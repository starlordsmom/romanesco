Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 19 Medical Information Extraction Using Natural Language Interpretation 1Gunjan Dhole and 2Dr. Nilesh Uke, 1Department of IT, PG Student, Sinhgad College of Engg, Pune, 411041,India 2Department of IT, Associate professor, Sinhgad College of Engg, Pune ,411041,India ABSTRACT NLP Based Retrieval of Medical Information is the extraction of medical data from narrative clinical documents. In this paper, we review Natural Language Processing (NLP) applications designed to extract medical problems from narrative text clinical documents. This paper also covers the methods used in this field and it also describes the architecture of the proposed system. However extraction of medical information is the difficult task due to complex symptom names and complex disease names. Proposed system is an expert system which will try to understand the input that can be the question about disease or list of symptoms and the system will try to give out the proper answer. Proposed system will consist of modules such as data processing, query processing, data extraction, answer matching, user interface etc. Lots of data are available in the medical field in free text form which is not used by normal rule based systems, so with the help of NLP we can use this free text data such as it will try to give out the answers that we search on our own. Keywords: Extraction, medical information, narrative text, NLP 1. Introduction : The increase in use of electronic health records and the corresponding interest in using these data for quality improvement and research in this field states that the interpretation of free text contained in the records is a difficult step. The biomedical text is another important information source which will benefit from structuring of data in narrative text. Different approaches are implemented for extraction of the medical text. Natural language processing approach uses tools like noun entity recognizers, coreference resolution, part of speech taggers and relationship extractors. However medical text is different from normal text as it contains complex terminologies, so medical information needs advanced versions of these tools. 1.1. Motivation : Following points are the points express the need for an automated system. Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 20 • Need for Text Processing: If we consider the data that we want to use as a database it should always be present in the database format. We miss out so much of data just because we don't have it in a proper format. It states that there is a need to develop such a system which can process this data. So that we never should face the problem of lack of data and the data which is already available in free text form can be used for better purpose. • Need of Medical Text Processing: In case of the medical field, there are lots of data available. But medical field does not have that much development with the help of Natural Language Processing. Medical data such as Electronic Medical Records are available. But there are very fewer systems that can take data from these medical documents. So there is a need to process this data which can be achieved with the help of natural language processing. • Need for an automated system for diagnosis of disease: The normal patient doesn't have the facility to predict the disease he is having until he gets to the doctor. Sometimes patients ignore their symptoms in early stages on any disease, which can be harmful. If the patients will be provided with the automated disease diagnosis system then can at least know the severity of the disease • Need for Automated system for doctors: The doctors have to keep track of very big chunk of knowledge. If they can take help of some automated system which can help them to diagnose a disease it will be good. It states that there is a need for automated system for doctors also. These denote that there is a requirement for developing the system that can process medical documents that can give the diagnosis of the disease and severity of the disease. The aim of this work is to evaluate an automated approach to the risk stratification of general diseases using Natural Language Processing (NLP) on medical documents. The final goal of the research is to increase patient safety by providing him the information about his diseases and severity of his diseases. 1.2. Why To Use Nlp For Information Retrieval : Most of the systems are using Rule based systems for extraction of information. In rule based systems, the number of rules is limited. Because of the limited number of rules the information extraction gets limited. If some new information is required then the rule based system fails to extract information due to lack of rules. In natural language processing, we can extract data from free text form. Enormous amount of data is available in free text form is available today. But it is not properly utilized. The text such as Electronic Health Records has patient data that can be used for many purposes. The Free text such as disease information, its symptom and causes all can be found in the free text. Natural language processing(NLP) can be used to extract all these and get proper information. It is also found that doctors don't use their whole knowledge for any disease. The automated system for data extraction can help doctors for proper recognition of diseases. 2. Related Work : Different medical extraction systems like MedlEE, MetaMap, linguistic string project were proposed [9]. MedLEE is developed to extract, structure, and encode clinical information into textual patient reports so that data can be used properly. Carol Friedman developed MedLEE with the Department of Biomedical Informatics at Columbia University, the Radiology Department at Columbia University, and the Department of Computer Science at Queens College of CUNY[9]. Dr. Alan Aronson developed MetaMap that is highly configurable software developed at the National Library of Medicine (NLM) to Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 21 convert biomedical text to the UMLS Metathesaurus and also to discover Metathesaurus concepts referred to in text [9]. The Linguistic String Project (LSP) was a developed (1960-2005) in the computer processing of language which is based on the linguistic theory of Zellig Harris: linguistic string theory, transformation analysis, and sublanguage grammar [9]. Different tools that are used for natural language processing are NER, pos-taggers, co-ref resolutions and Relationship extractors. Branimir T. Todorovic and Svetozar R. Rancic proposed a system for Named Entity Recognition and Classification using Context Hidden Markov Model [14]. Mohamed Hashem proposed A Supervised Named-Entity Extraction System for Medical Text. Andreea Bodnari. Louise Deleger proposed system for Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain [14]. Jiaping Zheng proposed a system for coreference resolution for the clinical narrative [15]. Wafaa Tawfik, Abdel-moneim proposed a system for Clinical Relationships extraction techniques from patient narratives [7]. The Ontology Development corpus and Information Extraction corpus annotated for co reference relations consists of 7214 coreferential pairs, forming 5992 pairs and 1304 chains. Classifiers can be trained with semantic, syntactic, and surface features pruned by feature selection. For the three system components for the resolution of relative pronouns, personal pronouns, and noun phrases. Support vector machines with linear and radial basis function kernels, decision trees, and perceptrons can be used for machine learning [10]. Ddan shen PROPSED A MedPost: a part-of-speech tagger for biomedical text [13]. This tagger was developed to meet the need for the high accuracy part-of-speech tagger trained from the MEDLINE corpus. This program currently accepts text for the purpose of tagging in either native MEDLINE format or XML. MEDLINE is a database of publications in health sciences, biology and related fields. It currently contains over 12 million records and nearly 7 million include an abstract [13]. Semantic relations can be extracted with the help of annotation approach which relies on linguistic patterns and domain knowledge which consists of two steps [8]: (i) Recognition of medical entities (ii) Identification of the correct semantic relation between each pair of entities. The first step is obtained by enhanced use of metamap. The second step depends on linguistic patterns that are built semi-automatically from a corpus selected. According to semantic criteria, evaluation of the treatment relations between a treatment and a disease can be extracted. 3. Information Retrieval : The research in "Natural Language Processing" (NLP) is going on from many years which were formed in 1960 as a sub-field of Artificial Intelligence and Linguistics, with the aim of studying problems in the automatic generation and understanding of natural language. A general text retrieval system consists of three entities 1. Records or full text documents 2. Indexer 3. Information retrieval tools Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 22 With the help of indexer, records are indexed and retrieved with the help of extraction tools. Natural Language Processing can be added at any or all of these stages. NLP interprets and stores meaning at for both the query and the document. If we add NLP information retrieval following stages should be followed. Step 1: Document Processing Step 2: Query Processing Step 3: Query Matching Step 4: Ranking & Sorting 4. Proposed Work : The proposed system consists of two major modules: Document Processing with Natural Language processing and query processing with natural language processing. Both are very important phases of the project. The proposed system consists of 5 modules as shown in the architecture. Also, it contains the knowledge base. Knowledge base actually stores all the medical documents: Fig. 5.1 the Architecture of Proposed System 4.1. Flow Of The Proposed System: Query will be given as input to the query processing module. Important entities will be extracted from the query. According to these entities, documents will be processed for extraction proper answers to the queries. Knowledge Base Base Query Processing Answer Prediction Filtering the answers Data Processing User Interface User Data Extraction Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 23 Answer matching will try to find the best possible answer from set of answers Proposed system contains following modules: 1) Data Extraction 2) Data Processing 3) Query Processing 4) Answer Matching 5) Filtering the Answers 4.2. Data Extraction: This module will manage knowledge Base. It will try to extract some data from Internet. Knowledge base will also consist of different resources that contain biomedical texts regarding different systems. It can have any kind of free text that involves disease's information. Fig. 5.2. The flow of the project . 4.3. Document Processing: Document processing involves processing of documents with the help of Natural language processing. It will involve parts such as section splitting, tokenization, relationship extraction, etc. 4.4. Query Processing: Query processing will involve the processing of a query with the help of natural language processing. It will extract all the important relationships and keywords from the query. 4.5. Answer Prediction: Answer prediction will involve the prediction of answer from the given set of relations and keywords. 5. Outcome Of The Project: The aim of the project is to get the answers of the disease related queries with the best precision and recall. Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 24 If the user enters set of symptoms then system should be able to answer with the probable disease name. Fig.4.3.1. Document Processing 6. Conclusion: Lots of researches are going on in the field of extraction of medical text with the help of NLP. As medical text is different from normal text, it needs advanced tools as compared to the normal NLP tools. Also, some of the systems are proposed for text extraction. Still there is a need for better medical text extraction systems. The proposed system extracts disease information according to the query currently. References [1] Andreea Bodnari, Louise Deleger, Thomas Lavergne, "A Supervised Named-Entity Extraction System for Medical Text" [2] Ngô Thanh Nhàn "linguistic string project - medical language processor" at http://www.cs.nyu.edu/cs/projects/lsp/ [3] The Brandeis University, "MedLEE" at <http://www.medlingmap.org/taxonomy/term/80> [4] James Freeman-Hargis "Introduction to Rule-Based Systems" at <http://ai-depot.com/Tutorial/RuleBased.html> http://www.cs.nyu.edu/cs/projects/lsp/ http://www.medlingmap.org/taxonomy/term/80 Advances in Vision Computing: An International Journal (AVC), Vol.1, No.1, March 2014 25 [5] Asma Ben Abacha, Pierre Zweigenbaum, "Automatic extraction of semantic relations between medical entities: a rule based approach" From Fourth International Symposium on Semantic Mining in Biomedicine (SMBM) [6] D. Nagarani, Avadhanula Karthik, G. Ravi, "A Machine Learning Approach for Classifying Medical Sentences into Different Classes", IOSR Journal of Computer Engineering (IOSRJCE) Volume 7, Issue 5 (Nov-Dec. 2012), PP 19-24 [7] Dan Shen Jie Zhang Guodong Zhou, "Effective Adaptation of a Hidden Markov Model-based Named Entity [8] Faguo ZHOU Enshen WU, "The Design of Computer Aided Medical Diagnosis System Based on Maximum Entropy" 978-1-61284-729-0111 2011 IEEE [9] Hinxton, UK. 25-26 October 2010 [10] Jiaping Zheng,1 Wendy W Chapman,2 Timothy A Miller,1 Chen Lin, "A system for coreference resolution for the clinical narrative", J Am Med Inform Assoc (2012). doi:10.1136/amiajnl-2011-000599 [11] Khan Razik, Dhande Mayur , "To Identify Disease Treatment Relationship in Short Text Using Machine Learning & Natural Language Processing", Journal of Engineering, Computers & Applied Sciences (JEC&AS), Volume 2, No.4, April 2013 [12] Kyle D. Richardson1, Daniel G. Bobrow1, Cleo Condoravdi1, Richard Waldinger2, Amar Das3, "English Access to Structured Data", 2011 Fifth IEEE International Conference on Semantic Computing [13] L. Smith1, T. Rindflesch2 and W. J. Wilbur, "MedPost: a part-of-speech tagger for bioMedical text", Vol. 20 no. 14 2004, pages 2320–2321, bioinformatics/bth227 [14] Lucila Ohno-Machado, Editor-in-chief, Prakash Nadkarni, Kevin Johnson "Natural language processing: algorithms and tools to extract computable information from EHRs and from the biomedical literature", amiajnl-2013-002214 [15] Romer Rosales, Faisal Farooq, Balaji Krishnapuram, Shipeng Yu, Glenn Fung, "Automated Identification of Medical Concepts and Assertions in Medical Text Knowledge Solutions" , AMIA i2b2/VA text mining challenge [16] Stéphane M. Meystre, MD, MS, Peter J. Haug ,"Comparing Natural Language Processing Tools to Extract Medical Problems from Narrative Text", MD AMIA 2005 Symposium Proceedings [17] téphane Meystre, Peter J Haug, R. Engelbrecht et al., "Evaluation of Medical Problem Extraction from Electronic Clinical Documents Using MetaMap Transfer (MMTx) Connecting Medical Informatics and Bio-Informatics", ENMI, 2005 [18]Stphane Meystre, Peter J. Haug, "Natural language processing to extract medical problems from electronic clinical documents: Performance evaluation", Journal of Biomedical Informatics 39 (2006) 589–599 [19]Wafaa Tawfik Abdel-moneim1, Mohamed Hashem , "Clinical Relationships Extraction Techniques from Patient Narratives", JCSI International Journal of Computer Science Issues, Vol.10, Issue 1, January 2013 http://ai-depot.com/Tutorial/RuleBased.html Learning probabilistic phenotypes from heterogeneous EHR data
Authors:
Information retrieval in medicine: The electronic medical record as a new domain Catherine Arnott Smith, MA, MILS, MSIS, PhD Assistant Professor School of Library and Information Studies University of Wisconsin-Madison Room 4217 Helen C. White Hall 600 N. Park Street, Madison, WI 53706 Abstract "The medical record is a material form of public memory," Berg (1996) writes, "a structured distributing and collecting device, where all tasks concerning a patient's trajectory must begin and end..." [Italics original; p. 510]. Structured distributing and collecting devices are the natural interest of information science. Unfortunately, of the 130 articles published about medicine in almost 36 years of JASIST, although 70 (54%) deal with information retrieval, communication and the work processes behind them, only 2 of these articles (1.5%) have focused on the medical record. The body of existing information retrieval work most relevant to the medical record as a base for experiment is the work called "passage retrieval" defined as "the task of identifying and extracting fragments from large, or short but heterogeneous full text documents" (Melucci, 1998, p. 44). This paper presents a document-centered approach to the EHR as an information retrieval problem. It is clear that passage retrieval researchers working in the field of information science have seen similar values in document passages as have researchers in medical informatics. Without either literature acknowledging the other, workers in both camps have identified the same potential in document structure, labels, specificity and explicit hierarchies of knowledge for signaling relevance to the reader. The National Health Information Infrastructure Initiative (http://aspe.hhs.gov/sp/nhii/) identifies academics and researchers as natural stakeholders, like clinicians and caregivers, in enabling better healthcare through better information sharing (National Committee on Vital and Health Statistics, 2003). Information science has much to contribute to the health information technology arena and to electronic health records in particular: their development, their maintenance, and most importantly their improvement to serve the needs of diverse users. The problem "The medical record is a material form of public memory," Berg (1996) writes, "a structured distributing and collecting device, where all tasks concerning a patient's trajectory must begin and end..." [Italics original; p. 510]. Structured distributing and collecting devices are the natural interest of information science. 2 This paper presents the medical record as a body for information retrieval research. Medicine in JASIST A search was performed for citations to JASIS/T in the Library & Information Science & Technology Abstracts (LISTA; Ebsco) from 1970 through the January issue of 2006. To identify articles which focused on medicine or health in general, citations were selected that were indexed with the subject descriptors "Medicine" or "Health" or included the keywords medical or health in their titles or abstracts. This yielded a total of 214 citations. Of these 214, 45 were eliminated as false drops; for example, articles written by authors employed by schools of medicine but which did not have medical subjects, and one "article" listing books received for review. A further 39 were found to be duplicates imported from the Information Science & Technology Abstracts (ISTA) database provided by the same vendor. This left a remainder of 130 citations for analysis. The author and a second coder with an MLS examined these 130 citations to determine the one descriptor of the major subject of each article. The number of descriptors assigned by LISTA indexers ranged from as few as 1 to as many as 8. Interindexer agreement by the two coders was 99% and consensus was reached easily. These 130 LISTA descriptors were then mapped to the Medical Subject Headings (MeSH; National Library of Medicine) terms determined through the use of the MeSH Browser (http://www.nlm.nih.gov/mesh/MBrowser.html) to be the closest equivalent. The MeSH Browser utility permits searching not only of MeSH terms, but also the scope notes and annotations associated with each term. Because MeSH is the predominant controlled vocabulary for indexing biomedical bibliographic concepts, it permits the most specific description of medical subjects in one source of literature. MeSH is also a multiaxial vocabulary; that is, a MeSH term can exist simultaneously in multiple hierarchies. For example, "Down Syndrome" is located not only in the Nervous System Disease tree, but also in Congenital, Hereditary and Neonatal Diseases and Abnormalities. For the purposes of this study, to facilitate classification of this literature for generalist readers in information science, terms were chosen from the Information Science tree (L01) of MeSH whenever possible. The L01 tree appears in its entirety in Table 1.1 1 In a few cases, the only possible fit available was a bad one; Knowledge Representation, for example, in LISTA's thesaurus, is acknowledged in MeSH only when it is enabled by a computer (Knowledge Representation (Computer); representation of knowledge manually is not considered in the scope of this term. The two coders chose the term Semantics instead. 3 Table 1. Information Science as represented in MeSH (Medical Subject Headings; National Library of Medicine).2 Information Science [L01] Book Collecting [L01.040] Chronology [L01.080] Classification [L01.100] + Communication [L01.143] + Communications Media [L01.178] + Computer Security [L01.209] Computing Methodologies [L01.224] + Copying Processes [L01.240] + Data Collection [L01.280] + Data Display [L01.296] + Informatics [L01.313] + Information Canters [L01.346] + Information Management [L01.399] Information Services [L01.453] + Information Storage and Retrieval [L01.470] + Information Theory [L01.488] Library Science [L01.583] + Medical Informatics [L01.700] + Pattern Recognition, Automated [L01.725] + Publishing [L01.737] + Systems Analysis [L01.906] + Note: + symbol means that this portion of the tree is expandable. Table 2 shows the distribution of medical subject coverage within JASIST between 1970 and January, 2006. The descriptors for six citations had no MeSH equivalents. These appear in italics. Levels in the MeSH hierarchy are indicated by a box symbol [ ]. 2 Source: National Library of Medicine. (2006). MeSH Browser. Available at: http://www.nlm.nih.gov/cgi/mesh/2006/MB_cgi#TreeL01. (Date accessed: February 11, 2006). 4 Table 2. Medical subject coverage in JASIST according to specific MeSH categorization, 1970-2006. MeSH term3 Number of citations Administrative agencies 1 Behavior and behavior mechanisms* Peer review 1 Buildings 1 Communication 1 Linguistics Semantics 1 Vocabulary 1 Writing 1 Authorship 1 Language arts 1 Information dissemination 1 Communications media Publications 2 Computer systems Internet 4 Computing methodologies Algorithms 1 Artificial intelligence 1 Automatic data processing 1 Natural Language Processing 2 Computer systems 1 Hypermedia 1 Copying process 1 Data Collection Medical records 1 Surveys 1 Diagnosis* Diagnostic imaging* 3 Government information 1 Grammar, comparative and general 1 3 All descriptors are taken from the L01 tree of MeSH unless identified by an asterisk [*]. 5 Table 2. Medical subject coverage in JASIST continued Health care economics and organizations* Congresses 1 Health care facilities, manpower, and services* Delivery of health care 1 Health services accessibility 1 Mental health services 1 Health care quality, access, and evaluation* Health occupations* Medicine 2 Information Centers Libraries 4 Medical libraries 1 Information science 6 Information services 12 Abstracting and indexing 2 Cataloging 1 Documentation 1 Information storage and retrieval 18 Databases, bibliographic 4 Databases 5 MEDLINE 2 Literature 1 Publications Book reviews4 6 Bibliography 6 Dissertations, academic 1 Periodicals 9 Medical informatics 5 Persons* Patients 1 Psychological phenomena and processes* Task performance and analysis 1 Publication formats* Abstracts 1 Letter 1 Publishing 4 Training 1 Total 130 4 Citations to "Books Received but Not Reviewed" were not included in this total (e.g., Wallace, D.P. (1988), v. 39(1), pp. 26-27). 6 Categorizing the descriptors according to the most general level of the MeSH hierarchy results in the gross subject distribution shown in Table 3, below. The descriptors are presented in descending order by citation. Table 3. Medical subject coverage in JASIST according to general MeSH categorization, 1970-2006. MeSH term5 Number of citations Information storage and retrieval 29 Communications media 25 Information services 16 Communication 7 Computing methodologies 7 Information science 6 Information Centers 5 Medical informatics 5 Computer systems 4 Publishing 4 Diagnosis* 3 Data Collection 2 Health care quality, access, and evaluation* 2 Health occupations* 2 Publication formats* 2 Administrative agencies 1 Behavior and behavior mechanisms* 1 Buildings 1 Copying process 1 Government information 1 Grammar, comparative and general 1 Health care economics and organizations* 1 Health care facilities, manpower, and services* 1 Persons* 1 Psychological phenomena and processes* 1 Training 1 Total 130 It is clear from the numbers in Table 3 that information science's predominant interest in medicine and health, at least as evidenced in this flagship journal of the field, has been (1) information retrieval, (2) communication of the information once retrieved, and (3) the services that support retrieval and communication. 5 All descriptors are taken from the L01 tree of MeSH unless identified by an asterisk [*]. 7 This is not a surprising finding since these are three domains that would be considered predominant interests of many generalist information scientists as well. When we speak of medical information in information science, we usually are speaking of information sources and tools developed by the National Library of Medicine or its sister institute the National Center for Biotechnology Information—the codevelopers and maintainers of PubMed. For example, there are numerous articles about MEDLINE and its exploitation; Swanson's famous fish oil article (1987) is an early example, Leroy & Chen (2005) a more recent one. The Unified Medical Language System has also been featured in the pages of this journal (for example, Humphrey, Rogers, Kilicoglu, Demner-Fushman, & Rindflesch, 2006). But of the 130 articles published about medicine in almost 36 years of JASIST, 70 (54%) deal with information retrieval, communication and the work processes behind them, while only 2 of these (1.5%) have focused on the medical record. The first of these was authored by William Hersh, MD, a prominent medical informatics researcher and director of the NLM Medical Informatics training program at Oregon Health Sciences University. His article appeared in 1995, and served as a forecast of technology trends to come with implications for the information science profession. The second paper appeared 6 years later and is the only JASIST-published study focusing on the medical record as a document set for experimentation. "An Experimental Study in Automatically Categorizing Medical Documents", presented a study of automatic classification using the International Code of Diseases (Ribeiro-Neto, Laender, & de Lima, 2001). The document base in this study consisted of over 20,000 clinical documents from a contemporary healthcare system. It is this author's contention, which will be explored in the remainder of this paper, that the medical record deserves consideration as a document base for information retrieval research. The medical record In 1965, Berkeley lamented the "chaos of medical information-gathering" and concluded that "this information is more or less useless in terms of being retrievable by computer methods": The usual case-history form often represents a device for recording and reinforcing the interests and prejudices of the individual physician or clinical investigator reflecting current fashions in diagnosis. It is frequently no more than an essay by the physician of doubtful literary or scientific merit (1965, p. 4). Although Berkeley's motive was criticism of case histories and case history takers, he also calls our attention to the fundamental function of the medical record, whether paper or electronic. Throughout its development, it has always documented both the knowledge domains of clinical practice, and the work 8 processes and practices that support and maintain the operation of these domains. The EHR and its Contents The Institute of Medicine, in its report Key Capabilities of an EHR System, stresses that the modern motivation for an Electronic Health Record, or EHR— one of many acronyms in use for an electronic health record--is not a desire for "a paperless record per se, but to make important patient information and data readily available and useable." (Committee on Data Standards for Patient Safety, 2003). For this reason, the EHR has been defined as a "complete online record that is accessible to all that need it when it is needed" (Ondo, Wagner, & Gale, 2002, p. 2). The EHR is fundamentally a "container for a set of transactions". These transactions are both persistent ones, such as historical data pertaining to one patient, with long-term value; and records of individual events, such as EKG tracings of that same patient on one morning in a single clinic, data that has short-term value (Bird, Goodchild, & Beale, 2000). The need for longitudinal access to persistent information is one characteristic that distinguishes healthcare IT from other industries, which typically experience "heavy retrieval requirements initially and then a drop-off in the need to access records". In healthcare, conversely, "it is not unusual for a caregiver to need access to 20 years' worth of a patient's medical history .. there is no predictable retrieval pattern for medical records." (Cisco, 1996). The paper medical record has historically supported—and thus the EHR must continue to support—numerous work processes and subprocesses, with multiple authors and custodians; potential audiences; intended data lifespans; and trajectories documenting care in different locations and for different purposes. As Khare and Rifkin (1998) remind us, "Usage determines community, which in turn refines the common ontology" (p. 393). So researchers interested in exploring semistructured medical documents, both for processing and for EHR systems development purposes, have followed one of two paths: automated extraction of text from existing documents to build a new ontology, or manual analysis of existing text to build a new ontology. The point of the ontology is to use it to structure the next phase of the resource's development. The content of EHRs reflects this multiplicity of needs and audiences. It is a mix of highly structured numeric data and excessively unstructured and idiosyncratic narrative text; increasingly, images are included as well. In fact, any information can be part of the medical record that is relevant for clinical decision making. This data makes its way into the record via voice transcription, data feed from machines, or conversion from paper. Although there is considerable variation in the content and the structure in medical records, the current paper-based record 9 has these typical contents which are present, in various degrees, in EHRs as well. • patient problem list • patient history • operating room notes • physical exams • discharge summaries • allergies • health maintenance information • immunizations • medications dispensed • orders • diagnostic results • images • most recent vital signs • progress notes • nursing visits • consult documentation • genetic information • results of previous retrieval runs of any or all of the above, • and information generated outside the health care organization but maintained as part of the individual patient's history. Much of this information can be and is presented to the user in the context of a text-based document. Clinical documents may also refer to each other. These documents are frequently "nested" inside each other, also; for example, EKG narrative reports can appear within a cardiology report; a letter from a physician may include results of a genetic test (Smith, 2005). An example of a typical clinical document found in an electronic medical record system appears in Figure 1. Dates and ages have been pseudonymized. 10 Figure 1. Radiology report. NUCLEAR MEDICINE FDG PET SCAN: 6-4-01 0914 HRS. STATED REASON FOR REQUEST: 50 Y/O MAN WITH HISTORY OF LYMPHOMA. RADIOPHARMACEUTICAL ADMINISTERED: 10.69mCi F-18 FDG IV Emission scanning of the neck, chest, abdomen and pelvis was obtained approximately one hour post-injection. Images were reconstructed with and without attenuation correction. COMPARISON: Comparison is made with prior FDG PET scan dated 10-12-00. FINDINGS: Patient's blood glucose level was 92mg/dl. In comparison with prior FDG PET scan, previously seen diffuse FDG uptake in the right lower lobe of the lung is no longer seen on current PET scan. Previously seen focal increased FDG uptake in the right lobe of the liver has increased in size and standard uptake value, suggesting progression of the lesion. The standard uptake value of this lesion is approximately 8 which is well within the range typically associated with malignancy. Multiple foci of moderate increased FDG uptake are noted in the periaortic region in a linear fashion, suggesting extensive lymphadenopathy. The possibility of these foci representing FDG activity in the ureter is felt to be less likely, but cannot be excluded. In addition, foci of increased FDG uptake are noted in the right iliac and right posterior iliac region, suggesting metastatic lymphadenopathy. No other lesions are identified. IMPRESSION: 1. INTERVAL PROGRESSION OF A MALIGNANT LESION IN THE RIGHT LOBE OF THE LIVER. 2. COMPLETE RESOLUTION OF DIFFUSE FDG UPTAKE IN THE RIGHT LOBE OF THE LUNG. 3. TWO MALIGNANT FOCI IN THE RIGHT ILIAC AND RIGHT POSTERIOR ILIAC REGION. 4. MULTIPLE FOCI OF MODERATE INCREASED FDG UPTAKE IN THE PERIAORTIC REGION IN A LINEAR FASHION, MAY REPRESENT EXTENSIVE METASTATIC LYMPHADENOPATHY. THE POSSIBILITY OF THESE FOCI REPRESENTING FDG ACTIVITY IN THE URETER IS FELT TO BE LESS LIKELY, BUT CANNOT BE EXCLUDED. J4 My signature below is attestation that I have interpreted this/these examination(s) and agree with the findings as noted above. END OF IMPRESSION: 11 Medical Data and Work Practices "Insufficient information" has been implicated as one of many failure modes resulting in medical errors and adverse events: insufficient meaning a lack of information regarding the drugs the patient has been prescribed; previous dose-response relationships; pharmaceutical information; laboratory data; and known allergies (Kohn, Carrigan & Donaldson, 2000). The electronic medical record, as the principal information resource for clinical care, has the potential to solve these problems through making clinical information more represent-able and thus retrieve-able. However, considering the medical record as an information source requires knowledge of the work practices that the record supports. "The very possibility of understanding the record's entries is based on a shared, practical, and entitled understanding of common tasks, experiences, and expectations" (Atkinson & Heath, 1981, pp. 200-201). As one social historian explains: It is a mistake to separate the knowledge claims of medicine from its practices, institutions, and so on. All are socially fashioned, and so it may ultimately be more helpful to think of mentalities, modes of thought, and medical culture than in terms of "knowledge", which implies the exclusion of what is inadmissible. [Jordanova, 1995, p. 362]. Sociologists Garfinkel and Bittner remarked on the intertwining of practice and documentation in the 1960s when, as investigators, they attempted unsuccessfully to intervene in a medical clinic's recordkeeping practices: "Attempts to pluck even single strands can set the whole instrument resonating", they note diplomatically (1967, p. 192). Medical data have three characteristics relating directly to work context. Specificity and Purpose First, data are always produced with a given purpose, and their hardness and specificity is directly tailored to that purpose...the meaning, hardness and significance of a piece of information cannot be detached from the specific purpose that structured the gathering of that information (Berg & Goorman, pp. 53-54). This is a sociologist's perspective on the difference between free text and controlled vocabularies. In healthcare, the "rigidity" of coding and classification exists so that information can be generalized across a number of clinical situations. Consider the International Classification of Diseases (ICD) concept "Brain Neoplasms, Miscellaneous, Not Otherwise Specified". This is a "soft" 12 knowledge representation because it is imprecise. However, this very imprecision makes the concept portable across different clinical situations—for example, between African and European medicine—and even into domains outside of immediate clinical care, such as epidemiological reporting. Contrast this with the much "harder" representation, "Astrocytoma", a label attached to a specific kind of brain tumor. This is a representation achievable only by the clinician who exits the classification system of ICD-9 to express the diagnosis the only way she can--in free text. This label's purpose is to accurately, and with the greatest precision possible, represent the clinician's diagnosis. A clinical information system that has not allowed for the existence of an astrocytoma renders this diagnosis invisible—and unfindable, except via keyword searching. Therefore this diagnosis is unshareable, both within the dataset of patients represented in this information system, and across the system, to be analyzed with other types of data, such as surgical procedure, name of surgeon, or age of the patient. Just as the task of the statistician or third-party payer has a different purpose from the task of the clinician, so the granularity and quality of the diagnosis' representation will differ accordingly. Mutual Elaboration Second, medical data do not exist in isolation, but "mutually elaborate each other", as "bits and pieces of an emerging story" (Berg & Goorman, 1999, p. 54), subject to the effects of time. The context of data elements as being located near other data elements is thus also important. In fact, the "story" does not even have to be told in textual narrative; Description of the course of an illness, whether told in words or in laboratory test values, requires integration of individual data with the larger picture. For an illustration, refer to Table 4, below (Bergeron, 1998). This table shows the importance of context in interpretation of otherwise ambiguous notations in medical records; understanding of the vocabulary and world view of the specialty is necessary to make sense of acronyms and abbreviations. For example, as Bergeron comments, "often written abbreviations are unambiguous in the context of a specialty, such as 'rih' for 'right inguinal hernia', and 'GA' for 'General Anesthesia' in surgery and anesthesia." (p. 575). 13 Table 4. A Comparison of Traditional Handwritten Medical Record Entries (Left) and Their Equivalent Oral Translations (Right) (Bergeron, 1998, p. 575). Hand written tradition Translation Pt returns for F/U of HBP Patient returns for follow-up of high blood pressure On exam—class 4 airway, opens mouth ~5 cm, 3 fb thyromental dist, good neck extension On exam, class 4 airway, opens mouth approximately 5 centimeters, 3 finger breadth thyromental distance, good neck extension Uterus: NSSC, no mass Uterus of normal size, shape, and consistency, no masses ...debridement stsg R ear... ...debridement single thickness skin graft of the right ear ... NPO p MN NPO past midnight ...for rih repair... ...for right inguinal hernia repair... ...9 hours of GA... ...9 hours of general anesthesia... PMH: NSVD times 2 Past Medical History: Normal spontaneous vaginal delivery times two Chest: Clear to P&A Chest: Clear to percussion and auscultation RTC 2 weeks Return to clinic in 2 weeks ...pulmonary hypertension with DOE for one year... ...pulmonary hypertension with dyspnea on exertion for one year... WD obese BF NAD Well developed, obese black female in no acute distress ...for an Ax block... ...for an Axillary block... ...the ett tape was under tension... ...the endo tracheal tube tape was under tension... ...to the patient's cvp and palpation... ...to the patient's central venous pressure (or cvp) and palpation... Breasts: no mass or D/C Breasts, no masses or discharge Context of Production Finally, a third characteristic of medical data is that human readers of medical information interpret and reinterpret "in the light of who generated it" (Berg & Goorman, 1999, p. 55), whether that generator is a human being or a machine. Medical readers consciously perceive the context of production, and integrate an understanding of the producer into their understanding of the data. Berg and 14 Bowker (1997) have even made the case that the data produced mirrors the organizational structure of the organization that produces it. These authors commented that in considering the electronic medical record, it was tantalizing to assert a connection between the databases drawn upon and the work organization ... the hierarchical database echoes the hierarchical organization structure most favored in the 1960s; the relational database echoes more the team model of the 1970s; and object orientation is the nec plus ultra (sic) of radical outsourcing [p. 534, n. 14]. Clinical information retrieval Hersh (1996) has identified the two distinctly different goals of general versus clinical information retrieval. The purpose of the former process is to get a particular document that matches the seeker's specific information need; the goodness of fit of document to need is facilitated through the use of descriptors representing the document's subject matter and manipulated by an automated system. The document is indexed; the document is retrieved; the document is the deliverable. In contrast, the information need in the clinical setting is typically centered around a particular patient; whether the information-seeker's ultimate intent is to use this clinical data in isolation, or in aggregation with other data describing other patients. Thus the clinical data—"The digoxin level of patient 13 upon admission?"—is in fact the deliverable, not the document. This has tremendous implications for the accuracy and granularity of representation in these documents. A "false drop," or mismatch between query and result, in nonclinical situations constitutes only information noise; in clinical retrieval, it can literally be fatal. (For an excellent review of research into clinical information retrieval, see Mendonça, Cimino, Johnson, & Seol, 2001). Typical clinical tasks performed using electronic medical record systems include the following (Laerum et al., 2001). These tasks illustrate the range of information needs and information retrieval features required: • Review the patient's problems • Seek out specific information from patient records • Follow results of a test or investigation over time • Obtain results from new tests or investigations • Enter daily notes • Obtain data on investigation or treatment procedures • Answer questions concerning general medical knowledge • Produce data reviews for specific patient groups 15 • Order clinical biochemical laboratory analyses • Obtain results from clinical biochemical laboratory analyses • Order x ray, ultrasound, or CT investigations • Obtain results from x ray, ultrasound, or CT investigations • Order other supplemental investigations • Obtain results from other supplemental investigations • Refer patient to other departments or specialists • Order treatment directly (medical, surgery, or other) • Write prescriptions • Write sick leave notes • Collect patient data for various medical declarations • Give written specific information to patients • Give written general information to patients • Collect patient information for discharge reports • Check and sign typed dictations Safran and Chute (1995) have delineated four specific ways of reusing clinical data which are four general categories into which the clinical tasks described by Laerum et al. can be sorted: Results reporting: Displaying information about an individual patient Case-finding: Finding data about another patient similar to the current patient Cohort description: Describing a group of patients with at least one attribute in common Predictive modeling: Elucidating patterns in data in hopes of describing trends, or relationships, between attributes The first application, Results reporting, centers on the patient as an individual, and this is the most common of the uses of clinical data because the record's primary function is to represent that patient's encounters with the healthcare system. This does not mean, however, that retrieval of information about that specific individual is going to be easy. The most significant information management challenge posed by claims data is the fragmentation of patient information over time and geographic space as patients move through a fragmented treatment system. Despite the industry's belated compulsion toward horizontal and vertical integration, most patients still receive their care across 16 myriad settings and sites... [there is an] unimaginable volume of clinical rules necessary to develop coherent "episodes of care", a mind-bending data-handling task that must be completed before any meaningful clinical information can be developed and used. (Kleinke, 1998, p. 29). Chronic disease is a long-term variation on the same theme; one individual with multiple visits to multiple providers presents the same clinical information retrieval problem multiplied exponentially. Chronic disease "must incorporate far more information than just physicians' opinions, must be accessible from many different sites of care, and must capture accurately both the illness trends and their speed of change" (Holman, 1996, pp. 1-2). The other three tasks outlined by Safran and Chute—case-finding, cohort identification, and predictive modeling--require aggregation of individuals into coherent subgroups; thus, identification of, and retrieval according to, common attributes is critical to the clinical research process. Possible attributes include age; gender; ethnic origin; and patient experience, which may include all of the above: for example, a clinician might be interested in the postoperative infections found in all Black men undergoing a particular procedure in the same hospital over time. The case-finding application described by Safran and Chute is a "Get me more like this" request; physicians "may find it useful to recall past situations similar to the current one, but the process is often biased by the tendency of recalling only more recent cases" (Montani & Bellazzi, 2001, p. 499), and an automated retrieval system helps avoid this human bias. The definition of useful attributes and the ability to restrict searches to patients with those characteristics is thus another key requirement in clinical information retrieval system design. Thus, the importance of clinical data extends even beyond the limits of any individual patient's healthcare needs. Clinical information is necessary for retrospective studies; outcomes research; quality assurance audits and evaluation (Dambro et al., 1988; Marshall, Balas, & Reid, 1997); decision support; management of patient care (Hersh, 1996); distributed health care (Dambro et al., 1988); and, in fact, any form of scientific research that requires dealing with individuals as members of groups (Sujansky, 1998). Loosely structured documents Essin & Essin (1990) had proposed loosely structured documents as the ideal electronic patient record implementation, integrating the high-quality representation available through standardized coding with the flexibility and customization afforded by a paper-document structure. Loosely structured, or semistructured, documents were earlier defined by Essin (1993) as documents 17 that have much in common—enough in common that a general statement can be made about their components. Essin's strategy had two levels. The first consisted of understanding the data elements themselves. The second level was a "meta-level" that contained knowledge about the data elements. Essin's idea was to capitalize on both kinds of knowledge to be able to treat the document as a piece of text with fields like a database; to consider the meta-knowledge, or structure, as a different entity from its text, or content, and model the two separately (Royal College of General Practitioners, 1999). Essin formally proposed this as an SGML solution. Since SGML is intended to separate the document and its tags, or notation, from its application specifics, these authors argued that the "ability to manage loosely structured formats avoids rigid formalisms that have the sole purpose of making the data processable" (Lincoln & Essin, 1995, p. 229). This SGML initiative eventually bore fruit as the ANSI standard Clinical Document Architecture from ISO Health Level Seven. Health Level Seven, or HL7 (www.hl7.org), is an international ANSI-accredited Standards Developing Organization within the domain of healthcare, specifically concerned with standards for clinical and administrative data. The HL7 community includes not only academics and healthcare professionals, but representatives of every major vendor in the healthcare IT industry, which ensures industry input and compliance with the standards developed by the organization. For this reason, HL7 and the Institute of Medicine were the two entities charged by U.S. Secretary of Health and Human Services Tommy Thompson in 2003 with developing standards for a U.S. electronic health record. The intent of the CDA in health informatics is similar to that of the Encoded Archival Description (EAD) in archives; both architectures attempt to impose order on semi-structured text documents by standardizing frequently occurring segments within those documents. The EAD was developed in 1993 as an encoding standard for machine-readable, sharable text created by libraries, museums, archives and manuscript repositories. The CDA, similarly, standardizes templates for radiology reports, laboratory test results, history and physical notes, discharge summaries, operating room notes, and hundreds of other common healthcare documents. Both the CDA and EAD are communication standards that specify structure, but do not attempt to define semantics of the content being structured. (See Smith, 2002, for a complete review of SGML/XML in medicine and the evolution of the Clinical Document Architecture standard and Arnott Smith, 2002, for an information retrieval experiment testing the effect of the standard; for the current status of Health Level Seven's CDA and other standards, see HL7, 2005). 18 Features What are the important semantic features of loosely structured documents in medicine, and how do they make clinical information accessible to the clinical reader? Wolff, Flörke, and Cremers (2000) point out that the principal defining feature of structured documents is the presence of explicit semantics for their structural parts. The benefit is that the meaning of the structural components— the sections—can be exploited, as can the meaning of the text they contain. The sections thus make up the "meta-level" proposed by Essin (1993). Elements, their labels, and their granularity are three important structural dimensions of clinical documents that have their own particular implications for retrieval of these texts. These document sections can be considered from three different perspectives. First, there is the behavior of the sections themselves, considered to be distinct elements, or components, of the documents; second, there are the strings of text, section headings or "labels" by which these sections can be accessed by the reader; and third is the number of these elements into which the document has been divided, or "partitioned". Each of these perspectives has its own implications for the structured document as a knowledge representation. Elements Lambrix and Shahmahri (2000) state that "the logical structure of a document can be seen as defining a part-of hierarchy over the document and its parts" (p. 290). In the same vein, European medical record standardization efforts have defined a "record item" as Part of a "chain" (having a "name" and a "content") [that] is a part of a "record item complex" that in turn can be part of record item complexes of higher rank. This chain of complexes is the "context" for the record item. (Rossi Mori & Consorti, 1999, p. 132). Document parts, also called elements or components, need to be understood as having meaning, particularly in relation to each other. This chain of relations is the "information space" which Kluge (1996) sees as connecting the dots between individual points of clinical data: "Only data-in-relation are information" (p. 253). In the example of the radiology report (Figure 1), the bulleted list of clinical "findings" is a distinct element present in this electronic medical record system's radiology report document type, but nowhere else. 19 Labels Nygren, Johnson, and Henriksson (1992) were the first researchers to examine the process of clinicians' reading of the medical record as a specific information source. These authors identified three reading techniques: first, skipping over irrelevant sections; second, skimming sections identified as possibly relevant; and third, reading needed information carefully. Tange, Dreessen, et al. (1997) collapsed the first two filtering steps into one in an EHR setting, describing a user who searches through the record "guided by the internal structure" to select relevant sections, then reading the content (p. 158). Readers of clinical documents navigate by reading the labels assigned to the structural elements of the documents. These are variously called "section headings", "labels", and "segment labels" in the medical informatics literature (see for example Lincoln & Essin, 1995, and Tange et al., 1998). In Figure 1, some labels are "Comparison", "Findings", and "Impression". The labels alert the reader to content (Lincoln & Essin, 1995). In this fashion the labels themselves serve to denote the structure and define the domain of knowledge: The structured representation acts as an intensional definition, in the particular vision of a world embedded in a structure. (Rossi-Mori, Galeazzi, Consorti, & Bidgood, 1997, p. 650). This makes them useful even at shallow depths of detail. Rossi Mori and Consorti (1999) comment: Even without using further structures, document names or titles of sections may be useful .. to build a cumulative table of content from various record systems ... to limit the search within entries. (p. 132) In fact, the presence of specific labels in a clinical document may be a signature of a particular document type. The author's doctoral dissertation experiment used 8 common clinical document types from the Medical Archival Retrieval System (MARS) at the University of Pittsburgh Medical Center. For this experiment, two hundred and one individual labels were created from section headings present in document text. Of the 201 labels, not one was common to all 8 types. Seven of the 8 document types shared only four elements: "Addendum", "Impression", "Medications", and "Plan." 20 Granularity The number of elements into which a document can be partitioned has been called the document's "granularity" (Tange, Dreessen et al., 1997). "Granularity" of clinical documents is "the level of detail to which these elementary paragraphs are specified" (p. 158). These researchers outlined a continuum of granularity like that of sandpaper: Coarse; Intermediate; and Fine. The radiology report in Figure 1 is of intermediate granularity because although some elements of the document have labels (such as "Findings" and "Impression"), others do not; for example, the precise type of radiology study performed is implicit in the text of the report (a PET scan) but is not explicitly called out by a label. Passage Retrieval The body of existing information retrieval work most relevant to the medical record as a base for experiment is the work called "passage retrieval". This has been defined as "the task of identifying and extracting fragments from large, or short but heterogeneous full text documents" (Melucci, 1998, p. 44). It is a subset of research into "corpus-based" text processing. Here, the text collection itself is used to derive information needed for analysis and for characterization (Salton & Allan, 1993). Moffat, Sacks-Davis, Wilkinson, and Zobel complained in 1994 that relatively little work had been done on retrieving or ranking partial documents (p. 188); a good review of research through the late 1990s can be found in Melucci (1998). More recent explorations include those of Fuji, Iwayama & Kando, who used a patent collection (2004) and Cui, Sun, Li, Kan & Chua (2005), who were interested in answer passages in question-answering systems. Passage retrieval was inspired by the problem of large documents. As Melucci comments (1998): Large is relative; the less the power of computational resources, or the worse the system capabilities of providing the user with usable and useful access to large documents, the more the documents are to be considered as large (p. 43). O'Connor, one of the earliest investigators of passage retrieval, used a medical resource for his dataset, but this was a medical bibliographic database: CANCERLIT, the cancer-specific augmented Medline from the National Library of Medicine. Inspired by the success of LEXIS, the legal full-text system, O'Connor asked "Why not passage retrieval for scientists?" (1980, p. 227). Like "large", the precise definition of "passage" has varied in the literature. Yang, Maglaughlin and Newby (2001) call this one of its principal challenges: "How documents should be split into passages in order to maximize [passage 21 retrieval's] advantageous potential is an important consideration" (p. 527). But in general, passages have been defined as being "some semantic structural feature" of the document (Kaszkiel & Zobel, 1997). Callan (1994) summarizes the passage retrieval literature as treating three different types of passages: windows, semantic, and discourse (see also Hearst & Plaunt, 1993; Lalmas & Ruthven, 1998). Windows are sections of text defined as a certain number of words (see Callan, 1994, for an example, and Xi & Xu-Rong, 2001, for an experiment in which window size was an independent variable). Semantic passages are defined based on the subject or content of the text. Hearst and Plaunt (1993), for example, worked with "subtopics" denoted by the subheadings in a magazine article. Discourse passages are based on units of textual discourse: sentences, paragraphs, and sections. It is discourse passages that put the weightiest requirements on the composer (Callan, 1994) because these passages require consistency from writers. Thus, analysis by discourse seems to work best with highly structured and edited text, such as the encyclopedia text used by Salton and colleagues in numerous experiments (Salton & Allan, 1993; Salton, Allan, & Buckley, 1993; Salton & Buckley, 1991). In fact, if text is neither highly structured nor edited, passage retrieval is more difficult than document retrieval, because: Any pre-defined segmentation of the text is absent, unless the text author has provided the text itself with a structure reflecting the organization of the topic which might support the retrieval of passages relevant to the topics (Melucci, 1998, p. 44). Both subtopics and sections are understood to be visible, explicit structural features of the text that are available for semantic processing. Their presence is alerted by strings of text that are legible labels—subheadings, or section headings: "[T]ext structure can be a good approximation of topic organization" (Melucci, 1998, p. 47). Callan (1994) and Kaszkiel & Zobel (1997) argue that because of these elements of a document's structure, all information retrieval can be viewed as a passage retrieval task—or, at least, a task of retrieving documents that have an internal structure: "Each element is a source of evidence that can be used in retrieval" (p. 302). Blair (2002) similarly states that "the determinacy of document representation" is one of the three factors most influencing content retrieval (the other two being the size of the collection and the type of the search) (p. 303). However, Moffat, Sacks-Davis, Wilkinson, and Zobel (1994) consider it important to consider documents not only structures, but hierarchical structures in particular (p. 181). Panko et al. (1999) refer to these hierarchical displays as "outlines" and 22 "suboutlines", noting that the suboutlines deal with increasing levels of specialization, while outlines, as the most general level, offer "greatest stability". Benefits Passage retrieval attempts to address the significant problem of full-text document retrieval which rests in the sheer size of the documents. This characteristic of full text may have a confounding effect: It may be large and difficult to manage, and relevant information may be widely scattered, and therefore hard for the user to extract (Tombros & Sanderson, 1998, p. 2). Although Wolff et al. (2000) point out that passage retrieval allows the user to make very precise queries because of the "enhanced expressive power" it affords, passage retrieval research thus far has virtually entirely been concentrated not on the questions, but the document "answers". In a full-text system, documents that are returned as "answers" to queries pose problems; they are large and unwieldy objects and the "answer" located within the result returned may be difficult for users to extract, so that it is not really an "answer" at all (Moffat et al., 1994). Cormack, Clarke, Palmer, and To (2000) have examined the use of passages to refine queries; their Multi-Text System supports "the retrieval of passages defined at query time rather than at build time" (p. 152). Yang et al. (2001) had good results through allowing the user to select the passages; according to these researchers, the interactivity of their system renders the "arbitrary determination of passages", discussed above, a nonissue (p. 527). Proponents argue that the primary advantage of passage retrieval rests in its ability to enhance the relevance of results returned. O'Connor noticed this in his early CANCERLIT experiments: "Passage retrieval saves users valuable time by immediately presenting them with relevant material within a document" (1980, p. 228). But opinions are mixed. Moffat et al. (1994) state that although very long and diffuse documents may give the appearance of relevance, close examination will reveal the reverse. Furthermore, when relevance ranking is done using whole documents as the input, one passage's high relevance may be completely obscured by the low relevance of the whole document overall (Kaszkiel & Zobel, 1997). It may be the revelation of document structure through hierarchies and outlines of the content that can best help the user determine relevance of a passage (Salton & Buckley, 1991). As Lalmas and Ruthven (1998) put it, often only part of a document is relevant. Callan (1994) states that the problem with full-text retrieval using "long documents, documents with complex structure, and even short 23 documents summarizing many subjects" (p. 302) is that this presents a challenge when the user can't tell where in the text an answer lies. As Kaszkiel and Zobel (1997) said: "Since passages are relatively short, they embody locality." (p. 178). Passage retrieval helps because it concentrates the reader's attention on those parts of the text that have a "high density" of relevant information, while also giving the reader an "intuitive overview" of the way in which those relevant subsections are distributed throughout the corpus (Tombros & Sanderson, 1998, p. 2; see also Salton et al., 1993; Salton & Allan, 1993). This phenomenon was also noted early by O'Connor (1980). In one experiment, he obtained questions about cancer "from those used at a CANCERLIT terminal" (perhaps log files, but not directly stated; p. 228) and directed medical librarians to construct an answer-base, using any means except CANCERLIT to answer the question. In two cases, answers could not be found; a computer then selected passages from the full-text articles that contained answers to the rest. O'Connor found that answers tended to be located in particular places, and that this knowledge could be exploited for ranking purposes: Another interesting result is the following: About 20% of the falsely retrieved sentences .. were from sections of papers which were headed "Methods" or "Materials and Methods". Another 20% were from sections headed "Discussion". ... By ranking output passages from such sections lower than those from the rest of the paper, nearly the same recall ... could be achieved with a 40% reduction in false retrieval. (p. 236) Losee (1996) found similar results in another medical domain: the CF database within TREC, which consists of Medline citations to articles to which the MeSH term "Cystic Fibrosis" was attached. (For a description, see Shaw, Wood, Wood & Tibbo, 1991). Working with window passages, Losee hypothesized that text "windows" and phrases would differ according to the knowledge discipline from which they were drawn. His experimental results showed that the "statistically significant window" could be used as a distinguishing characteristic of text corpora, although the reasons Losee cites may surprise anybody who has worked with medical terminologies: Authors of articles in the medical literature use the terms in the abstract more frequently in the body of the text and with more regularity than is found in other disciplines. This may be due both to a consistent and unambiguous vocabulary for medical discussions...(p. 755). Salton et al. (1993), who "scored" sentences in ranking of passages, note that "the location of sentences in the text under consideration" affects a particular passage's ranking (p. 50). Salton and Allan (1993) point out too that an 24 understanding of local context allows the user and the system to avoid false retrievals "caused by linguistic ambiguities" which context obviously disambiguates (p. 132). Context is also important when considering the relationship of document components to each other, since these components will relate "both temporally and hierarchically" (Lalmas & Ruthven, 1998, p. 530). These improvements in delivery of relevant documents to the user may be due to the ability of passage retrieval systems to exploit user understanding of context in relevance assessment. Clinical documents as candidates for passage retrieval The author has been unable to identify any acknowledgment of the passage retrieval research in the medical informatics literature or vice versa. One principal medical research group ascribes to "structured retrieval" all the same advantages of passage retrieval. This group is located at Maastricht University in the Netherlands, and is headed by Huibert Tange (see Tange, Dreessen et al., 1997; Tange, Hasman, Robbe, & Schouten, 1997; Tange, 1996; Tange et al., 1998). Tange, Hasman et al. (1997) propose that the "search structure", or information retrieval process, in the domain of clinical information has two main aspects: the "granularity" of the paragraphs, as previously defined, and the relationship of those paragraphs to each other. The number of paragraphs being searched—an important aspect of granularity--is proposed to be inversely related to the ease of searching them, which relates obviously to the work of Salton et al. ascribing passage understanding to relevance. The same proposition was put forward by Lincoln and Essin in 1995, again with only implicit acknowledgment that relevance had anything to do with the desired result: "An ability to specify text searches as narrowly as necessary using additional tags [SGML] avoids secondary parsing or sorting to eliminate unwanted material" (italics by author; p. 229). In one experiment, Tange et al. (1998) found that high granularity of clinical documents (meaning documents with large numbers of sections) was associated with increased speed of information retrieval for progress notes only; certainly, a finding of high value in a high-need clinical situation. However, this finding did not hold for other types of documents, specifically Medical History or Physical Examination documents, where excessive partitioning caused more problems than it solved. The author's doctoral dissertation, which tested information retrieval in 1000 structured versus unstructured clinical documents, found that structuring had no effect on precision (Smith, 2002). These findings agreed with those of Moffat et al. (1994). The effect of document type on the passage retrieval process clearly requires further research. In addition, results from the relatively small body of passage retrieval literature to retrieval of structured documents in medicine—an even smaller body--need to be considered in light of the nature of the documents themselves. Salton and his 25 colleagues (Salton & Allan, 1993; Salton & Buckley, 1991; Salton et al., 1993) in their ranking algorithms concentrated in similarity of passages within documents, focusing on finding "subparts of a large document" (an electronic encyclopedia) that "co-refer or are very similar in content" (Hearst & Plaunt, 1993, p. 61). This was done because similarity of passages was used as a signal of the relevance of one passage to another: An attempt was made to recognize text portions within which text meanings are homogeneous (that is, sufficiently similar to conclude that the texts are closely related (Salton & Buckley, 1991, p. 22). Clinical documents are typically extremely short. The author's doctoral dissertation used 1000 reports from a long-running electronic medical record system at the University of Pittsburgh. These documents consumed no more than 2 pages of printed text on average, were often only a few paragraphs long, and featured unique and nonredundant text organized akin to Hearst and Plaunt's "subtopics" (1993). In addition, unlike the case of the magazine articles mined by Hearst and Plaunt, the section heading labels are not content summarizations of the paragraphs they signal. The section headings used in clinical documents are more like fields of a database than the discourse-structured text of passage retrieval experiment. Finally, since each clinical document represents only one patient and one clinical event (e.g., a PET scan for the 50-year-old male represented in Figure 1), similarity of passages will probably seldom occur within documents. Instead, it is likely to occur across aggregations of documents that are of the same or similar type, e.g., all PET scans performed on 50-year-olds in all clinics in the healthcare system. Since, as previously described, three of the four principal types of clinical queries rely on identification of subgroups by common attributes, passage similarity is still an important factor to consider. Conclusion This paper has outlined a document-centered approach to the EHR as an information retrieval problem. It is clear that passage retrieval researchers working in the field of information science have seen similar values in document passages as have researchers in medical informatics. Without either literature acknowledging the other, workers in both camps have identified the same potential in document structure, labels, specificity and explicit hierarchies of knowledge for signaling relevance to the reader. The National Health Information Infrastructure Initiative (http://aspe.hhs.gov/sp/nhii/) identifies academics and researchers as natural stakeholders, like clinicians and caregivers, in enabling better healthcare through better information sharing (National Committee on Vital and Health Statistics, 26 2003). Information science has much to contribute to the health information technology arena and to electronic health records in particular: their development, their maintenance, and most importantly their improvement to serve the needs of diverse users. References Arnott Smith, C. (2003). Effect of XML markup on retrieval of clinical documents. Proceedings/AMIA :614-8. Berg, M., & Bowker, G. (1997). The multiple bodies of the medical record: Toward a sociology of an artifact. Sociological Quarterly, 38(3), 513-537. Berg, M., & Goorman, E. (1999). The contextual nature of medical information. International Journal of Medical Informatics, 56, 51-60. Bergeron, B. (1998). The effect of technology on the written tradition of medicine. Perspectives in Biology and Medicine, 41(4), 572-578. Bird, L.J., Goodchild, A., & Beale, T. (2000). Integrating health care information using XML-based metadata. HIC 2000. Retrieved February 12, 2006, from http://titanium.dstc.edu.au/papers/HIC2000.pdf. Blair, D. C. (2002). The challenge of commercial document retrieval, Part II: A strategy for document searching based on identifiable document partitions. Information Processing & Management, 38(2), 293-304. Callan, J. P. (1994). Passage-level evidence in document retrieval. Proceedings of ACM SIGIR Conference (pp. 302-310). New York: ACM. Cisco, S.L. (1996). Document imaging in the United States: A survey of several hundred hospital installations. (p. 8). Newton, MA: Medical Records Institute. Committee on Data Standards for Patient Safety, Institute of Medicine. (2003). Key Capabilities of an Electronic Health Record System: Letter Report. Washington, DC: National Academies Press. Retrieved February 12, 2006, from http://fermat.nap.edu/catalog/10781.html]. 27 Cormack, G., Clarke, C., Palmer, C., & To, S.S.L. (2000). Passage-based query refinement. Information Processing & Management, 36(1), 133-153. Cui, H., Sun, R., Li, K., Kan, M-Y., & Chua, T-S. (2005). Question answering passage retrieval using dependency relations. SIGIR Forum, 39(2), 400-407. Dambro, M.R., Weiss, B.D., McClure, C.L., & Vuturo, A.F. (1988). An unsuccessful experience with computerized medical records. Journal of Medical Education, 63(8), 617-623. Essin, D. J. (1993). Intelligent processing of loosely structured documents as a strategy for organizing electronic health care records. Methods of Information in Medicine, 32, 265-268. Essin, D. J., & Essin, C. D. (1990). Computerizing medical records: Software criteria for systems to document patient encounters. Critical Care Medicine, 18(1), 100-102. Fuji, A., Iwayam, M., & Kando, N. (2004). The patent retrieval task in the Fourth NTCIR Workshop. SIGIR Forum, 560-561. Garfinkel, H., & Bittner, E. (1967). "Good" organizational reasons for "bad" clinic records. In H. Garfinkel (Ed.), Studies in ethnomethodology (pp. 186-207). Englewood Cliffs, NJ: Prentice-Hall. HL7 [Health Level Seven.] (2005, Oct. 13). HL7 position statement on CCR/CDA Harmonization. Retrieved February 12, 2006, from http://www.hl7.org/documentcenter/public/mou/HL7%20Position%20Statement% 20on%20CCR%20updated%20Oct%2014.pdf. Hearst, M. A., & Plaunt, C. (1993). Subtopic structuring for full-length document access. In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 59-68). New York: ACM. Hersh, W. R. (1996). Information retrieval: A health care perspective. New York: Springer-Verlag. 28 Hersh, W.R. (1995). The electronic medical record: Promises and problems. Journal of the American Society for Information Science, 46(10), 772-776. Humphrey, S.M., Rogers, W.J., Kilicoglu, H., Demner-Fushman, D., & Rindflesch, T.C. (2006). Word sense disambiguation by selecting the best semantic type based on Journal Descriptor Indexing: Preliminary experiment. Journal of the American Society for Information Science & Technology, 57(1), 96-113. Jordanova, L. (1995). The social construction of medical knowledge. Social History of Medicine, 8(3):361-381. Kaszkiel, M., & Zobel, J. (1997). Passage retrieval revisited. SIGIR '97 (pp. 178-185). New York: ACM. Kleinke, J. (1998). Release 0.0: Clinical information technology in the real world. Health Affairs, 17, 23-38. Kluge, E.-H. W. (1996). The medical record: Narration and story as a path through patient data. Methods of Information in Medicine, 35, 88-92. Laerum, H., Ellingesen, G., & Faxvaag, A. (2001). Doctors' use of electronic medical records systems in hospitals: Cross sectional survey. BMJ, 323(7325), 1344-1348. Lalmas, M., & Ruthven, I. (1998). Representing and retrieving structured documents using the Dempster-Shafer theory of evidence: Modelling and evaluation. The Journal of Documentation, 54(5), 529-565. Lambrix, P., & Shahmehri, N. (2000). Querying documents using content, structure and properties. Journal of Intelligent Information Systems, 15, 287-307. Leroy, G., & Chen, H. (2005). Genescene: An ontology-enhanced integration of linguistic and co-occurrence based relations in biomedical texts. Journal of the American Society for Information Science & Technology, 56(5), 457-468. Lincoln, T. L., & Essin, D. J. 1995. A document processing architecture for electronic medical records. Medinfo, 227-230. 29 Kohn, L.T., Corrigan, J.M., & Donaldson, M.S. (Eds.) (2000). To Err Is Human: Building a Safer Health System.. Washington, DC: National Academies Press. Losee, R. (1996). Text windows and phrases differing by discipline, location in document, and syntactic structure. Information Processing & Management, 32(6), 747. Marshall, J., Balas, E. A., & Reid, J. C. (1997). Technique for efficient information retrieval in outpatient systems. Journal of the American Medical Informatics Association, 76-80. Melucci, M. (1998). Passage retrieval: A probabilistic technique. Information Processing & Management, 34(1), 43-67. Mendonça, E. A., Cimino, J. J., Johnson, S. B., & Seol, Y. H. (2001). Accessing heterogeneous sources of evidence to answer clinical questions. Journal of Biomedical Informatics, 34(2), 85-98. Moffat, A., Sacks-Davis, R., Wilkinson, R., & Zobel, J. (1994). Retrieval of partial documents. In D. Harman (Ed.), Proceedings of the Second Text Retrieval Conference (TREC-2) (pp. 181-190). Washington, DC: Department of Commerce/NIST. Montani, M., & Bellazzi, R. (2001). Intelligent knowledge retrieval for decision support in medical applications. Medinfo, 10(Pt 1), 498-502. National Committee on Vital and Health Statistics (2001). Information for Health: A Strategy for Building the National Health Information Infrastructure. Retrieved February 12, 2006, from http://www.ncvhs.hhs.gov/nhiilayo.pdf. Nygren, E., Johnson, S., & Henriksson, P. (1992). Reading the medical record: I. Analysis of physicians ways of reading the medical record. Computer Methods and Programs in Biomedicine, 39, 1-12. O'Connor, J. (1980). Answer-passage retrieval by text searching. Journal of the American Society for Information Science, 31(4), 227-39. Ondo, K., Wagner, J., & Gale, K. (2002). The electronic medical record: Hype or reality? Journal of Healthcare Information Management, 17(4): p. 2. 30 Panko, W., Silverstein, J., & Lincoln, T. (1999). Technologies for extracting full value from the electronic patient record. Proceedings of the 32nd Hawaii International Conference on System Sciences. New York: IEEE. Rees, C. (1981). Records and hospital routine. Atkinson P, & Heath C (Eds.), Medical work: Realities and routines (pp. 55-70) . Farnborough, England: Gower. Ribeiro-Neto, B., Laender, A.H.F.d.L., & Luciano, R.S. (2001). An experimental study in automatically categorizing medical documents. Journal of the American Society for Information Science & Technology, 52(5), 391-401. Rossi Mori, A., & Consorti, F. (1999). Structures of clinical information in patient records. Proceedings/AMIA Annual Symposium, 132-6. Royal College of General Practitioners Health Informatics Task Force. (1999). Scoping the primary care view of the option for the development of the EPR (ScopeEPR). Retrieved February 11, 2006, through Google.com cache: http://www.schin.ncl.ac.uk/rcgp/scopeEPR/report/index22.htm. Salton, G., & Allan, J. (1993). Selective text utilization and text traversal. Hypertext '93 (pp. 131-144). ACM. Salton, G., & Buckley, C. (1991). Automatic text structuring and retrieval: Experiments in automatic encyclopedia searching. Proceedings of the 14th International SIGIR. (pp. 21-30). New York: ACM. Salton, G., Allan, J., & Buckley, C. (1993). Approaches to passage retrieval in full text information systems. SIGIR '93 (pp. 49-55). ACM. Shaw, W. M., Jr., Wood, J. B., Wood, R. E., & Tibbo, H. R. (1991). The cystic fibrosis database: Content and research opportunities. Library and Information Science Research, 13, 347-366. Smith, C.A. (2005). Electronic health records: Sharing knowledge while preserving privacy. IBM Center for Healthcare Management Research Report. Retrieved February 12, 2006, from http://www-1.ibm.com/services/us/bcs/html/chm_pubs.html. 31 Smith, C.A. (2002). The Clinical Document Architecture: XML semantic markup for enhanced clinical information retrieval. (Doctoral dissertation, University of Pittsburgh, Pittsburgh). [Dissertation Abstracts Online, 64/01A , 9.] Sujansky, W. (1998). A document-centric electronic medical record system with database-centric reporting capabilities. In Toward an Electronic Patient Record '98: Proceedings (Vol. I) (pp. 398-403). Newton, MA: Medical Records Institute. Swanson DR. Two medical literatures that are logically but not bibliographically connected. JASIS 1987; 38: 228-233. Tange, H. (1996). How to approach the structuring of the medical record? Toward a model for flexible access to free text medical data. International Journal of Bio-Medical Computing, 42, 27-34. Tange, H. J., Dreessen, V. A. B., Hasman, A., & Donkers, H. H. L. M. (1997). An experimental electronic medical-record system with multiple views on medical narratives. Computer Methods and Programs in Biomedicine, 54, 157-172. Tange, H. J., Hasman, A., Robbé, P. F. d. V., & Schouten, H. C. (1997). Medical narratives in electronic medical records. International Journal of Medical Informatics, 46, 7-29. Tange, H. J., Schouten, H. C., Kester, A. D. M., & Hasman, A. (1998). The granularity of medical narratives and its effect on the speed and completeness of information retrieval. Journal of the American Medical Informatics Association, 5, 571-582. Tombros, A., & Sanderson, M. (1998). Advantages of query biased summaries in information retrieval. SIGIR '98 (pp. 2-10). New York : ACM. Wolff, J. E., Flörke, H., & Cremers, A. B. (2000). Searching and browsing collections of structural information. In Proceedings of the IEEE Advances in Digital Libraries, pp. 141-150. New York: IEEE. Xi, W., & Xu-Rong, R. (2001). Incorporating window-based passage-level evidence in document retrieval. Journal of Information Science, 27(2), 73-88. Yang, K., Maglaughlin, K., & Newby, G. (2001). Passage feedback with IRIS. Information Processing & Management, 37(3), 521-. Privacy & Opting Out of Cookies
A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2018 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
Privacy & Opting Out of Cookies
A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2018 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
Privacy & Opting Out of Cookies
A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2018 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
* E-mail: josh.denny@vanderbilt.edu
Affiliation Departments of Biomedical Informatics and Medicine, Vanderbilt University School of Medicine, Nashville, Tennessee, United States of America
⨯
Chapter 13: Mining Electronic Health Records in the Genomics Era
Joshua C. Denny
Figures
Abstract
Abstract: The combination of improved genomic analysis methods, decreasing genotyping costs, and increasing computing resources has led to an explosion of clinical genomic knowledge in the last decade. Similarly, healthcare systems are increasingly adopting robust electronic health record (EHR) systems that not only can improve health care, but also contain a vast repository of disease and treatment data that could be mined for genomic research. Indeed, institutions are creating EHR-linked DNA biobanks to enable genomic and pharmacogenomic research, using EHR data for phenotypic information. However, EHRs are designed primarily for clinical care, not research, so reuse of clinical EHR data for research purposes can be challenging. Difficulties in use of EHR data include: data availability, missing data, incorrect data, and vast quantities of unstructured narrative text data. Structured information includes billing codes, most laboratory reports, and other variables such as physiologic measurements and demographic information. Significant information, however, remains locked within EHR narrative text documents, including clinical notes and certain categories of test results, such as pathology and radiology reports. For relatively rare observations, combinations of simple free-text searches and billing codes may prove adequate when followed by manual chart review. However, to extract the large cohorts necessary for genome-wide association studies, natural language processing methods to process narrative text data may be needed. Combinations of structured and unstructured textual data can be mined to generate high-validity collections of cases and controls for a given condition. Once high-quality cases and controls are identified, EHR-derived cases can be used for genomic discovery and validation. Since EHR data includes a broad sampling of clinically-relevant phenotypic information, it may enable multiple genomic investigations upon a single set of genotyped individuals. This chapter reviews several examples of phenotype extraction and their application to genetic research, demonstrating a viable future for genomic discovery using EHR-linked data.
Citation: Denny JC (2012) Chapter 13: Mining Electronic Health Records in the Genomics Era. PLoS Comput Biol 8(12):            e1002823.                  https://doi.org/10.1371/journal.pcbi.1002823
Editors: Fran Lewitter (         Whitehead Institute, United States of America       ), and Maricel Kann (         University of Maryland, Baltimore County, United States of America       )
Published: December 27, 2012
Copyright: © 2012 Joshua C. Denny. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Funding: This article was supported in part by grants from the National Library of Medicine R01 LM 010685 and the National Human Genome Research Institute U01 HG004603. The funders had no role in the preparation of the manuscript.
Competing interests: The author has declared that no competing interests exist.
What to Learn in This Chapter
Describe the types of information available in Electronic Health Records (EHRs), and the relative sensitivity and positive predictive value of each
Describe the difference between unstructured and structured information in the EHR
Describe methods for developing accurate phenotype algorithms that integrate structured and unstructured EHR information, and the roles played by billing codes, laboratory values, medication data, and natural language processing
Describe recent uses of EHR-derived phenotypes to study genome-phenome relationships
Describe the cost advantages unique to EHR-linked biobanks, and the ability to reuse genetic data for many studies
Understand the role of EHRs to enable phenome-wide association studies of genetic variants
This article is part of the "Translational Bioinformatics" collection for PLOS Computational Biology.
1. Introduction and Motivation
Typical genetic research studies have used purpose-built cohorts or observational studies for genetic research. As of 2012, more than 1000 genome-wide association analyses have been performed, not to mention a vast quantity of candidate gene studies [1] . Many of these studies have investigated multiple disease and phenotypic traits within a single patient cohort, such as the Wellcome Trust [2] and Framingham research cohorts [3] – [5] . Typically, patient questionnaires and/or research staff are used to ascertain phenotypic traits for a patient. While these study designs may offer high validity and repeatability in their assessment of a given trait, these models are typically very costly and often represent only a cross-section of time. In addition, rare diseases may take a significant time to accrue in these datasets.
Another model that is gaining acceptance is genetic discovery based solely or partially from phenotype information derived solely from the electronic health record (EHR) [6] . In these models, a hospital collects DNA for research, and maintains a linkage between the DNA sample and the EHR data for that patient. The primary source of phenotypic information, therefore, is the EHR. Depending on the design of the biobank model, some EHR-linked biobanks have the ability to supplement EHR-accrued data with purpose-collected research data.
The EHR model for genetic research offers several key advantages, but also faces prominent challenges to successful implementation. A primary advantage is cost. EHRs contain a longitudinal record of robust clinical data that is produced as a byproduct of routine clinical care. Thus, it is a rich, real-world dataset that requires little additional funding to obtain. Both study designs share costs for obtaining and storing DNA.
Another advantage of EHR-linked DNA databanks is the potential to reuse genetic information to investigate a broad range of additional phenotypes beyond the original study. This is particularly true for dense genetic data such as generated through genome-wide association studies or large-scale sequencing data. For instance, a patient may be genotyped once as part of a study on diabetes, and then later participate in another analysis for cardiovascular disease.
Major efforts in EHR DNA biobanking are underway at a number of institutions. One of the major driving forces has been the National Human Genome Research Institute (NHGRI)-sponsored Electronic Medical Records and Genomics (eMERGE) network [7] , which began in 2007 and, as of 2012, consists of nine sites that are performing genome-wide association studies using phenotypic data derived from EHR. The National Institutes of Health (NIH)-sponsored Pharmacogenomics Research Network (PGRN) also include sites performing genetic research using EHR data as their source of phenotypic data. Another example is the Kaiser Permanente Research Program on Genes, Environment and Health, which has genotyped 100,000 members with linked EHR data [8] .
2. Classes of Data Available in EHRs
EHRs are designed primarily to support clinical care, billing, and, increasingly, other functions such as quality improvement initiatives aimed at improving the health of a population. Thus, the types of data and their methods of storing this data are optimized to support these missions. The primary types of information available from EHRs are: billing data, laboratory results and vital signs, provider documentation, documentation from reports and tests, and medication records. Billing data and many laboratory results are available in most systems as structured "name-value pair" data. Clinical documentation, many test results (such as echocardiograms and radiology testing), and medication records are often found in narrative or semi-narrative text formats. Researchers creating "electronic phenotype algorithms" (discussed in Section 6.2) typically utilize multiple types of informatics (e.g., billing codes, laboratory results, medication data, and/or NLP) to achieve high accuracy when identifying cases and controls from the EHR.
Table 1 summarizes the types of data available in the EHR and their strengths and weaknesses.
Download:
https://doi.org/10.1371/journal.pcbi.1002823.t001
2.1 Billing Data
Billing data typically consists of codes derived from the International Classification of Diseases (ICD) and Current Procedural Terminology (CPT). ICD is a hierarchical terminology of diseases, signs, symptoms, and procedure codes maintained by the World Health Organization (WHO). While the majority of the world uses ICD version 10, the United States (as of 2012) uses ICD version 9-CM; the current Center for Medicare and Medicaid Services guidelines mandate a transition to ICD-10-CM in the United States by October 1, 2014. Because of their widespread use as required components for billing, and due to their ubiquity within EHR systems, billing codes are frequently used for research purposes [9] – [14] . Prior research has demonstrated that such administrative data can have poor sensitivity and specificity [15] , [16] . Despite this, they remain an important part of more complex phenotype algorithms that achieve high performance [17] – [19] .
CPT codes are created and maintained by the American Medical Association. They serve as the chief coding system providers use to bill for clinical services. Typically, CPTs are paired with ICD codes, the latter providing the reason (e.g., a disease or symptom) for a clinical encounter or procedure. This satisfies the requirements of insurers, who require certain allowable diagnoses and symptoms to pay for a given procedure. For example, insurance companies will pay for a brain magnetic resonance imaging (MRI) scan that is ordered for a number of complaints (such as known cancers or symptoms such as headache), but not for unrelated symptoms such as chest pain.
Within the context of establishing a particular diagnosis from EHR data, CPT codes tend to have high specificity but low sensitivity, while ICD9 codes have comparatively lower specificity but higher sensitivity. For instance, to establish the diagnosis of coronary artery disease, one could look for a CPT code for "coronary artery bypass surgery" or "percutaneous coronary angioplasty" disease, or for one of several ICD9 codes. If the CPT code is present, there is a high probability that the patient has corresponding diagnosis of coronary disease. However, many patients without these CPT codes also have coronary disease, but either have not received these interventions or received them at a different hospital. In contrast, a clinician may bill an ICD9 code for coronary disease based on clinical suspicion without a firm diagnosis. Figure 1 shows the results of a study that compared the use of natural language processing (NLP) and CPT codes to detect patients who have received colorectal cancer screening, via a colonoscopy within the last ten years, at one institution. In this study, only 61% (106 out of 174 total) of the documented completed colonoscopies were found via CPT codes [20] . The most common cause of false negatives was a colonoscopy completed at another hospital. CPT codes, however, had a very high precision (i.e., positive predictive value; see Box 1 ), with only one false positive.
Download:
original image
Figure 1. Comparison of natural language processing (NLP) and CPT codes to detect completed colonoscopies in 200 patients.
In this study, more completed colonoscopies were found via NLP than with billing codes alone, and only one colonoscopy was found with billing codes that was not found with NLP. NLP examples were reviewed for accuracy.
Box 1. Metrics Commonly Used to Evaluate Phenotype Selection Algorithms
2.2 Laboratory and Vital Signs
Laboratory data and vital signs form a longitudinal record of mostly structured data in the medical record. In addition to being stored as name-value pair data, these fields and values can be encoded using standard terminologies. The most common controlled vocabulary used to represent laboratory tests and vital signs is the Logical Observation Identifiers Names and Codes (LOINC®), which is a Consolidated Health Informatics standard for representation of laboratory and test names and is part of Health Language 7 (HL7) [21] , [22] . Despite the growing use of LOINC, many (perhaps most) hospital lab systems still use local dictionaries to encode laboratory results internally. Hospital laboratory systems or testing companies may change over time, resulting in different internal codes for the same test result. Thus, care is needed to implement selection logic based on laboratory results. Indeed, a 2009–2010 data standardization effort at Vanderbilt University Medical Center found that the concept of "weight" and "height" each had more than five internal representations. Weights and heights were also recorded by different systems using different field names and stored internally with different units (e.g., kilograms, grams, and pounds for weight; centimeters, meters, inches, and feet for height).
Structured laboratory results are often a very important component of phenotype algorithms, and can represent targets for genomic investigation [3] , [4] , [23] . An algorithm to identify type 2 diabetes (T2D) cases and controls, for instance, used laboratory values (e.g., hemoglobin A1c and glucose values) combined with billing codes and medication mentions [17] . Similarly, an algorithm to determine genomic determinants of normal cardiac conduction required normal electrolyte (potassium, calcium, and magnesium) values [16] . In these settings, investigation of the determinants of the values requires careful selection of the value to be investigated. For instance, an analysis of determinants of uric acid or red blood cell indices would exclude patients treated with certain antineoplastic agents (which can increase uric acid or suppression of erythrocyte production), and, similarly, an analysis of white blood cell indices also excludes patients with active infections and certain medications at the time of the laboratory measurement.
2.3 Provider Documentation
Clinical documentation represents perhaps the richest and most diverse source of phenotype information. Provider documentation is required for nearly all billing of tests and clinical visits, and is frequently found in EHR systems. To be useful for phenotyping efforts, clinical documentation must be in the form of electronically-available text that can be used for subsequent manual review, text searches, or NLP. They can be created via computer-based documentation (CBD) systems or dictated and transcribed. The most common form of computable text is in unstructured narrative text documents, although a number of developers have also created structured documentation tools [24] . Narrative text documents can be processed by text queries or by NLP systems, as discussed in the following section.
For some phenotypes, crucial documents may only be available as hand-written documents, and thus not amenable to text searching or NLP. Unavailability may result from clinics that are slow adopters, have very high patient volumes, or have specific workflows not well accommodated by the EHR system [25] . However, these hand-written documents may be available electronically as scanned copies. Recent efforts have shown that intelligent character recognition (ICR) software may be useful for processing scanned documents containing hand-written fields ( Figure 2 ) [26] , [27] . This task can be challenging, however, and works best when the providers are completing pre-formatted forms.
Download:
Figure 2. Use of Intelligent Character Recognition to codify handwriting.
Figure courtesy of Luke Rasmussen, Northwestern University.
https://doi.org/10.1371/journal.pcbi.1002823.g002
2.4 Documentation from Reports and Tests
Provider-generated reports and test results include radiology and pathology reports and some procedure results such as echocardiograms. They are often in the form of narrative text results. Many of these contain a mixture of structured and unstructured results. Examples include an electrocardiogram report, which typically has structured interval durations and may contain a structured field indicating whether the test was abnormal or not. However, most electrocardiogram (ECG) reports also contain a narrative text "impression" representing the cardiologist's interpretation of the result (e.g., "consider anterolateral myocardial ischemia" or "Since last ECG, patient has developed atrial fibrillation") [28] . For ECGs, the structured content (e.g., the intervals measured on the ECG) are generated using automated algorithms and have varying accuracy [29] .
2.5 Medication Records
Medication records serve an important role in accurate phenotype characterization. They can be used to increase the precision of case identification, and to help ensure that patients believed to be controls do not actually have the disease. Medications received by a patient serve as confirmation that the treating physician believed the disease was present to a sufficient degree that they prescribed a treating medication. It is particularly helpful to find presence or absence of medications highly specific or sensitive for the disease. For instance, a patient with diabetes will receive either oral or injectable hypoglycemic agents; these medications are both highly sensitive and specific for treating diabetes, and can also be used to help differentiate type I diabetes (treated almost exclusively with insulin) from T2D (which is typically a disease of insulin resistance and thus can be treated with a combination of oral and injectable hypoglycemic agents).
Medication records can be in varying forms within an electronic record. With the increased use of computerized provider order entry (CPOE) systems to manage hospital stays, inpatient medication records are often available in highly structured records that may be mapped to controlled vocabularies. In addition, many hospital systems are installing automated bar-code medication administration records by which hospital staff record each individual drug administration for each patient [30] . With this information, accurate drug exposures and their times can be constructed for each inpatient. Even without electronic medication administration records (such as bar-code systems), research has shown that CPOE-ordered medications are given with fairly high reliability [31] .
Outpatient medication records are often recorded via narrative text entries within clinical documentation, patient problem lists, or communications with patients through telephone calls or patient portals. Many EHR systems have incorporated outpatient prescribing systems, which create structured medical records during generation of new prescriptions and refills. However, within many EHR systems, electronic prescribing tools are optional, not yet widely adopted, or have only been used within recent history. Thus, accurate construction of a patient's medication exposure history often requires NLP techniques. For specific algorithms, focused free-text searching for a set of medications can be efficient and effective [17] . This approach requires the researcher to generate the list of brand names, generics, combination medications, and abbreviations that would be used, but has the advantage that it can be easily accomplished using relational database queries. The downside is that this approach requires re-engineering for each medication or set of medications to be searched, and does not allow for the retrieval of other medication data, such as dose, frequency, and duration. A more general-purpose approach can be achieved with NLP, which is discussed in greater detail in Section 3 below.
3. Natural Language Processing to Support Clinical Knowledge Extraction
Although many documentation tools include structured and semi-structured elements, the vast majority of computer based documentation (CBD) remains in "natural language" narrative formats [24] . Thus, to be useful for data mining, narrative data must be processed through use of text-searching (e.g., keyword searching) or NLP systems. Keyword searching can effectively identify rare physical exam findings in text [32] , and extension to use of regular expression pattern matching has been used to extract blood pressure readings [33] . NLP computer algorithms scan and parse unstructured "free-text" documents, applying syntactic and semantic rules to extract structured representations of the information content, such as concepts recognized from a controlled terminology [34] – [37] . Early NLP efforts to extract medical concepts from clinical text documents focused on coding in the Systematic Nomenclature of Pathology or the ICD for financial and billing purposes [38] , while more recent efforts often use complete versions of the Unified Medical Language System (UMLS) [39] – [41] , SNOMED-CT [16] , and/or domain-specific vocabularies such as RxNorm for medication extraction [42] . NLP systems utilize varying approaches to "understanding text," including rule-based and statistical approaches using syntactic and/or semantic information. Natural language processors can achieve classification rates similar to those of manual reviewers, and can be superior to keyword searches. A number of researchers have demonstrated the effectiveness of NLP for large-scale text-processing tasks. Melton and Hripcsak used MedLEE to recognize instances of adverse events in hospital discharge summaries [43] . Friedman and colleagues evaluated NLP for pharmacovigilance to discover adverse drug events from clinical records by using statistical methods that associate extracted UMLS disease concepts with extracted medication names [40] . These studies show the potential for NLP to aid in specific phenotype recognition.
Using either NLP systems or keyword searching, the primary task in identifying a particular phenotype is to filter out concepts (or keywords) within a corpus of documents that indicate statements other than the patient having the disease. Researchers may desire to specify particular document types (e.g., documents within a given domain, problem lists, etc.) or particular types of visits or specialists (e.g., requiring a visit with an ophthalmologist). Some common NLP tasks needed in phenotype classification include identifying family medical history context and negated terms (e.g., "no cardiac disease"), and removing drug allergies when searching for patients taking a certain medication. Recognition of sections within documents can be handled using structured section labels, specialized NLP systems such as SecTag [44] , or more general-purpose NLP systems such as MedLEE [45] or HITEX [46] . A number of solutions have been proposed for negation detection; among the more widespread are adaptations of the NegEx algorithm developed by Chapman et al., which uses a series of negation phrases and boundary words to identify negated text [47] . NegEx or similar algorithms can be used as a standalone system or be integrated within a number of general-purpose NLP systems including MedLEE [48] , the KnowledgeMap concept identifier [49] , cTAKES [50] , and the National Library of Medicine's MetaMap [51] .
Medication information extraction is an important area for clinical applications that benefits from specialized NLP tools. Most general-purpose NLP systems will recognize medications by the medication ingredient mentioned in the text but may not identify the relevant medication metadata such as dose, frequency, and route. In addition, a general purpose NLP system using as its vocabulary the UMLS will likely recognize "atenolol" and "Tenormin" (a United States brand name for atenolol) as two different concepts, since each is represented by separate concepts in the UMLS. Medication-specific NLP systems focus on extracting such metadata for a medication. Sirohl and Peissig applied a commercial medication NLP system to derived structured medication information [52] , which was later linked to laboratory data and used to explore the pharmacodynamics of statin efficacy (a cholesterol-lowering medication) [53] . Xu et al. developed a similar system at Vanderbilt called MedEx, which had recall and precision ≥0.90 for discharge summaries and clinic notes on Vanderbilt clinical documents [42] . Additionally, the 2009 i2b2 NLP challenge focused on medication extraction using de-identified discharge summaries from Partners Healthcare, and 20 teams competed to identify medications and their signatures. The best systems achieved F-measures ≥0.80 [54] . Much work remains to be done in this area, as extraction of both medication names and associated signature information can be challenging when considering the full breadth of clinical documentation formats available, including provider-staff and provider-patient communications, which often contain less formal and misspelled representations of prescribed medications.
For more information on NLP methods and applications, please see the article on text mining elsewhere in this collection (submitted).
4. EHR-Associated Biobanks: Enabling EHR-Based Genomic Science
DNA biobanks associated with EHR systems can be composed of either "all comers" or a focused collection, and pursue either a conventional consented "opt-in" or an "opt-out" approach. Currently, the majority of DNA biobanks have an opt-in approach that selects patients for particular research studies. Two population-based models in the eMERGE network are the Personalized Medicine Research Population (PMRP) project of the Marshfield Clinic (Marshfield, WI) [55] and Northwestern University's NUgene project (Chicago, IL). The PMRP project selected 20,000 individuals who receive care in the geographic region of the Marshfield Clinic. These patients have been consented, surveyed, and have given permission to the investigators for recontact in the future if additional information is needed. The NUgene project, which has enrolled nearly 10,000 people through 2012, uses a similar approach, obtaining patients' consent during outpatient clinic visits [56] . Another example of an EHR-associated biobank is the Kaiser-Permanente biobank, which has genotyped 100,000 individuals [57] .
The alternative "opt-out" approach is evidenced by Vanderbilt University's BioVU, which associates DNA with de-identified EHR data [58] . In this model, patients have the opportunity to "opt out" of the DNA biobank by checking a box on the standard "Consent to Treatment" form signed as part of routine clinical care. A majority of patients (>90%) do not check this box, indicating assent to the use of their DNA in the biobank [58] . If the patient does not opt-out, blood that is scheduled to be discarded after routine laboratory testing is instead sent for DNA extraction, which is stored for potential future use. To ensure that no one knows with certainty if a subject's DNA is in BioVU, an additional small percentage of patients are randomly excluded.
The BioVU model requires that the DNA and associated EHR data be de-identified in order to assure that the model complies with the policies of non-human subjects research. The full-text of the EHR undergoes a process of de-identification with software programs that remove Health Insurance Portability and Accountability Act (HIPAA) identifiers from all clinical documentation in the medical record. At the time of this writing, text de-identification for BioVU is performed using the commercial product DE-ID [59] with additional pre- and post-processing steps. However, a number of other clinical text de-identification software packages have been studied, some of which are open source [60] , [61] . Multiple reviews by both the local institutional review board and the federal Office for Human Research Protections have affirmed this status as nonhuman subjects research according to 45 CFR 46 [58] . Nonetheless, all research conducted within BioVU and the associated de-identified EHR (called the "Synthetic Derivative") is overseen by the local Institutional Review Board. An opt-out model similar to BioVU is used by Partners Healthcare for the Crimson biobank, which can accrue patients who meet specific phenotype criteria as they have routine blood draws.
An advantage of the opt-out approach is rapid sample accrual. BioVU began collecting DNA samples in 2007, adding about 500 new samples weekly, and has over 150,000 subjects as of September 2012. Since it enrolls subjects prospectively, investigation of rare phenotypes may be possible with such systems. The major disadvantage of the opt-out approach is that it precludes recontact of the patients since their identity has been removed. However, the Synthetic Derivative is continually updated as new information is added to the EHR, such that the amount of phenotypic information for included patients grows over time.
5. Race and Ethnicity in EHR-Derived Biobanks
Given that much genetic information varies greatly within ancestral populations, accurate knowledge of genetic ancestry information is essential to allow for proper genetic study design and control of population stratification. Without it, one can see numerous spurious genetic associations due solely to race/ethnicity [62] . Single nucleotide polymorphisms (SNPs) common in one population may be rare in another. In large-scale GWA analyses, one can tolerate less accurate knowledge of ancestry a priori, since the large amount of genetic data allows one to calculate the genetic ancestry of the subject using catalogs of SNPs known to vary between races. Alternatively, one can also adjust for genetic ancestry using tools such as EIGENSTRAT [63] . However, in smaller candidate gene studies, it is important to know the ancestry beforehand.
Self-reported race/ethnicity data is often used in genetic studies. In contrast race/ethnicity as recorded within an EHR may be entered through a variety of sources. Most commonly, administrative staff record race/ethnicity via structured data collection tools in the EHR. Often, this field can be ignored (left as "unknown"), especially in busy clinical environments, such as emergency departments. "Unknown" percentages of patients can range between 9% and 23% of subjects [17] , [18] . Among those patients for whom data is entered, a study of genetic ancestry informative markers correlated well with EHR-reported race/ethnicities [64] . In addition, a study within the Veterans Administration (VA) hospital system noted that over 95% of all EHR-derived race/ethnicity agreed with self-reported race/ethnicity using nearly one million records [65] . Thus, despite concerns over EHR-derived ancestral information, such information, when present, appears similar to self-report ancestry information.
6. Phenotype-Driven Discovery in EHRs
6.1 Measure of Phenotype Selection Logic Performance
The evaluation of phenotype selection logic can use metrics similar to information retrieval tasks. Common metrics are sensitivity (or recall), specificity, positive predictive value (PPV, also known as precision), and negative predictive value (see Box 1 ). If a population is assessed for case and control status, then another useful metric is comparing the receiver operator characteristic (ROC) curves. ROC curves graph the sensitivity vs. false positive rate (or, 1-specificity) given a continuous measure of the outcome of the algorithm. By calculating the area under the ROC curve (AUC), one has a single measure of the overall performance of an algorithm that can be used to compare two algorithms or selection logics. Since the scale of the graph is 0 to 1 on both axes, the performance of a perfect algorithm is 1, and random chance is 0.5.
6.2 Creation of Phenotype Selection Logic
Initial work in phenotype detection has often focused on a single modality of EHR data. A number of studies have used billing data, some comparing directly to other genres of data, such as NLP. Li et al. compared the results of ICD-9 encoded diagnoses and NLP-processed discharge summaries for clinical trial eligibility queries, finding that use of NLP provided more valuable data sources for clinical trial pre-screening than ICD-9 codes [15] . Savova et al. has used cTAKES to discover peripheral arterial disease cases by looking for particular key words in radiology reports, and then aggregating the individual instances using "AND-OR-NOT" Boolean logic to classify cases into four categories: positive, negative, probable, and unknown [66] .
Phenotype algorithms can be created multiple ways, depending of the rarity of the phenotype, the capabilities of the EHR system, and the desired sample size of the study. Generally, phenotype selection logics (algorithms) are composed of one or more of four elements: billing code data, other structured (coded) data such as laboratory values and demographic data, medication information, and NLP-derived data. Structured data can be retrieved effectively from most EHR systems. These data can be combined through simple Boolean logic [17] or through machine learning methods such as logistic regression [18] , to achieve a predefined specificity or positive predictive value. A drawback to the use of machine learning data (such as logistic regression models) is that it may not be as portable to other EHR systems as more simple Boolean logic, depending on how the models are constructed. The application of many phenotype selection logics can be thought of partitioning individuals into four buckets – definite cases (with sufficiently high PPV), possible cases (which can be manually reviewed if needed), controls (which do not have the disease with acceptable PPV), and individuals excluded from the analysis due to either potentially overlapping diagnoses or insufficient evidence ( Figure 3 ).
Download:
Article-Level Metrics
Submit Your Manuscript
Discover a faster, simpler path to publishing in a high-quality journal. PLOS ONE promises fair, rigorous peer review,   broad scope, and wide readership – a perfect fit for your research every time.
Article-Level Metrics
Submit Your Manuscript
Discover a faster, simpler path to publishing in a high-quality journal. PLOS ONE promises fair, rigorous peer review,   broad scope, and wide readership – a perfect fit for your research every time.
A Biomedical Information Retrieval System based on Clustering for Mobile Devices * Un Sistema de Recuperación de Información Biomédica en Dispositivos Móviles basado en Agrupamiento * This work has been partially funded by the Spanish Ministry of Science and Innovation and the European Union from the ERDF (TIN2009-14057-C03-03) Manuel Millán, Alejandro Muñoz Escuela Politécnica Superior Universidad de Huelva {manuel.millan, alejandro.munoz}@alu.uhu.es Manuel de la Villa, Manuel J. Maña Depto. de Tecnologías de la Información Universidad de Huelva {manuel.villa, manuel.mana}@dti.uhu.es Resumen: La sobrecarga de información producida por la creciente disponibilidad en internet de textos y publicaciones de interés es un problema que se acrecienta cuando esa información es necesaria para la toma de decisiones, como ocurre en el ámbito biomédico. Es en este dominio donde se ubica este sistema de recuperación de información dirigido a dispositivos de consulta móviles, que a los tradicionales procesos de indexado y búsqueda, añade la característica de la devolución de los resultados de manera agrupada en función de su contenido. Palabras clave: Recuperación de información, dispositivos móviles, agrupamiento de documentos, dominio biomédico. Abstract: Information overload caused by the increasing availability of online texts and publications of interest is a problem that increases when such information is necessary for decision making, as in the biomedical field. It is in this domain where we present an information retrieval system for mobile devices. Traditional indexing and search processes are enriched with the feature of returning the results in clusters according to their content. Keywords: Information retrieval, mobile devices, clustering, biomedical domain 1 Introduction In general, work at hospitals requires mobility and coordination. Hospitals staff might be distributed in space or time and their information needs are highly dependent on contextual conditions (Munoz et al, 2003). For this reason, many hospitals are encouraging the use of PDAs connected via wireless networks to the information systems of hospitals and allowing access to a wide variety of information sources (León et al., 2007). A recent study estimated PDA use by health professionals shows an evolution in the use ranging from 30% in 2000 to 60% in 2006 (Garrity and El Emam, 2006). In this context it is necessary to make available to physicians and other health professionals, systems that allow access to the information they need from anywhere in the hospital. In this way, doctors could make more informed decisions and based on evidence from the point of patient care, either in a doctor's office or beside their bed. The post-retrieval clustering is a technique that has been investigating for at least 15 years in order to improve the organization of the results returned by an Information Retrieval System (IRS) and facilitate navigation between them (e.g., Scatter/gather algorithm (Hearst and Pedersen, 1996), (Maña, Buenaga and Gómez, 2004), (Dunlavy et al., 2007). We propose to integrate the clustering of documents in a SRI in the biomedical domain. Clummed (CLUstering on Mobile MEdical Devices) is the result of this integration. The Procesamiento del Lenguaje Natural, Revista nº 45, septiembre 2010, pp 255-258 recibido 26-06-10 revisado 03-07-10 aceptado 06-07-10 ISSN 1135-5948 © 2010 Sociedad Española para el Procesamiento del Lenguaje Natural Fig. 1 System operation scheme system organizes search results into folders that store documents that are semantically related. The complexity of the information that the user must handle is reduced drastically. Simply reading the names of the folders, the user can choose the group of documents that is closest to their information needs. In a second step, users navigate among the documents of the group, a considerably smaller number of documents that the total recovered, to find those that are of interest. The paper is organized as follows. In the next section we present the related work presented here. Section 3 describes the proposed system. Finally, in Sections 4 are showed briefly the conclusions. 2 Related work The biomedical information retrieval systems developed for portable devices appeared relatively recently. Nevertheless, still few systems had taken advantage of accessing information across the Internet or other external real-time information resources. We can find stand-alone tools, capables of made available data already stored, like medicine's reference guides, clinic practice guides, protocols, algorithms, programs for diverse calculations and e-books. But the widely extended use is the wireless internet access to perform, besides the previous functions, searches of information on remote resources and communicate with remote clinic information systems, as medical history. (Hauser et al., 2007) publish a prospective study where results prove MEDLINE, accessed from mobile devices, is a viable information source for supporting clinical decision-making. Currently, the available resources accessible from the PDA at the bedside provided response to 86% of clinical questions, most of them (88.9% - 97.7%) during the rounds of visits. As already noted, there are several works in other domains that suggest that the grouping or clustering improves the organization of the results returned by an information retrieval system (IRS) but the use of clustering in the biomedical field is a novelty. In previous work we have presented some prototypes of IRS who made use of clustering of results, as in (Maña, Buenaga and Gómez, 2004) and (Buenaga et al., 2008) where each group is identified by their similarities and each individual document by differences. 3 System overview Our system follows a traditional pattern, dividing the process in the work of indexing and retrieval. In indexing, a preprocessor prepares and creates the structure that supports the documents, in our case a set of biomedical documents (retrieved from BioMed Central 2 , an online repository of biomedical papers mainly open access). With the support of Lucene (Gospodnetic, Hatcher and McCandless, 2009), a library of functions for indexing and searching of texts, written in Java, perform manipulations on the text of each document to obtain a representation of the document in terms or tokens that will form the set of index files. The retrieval process (Fig. 1) starts with the input of a search string in the user interface. The search engine will be able to understand and process the query to search in the collection of documents whose index contains some elements of the search string. A similarity algorithm scores the adequacy of each document to the search string and selects a group of documents. Among the innovations of our system are clustering post-retrieval and the adaptation of the system and its interface for access from mobile devices, showing the user the different groups of documents obtained, among the user navigate, selecting the group he most wants. The system will respond expanding it and listing from high to low relevance the documents included in the group. 3.1 Clustering Given a set of documents, clustering is the task of dividing the set into groups according to their semantic content. Depending on the structure of groups which produce, clustering 2 http://www.biomedcentral.com Manuel Millán, Alejandro Muñoz, Manuel de la Villa, Manuel J. Maña 256 methods are usually classified as non-hierarchical and hierarchical or partition-based (Rasmussen, 1992). Methods based on partitions simply divide the set of documents in a number of disjoint groups at the same level, identified by a centroid or center of gravity, representative of the semantic features of the documents it contains. The latter group includes the two clustering algorithms we have worked with, the K-Means (MacQueen, 1966) and the Expectation-Maximization (Dempster, Laird and Rubin, 1977), selected mainly for its availability in the Weka library (Witten and Frank, 2005) a collection of machine learning algorithms for data mining tasks, included in a Java API for easy integration. The K-Means algorithm is a simple and easy procedure to classify a set of objects in a number of K groups (clusters), determining K a priori. It represents each of the groups by the average (or weighted average) of its points, that is, by its centroid, which is located in the center of the elements of each group. Representation by centroids has the advantage of having a graphic and statistical significance immediately. The Expectation-Maximization algorithm (EM) is an iterative method of maximum likelihood estimation. To decide which of both algorithms to implement in our project we have focused on the time spent in its execution, since the results of the cluster were very similar between them. We measure the time it takes to stage grouping, using the files generated by our application after four queries. Simple K-Means algorithm proved being much faster than EM algorithm, between 3 and 6 times faster, depending on the size of the documents set. Based on these findings, we chose to continue working with the algorithm Simple-K-Means (with K=4), the best suited to our objectives. 3.2 Visualization on mobile devices Access to the Web from mobile devices has a large number of constraints, as could be: screen size, no multiple windows, navigation limited, no JavaScript and cookies, page types available, slow speed, unbalanced pages from automatic conversion to mobile format, size messages, cost of shipping by size, etc... The World Wide Web Consortium (W3C) is an international organization that works to develop web standards with general agreement. One of its working groups, the Mobile Web Initiative is a committee working on the creation of catalogs of best practices for creating mobile-friendly content, the easy access to device descriptions, the creation of test benches interoperability of mobile browsers Fig.2. Viewing a cluster in high resolution format A Biomedical Information Retrieval System based on Clustering for Mobile Devices 257 and exploring ways to use the Web on mobile devices to reduce the digital divide. (W3C, 2008) compiled a set of guidelines for designing and developing Web content for mobile devices. We've followed strictly the guide in the development of the tool presented in this work. As a result, we have implemented two user interfaces, one for devices with a larger screen (Fig. 2) (PC, notebook, Tablet PCs or UltraPCs) and one for smaller mobile devices (PDA's, Handhelds, Mobile, etc.). 4 Conclusions The clustering of documents provides a mechanism for effective and intuitive navigation through the organization of the recovery results. When the system returns a large number of documents, organizing them in a small number of groups is very useful, especially on mobile devices with reduced screens. In this paper we have presented a retrieval system for biomedical domain with post-retrieval clustering. Besides, we have presented two interfaces, one for desktop and one for mobile device. Future efforts will focus on using a biomedical ontology, like UMLS Metathesaurus, to improve the clustering quality and the labeling of the groups. Bibliography Buenaga, M. de, Gachet, D., Maña, M., de la Villa, M., Mata J.:Clustering and Summarizing Medical Documents to Improve Mobile Retrieval. Workshop on Mobile Information Retrieval (MobIR'08) Singapore (2008) Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39 (1): 1–38 (1977) Dunlavy, D.M., O'Leary, D.P., Conroy, J.M., Schlesinger, J.D. QCS: A system for querying, clustering and summarizing documents. Information Processing and Management 43(6), 1588--1605 (2007) Garritty, C., El Emam, K.: Who's using PDAs? Estimates of PDA use by health care providers: a systematic review of surveys. J. of Medical Internet Research 8(2), e7 (2006) Gospodnetic, O., Hatcher E., McCandless M.: Lucene in Action (2nd ed.). Manning Publications (2009) Hauser, S.E., Demner-Fushman, D. et al.: Using Wireless Handheld Computers to Seek Information at the Point of Care: An Evaluation by Clinicians. Journal of the American Medical Informatics Association; Nov-Dec; 14(6): 807-15 (2007) Hearst, M., Pedersen, P.: Reexamining the Cluster Hypothesis: Scatter/Gather on Retrieval Results. In: 19th Annual International ACM SIGIR Conference, pp. 76--84 (1996) León, S.A., Fontelo, P., Green, L., Ackerman, M., Liu, F.: Evidence-based medicine among internal medicine residents in a community hospital program using smart phones. BMC Medical Informatics and Decision Making 7(5) (2007) MacQueen, J. B.: Some Methods for classification and Analysis of Multivariate Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press. pp. 281–297 (1966) Maña, M.J., Buenaga, M., Gómez, J.M.: Multidocument summarization: An added value to clustering in interactive retrieval. ACM TOIS 22(2), 215--241 (2004) Muñoz, M.A., Rodríguez, M., Favela, J., Martínez-García, A.I., González, V.M.: Context-aware mobile communication in hospitals. IEEE Computer 36(8), 38-46 (2003) Rasmussen, E.: Clustering algorithms. En W. Frakes y R. Baeza-Yates, eds., Information Retrieval: Data Structures & Algorithms, pags. 419-442. Prentice-Hall International, London (1992) Witten, I. H. y Frank, E.: Data Mining: Practical Machine Learning Tools and Techniques (2nd ed.). Morgan Kaufmann, San Francisco. CA (2005) W3C: Mobile Web Best Practices 1.0, Basic Guidelines. W3C Recommendation 29 July 2008. http://www.w3.org/TR/2008/REC-mobile-bp-20080729/ (2008) Manuel Millán, Alejandro Muñoz, Manuel de la Villa, Manuel J. Maña 258 View Original Download Slide
 
Scatterplot. A scatterplot of manually extracted VAs (x-axis) versus TOVA extracted VAs (y-axis). Visual acuities are represented as logMAR values with CF = 2.0, HM = 2.4, LP = 2.7, and NLP = 3.0. Regression line of best fit shown.
Figure 4
 
Scatterplot. A scatterplot of manually extracted VAs (x-axis) versus TOVA extracted VAs (y-axis). Visual acuities are represented as logMAR values with CF = 2.0, HM = 2.4, LP = 2.7, and NLP = 3.0. Regression line of best fit shown.
View Original Download Slide
Discussion
Our study demonstrated that VA data extracted using TOVA correlates to manually extracted data with considerable accuracy. Less than one second was required to run TOVA on the corpus of 6266 notes to extract VA and laterality data while the manual extraction of a subset took several days. The total VA extraction algorithm is scalable for much larger datasets, such as the Veteran Affairs National Patient Care Database with more than 20 million free text eye clinic notes. 
Our algorithm differs from another recently developed by Mbagwu et al. 12 for extracting VA from EPIC EHR (Epic Systems Corporation, Madison, WI) notes. Their algorithm, written in structured query language, was designed to extract Snellen VAs from structured laterality fields created by the EPIC EHR. It performed keyword searches for text strings within the laterality field that were manually mapped to 1 of 18 defined VA categories (e.g., 20/20, 20/30, and so forth). To assign the best documented VA within a note, they implemented a ranking logic for the 18 categories. They found 5668 unique responses from 298,096 clinical notes, but validated only 100 of these notes using manual chart review and had a match rate of 99%. The total VA extraction algorithm is fundamentally different than the Mbagwu et al. 12 algorithm. Firstly, the use of natural language processing in our algorithm allows for extraction from free text, unlike the Mbagwu et al. 12 algorithm. Their algorithm, while relatively accurate, is designed around structured laterality fields. These fields tell their algorithm which eye the VA belongs to. These fields are not present in many ophthalmology notes and, thus, their algorithm only applies to notes that supply laterality information imbedded in the structure of the note. The total VA extraction algorithm, on the other hand, assigns laterality with the tokenized scoring system which is effective with a block of free text. Furthermore, since the data within the EPIC EHR laterality fields were free text and their algorithm did not implement natural language processing, they were required to manually map each response to a category, making it difficult to anticipate the full range of possible responses. This also highlights the fact that even in structured notes, VA often is recorded as free text. 
In a retrospective study within the Kaiser Permanente Northwest health care system, Smith et al. 13 extracted best corrected VA from 2074 free text notes using a computer program written in Python programming language. They validated their results by manual chart review of 100 notes, but no details about the algorithm logic or results of the validation were reported. Furthermore, their analysis excluded any patient note without VA detected by their algorithm and, therefore, was unable to account for VAs potentially missed. 
Natural language processing was used as part of a multimodal approach for extracting cataract cases from broader datasets. In a retrospective review of the Personalized Medicine Research Project (PMRP) cohort, Waudby et al. 14 identified 16,336 cataract patients by combining structured database querying of CPT and ICD-9 codes, natural language processing for data mining of text-based notes, and intelligent character recognition (ICR) of handwritten notes. The results of this combined search were validated by manual extraction of each note. They found a positive predictive value of 95.6% for the combined search when compared to manual extraction. Due to limitations in their automated search, manual extraction was necessary to retrieve data on VA, laterality, and type and severity of cataract. This illustrates the potential for combining a natural language processing algorithm with other tools for comprehensive automated retrospective review. 
Our study has several limitations. We analyzed notes at a single site and, therefore, may not have encountered all variations in VA documentation. However, to the best of our knowledge this is the largest set of notes validated by human extraction and encompasses many styles of note-writers. Multicenter validation of the algorithm is planned in a subsequent study. Our analysis included only inpatient consultation notes, which may be systematically different from outpatient clinic notes. The total VA extraction algorithm is designed to extract from free text notes, and some EHR systems may move toward more structured notes with increased use of drop-down menus or checkboxes. These notes provide more discrete VA data elements and an algorithm designed within that framework may be more accurate. However, EHR systems typically have the capability of exporting notes as free-text, no matter the method of generating the note, and, thus, our algorithm is widely generalizable. While manual extraction currently is the most common method of chart review and was used as the gold standard in our analysis, this method is known to result in transcription error. 3 Indeed, even in our study the interhuman concordance rate was on-par with the concordance of TOVA to final arbitrated data. The total VA extraction algorithm is designed to detect Snellen VAs with imperial measurements and would require modification to detect Snellen metric, logMAR, or other types of VA. Lastly, TOVA was not designed to categorize VAs by the method of measurement (e.g., pinhole aperture testing or unaided VA testing). This is a serious limitation in the current version of the algorithm. The functionality to link the method of measurement to the VA could be added as an extension of the current algorithm. For example, after the best corrected VA is determined, surrounding text then could be searched for the method of measurement and these data could be linked to the VA. Such an extension is planned in an updated version of TOVA. 
Despite these limitations, TOVA provides a validated tool for extraction of VA from free text clinical notes, such as those found in large datasets currently available for analysis. The majority of both structured and unstructured notes contain free text VAs making natural language processing a logical approach for extraction. The application of such algorithms has the potential to provide fast, accurate, large-scale data extraction from EHRs allowing more possibilities for future clinical studies. 
Acknowledgments
Supported by Grant NEI K23EY02492 (CSL), Research to Prevent Blindness (CSL, AYL). 
Disclosure: D.M. Baughman, None; G.L. Su, None; I. Tsui, None; C.S. Lee, None; A.Y. Lee, None 
References
1.
Charles        D,       Gabriel        M,       Furukawa        MF.            Adoption of electronic health record systems among US non-federal acute care hospitals: 2008-2012. ONC Data Brief.      2013      ;      9:      1–9.
2.
US Department of Veterans Affairs.      172VA10P2: VHA Corporate Data Warehouse–VA. 79 FR 4377. Updated December 8,      2016.      Accessed December 8, 2016.
3.
Knake        LA,       Ahuja        M,       McDonald        EL,       et al.            Quality of EHR data extractions for studies of preterm birth in a tertiary care center: guidelines for obtaining reliable data. BMC Pediatr.      2016      ;      16:59.
4.
Kimia        AA,       Savova        G,       Landschaft        A,       Harper        MB.            An introduction to natural language processing: how you can get more from those electronic notes you are generating. Pediatr Emerg Care.      2015      ;      31:      536–541.
5.
Zhang        W,       Skiena        S.            Trading strategies to exploit blog and news sentiment.      In: Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.      Menlo Park, CA: AAAI Press;      2010:      375–378. http://www.aaai.org/ocs/index.php/ICWSM/ICWSM10/paper/download/1529/1904/ . Accessed September 29, 2016.
6.
Li        H.            Learning to rank for information retrieval and natural language processing. Synth Lect Hum Lang Technol.      2014      ;      7:      1–121.
7.
Balahur        A,       Turchi        M.            Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis. Comput Speech Lang.      2014      ;      28:      56–75.
8.
Hughes        K,       Buckley        J,       Coopey        S,       et al.            The feasibility of using natural language processing to extract clinical information from breast pathology reports. J Pathol Inform.      2012      ;      3:      23.
9.
Feder        RS,       Olsen        TW,       Prum        BE,       et al.            Comprehensive adult medical eye evaluation preferred practice pattern guidelines. Ophthalmology.      2016      ;      123:      P209–P236.
10.
Vitale        S,       Cotch        MF,       Sperduto        RD.            Prevalence of visual impairment in the United States. JAMA.      2006      ;      295:      2158–2163.
11.
Lange        C,       Feltgen        N,       Junker        B,       Schulze-Bonsel        K,       Bach        M.            Resolving the clinical acuity categories "hand motion" and "counting fingers" using the Freiburg Visual Acuity Test (FrACT). Graefes Arch Clin Exp Ophthalmol.      2009      ;      247:      137–142.
12.
Mbagwu        M,       French        DD,       Gill        M,       et al.            Creation of an accurate algorithm to detect snellen best documented visual acuity from ophthalmology electronic health record notes. JMIR Med Inform.      2016      ;      4:      e14.
13.
Smith        DH,       Johnson        ES,       Russell        A,       et al.            Lower visual acuity predicts worse utility values among patients with type 2 diabetes. Qual Life Res.      2008      ;      17:      1277–1284.
14.
Waudby        CJ,       Berg        RL,       Linneman        JG,       et al.            Cataract research using electronic health records. BMC Ophthalmol.      2011      ;      11:      1.
Figure 1
View Original Download Slide
 
Total VA extraction algorithm logic. A diagram outlining the rule-based natural language processing algorithm, TOVA, for extracting VAs from clinical notes. Stepwise algorithm logic is given above with examples (a–d) of free text applications for each step shown below.
Figure 1
 
Total VA extraction algorithm logic. A diagram outlining the rule-based natural language processing algorithm, TOVA, for extracting VAs from clinical notes. Stepwise algorithm logic is given above with examples (a–d) of free text applications for each step shown below.
Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid molecular diagnostic technologies Expert opinion of the European Tuberculosis Laboratory Initiative core group members for the WHO European Region Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid molecular diagnostic technologies Expert opinion of the European Tuberculosis Laboratory Initiative core group members for the WHO European Region © World Health Organization 2017 All rights reserved. The Regional Office for Europe of the World Health Organization welcomes requests for permission to reproduce or translate its publications, in part or in full. The designations employed and the presentation of the material in this publication do not imply the expression of any opinion whatsoever on the part of the World Health Organization concerning the legal status of any country, territory, city or area or of its authorities, or concerning the delimitation of its frontiers or boundaries. Dotted lines on maps represent approximate border lines for which there may not yet be full agreement. The mention of specific companies or of certain manufacturers' products does not imply that they are endorsed or recommended by the World Health Organization in preference to others of a similar nature that are not mentioned. Errors and omissions excepted, the names of proprietary products are distinguished by initial capital letters. All reasonable precautions have been taken by the World Health Organization to verify the information contained in this publication. However, the published material is being distributed without warranty of any kind, either express or implied. The responsibility for the interpretation and use of the material lies with the reader. In no event shall the World Health Organization be liable for damages arising from its use. The views expressed by authors, editors, or expert groups do not necessarily represent the decisions or the stated policy of the World Health Organization. Keywords TUBERCULOSIS, PULMONARY – DIAGNOSIS TUBERCULOSIS, MULTIDRUG-RESISTANT – DIAGNOSIS CLINICAL LABORATORY TECHNIQUES DIAGNOSTIC TECHNIQUES AND PROCEDURES EUROPE ISBN 978 92 890 5237 5 Address requests about publications of the WHO Regional Office for Europe to: Publications WHO Regional Office for Europe UN City, Marmorvej 51 DK-2100 Copenhagen Ø, Denmark Alternatively, complete an online request form for documentation, health information, or for permission to quote or translate, on the Regional Office website (http://www.euro.who. int/pubrequest). Cover graphic: Venomous Vectors, Shutterstock Photo page vi: Paulista, Shutterstock Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Principles of laboratory diagnosis of TB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Prerequisites of a good laboratory network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Algorithms for laboratory diagnostic and follow up of TB cases . . . . . . . . . . . . . . . . . . . . . . 8 Algorithm for the initial laboratory diagnosis of individuals with symptoms consistent with pulmonary TB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Algorithm for monitoring the follow-up of patients with drug-sensitive pulmonary TB . . . . .11 Algorithm for the monitoring of follow-up of MDR-TB and RIF-resistant patients . . . . . . . . . .13 Practical considerations for the diagnostic algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15 Diagnostic methods of preference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16 Interpretation and reporting of laboratory results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .18 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20 Annex 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26 Contents Acknowledgements This technical document was developed as a collaborative product by the European Tuberculosis Laboratory Initiative (ELI) core group members. Document development was guided by Dr Masoud Dara (WHO Regional Office for Europe) and led by Dr Soudeh Ehsani (WHO Regional Office for Europe), with ELI core group members Professor Francis Drobniewski (Imperial College London, United Kingdom), Dr Irina Felker (Novosibirsk Tuberculosis Research Institute, WHO National Centre of Excellence, Russian Federation), Dr Sven Hoffner (the Public Health Agency of Sweden, Supranational Reference Laboratory Stockholm), Dr Gulmira I. Kalmambetova (National Reference Laboratory, Kyrgyzstan), Dr Hasmik Margaryan (Tuberculosis Laboratory Expert, Armenia), Mr Evgeni Sahalchyk (IML red GmbH, Supranational Reference Laboratory Gauting, Germany), Dr Elina V. Sevastyanova (Central Tuberculosis Research Institute, WHO National Centre of Excellence, Russian Federation), Dr Natalia Shubladze (Laboratory Consultant, National Centre of Tuberculosis and Lung Diseases and the National Reference Mycobacteriology Laboratory, Georgia), Ms Nukra Sinavbarova (National Public Health Laboratory, Tajikistan), Dr Alena Skrahina (Republican Research and Practical Centre for Pulmonology and Tuberculosis, Belarus), Dr Rasim Tahirli (WHO Collaborating Centre on Prevention and Control of Tuberculosis in the Penitentiary System, Azerbaijan) and former ELI core group secretariat Dr Kristin Kremer and former ELI core group member Dr Sabine Rüsch-Gerdes (Senior WHO consultant). Technical inputs and/or review were provided by Dr Christopher Gilpin (WHO headquarters), Mr Wayne Van Gemert (WHO headquarters) and regional Green Light Committee members Dr Gunta Dravniece (Senior Consultant, Access to Care Team, Technical Division, KNCV Tuberculosis Foundation) and Dr Elmira Gurbanova (WHO Collaborating Centre on Prevention and Control of Tuberculosis in the Penitentiary System, Azerbaijan). Development and publication of the document were enabled with financial support from the United States Agency for International Development. Acronyms CM common mycobacteria CRI colorimetric redox indicator CrI credible interval DR-TB drug-resistant tuberculosis DST drug-susceptibility testing ELI European Tuberculosis Laboratory Initiative ESTC European Union Standards for Tuberculosis Care FL first-line FLQ fluoroquinolone gDST genotypic DST INH isoniazid ISTC International Standards for Tuberculosis Care LAMP loop-mediated isothermal amplification LED light-emitting diode LJ Löwenstein-Jensen LPA line probe assay MDR-TB multidrug-resistant tuberculosis MGIT mycobacteria growth indicator tube MIC minimal inhibitory concentrations MODS microscopic-observation drug-susceptibility MTB Mycobacterium tuberculosis MUT mutant M/XDR-TB multidrug and extensively drug-resistant tuberculosis NRA nitrate reduction assay NRL National Reference Laboratory NTM non-tuberculous mycobacteria PCR polymerase chain reaction pDST phenotypic drug susceptibility testing PPV positive predictive value R resistant RIF rifampicin RRDR Rifampicin Resistance Determining Region S susceptible SL second-line SLID second-line injectable drug TB tuberculosis WT wild type XDR-TB extensively drug-resistant tuberculosis iv The European Tuberculosis Laboratory Initiative (ELI), with its secretariat at the WHO Regional Office for Europe, has developed this technical document to address the need in the WHO European Region for increasing timely and accurate detection of tuberculosis (TB) and multidrug-resistant TB (MDR-TB) through scaling-up the appropriate use of WHO-recommended rapid molecular diagnostic techniques. The document presents comprehensive algorithms for diagnosis and treatment-monitoring of pulmonary TB and MDR-TB using rapid molecular techniques recommended by WHO. With strong commitment of the Member States and continuous support from donors and partners, most techniques have already been introduced to the majority of countries of the Region, particularly in the high MDR-TB burden countries. However, to yield the maximum benefit of each technique, the appropriate and accurately timed sequence of different laboratory tests and correct interpretation and communication of results between laboratories and clinicians need to be ensured. For effective operation and efficient outcomes, sustainable financial and human resources need to be directed towards increasing testing capacities and optimizing sample transportation and data communication. This document aims to address these issues, taking the challenges and opportunities of the countries of the Region into account. ABSTRACT v Although the WHO European Region accounts for less than 5% of TB cases worldwide, about 25% of the worldwide burden of multidrug-resistant TB (MDR-TB) occurs in this Region (3). Of the 30 countries classified as high MDR-TB burden countries, nine are in the European Region (Azerbaijan, Belarus, Kazakhstan, Kyrgyzstan, the Republic of Moldova, the Russian Federation, Tajikistan, Ukraine and Uzbekistan). The Region includes 18 high TB priority countries, as defined in 2008 (3, 4), and 99% of the MDR-TB cases in the Region occur in these countries (5). Introduction Tuberculosis (TB) accounts for over 40% of all mortality cases from communicable diseases in the WHO European Region and is the most common cause of death among people living with HIV (1, 2) Of the 30 countries classified as high MDR-TB burden countries, nine are in the European Region (Azerbaijan, Belarus, Kazakhstan, Kyrgyzstan, the Republic of Moldova, the Russian Federation, Tajikistan, Ukraine and Uzbekistan). The Region includes 18 high TB priority countries, as defined in 2008, and 99% of the MDR-TB cases in the Region occur in these countries. Introduction 1 The Region includes high-, middle- and low-income countries with diverse national health system structures for TB control activities. Latest data from the Region on MDR-TB prevalence amount to 16% among new TB cases and 48% among previously treated cases (3, 6). Extensively drug-resistant TB (XDR-TB) is estimated to occur in 23.4% of all MDR-TB cases subjected to second-line drug-susceptibility testing (DST) (6). In 2015, treatment success rates were at 76% in new and relapse, 63% in previously treated (excluding relapses) and 51% in rifampicin-resistant/ MDR-TB cohorts (6). In 2015 with the development of an ambitious post-2015 global End TB Strategy, to continue the progress and address the challenges in TB and M/XDR-TB detection, prevention and care, the Regional Office developed the TB action plan for the WHO European Region covering the period 2016–2020 (5, 6, 8). Much work needs to be done to reach the targets set in the 2016–2020 action plan for the laboratory diagnosis of TB in the Region, particularly in the appropriate use of molecular diagnostics, and increasing the MDR-TB case-detection rate and coverage of quality-assured second-line DST among MDR-TB cases. Currently about 311 910 TB cases are registered in the 51 reporting countries of the Region (6). Bacteriological confirmation of TB diagnosis was reported for 61.4% of all new and relapse pulmonary cases in the Region and in four countries this was below 50% (5, 6). Only 57.9% of an estimated 74 000 MDR-TB cases were detected, against a regional target of diagnosing at least 85% (6, 8). Coverage of rifampicin (RIF)-resistance testing among laboratory-confirmed pulmonary TB cases amounted to 44% of new cases and 49% of previously treated cases in 2015. Coverage of second-line DST among laboratory confirmed drug-resistant TB cases was 52.2% (3). Although DST coverage has improved significantly, scale-up of testing and the use of WHO-recommended rapid molecular tests are urgently needed to reach the target of performing DST for close to 100% of all laboratory-confirmed cases by 2020. More directive guidance and advocacy is also needed to address gaps in the laboratory diagnosis of TB in the Region, especially in the nine high MDR-TB burden countries. In response to the need to strengthen TB laboratory capacity for accurate diagnosis and early detection of drug-resistant TB (DR-TB) in the Region and ensure implementation of the regional action plans (8, 9), the WHO Regional Office for Europe established the European Tuberculosis Laboratory Initiative (ELI) in 2012 (10). The mission of ELI is to strengthen TB laboratory capacity in the Region, with a focus on the 18 high TB priority countries. ELI members consist of national and Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 2 international TB laboratory experts in the Region and international partners dedicated to accelerating and expanding access to quality-assured TB diagnostic laboratory services. ELI has a core group of members who function as an independent, technical advisory and support group for WHO and partners (10). ELI has been working to develop a diagnostic algorithm, taking into consideration the substantial heterogeneity of the Region. The algorithm combines different types of tests synergistically by targeting patient groups appropriately and will help to make the best use of available resources by maximizing their respective strengths and mitigating their weaknesses (11, 12). The draft algorithm developed by the previous core group members (2012–2014) was revised during and after the last ELI core group meeting, held in Copenhagen, Denmark on 25 February 2016 (13). The algorithm was finalized and agreed by members during the ELI core group meeting in Tbilisi, Georgia on 30 November and 2 December 2016 and is presented here. The recommendations presented here take into account and supplement previously published recommended standards for modern TB laboratory services, including the WHO global policy framework on implementing TB diagnostics and the European Union Standards for Tuberculosis Care (ESTC) and International Standards for Tuberculosis Care (ISTC) (11, 14, 15). The ESTC adapted the ISTC (14) to reflect the European Union setting and practices (11). The ESTC builds on previous recommendations on laboratory methods for diagnosing TB (16) to propose that: In countries, settings or populations in which MDR-TB is suspected in a patient, rapid testing for the identification of rifampicin- and isoniazid-resistance, using validated tools in a quality-assured laboratory, should be performed. To reassure rapid diagnosis of TB in the entire Region, ELI suggests further extension of this standard by using rapid molecular diagnosis as an initial method for all cases with clinical suspicion of TB, to be applied in all countries of the Region. With high MDR-TB rates being present in Eastern Europe, every presumptive TB case could also be an MDR-TB case. Only 57.9% of an estimated 74 000 MDR-TB cases were detected, against a regional target of diagnosing at least 85% Introduction 3 Specific laboratory policies and diagnostic algorithms should be available at country level based on the local epidemiological situation; limited resources initially will mean that not all the required improvements can be implemented immediately, but resource limitation should not influence the recommendation, only the rate at which the recommendation is implemented. WHO-endorsed rapid diagnostic tests should be key to the diagnostic work-up for all TB presumptive cases (17). Countries shall prioritize the use of recommended rapid molecular tests, rather than conventional microscopy, culture or DST, as the initial diagnostic test for adults and children presumed to have pulmonary TB/ MDR-TB and/or HIV-associated TB and/or TB meningitis (17). This will ensure the availability of early and accurate diagnosis. Conventional microscopy should be used as an initial diagnostic test only in laboratory settings without rapid molecular tests and in the absence of systems for timely sample transportation to a setting in which these techniques are available. In settings with high risk of transmission of TB and/or MDR-TB, such as prisons in countries of the former Soviet Union, implementation of sputum polymerase chain reaction (PCR)-based screening (GeneXpert MTB/RIF assay) as an annual screening tool has been shown to most cost-effectively reduce TB and MDR-TB when compared to more traditional interventions (18). Principles of laboratory diagnosis of TB Laboratory diagnosis of TB should start with appropriate clinical screening procedures to identify individuals with clinical suspicion of TB Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 4 In the event of discrepancies between the results of conventional and molecular assays, it is recommended that the respective tests be repeated on the same or another sample from the same patient to exclude technical errors. If the discrepancy between microscopy and molecular tests is confirmed, the laboratory shall report the molecular test result to the clinician instead of the microscopy result due to the higher sensitivity and specificity of molecular tests compared to microscopy. If the culture becomes positive after initial molecular and microscopy tests were negative, this positive culture result should be reported as well (as this reflects the proportion of samples for which culture remains the most sensitive test). The proposed table in Annex 1 has been developed to help the interpretation and explanation of all potential discordant results that may occur, although most of these cases will be observed very rarely. The table will also assist laboratories to explain discrepant or conflicting results to clinicians. WHO-endorsed rapid diagnostic tests should be key to the diagnostic work-up for all TB presumptive cases Principles of laboratory diagnosis of TB 5 Laboratories should have appropriate infrastructure, biosafety measures, equipment, and access to regular maintenance of equipment and infrastructure. Laboratory commodities and supplies should be managed well and laboratory data properly recorded, preferably in an electronic format. Ideally, the laboratory would have a clear and separate budget line within the programme/hospital budget. Specimen transportation and referral mechanisms within a laboratory network should be well described, with clear transportation arrangements in place. Patient specimens kept in appropriate secure containers can be transported using most health-care vehicles to increase transport frequency and reduce delay; the quality of the specimens arriving should be monitored (15, 20, 21). A laboratory quality management system in line with WHO recommendations should be in place. The laboratory work-up of a clinical sample should be defined by the patient's category (new, relapse or previously treated case), the purpose of the analysis (for diagnosis or treatment success) and the patient's risk assessment (HIV, or risk of MDR-TB, for instance). This information is to be recorded on the laboratory request form together with the laboratory tests to ensure that the most appropriate, efficient and cost-effective laboratory work-up is achieved, and to facilitate result Prerequisites of a good laboratory network Countries should have sufficient funding and appropriately trained human resources available for laboratories, as well as a laboratory maintenance and development plan (19) Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 6 interpretation – in the case of discordant results – and communication between clinical doctors and laboratory staff. Frequent repetitions of DST are usually unnecessary and often not helpful. Once a patient has been shown to have MDR-TB, for example, subsequent repetition of isoniazid (INH) and RIF testing will be unhelpful. An updated manual detailing recommendations for biosafety was published in 2012 by WHO (22). The current recommendations are based on assessments of the risks associated with different technical procedures performed in different types of TB laboratories. It is recommended that culture and DST be used only in laboratories at regional and central reference level that have appropriate biosafety standards (22). Similarly, line probe assays (LPAs) should be placed at regional and central reference level laboratories (or any laboratory currently performing not automated PCR-based amplification methodologies for infections, as the infrastructure will be the same) because of the complexity of the assay and required infrastructure (23). Many countries in the Region follow the principle of patient (instead of specimen) referral to confirm the diagnosis of TB at specialized TB centres or hospitals. By using WHO recommended rapid molecular techniques for the initial diagnosis of TB and MDR-TB at district or subdistrict level (24, 25), a highly specific diagnosis of TB can be achieved at a lower level in the health system, which can help countries to reduce diagnostic delay, apply the most appropriate infection control measures and simultaneously move towards ambulatory treatment of TB cases (9). Similarly, line probe assays (LPAs) should be placed at regional and central reference level laboratories, because of the complexity of the assay and required infrastructure Prerequisites of a good laboratory network 7 Where resources and/or the availability of molecular diagnostics or other tests are limited, different algorithms could be used for different patient groups based on careful risk assessment and prioritization of molecular diagnostic tests and liquid culture for those patients presumed to have pulmonary MDR-TB and/or HIV-associated TB and/or TB meningitis. Algorithm for the initial laboratory diagnosis of individuals with symptoms consistent with pulmonary TB This starts by subjecting two clinical specimens (preferably including the morning sample) to a WHO-endorsed molecular diagnostic test and one specimen for culture (Fig. 1). Algorithms for laboratory diagnostic and follow up of TB and MDR-TB cases Three algorithms are proposed in this document: 1. for the initial diagnostics of all presumptive TB cases 2. for follow-up of patients under first-line anti-TB treatment 3. for follow-up of MDR-TB patients (see Fig. 1–3). Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 8 Only when a positive rapid molecular test result is obtained would a subsequent microscopy test be helpful to identify the bacterial load and infectivity. Results of molecular tests should be communicated to the clinician without waiting for culture results. If GeneXpert MTB/RIF is negative, no microscopy is necessary and the sample should be sent for culture only (as culture would still be more sensitive for TB detection). Culture-positive samples are subjected to identification, and those cultures identified as M. tuberculosis (MTB) complex should be subjected to DST. If the sample that was subjected to the molecular test is culture negative (as well as negative on the molecular test), further clinical and other non-TB specific laboratory investigations should follow. If the molecular diagnostic test predicts susceptibility to RIF and if first-line (FL) LPA is available, it should be performed to identify INH mono-resistant cases. If INH resistance is identified, INH mono-resistant regimen should be initiated. In case FL-LPA is not available or no INH resistance is identified, the FL regimen needs to be initiated. In both cases, culture and subsequent phenotypic drug-susceptibility testing (pDST) should be done. Performing a WHO-recommended rapid molecular test for drug resistance (such as Xpert or LPA) on the primary samples reduces the delay to appropriate phenotypic resistance results. If RIF resistance (with or without INH resistance) is confirmed, it is recommended that the patient be referred to a DR-TB treatment initiation site for appropriate therapy and to proceed with second-line (SL) LPA (26): this recommendation applies to the direct testing of sputum specimens from RIF-resistant TB or MDR-TB irrespective of the smear status, while acknowledging that the indeterminate rate is higher when testing smear-negative sputum specimens compared with smear-positive sputum specimens (26). SL-LPA is suitable for use at central or national reference laboratory level; it has the potential to be used at regional level if the appropriate infrastructure and trained staff can be ensured (26). Once the culture becomes positive, first- and second-line pDST should be set up, irrespective of SL-LPA results, for patients with negative and positive SL-LPA results. This streamlining reduces any delay for initiating an appropriate M/XDR-TB treatment regimen. Depending on the LPA results (FL and SL), treatment should start accordingly and be adjusted if needed once pDST results are available. SL-LPA results that lead to the exclusion of both fluoroquinolones (FLQs) and second-line injectable drug (SLID) resistance means that the use of a shorter MDR-TB regimen could be considered, providing the other criteria are met (27). Algorithms for laboratory diagnostic and follow up of TB and MDR-TB cases 9 Fig. 1. Algorithm for the initial laboratory diagnosis of individuals with symptoms consistent with pulmonary TB WHO-endorsed rapid molecular TB diagnostic techniques Conventional diagnostic assay (microscopy or culture-based phenotypic techniques) Result of the diagnostics Treatment Culture MTB detected Solid or liquid pDST Individualized regimen MTB not detected FL regimen Microscopy (bacterial load, infectivity) 7 RIF resistant 2,8 Not interpretable results No result Culture 8 Interpretable results FLQ (S) SLID (S) FLQ (S) SLID (R) FLQ (R) SLID (S) FLQ (R) SLID (R) 9 FL-LPA 5,6 Regimen for INH-mono-resistance INH susceptible INH resistant Xpert MTB/RIF 1,2,3 4 No result/error/ Patient has TB SL-LPA (direct testing) invalid MTB detected RIF indeterminate repeat repeat MTB not detected Re-evaluate the patient clinically, conduct additional testing (e.g., X-ray) in accordance with national guidelines; consider repeating Xpert MTB/RIF testing; use clinical judgment for treatment decisions; alternative diagnosis Refer patient to DR-TB treatment initiation site for appropriate therapy MTB detected RIF susceptible/resistant RIF susceptible SL-LPA (indirect testing)Individua-lized MDR-TB regimen (WHO-endorsed molecular diagnostic test) Shorter MDR-TB regimen Presumptive pulmonary TB cases, sputum specimen Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 10 Algorithm for monitoring the follow-up of patients with drug-sensitive pulmonary TB This algorithm is shown in Fig. 2. Monthly microscopy and culture during the intensive phase of two sputum samples and immediately after completion of the intensive phase (at month two for new cases and month three for previously treated cases) is suggested, as well as during the fifth and last month of treatment, according to the timeframe of the treatment protocol in a particular country. If microscopy and/or culture is/are positive from a sample taken after two months' treatment, a WHO-endorsed molecular diagnostic test and DST are suggested to confirm the presence/absence of MTB and the anti-TB drug-resistance pattern. If the WHO-endorsed molecular tests (X-pert, FL-LPA) indicate resistance to RIF with or without INH, then SL-LPA should be performed. SL-LPA results that lead to the exclusion of both FLQs and SLID resistance means that the use of a shorter MDR-TB regimen could be considered providing the other criteria are met (27). Once culture results are available and RIF resistance is known (through WHO-endorsed molecular tests), first- and second-line pDST should be performed simultaneously. Results of molecular tests should be communicated to the clinician without waiting for culture results1 Performing a rapid molecular test for drug resistance (e.g. Xpert or LPA) on the primary samples reduces the delay to appropriate phenotypic resistance results2 In the absence of Xpert MTB/RIF, sputum smear microscopy can be used as the initial test and the sample should be sent as quickly as possible to a laboratory with the capacity to perform WHO recommended molecular tests in addition to culture 3 In case Xpert MTB/RIF is absent but FL LPA is available, this test should be used for smear-positive sputum samples, to detect rifampicin and in addition INH resistance. If INH-R is detected this information should guide further diagnostic work and be considered in clinical treatment decisions 4 To be performed when FL-LPA is available and has not been done already5 In the absence of FL-LPA, FL regimen is suggested to be initiated and adjusted once additional DST results are available6 In cases SL-LPA results would not become available within one week, empirical MDR-TB treatment may be initiated7 SL-LPA are suitable for use at the central or national reference laboratory level; or at regional level with appropriate infrastructure8 Following eligibility criteria9 Footnotes for Fig. 1 Algorithms for laboratory diagnostic and follow up of TB and MDR-TB cases 11 Fig. 2. Algorithm for monitoring the follow-up of patients with drug-sensitive pulmonary TB Two sputum specimens Microscopy (bacterial load, infectivity) Culture Individualized regimen MTB not detected First-line and second-line pDST WHO-endorsed molecular test4 3 No resistance to RIF or INH Resistance to RIF with/without INH Not interpretable results No result Culture SL-LPA (indirect testing)2 Interpretable results FLQ (S) SLID (S) FLQ (S) SLID (R) FLQ (R) SLID (S) FLQ (R) SLID (R) Individua-lized MDR-TB regimen Shorter MDR-TB regimen5 1,2 WHO-endorsed rapid molecular TB diagnostic techniques Conventional diagnostic assay (microscopy or culture-based phenotypic techniques) Result of the diagnostics Treatment Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 12 The purpose of diagnostic testing here is to determine the success of treatment; repeating DST is usually not required unless the patient continues to remain smear and culture positive or there is evidence of concurrent MDR-TB exposure (that is, potential superinfection) or clinical deterioration. The laboratory needs to be told that the sample has been sent only to assess treatment success. Algorithm for the monitoring of follow-up of MDR-TB and RIF-resistant patients This algorithm is shown in Fig. 3. Culture and microscopy of 1–2 sputum samples should be performed every month (beginning from the third month) for the complete intensive phase of treatment (28-30). After culture conversion (that is, after two consecutive negative cultures taken at least 30 days apart), culture can be performed every third month and smear microscopy every month (28), or following the timeframe of the treatment protocol in a particular country. If microscopy becomes positive and once the culture becomes positive, SL-LPA and pDST to additional anti-TB drugs should be performed (there is no need to repeat the initial RIF-resistance tests, as these will remain resistant). Performing a rapid molecular test for drug resistance (e.g. Xpert or LPA) on the primary samples reduces the delay to appropriate phenotypic resistance results If microscopy and/or culture is positive from a sample taken after two months treatment, a WHO-endorsed molecular diagnostic test and DST are suggested to be performed to confirm the presence/ absence of MTB and to determine the resistance pattern SL-LPA are suitable for use at the central or national reference laboratory level; they have the potential to be used at the regional level with appropriate infrastructure Before initiating the WHO-endorsed test, the results of the molecular tests conducted on previous isolates of the respective patient should be checked in order to ensure that the molecular test of choice can detect additional resistance Following eligibility criteria Footnotes for Fig. 2 1 2 3 4 5 Algorithms for laboratory diagnostic and follow up of TB and MDR-TB cases 13 Fig. 3. Algorithm for the monitoring of follow-up of MDR-TB and RIF-resistant patients To be performed only if no resistance to second line drugs (FLQ and SLID) was detected previously There is no need to repeat the initial rifampicin resistance tests as these will remain resistant Footnotes for Fig. 3 Two sputum specimens Culture MTB not detected MTB detectedAFB negative AFB positive Individualized regimen 2 SL-LPA1 Microscopy (bacterial load, infectivity) pDST to additional anti-TB drugs WHO-endorsed rapid molecular TB diagnostic techniques Conventional diagnostic assay (microscopy or culture-based phenotypic techniques) Result of the diagnostics Treatment 1 2 Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 14 The results of all tests performed at lower levels of laboratories should be reported and referred together with the samples and/or cultured isolates to the National Reference Laboratory (NRL). This will prevent wasteful duplication of tests (except in a proportion used for quality control purposes). A handbook with the detailed methodologies needed for assay performance has been published by the European Centre for Disease Prevention and Control European Reference Laboratory Network (31). WHO recommends replacing conventional microscopy with rapid molecular tests (such as GeneXpert MTB/RIF) for the initial diagnosis of TB (3, 17, 24, 25, 32). If resources permit, two specimens should be tested to increase the sensitivity. Microscopy would be limited to test TB positive samples detected by rapid molecular techniques to ascertain patient infectivity and infection control purposes and for treatment monitoring. Continuing with microscopy also maintains quality-assured skills in performing microscopy if molecular tests are not available. Practical considerations for the diagnostic algorithm Various tests are performed at different levels of laboratories and within a laboratory network. In some countries, laboratories that perform PCR-based diagnostics for infections other than TB will have the skills and infrastructure necessary for LPAs, as TB diagnosis in many countries is incorporated with other clinical microbiology tests for other diseases Practical considerations for the diagnostic algorithm 15 Diagnostic methods of preference Capacity for microscopy, culture and DST needs to remain, despite molecular diagnostics. Microscopy and culture are particularly important for treatment monitoring. The availability of molecular diagnostic tests does not eliminate the need for conventional microscopy, culture and DST capability; microscopy and culture remain necessary for the follow-up of treatment, and culture currently still provides maximum diagnostic sensitivity, while conventional DST is required to support a diagnosis of XDR-TB and provide a tailored patient-regime for M/XDR-TB patients. Demands for conventional techniques might change in the future based on the epidemiological situation. Light-emitting diode (LED) fluorescence microscopy is the recommended method for microscopy at all levels of laboratory (33). Both LED microscopy and conventional fluorescence microscopy are at least 10% more sensitive than Ziehl-Neelsen microscopy. Moreover, LED microscopy is less costly compared to conventional fluorescence microscopy (33). For culture, both solid and liquid media are recommended. Liquid culture is more sensitive and quicker than culture on solid media (increased yield 10%) (34). Liquid culture results may become available within days but are more prone to contamination, more expensive and associated with a higher biosafety risk. Positive cultures should be differentiated into MTB complex and non-tuberculous mycobacteria. Confirmation of MTB complex is done through biochemical reaction, molecular amplification tests or immunochromatographic assays. The latter two are recommended for species identification on culture isolates, as they provide a rapid and definite identification of MTB complex (34). Drug resistance can be detected by genotypic and phenotypic methods. Automated liquid systems are the current gold standard for FL and SL DST (34). DST should follow WHO guidelines with stringent quality assurance methods (29, 34, 35). SL DST should aim to include testing of the aminoglycosides, polypeptides and FLQs used in the country. DST results on these drugs have good reliability and reproducibility and allow a quality-assured diagnosis of XDR-TB. With the introduction of SL-LPA for detecting resistance to FLQs and SLIDs, resistant results to these drugs can be obtained more rapidly. By direct testing, SL-LPA will detect 86% of patients with FLQ resistance, 87% with SLID resistance and 69% of XDR-TB; in all cases, the test will rarely give a false Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 16 positive result (26). In interpreting the results, it should be considered that SL-LPA cannot determine resistance to individual FLQs. Resistance-conferring mutations detected by SL-LPA are highly correlated with phenotypic resistance to ofloxacin and levofloxacin. The correlation of these mutations with phenotypic resistance to moxifloxacin and gatifloxacin is unclear, however, and the inclusion of moxifloxacin or gatifloxacin in an MDR-TB regimen is best guided by pDST results (26). Mutations in some regions of the MTB complex genome (such as the eis promoter region) may be responsible for causing resistance to one drug in a class (group) more than other drugs within that class (group). For example, the eis C14T mutation is associated with kanamycin resistance in strains from eastern Europe (26). Non-commercial methods for culture and DST, including microscopic-observation drug-susceptibility (MODS), colorimetric redox indicator (CRI) methods and the nitrate reduction assay (NRA), are also recommended by WHO (36). These tests are currently considered as an interim solution while scale-up of genotypic testing is developed. The systems are less expensive than commercial systems, but may be more prone to errors due to lack of standardization, are highly operator-dependent and only suitable for use at reference laboratory level. Currently, the WHO-recommended molecular diagnostic tests for TB and DR-TB include LPAs and the Xpert MTB/RIF assay (as well as loop-mediated isothermal amplification (LAMP) for TB diagnosis only). Data from systematic reviews and meta-analyses show that in comparison to conventional DST, LPAs are highly sensitive (≥97%) and specific (≥99%) for the detection of RIF resistance, alone or in combination with INH (sensitivity ≥90%; specificity ≥99%), on isolates of MTB and on smear-positive sputum specimens (23, 37, 38). An extensive review (17, 32) of the Xpert MTB/RIF assay to detect pulmonary TB disease, including studies involving almost 10 000 participants, showed the high specificity of the Xpert MTB/RIF assay to detect TB (99% (95% CrI 98–99%)). Sensitivity varied by smear status: 68% (95% CrI 61–74%) for smear-negative By direct testing, SL-LPA will detect 86% of patients with FLQ resistance, 87% with SLID resistance and 69% of XDR-TB; in all cases, the test will rarely give a false positive result Practical considerations for the diagnostic algorithm 17 culture-positive to 98% (95% CrI 97–99%) for smear-positive culture-positive samples. Performance of the Xpert MTB/RIF assay for detection of extrapulmonary TB varied by specimen type, with lower sensitivity for pleural and cerebrospinal fluid (ranging from 17% to 80%, respectively) and somewhat lower specificity (93%) for lymph node specimens, but with good sensitivity (>81%) and specificity (>98%) for other specimen types (25). Overall, the Xpert MTB/RIF assay had 95% (95% CrI 90–97%) sensitivity and 98% (95% CrI 97–99%) specificity to detect RIF. When studies were analysed separately for settings with different levels of RIF resistance, however, the sensitivity was 96% (95% CrI 91–98%) for settings with >15% RIF resistance among the tested population, and 91% (95% CrI 79–97%) for settings with ≤ 15% RIF resistance. The corresponding pooled specificities were 97% (95% CrI 94–99%) and 99% (95% CrI 98–99%) (17, 32). Similar data have been published in a systematic review with a detailed health economic analysis (39). The specificity of Xpert MTB/RIF for detecting TB is very high (99%), and false-positive results are likely to be linked to the detection by Xpert MTB/RIF of dead MTB bacilli that would not be detected by culture, which is the present reference standard. Given that the specificity of Xpert MTB/RIF is not 100%, the positive predictive value (PPV) of Xpert MTB/RIF testing for RIF resistance testing is adversely affected in settings with a low prevalence of drug-resistant TB disease or in populations with a low prevalence of TB. Testing for TB is not usually implemented in a general, asymptomatic population but in individuals with clinical suspicion of TB following some form of screening involving, for example, symptom assessment or chest X-ray. Such screening procedures increase the prevalence of TB in the group tested and improve the PPV of the test, reducing but not eliminating concerns related to false-positive results (32). Interpretation and reporting of laboratory results Results from laboratory tests, including microscopy, molecular tests, culture and DST, should be reported to clinicians as soon as possible after they become available, and all means of communication, including telephone, fax, email and SMS, should be considered to facilitate communication. Interpretation of laboratory results by laboratory doctors or equivalent is vital, especially in situations when results are apparently inconsistent or discrepant. Subsequently, appropriate Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 18 isolation of the respective patients and adjustment of their treatment courses according to the anti-TB drug susceptibility pattern of the causative pathogens should be initiated promptly (29) to prevent further spread of the disease and/or the development of further resistance. It should be noted that mutations in the rpoB gene are a very good marker for MDR-TB in the Region; the percentage of RIF mono-resistant isolates has been shown to be only 0.5% among new cases and 0.9% among previously treated cases (5). Under these circumstances, it seems reasonable to initiate patients on MDR treatment if the Xpert MTB/RIF or LPA results indicate RIF resistance. More caution is necessary when interpreting the results of molecular tests for SL anti-TB drugs. Results from laboratory tests, including microscopy, molecular tests, culture and DST, should be reported to clinicians as soon as possible after they become available Practical considerations for the diagnostic algorithm 19 1. The European health report 2012: charting the way to well-being. Copenhagen: WHO Regional Office for Europe; 2012 (http://www.euro.who.int/__data/ assets/pdf_file/0004/197113/EHR2012-Eng.pdf, accessed 15 February 2017). 2. Roadmap to implement the tuberculosis action plan for the WHO European Region 2016–2020. WHO Regional Office for Europe; 2016 (http://www.euro. who.int/__data/assets/pdf_file/0020/318233/Roadmap-implement-TBC-action-plan-20162020.pdf, accessed 15 February 2017). 3. Global Tuberculosis Report 2016. Geneva: World Health Organization; 2016 (WHO/HTM/TB/2016.13; http://apps.who.int/iris/bitstr eam/10665/250441/1/9789241565394-eng.pdf, accessed 15 February 2017). 4. Plan to Stop TB in 18 priority countries of the WHO European Region, 2007– 2015. Copenhagen: WHO Regional Office for Europe; 2007 (http://www.euro. who.int/document/E91049.pdf, accessed 15 February 2017). 5. Tuberculosis surveillance and monitoring in Europe 2016. Stockholm: European Centre for Disease Prevention and Control/WHO Regional Office for Europe; 2016 (http://ecdc.europa.eu/en/publications/Publications/ecdc-tuberculosis-surveillance-monitoring-Europe-2016.pdf, accessed 15 February 2017). 6. Tuberculosis serveillance and monitoring in Europe 2017. Stockholm: European Centre for Disease Prevention and Control/WHO Regional Office for Europe; 2017. References Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 20 http://www.euro.who.int/__data/assets/pdf_file/0004/197113/EHR2012-Eng.pdf http://www.euro.who.int/__data/assets/pdf_file/0004/197113/EHR2012-Eng.pdf http://www.euro.who.int/__data/assets/pdf_file/0020/318233/Roadmap-implement-TBC-action-plan-20162020.pdf http://www.euro.who.int/__data/assets/pdf_file/0020/318233/Roadmap-implement-TBC-action-plan-20162020.pdf http://www.euro.who.int/__data/assets/pdf_file/0020/318233/Roadmap-implement-TBC-action-plan-20162020.pdf http://apps.who.int/iris/bitstream/10665/250441/1/9789241565394-eng.pdf http://apps.who.int/iris/bitstream/10665/250441/1/9789241565394-eng.pdf http://www.euro.who.int/document/E91049.pdf http://www.euro.who.int/document/E91049.pdf http://ecdc.europa.eu/en/publications/Publications/ecdc-tuberculosis-surveillance-monitoring-Europe-2016.pdf http://ecdc.europa.eu/en/publications/Publications/ecdc-tuberculosis-surveillance-monitoring-Europe-2016.pdf 7. Tuberculosis surveillance and monitoring in Europe 2013. Stockholm: European Centre for Disease Prevention and Control/WHO Regional Office for Europe; 2013 (http://ecdc.europa.eu/en/publications/Publications/ Tuberculosis-surveillance-monitoring-2013.pdf, accessed 15 February 2017). 8. Tuberculosis action plan for the WHO European Region 2016–2020. Copenhagen: WHO Regional Office for Europe; 2015 (http://www. euro.who.int/__data/assets/pdf_file/0007/283804/65wd17e_Rev1_ TBActionPlan_150588_withCover.pdf, accessed 15 February 2017). 9. Roadmap to prevent and combat drug-resistant tuberculosis. The Consolidated Action Plan to Prevent and Combat Multidrug- and Extensively Drug-Resistant Tuberculosis in the WHO European Region, 2011–2015. Copenhagen: WHO Regional Office for Europe; 2011 (http://www.euro.who. int/__data/assets/pdf_file/0014/152015/e95786.pdf, accessed 15 February 2017). 10. European Tuberculosis Laboratory Initiative. In: Health topics [website]. Copenhagen: WHO Regional Office for Europe; 2017 (Report Number; http:// www.euro.who.int/en/health-topics/communicable-diseases/tuberculosis/ activities/european-tuberculosis-laboratory-initiative, accessed 15 February). 11. Migliori GB, Zellweger JP, Abubakar I, Ibraim E, Caminero JA, De Vries G, et al. European Union standards for tuberculosis care. Eur Respir J 2012;39(4):807-19; doi: 10.1183/09031936.00203811. 12. Ehsani S, van den Boom M, Gilpin C, Dara M, Europe WHOROf. The role of novel molecular techniques for tuberculosis diagnostics in the WHO European Region. J Public Health (Oxf) 2016; doi: 10.1093/pubmed/fdv200. 13. Meeting of the European Tuberculosis Laboratory Initiative (ELI) Core Group. Copenhagen: WHO Regional Office for Europe; 2016 (http://www.euro. who.int/__data/assets/pdf_file/0020/307433/ELI-TB-Core-Group-mtg.pdf, accessed 15 February 2017). 14. International Standards for Tuberculosis Care, third edition. The Hague: TB CARE I; 2014 (http://www.who.int/tb/publications/ISTC_3rdEd.pdf, accessed 15 February 2017). References 21 http://ecdc.europa.eu/en/publications/Publications/Tuberculosis-surveillance-monitoring-2013.pdf http://ecdc.europa.eu/en/publications/Publications/Tuberculosis-surveillance-monitoring-2013.pdf http://www.euro.who.int/__data/assets/pdf_file/0007/283804/65wd17e_Rev1_TBActionPlan_150588_withCover.pdf http://www.euro.who.int/__data/assets/pdf_file/0007/283804/65wd17e_Rev1_TBActionPlan_150588_withCover.pdf http://www.euro.who.int/__data/assets/pdf_file/0007/283804/65wd17e_Rev1_TBActionPlan_150588_withCover.pdf http://www.euro.who.int/__data/assets/pdf_file/0014/152015/e95786.pdf http://www.euro.who.int/__data/assets/pdf_file/0014/152015/e95786.pdf http://www.euro.who.int/en/health-topics/communicable-diseases/tuberculosis/activities/european-tuberculosis-laboratory-initiative http://www.euro.who.int/en/health-topics/communicable-diseases/tuberculosis/activities/european-tuberculosis-laboratory-initiative http://www.euro.who.int/en/health-topics/communicable-diseases/tuberculosis/activities/european-tuberculosis-laboratory-initiative http://www.euro.who.int/__data/assets/pdf_file/0020/307433/ELI-TB-Core-Group-mtg.pdf http://www.euro.who.int/__data/assets/pdf_file/0020/307433/ELI-TB-Core-Group-mtg.pdf http://www.who.int/tb/publications/ISTC_3rdEd.pdf 15. Implementing tuberculosis diagnostics: policy framework. Geneva: World Health Organization; 2015 (WHO/HTM/TB/2015.11; http://apps.who.int/iris/ bitstream/10665/162712/1/9789241508612_eng.pdf, accessed 15 February 2017). 16. Drobniewski FA, Hoffner S, Rusch-Gerdes S, Skenders G, Thomsen V, Force WHOELST. Recommended standards for modern tuberculosis laboratory services in Europe. Eur Respir J 2006;28(5):903-9; doi: 10.1183/09031936.06.00084906. 17. Automated real-time nucleic acid amplification technology for rapid and simultaneous detection of tuberculosis and rifampicin resistance: Xpert MTB/ RIF assay for the diagnosis of pulmonary and extrapulmonary TB in adults and children. Policy update. Geneva: World Health Organization; 2013 (WHO/ HTM/TB/2013.16; http://www.stoptb.org/wg/gli/assets/documents/WHO Policy Statement on Xpert MTB-RIF 2013 pre publication 22102013.pdf, accessed 15 February 2017). 18. Winetsky DE, Negoescu DM, DeMarchis EH, Almukhamedova O, Dooronbekova A, Pulatov D, et al. Screening and rapid molecular diagnosis of tuberculosis in prisons in Russia and Eastern Europe: a cost-effectiveness analysis. PLoS Med 2012;9(11):e1001348; doi: 10.1371/journal.pmed.1001348. 19. The European health report 2015: Targets and beyond – reaching new frontiers in evidence. Copenhagen: WHO Regional Office for Europe; 2015 (http://www. euro.who.int/__data/assets/pdf_file/0006/288645/European-health-report-2015-full-book-en.pdf, accessed 15 February 2017). 20. Policy Framework for Implementing New Tuberculosis Diagnostics. Geneva: World Health Organization; 2010 (http://www.who.int/tb/laboratory/ whopolicyframework_rev_june2011.pdf, accessed 15 February 2017). 21. Guidance on regulations for the Transport of Infectious Substances 2015–2016. Geneva: World Health Organization; 2015 (WHO/HSE/GCR/2015.2; http://apps. who.int/iris/bitstream/10665/149288/1/WHO_HSE_GCR_2015.2_eng.pdf, accessed 15 February 2017). Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 22 http://apps.who.int/iris/bitstream/10665/162712/1/9789241508612_eng.pdf http://apps.who.int/iris/bitstream/10665/162712/1/9789241508612_eng.pdf http://www.stoptb.org/wg/gli/assets/documents/WHO Policy Statement on Xpert MTB-RIF 2013 pre publication 22102013.pdf http://www.stoptb.org/wg/gli/assets/documents/WHO Policy Statement on Xpert MTB-RIF 2013 pre publication 22102013.pdf http://www.euro.who.int/__data/assets/pdf_file/0006/288645/European-health-report-2015-full-book-en.pdf http://www.euro.who.int/__data/assets/pdf_file/0006/288645/European-health-report-2015-full-book-en.pdf http://www.euro.who.int/__data/assets/pdf_file/0006/288645/European-health-report-2015-full-book-en.pdf http://www.who.int/tb/laboratory/whopolicyframework_rev_june2011.pdf http://www.who.int/tb/laboratory/whopolicyframework_rev_june2011.pdf http://apps.who.int/iris/bitstream/10665/149288/1/WHO_HSE_GCR_2015.2_eng.pdf http://apps.who.int/iris/bitstream/10665/149288/1/WHO_HSE_GCR_2015.2_eng.pdf 22. Tuberculosis laboratory biosafety manual. Geneva: World Health Organization; 2012 (WHO/HTM/TB/2012.11; http://apps.who.int/iris/ bitstream/10665/77949/1/9789241504638_eng.pdf, accessed 15 February 2017). 23. Molecular line probe assays for rapid screening of patients at risk of multidrug-resistant tuberculosis (MDR-TB): policy statement. Geneva: World Health Organization; 2008 (http://www.who.int/tb/features_archive/policy_ statement.pdf, accessed 15 February 2017). 24. Automated Real-time Nucleic Acid Amplification Technology for Rapid and Simultaneous Detection of Tuberculosis and Rifampicin Resistance: Xpert MTB/RIF System. Policy statement. Geneva: World Health Organization; 2011 (WHO/HTM/TB/2011.4; http://apps.who.int/iris/ bitstream/10665/44586/1/9789241501545_eng.pdf, accessed 15 February 2017). 25. Xpert MTB/RIF implementation manual. Technical and operational 'how-to': practical considerations. Geneva: World Health Organization; 2014 (WHO/HTM/ TB/2014.1; http://apps.who.int/iris/bitstream/10665/112469/1/9789241506700_ eng.pdf, accessed 15 February 2017). 26. The use of molecular line probe assays for the detection of resistance to second-line anti-tuberculosis drugs: Policy guidance. Geneva: World Health Organization; 2016 (WHO/HTM/TB/2016.07; http://apps.who.int/iris/bitstr eam/10665/246131/1/9789241510561-eng.pdf, accessed 15 February 2017). 27. WHO treatment guidelines for drug-resistant tuberculosis, 2016 update. Online Annexes 4,5,6. Geneva: World Health Organization; 2016 (WHO/HTM/TB/2016.04; http://apps.who.int/iris/bitstr eam/10665/250125/5/9789241549639-webannexes-eng.pdf, accessed 15 February 2017). 28. Definitions and reporting framework for tuberculosis – 2013 revision. Geneva: World Health Organization; Updated December 2014 (WHO/HTM/TB/2013.2; http://apps.who.int/iris/bitstream/10665/79199/1/9789241505345_eng.pdf, accessed 15 February 2017). References 23 http://apps.who.int/iris/bitstream/10665/77949/1/9789241504638_eng.pdf http://apps.who.int/iris/bitstream/10665/77949/1/9789241504638_eng.pdf http://www.who.int/tb/features_archive/policy_statement.pdf http://www.who.int/tb/features_archive/policy_statement.pdf http://apps.who.int/iris/bitstream/10665/44586/1/9789241501545_eng.pdf http://apps.who.int/iris/bitstream/10665/44586/1/9789241501545_eng.pdf http://apps.who.int/iris/bitstream/10665/112469/1/9789241506700_eng.pdf http://apps.who.int/iris/bitstream/10665/112469/1/9789241506700_eng.pdf http://apps.who.int/iris/bitstream/10665/246131/1/9789241510561-eng.pdf http://apps.who.int/iris/bitstream/10665/246131/1/9789241510561-eng.pdf http://apps.who.int/iris/bitstream/10665/250125/5/9789241549639-webannexes-eng.pdf http://apps.who.int/iris/bitstream/10665/250125/5/9789241549639-webannexes-eng.pdf http://apps.who.int/iris/bitstream/10665/79199/1/9789241505345_eng.pdf 29. Guidelines for the programmatic management of drug-resistant tuberculosis: update 2011. Geneva: World Health Organization; 2011 (WHO/HTM/TB/2011.6; http://apps.who.int/iris/bitstream/10665/44597/1/9789241501583_eng.pdf, accessed 15 February 2017). 30. Same-day diagnosis of tuberculosis by microscopy: policy statement. Geneva: World Health Organization; 2011 (WHO/HTM/TB/2011.7; http://apps.who.int/ iris/bitstream/10665/44603/1/9789241501606_eng.pdf, accessed 15 February 2017). 31. Handbook on TB laboratory diagnostic methods for the European Union. Stockholm: ECDC: European Centre for Disease Prevention and Control; 2016 (http://ecdc.europa.eu/en/publications/publications/tuberculosis-laboratory-diagnostic-methods-eu.pdf, accessed 15 February 2017). 32. Using the Xpert MTB/RIF assay to detect pulmonary and extrapulmonary tuberculosis and rifampicin resistance in adults and children. Expert Group Meeting Report. Geneva: World Health Organization; 2013 (WHO/HTM/ TB/2013.14; http://apps.who.int/iris/bitstream/10665/112659/1/WHO_HTM_ TB_2013.14_eng.pdf, accessed 15 February 2017). 33. Fluorescent light-emitting diode (LED) microscopy for diagnosis of tuberculosis: policy statement. Geneva: World Health Organization; 2011 (WHO/HTM/TB/2011.8; http://apps.who.int/iris/ bitstream/10665/44602/1/9789241501613_eng.pdf, accessed 15 February 2017). 34. Use of liquid TB culture and drug susceptibility testing in low- and medium-income settings. Summary report of the Expert Group Meeting on the use of liquid culture media. Geneva: World Health Organization; 2007 (http:// www.who.int/tb/laboratory/use_of_liquid_tb_culture_summary_report.pdf, accessed 15 February 2017). 35. Policy guidance on drug-susceptibility testing (DST) of second-line antituberculosis drugs. Geneva: World Health Organization: The STOP TB department; 2008 (WHO/HTM/TB/2008.392; http://www.who.int/tb/ publications/2008/who_htm_tb_2008_392.pdf, accessed 15 February 2017). Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 24 http://apps.who.int/iris/bitstream/10665/44597/1/9789241501583_eng.pdf http://apps.who.int/iris/bitstream/10665/44603/1/9789241501606_eng.pdf http://apps.who.int/iris/bitstream/10665/44603/1/9789241501606_eng.pdf http://ecdc.europa.eu/en/publications/publications/tuberculosis-laboratory-diagnostic-methods-eu.pdf http://ecdc.europa.eu/en/publications/publications/tuberculosis-laboratory-diagnostic-methods-eu.pdf http://apps.who.int/iris/bitstream/10665/112659/1/WHO_HTM_TB_2013.14_eng.pdf http://apps.who.int/iris/bitstream/10665/112659/1/WHO_HTM_TB_2013.14_eng.pdf http://apps.who.int/iris/bitstream/10665/44602/1/9789241501613_eng.pdf http://apps.who.int/iris/bitstream/10665/44602/1/9789241501613_eng.pdf http://www.who.int/tb/laboratory/use_of_liquid_tb_culture_summary_report.pdf http://www.who.int/tb/laboratory/use_of_liquid_tb_culture_summary_report.pdf http://www.who.int/tb/publications/2008/who_htm_tb_2008_392.pdf http://www.who.int/tb/publications/2008/who_htm_tb_2008_392.pdf 36. Noncommercial culture and drug-susceptibility testing methods for screening patients at risk for multidrugresistant tuberculosis: policy statement. Geneva: World Health Organization; 2011 (WHO/HTM/TB/2011.9; http://apps.who.int/ iris/bitstream/10665/44601/1/9789241501620_eng.pdf, accessed 15 February 2017). 37. Bwanga F, Hoffner S, Haile M, Joloba ML. Direct susceptibility testing for multi drug resistant tuberculosis: a meta-analysis. BMC Infect Dis 2009;9:67; doi: 10.1186/1471-2334-9-67. 38. Arentz M, Sorensen B, Horne DJ, Walson JL. Systematic review of the performance of rapid rifampicin resistance testing for drug-resistant tuberculosis. PLoS One 2013;8(10):e76533; doi: 10.1371/journal.pone.0076533. 39. Drobniewski F, Cooke M, Jordan J, Casali N, Mugwagwa T, Broda A, et al. Systematic review, meta-analysis and economic modelling of molecular diagnostic tests for antibiotic resistance in tuberculosis. Health Technol Assess 2015;19(34):1-188, vii-viii; doi: 10.3310/hta19340. References 25 http://apps.who.int/iris/bitstream/10665/44601/1/9789241501620_eng.pdf http://apps.who.int/iris/bitstream/10665/44601/1/9789241501620_eng.pdf Both methods, Xpert MTB/RIF and first-line line probe assays (LPA), have a high sensitivity and specificity to detect tuberculosis (TB) and rifampicin resistance. The Xpert MTB/RIF assay has 95% sensitivity and 98% specificity for rifampicin detection and LPA 97% and 99% respectively (25, 26). That means there are few false susceptible and false resistant strain results reported to clinicians. Genotypic results are recommended to be reported first to clinicians, due to the rapidity of the techniques and their high sensitivity and specificity. With this, the clinician can start treatment with first-line drugs or a multidrug-resistance (MDR) regimen based on the initial rapid molecular test results. After repeating results with the other molecular-based technique (often LPA, as Xpert MTB/RIF is proposed to be used as the initial test) and phenotypic drug-susceptibility testing (pDST), the results can in rare cases differ. If both genotypic DST (gDST) methods show resistance but pDST results are susceptible, there is likely to be an error in performing conventional DST. Before clarification of the reasons behind the discordance, the clinician shall proceed with the initiation of treatment based on the gDST results. Annex 1 Results obtained by genotypic (Xpert MTB/RIF and first-line LPA) and phenotypic methods (MGIT and LJ) to support primarily NRLs or regional reference laboratories for better understanding and interpreting discrepant results Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 26 Silent mutations, which are rare and can be detected only with genotypic methods, do not result in amino acid changes and therefore do not confer resistance. Rare or disputed mutations are detected by gDST, but there may be some discordance when results are compared to pDST results, as they may indicate low-level resistance that may be missed by some pDST methods. Rare cases due to mutations outside the hotspot region will not be detected by Xpert MTB/RIF and LPA but will be detected by phenotypic methods. Sequencing techniques would be the preferred methodology to further confirm silent, rare and disputed mutations. Hetero-resistance can occur mainly in countries with a high MDR-TB rate, due to the fact that patients can be infected with two or more different strains (so-called mixed infections). In the LPA, heteroresistance can be visualized because both the wildtype and the mutation probes will be positive. Heteroresistance may not always be visible in the pDST, however. If in a sputum sample drug-susceptible bacteria exist at a higher concentration, pDST results would normally be "susceptible". In this case, DST should be repeated with another sample to detect the resistant strain, preferably with all techniques. In summary: in high MDR prevalence settings, rifampicin-resistant results obtained via rapid molecular techniques (GeneXpert and first-line LPA) should be communicated to clinicians to inform the drugs used in the initiation of treatment. In case of discordance between pDST and gDST results, absence of laboratory errors should be guaranteed for both approaches (gDST and pDST) by the laboratory and as a second step the different possibilities for potential discordant results explained in Table A.1 should be considered. Annex 1 27 Xpert MTB/RIF – primary sample FL-LPA Report to the clinician Treatment to be initiated by clinician pDST Explanation / Possible laboratory errors Action Resistant WT: NO, MUT: YES RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Resistant Typical mutation in the rpoB RRDR Issue report Susceptible WT:YES, MUT: NO RIF susceptible Initiate first-line regimen Susceptible Issue report Resistant WT: NO, MUT: YES RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Susceptible Faulty pDST e.g.:• no or too little bacteria suspension added • too much RIF added • RIF stock concentration too high If erroneous pDST can be excluded mutations that cause low level resistance could be expected. • Repeat pDST • Review RIF stock/concentration and quality controls Resistant WT: NO, MUT: NO RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Resistant Mutation not available on HAIN strip Issue report Resistant WT: YES, MUT: YES RIF resistant Note: evidence of mixed infection with a RIF resistant and sensitive strain 1. Initiate second-line regimen 2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Resistant Hetero-resistance • Repeat FL-LPA from pDST• sequencing (double peaks) Resistant WT: NO, MUT: NO RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Susceptible Faulty pDST (e.g. TB suspension not added) • Repeat pDST• Review RIF stock/concentration and quality controls If sequencing available and silent mutation confirmed release as "RIF susceptible", otherwise release as "RIF resistant". Determine RIF MIC if possible in order to identify low level resistance Silent or rare mutations Sequencing R esistant WT: YES, MUT: NO Depends on outcome of investigation Depends on outcome of investigation Susceptible Faulty Xpert MTB/RIF e.g.:• sample mix up • contamination • Repeat FL-LPA, pDST testing of another isolate of the patient • Review samples tested before and after the sample in question • Review maintanance record RIF susceptible RIF resistant Note: evidence of mixed infection with a RIF resistant and sensitive strain Hetero-resistance • Repeat Xpert testing on another sputum sample • Repeat FL-LPA • pDST testing of another isolate of the patient Susceptible WT: YES, MUT: NO RIF susceptible Depends on outcome of investigation Resistant Faulty pDST e.g.:• no drug added • contamination • Repeat pDST • Microscopy • Blood plate from pDST Issue report with result: "No genetic markers of RIF resistance detected; unable to perform pDST due to mixed culture with NTMs" Mixed culture with MTB and NTM HAIN CM from RIF DST MGIT or from solid RIF DST; review original solid culture, to check for NTM, repeat pDST on another positive isolat RIF resistant Mutation outside the rpoB 511-524 hotspot region rpoB sequencing (outside RRDR) RIF resistant Note: evidence of mixed infection with a RIF resistant and sensitive strain Hetero-resistance • FL-LPA: check for faint signals • Repeat FL-LPA from pDST • Sequencing (double peaks) Susceptible WT: YES, MUT: YES Depends on outcome of investigation Depends on outcome of investigation S usceptible Laboratory error (e.g. contamination of the FL-LPA) Check negative control in the FL-LPA run. Repeat FL-LPA. Table A.1. Interpretation and explanation of potential discordant results Algorithm for laboratory diagnosis and treatment-monitoring of pulmonary tuberculosis and drug-resistant tuberculosis using state-of-the-art rapid diagnostic technologies 28 Xpert MTB/RIF – primary sample FL-LPA Report to the clinician Treatment to be initiated by clinician pDST Explanation / Possible laboratory errors Action Resistant WT: NO, MUT: YES RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Resistant Typical mutation in the rpoB RRDR Issue report Susceptible WT:YES, MUT: NO RIF susceptible Initiate first-line regimen Susceptible Issue report Resistant WT: NO, MUT: YES RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Susceptible Faulty pDST e.g.:• no or too little bacteria suspension added • too much RIF added • RIF stock concentration too high If erroneous pDST can be excluded mutations that cause low level resistance could be expected. • Repeat pDST • Review RIF stock/concentration and quality controls Resistant WT: NO, MUT: NO RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Resistant Mutation not available on HAIN strip Issue report Resistant WT: YES, MUT: YES RIF resistant Note: evidence of mixed infection with a RIF resistant and sensitive strain 1. Initiate second-line regimen 2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Resistant Hetero-resistance • Repeat FL-LPA from pDST• sequencing (double peaks) Resistant WT: NO, MUT: NO RIF resistant based on Xpert MTB/RIF results 1. Initiate second-line regimen2. Adjust treatment regimen if needed once SL-LPA and pDST results are available Susceptible Faulty pDST (e.g. TB suspension not added) • Repeat pDST• Review RIF stock/concentration and quality controls If sequencing available and silent mutation confirmed release as "RIF susceptible", otherwise release as "RIF resistant". Determine RIF MIC if possible in order to identify low level resistance Silent or rare mutations Sequencing R esistant WT: YES, MUT: NO Depends on outcome of investigation Depends on outcome of investigation Susceptible Faulty Xpert MTB/RIF e.g.:• sample mix up • contamination • Repeat FL-LPA, pDST testing of another isolate of the patient • Review samples tested before and after the sample in question • Review maintanance record RIF susceptible RIF resistant Note: evidence of mixed infection with a RIF resistant and sensitive strain Hetero-resistance • Repeat Xpert testing on another sputum sample • Repeat FL-LPA • pDST testing of another isolate of the patient Susceptible WT: YES, MUT: NO RIF susceptible Depends on outcome of investigation Resistant Faulty pDST e.g.:• no drug added • contamination • Repeat pDST • Microscopy • Blood plate from pDST Issue report with result: "No genetic markers of RIF resistance detected; unable to perform pDST due to mixed culture with NTMs" Mixed culture with MTB and NTM HAIN CM from RIF DST MGIT or from solid RIF DST; review original solid culture, to check for NTM, repeat pDST on another positive isolat RIF resistant Mutation outside the rpoB 511-524 hotspot region rpoB sequencing (outside RRDR) RIF resistant Note: evidence of mixed infection with a RIF resistant and sensitive strain Hetero-resistance • FL-LPA: check for faint signals • Repeat FL-LPA from pDST • Sequencing (double peaks) Susceptible WT: YES, MUT: YES Depends on outcome of investigation Depends on outcome of investigation S usceptible Laboratory error (e.g. contamination of the FL-LPA) Check negative control in the FL-LPA run. Repeat FL-LPA. Table A.1. Interpretation and explanation of potential discordant results Annex 1 29 The WHO Regional Office for Europe The World Health Organization (WHO) is a specialized agency of the United Nations created in 1948 with the primary responsibility for international health matters and public health. The WHO Regional Office for Europe is one of six regional offices throughout the world, each with its own programme geared to the particular health conditions of the countries it serves. Member States Albania Andorra Armenia Austria Azerbaijan Belarus Belgium Bosnia and Herzegovina Bulgaria Croatia Cyprus Czechia Denmark Estonia Finland France Georgia Germany Greece Hungary Iceland Ireland Israel Italy Kazakhstan Kyrgyzstan Latvia Lithuania Luxembourg Malta Monaco Montenegro Netherlands Norway Poland Portugal Republic of Moldova Romania Russian Federation San Marino Serbia Slovakia Slovenia Spain Sweden Switzerland Tajikistan The former Yugoslav Republic of Macedonia Turkey Turkmenistan Ukraine United Kingdom Uzbekistan World Health Organization Regional Office for Europe UN City, Marmorvej 51, DK-2100 Copenhagen Ø, Denmark Tel.: +45 45 33 70 00. Fax: +45 45 33 70 01. E-mail: euwhocontact@who.int Website: www.euro.who.int 9 789289 052375 > ISBN 9789289052375 mailto:euwhocontact%40who.int?subject= Glowbase Graduate Recruitment Platform - http://www.glowbase.com - © Glowbase GmbH - 2016-01-22 13:42:52 Phenotype-related information extraction from routine data for biomarker research Summary Stefan Schulz; Institute for Medical Informatics, Statistics and Dokumentation, Medical Universität of Graz Supervisor: Prof. Dr. Stefan Schulz Status: open Offered by: Medical University of Graz Description Phenotype-related information extraction from routine data for biomarker research Background: One grand challenge in biomarker research is to complete the picture of information about potential biomarkers using a broad range of clinical data sources. This project addresses the reuse of documented information on clinical phenotypes, past diseases, findings, procedures, lifestyle data, drugs, family history, etc. As most of this routine data resides in free-text narratives such as findings reports and low-structured patient summaries in electronic health record systems, a semantic data extraction platform for the optimized retrieval of biobank samples and a customizable toolkit for content retrieval is currently being built. An important use case is the optimized retrieval of biobank samples, such as needed for bio-marker research, which requires information about the clinical context of the patient from which the sam-ples had been taken. This data requires a thoughtful selection of important features like diseases, signs, symptoms, therapies, clinical evolution and laboratory parameters, altogether put into a temporal context, for the best ranking on a cohort search engine. Hypothesis and Objectives: The hypothesis is that there is a common core of information needs for biomarker research that can only be addressed by either labour-intensive manual reworking of routine data or by machine processing of the EHR. The objective is to lay the foundations for the development of a customizable toolkit for content extraction. Existing approaches like i2b2 will be capitalized on, as well as existing terminologies and data models (SNOMED CT, clinical models). Principal objectives are to develop, customize and assess components of a processing pipeline that takes raw clinical texts as they are and enriches them by semantic metadata. The specific tasks are manifold: identification of short forms (abbreviations, acronyms), correction of misspellings, identification of attribute – number – unit triples, identification of temporal contexts, identification of epistemic contexts (diagnostic (un)certainty, intentions), negations. Main purpose of these processing steps are to accurately map textual content to clinical terminologies (SNOMED CT, LOINC, ICD) and pre-defined information models, which could affect, for example, how a patient is ranked on a cohort search engine. Methodology: Diverse computational linguistics methods will be used (rule-based, machine learning-based), together with different information retrieval approaches. The student should therefore have a background in computer science, with a focus on computational linguistics and text mining. Familiarity with the medical domain, as well as with medical ontologies and terminologies is desirable. References: http://www.glowbase.com Glowbase Graduate Recruitment Platform - http://www.glowbase.com - © Glowbase GmbH - 2016-01-22 13:42:52 1. Patterson O., Igo S., and Hurdle J. F. Automatic acquisition of sublanguage semantic schema: To-wards the word sense disambiguation of clinical narratives. In AMIA Annual Symposium Proceedings, volume 2010, pages 612–616. American Medical Informatics Association, 2010. 2. Joachims T. Text categorization with support vector machines: Learning with many relevant features. In European Conference on Machine Learning (ECML), pages 137–142, Berlin, 1998. Springer. 3. Baharudin B., Lee L. H., and Khan K. A review of machine learning algorithms for text-documents classification. Journal of Advances in Information Technology, 1(1): 4–20, 2010. 4. Chiticariu L., Li Y., and Reiss F. R. Rule-based information extraction is dead! Long live rule-based information extraction systems! In EMNLP, pages 827–832, 2013. 5. Chapman W. W., Chu D., and Dowling J. N. ConText: an algorithm for identifying contextual features from clinical text. In Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing, pages 81–88. Association for Computational Linguistics, 2007. 6. Meystre S. M., Savova G., Kipper-Schuler K., and Hurdle J. Extracting information from textual doc-uments in the electronic health record: A review of recent research. Yearbook of Medical Informatics, 35:128–144, 2008. 7. Patterson O., Forbush T., Saini S., Moser S., and DuVall S. Classifying the indication for colonoscopy procedures: A comparison of NLP approaches in a diverse national healthcare system. Studies in Health Technology and Informatics, 216:614–618, 2015. 8. Kreuzthaler M., Daumke P., and Schulz S. Semantic retrieval and navigation in clinical document col-lections. EHealth2015–Health Informatics Meets EHealth: Innovative Health Perspectives: Personal-ized Health, 212:9–14, 2015. 9. Savova G. K., Masanz J. J., Ogren P. V., Zheng J., Sohn S., Kipper-Schuler K. C., and Chute C. G. Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association, 17(5):507–513, 2010. 10. Friedman C., Hripcsak G., DuMouchel W., Johnson S. B., and Clayton P. D. Natural language pro-cessing in an operational clinical information system. Natural Language Engineering, 1(01):83–108, 1995. To get more information or to apply online, visit https://mug.glowbase.com/ positions/55 or scan the the code on the left with your smartphone. http://www.glowbase.com https://mug.glowbase.com/positions/55 https://mug.glowbase.com/positions/55 Phenotype-related information extraction from routine data for biomarker research Summary Description Information Retrieval
Definitions
Information Retrieval is "The study of systems for indexing, searching, and recalling data, particularly text or other unstructured forms." [ virtechseo.com ]
"Information retrieval is the art and science of searching for information in documents, searching for documents themselves,  searching for metadata which describes documents, or searching within databases, whether relational stand alone databases or  hypertext networked databases such as the Internet or intranets, for text, sound, images or data" [ Wikipedia ]
Information Retrieval is "a field at the intersection of information science and computer science. [It] concerns itself with the indexing and retrieval of information from heterogeneous and mostly-textual information resources. The term was coined by Mooers in 1951, who advocated that it be applied to the intellectual aspects of description of information and systems for its searching (Mooers, 1951)." [ Hersh, 2003 ]
Information retrieval in medicine
Digital information indexing and retrieval systems are becoming increasingly important for the  dissemination of medical knowledge and have the potential to impact considerably on the quality of care provided by clinicians.  Technologies and applications providing easy access to validated, well-maintained sources (libraries, databases ...) of up-to-date clinical knowledge over a variety of platforms have become increasingly visible. These range from PubMed through which the US National Library of Medicine offers free access to the world's medical research literature, to a large number of commercial medical reference software products (see links below).
references: general
William Hersh, M.D.  Information Retrieval:  A Health & Biomedical Perspective (Second Edition).  Springer-Verlag , 2003
[ Official Web site for the book maintained by William Hersh ]   [ Chapter 1 (free) ]  [ Chapter updates (free) ]
Excerpts from the book ook are available on this site (Preface,  table of contents,  Chapter 1 [terms, models, and resources], bibliography, full Index and, in particular, detailed updates for each chapter).
" The book is divided into three sections.   The first section covers the basic concepts of information science.    The first chapter provides basic definitions and models that will be used throughout the book.  The next chapter gives an  overview of health and biomedical information, covering issues related to its generation and use.   The third chapter discusses the evaluation of IR systems, highlighting the methods and their limitations.   ...  The second section covers the current state-of-the-art in commercial and other widely used retrieval systems.   The first chapter in this section gives an overview of the great deal of content that is currently available.   Next come chapters on the two fundamental intellectual tasks of IR, indexing and retrieval.   The predominant paradigms of each are discussed in detail.   The final chapter covers evaluation of these systems. ...  The third section covers the major threads of research and development in efforts to build better IR systems.   The focus is initially on details of indexing and retrieval, with a chapter each on the two major thrusts, which are lexical-statistical and linguistic systems.  In the next chapter, a survey of various efforts to augment other systems is described.  This is followed by a chapter on information extraction, a topic of growing importance.  Throughout this section, a theme of implementational feasibility and evaluation is maintained. "
References: Information Retrieval in medicine
Westbrook JI, Coiera EW, Gosling AS.   Do online information retrieval systems help experienced clinicians answer clinical questions? J Am Med Inform Assoc. 2005 May-Jun;12(3):315-21.
[ PubMed ]   [ ]
" OBJECTIVE: To assess the impact of clinicians' use of an online information retrieval system on their performance in answering clinical questions. DESIGN: Pre-/post-intervention experimental design. MEASUREMENTS: In a computer laboratory, 75 clinicians (26 hospital-based doctors, 18 family practitioners, and 31 clinical nurse consultants) provided 600 answers to eight clinical scenarios before and after the use of an online information retrieval system. We examined the proportion of correct answers pre- and post-intervention, direction of change in answers, and differences between professional groups. RESULTS: System use resulted in a 21% improvement in clinicians' answers, from 29% (95% confidence interval [CI] 25.4-32.6) correct pre- to 50% (95% CI 46.0-54.0) post-system use. In 33% (95% CI 29.1-36.9) answers were changed from incorrect to correct. In 21% (95% CI 17.1-23.9) correct pre-test answers were supported by evidence found using the system, and in 7% (95% CI 4.9-9.1) correct pre-test answers were changed incorrectly. For 40% (35.4-43.6) of scenarios, incorrect pre-test answers were not rectified following system use. Despite significant differences in professional groups' pre-test scores [family practitioners: 41% (95% CI 33.0-49.0), hospital doctors: 35% (95% CI 28.5-41.2), and clinical nurse consultants: 17% (95% CI 12.3-21.7; chi(2) = 29.0, df = 2, p < 0.01)], there was no difference in post-test scores. (chi(2) = 2.6, df = 2, p = 0.73). CONCLUSIONS: The use of an online information retrieval system was associated with a significant improvement in the quality of answers provided by clinicians to typical clinical problems. In a small proportion of cases, use of the system produced errors. While there was variation in the performance of clinical groups when answering questions unaided, performance did not differ significantly following system use. Online information retrieval systems can be an effective tool in improving the accuracy of clinicians' answers to clinical questions. "
Westbrook JI, Coiera EW, Gosling AS.    Do online information retrieval systems help experienced clinicians answer clinical questions? J Am Med Inform Assoc. 2005 May-Jun;12(3):315-21.
[ PubMed ]   [ ]
" OBJECTIVE: To assess the impact of clinicians' use of an online information retrieval system on their performance in answering clinical questions. DESIGN: Pre-/post-intervention experimental design. MEASUREMENTS: In a computer laboratory, 75 clinicians (26 hospital-based doctors, 18 family practitioners, and 31 clinical nurse consultants) provided 600 answers to eight clinical scenarios before and after the use of an online information retrieval system. We examined the proportion of correct answers pre- and post-intervention, direction of change in answers, and differences between professional groups. RESULTS: System use resulted in a 21% improvement in clinicians' answers, from 29% (95% confidence interval [CI] 25.4-32.6) correct pre- to 50% (95% CI 46.0-54.0) post-system use. In 33% (95% CI 29.1-36.9) answers were changed from incorrect to correct. In 21% (95% CI 17.1-23.9) correct pre-test answers were supported by evidence found using the system, and in 7% (95% CI 4.9-9.1) correct pre-test answers were changed incorrectly. For 40% (35.4-43.6) of scenarios, incorrect pre-test answers were not rectified following system use. Despite significant differences in professional groups' pre-test scores [family practitioners: 41% (95% CI 33.0-49.0), hospital doctors: 35% (95% CI 28.5-41.2), and clinical nurse consultants: 17% (95% CI 12.3-21.7; chi(2) = 29.0, df = 2, p < 0.01)], there was no difference in post-test scores. (chi(2) = 2.6, df = 2, p = 0.73). CONCLUSIONS: The use of an online information retrieval system was associated with a significant improvement in the quality of answers provided by clinicians to typical clinical problems. In a small proportion of cases, use of the system produced errors. While there was variation in the performance of clinical groups when answering questions unaided, performance did not differ significantly following system use. Online information retrieval systems can be an effective tool in improving the accuracy of clinicians' answers to clinical questions. "
Haynes RB, Walker CJ, McKibbon KA et al. Performances of 27 MEDLINE systems tested by searches with clinical questions. J Am Med Inform Assoc. 1994 May-Jun;1(3):285-95.
[ PubMed ]   [ PubMed Central ]
" OBJECTIVE: To compare the performances of online and compact-disc (CD-ROM) versions of the National Library of Medicine's (NLM) MEDLINE database. DESIGN: Analytic survey. INTERVENTION: Clinical questions were drawn from 18 searches originally conducted spontaneously by clinicians from wards and clinics who had used Grateful Med Version 4.0. Clinicians' search strategies were translated to meet the specific requirements of 13 online and 14 CD-ROM MEDLINE systems. A senior librarian and vendors' representatives constructed independent searches from the clinicians' questions. The librarian and clinician searches were run through each system, in command mode for the librarian and menu mode for clinicians, when available. Vendor searches were run through the vendors' own systems only. MAIN MEASUREMENTS: Numbers of relevant and irrelevant citations retrieved, cost (for online systems only), and time. RESULTS: Systems varied substantially for all searches, and for librarian and clinician searches separately, with respect to the numbers of relevant and irrelevant citations retrieved (p < 0.001 for both) and the cost per relevant citation (p = 0.012), but not with respect to the time per search. Based on combined rankings for the highest number of relevant and the lowest number of irrelevant citations retrieved, the SilverPlatter CD-ROM MEDLINE clinical journal subset performed best for librarian searches, while the PaperChase online system worked best for clinician searches. For cost per relevant citation retrieved, Dialog's Knowledge Index performed best for both librarian and clinician searches. CONCLUSIONS: There were substantial differences in the performances of competing MEDLINE systems, and performance was affected by search strategy, which was conceived by a librarian or by clinicians. "
links  
A Part-Of-Speech term weighting scheme for biomedical IR is proposed.
•
A machine learning algorithm is proposed to compute the optimal POS weights.
•
The proposed POS-BoW and POS-MRF outperform the BoW and MRF models, respectively.
Abstract
In the era of digitalization, information retrieval (IR), which retrieves and ranks documents from large collections according to users' search queries, has been popularly applied in the biomedical domain. Building patient cohorts using electronic health records (EHRs) and searching literature for topics of interest are some IR use cases. Meanwhile, natural language processing (NLP), such as tokenization or Part-Of-Speech (POS) tagging, has been developed for processing clinical documents or biomedical literature. We hypothesize that NLP can be incorporated into IR to strengthen the conventional IR models. In this study, we propose two NLP-empowered IR models, POS-BoW and POS-MRF, which incorporate automatic POS-based term weighting schemes into bag-of-word (BoW) and Markov Random Field (MRF) IR models, respectively. In the proposed models, the POS-based term weights are iteratively calculated by utilizing a cyclic coordinate method where golden section line search algorithm is applied along each coordinate to optimize the objective function defined by mean average precision (MAP). In the empirical experiments, we used the data sets from the Medical Records track in Text REtrieval Conference (TREC) 2011 and 2012 and the Genomics track in TREC 2004. The evaluation on TREC 2011 and 2012 Medical Records tracks shows that, for the POS-BoW models, the mean improvement rates for IR evaluation metrics, MAP, bpref, and P@10, are 10.88%, 4.54%, and 3.82%, compared to the BoW models; and for the POS-MRF models, these rates are 13.59%, 8.20%, and 8.78%, compared to the MRF models. Additionally, we experimentally verify that the proposed weighting approach is superior to the simple heuristic and frequency based weighting approaches, and validate our POS category selection. Using the optimal weights calculated in this experiment, we tested the proposed models on the TREC 2004 Genomics track and obtained average of 8.63% and 10.04% improvement rates for POS-BoW and POS-MRF, respectively. These significant improvements verify the effectiveness of leveraging POS tagging for biomedical IR tasks.
Graphical abstract
Cookies are used by this site. For more information, visit the cookies page .
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
Abstract
Objective
Unlocking the data contained within both structured and unstructured components of electronic health records (EHRs) has the potential to provide a step change in data available for secondary research use, generation of actionable medical insights, hospital management, and trial recruitment. To achieve this, we implemented SemEHR, an open source semantic search and analytics tool for EHRs.
Methods
SemEHR implements a generic information extraction (IE) and retrieval infrastructure by identifying contextualized mentions of a wide range of biomedical concepts within EHRs. Natural language processing annotations are further assembled at the patient level and extended with EHR-specific knowledge to generate a timeline for each patient. The semantic data are serviced via ontology-based search and analytics interfaces.
Results
SemEHR has been deployed at a number of UK hospitals, including the Clinical Record Interactive Search, an anonymized replica of the EHR of the UK South London and Maudsley National Health Service Foundation Trust, one of Europe's largest providers of mental health services. In 2 Clinical Record Interactive Search–based studies, SemEHR achieved 93% (hepatitis C) and 99% (HIV) F-measure results in identifying true positive patients. At King's College Hospital in London, as part of the CogStack program (github.com/cogstack), SemEHR is being used to recruit patients into the UK Department of Health 100 000 Genomes Project (genomicsengland.co.uk). The validation study suggests that the tool can validate previously recruited cases and is very fast at searching phenotypes; time for recruitment criteria checking was reduced from days to minutes. Validated on open intensive care EHR data, Medical Information Mart for Intensive Care III, the vital signs extracted by SemEHR can achieve around 97% accuracy.
Conclusion
Results from the multiple case studies demonstrate SemEHR's efficiency: weeks or months of work can be done within hours or minutes in some cases. SemEHR provides a more comprehensive view of patients, bringing in more and unexpected insight compared to study-oriented bespoke IE systems. SemEHR is open source, available at https://github.com/CogStack/SemEHR .
secondary use of EHR , information extraction , NLP , semantic search , ontology , FHIR , patient recruitment
BACKGROUND
The opportunity for secondary use of the wealth of information contained within electronic health records (EHRs) has attracted researchers interested in investigating approaches to provide more tailored and timely care, improve efficiency of services, and derive new scientific and medical insights. 1–4 In addition to structured data contained within relational database tables (such as International Classification of Diseases, Tenth Revision [ICD-10] diagnoses codes), EHR documents are filled with unstructured clinical notes, such as nursing records, radiology reports, and discharge summaries. These notes add a richness and depth to EHR-based studies, 5–7 providing data and insight beyond what is available within the thin layer of data stored within structured fields.
Deriving actionable insights from the EHR, including the unstructured component, is challenging. It requires bringing together expertise in the clinical domain, the underlying health care information systems, and text analytics techniques, eg, natural language processing (NLP). For example, the Clinical Record Interactive Search (CRIS) system, 8 an anonymized replica of the EHR used in the South London and Maudsley (SLaM) National Health Service (NHS) Foundation Trust in the UK, was designed to support clinical and scientific studies. Since its launch in 2009, a large number of studies ( 9–13 to name a few) have used the CRIS resource in conjunction with NLP or text-mining techniques. Although these studies answered different clinical questions, the technical requirements for extracting, structuring, and making sense of the data largely overlapped, and included (1) preprocessing and cleansing corpus-related documents (eg, removing misleading form headings from scanned documents); (2) compiling and recognizing common medical terminology (eg, the antipsychotic medication identification problem is almost the same in 10 , 11 ); and (3) deriving patient-level clinical signals from document-level NLP annotations (eg, understanding that a medication prescribed at admission was removed from the patient's discharge medication list).
As unstructured EHR data are inevitably needed by many research projects and clinical studies, more cost-effective and systematic solutions are needed to address the common challenges presented by different use cases, while also ensuring that study-specific requirements are not compromised by the unified approach.
To address such challenges, we propose SemEHR, a semantic search and analytical system that generates a complete and processable view of patients from their clinical notes.
To realize a general-purpose biomedical information extraction (IE) system for EHRs, there are at least 3 fundamental challenges: (1) syntactic heterogeneity: how to effectively access multimodal/multisource EHR data that are almost certainly heterogeneous in format, data model, and access interface; (2) knowledge coverage: how to cover all possible biomedical concepts that are required by potential use cases; and (3) context capturing: how to represent and capture the contexts associated with extracted concepts and determine which are critical to understand the clinical domain. To address these challenges, SemEHR uses a production infrastructure that integrates our previous work in the CogStack pipeline 14 to harmonize and cleanse heterogeneous records, using them to identify contextualized mentions (negation, temporality, and experiencer) of a wide range of biomedical concepts, including Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) ( http://www.snomed.org/snomed-ct ), ICD-10 ( http://apps.who.int/classifications/icd10/browse/2010/en ), Logical Observation Identifiers Names and Codes (LOINC) ( https://loinc.org/ ), and Drug Ontology ( https://ontology.atlassian.net/wiki/spaces/DRON/overview ). In addition, SemEHR automatically associates semantic types of annotations and their clinical contexts (derived from documents or sections) with dedicated extraction rules, which enables better IE capabilities, such as populating structured vital sign data from observation notes.
It is well appreciated that a one-size-fits-all approach needs to be adapted to work effectively in different scenarios. Therefore, to serve different use cases well, we require the capability to extend the terminology of the general-purpose IE system to cover unseen concepts, deal with language specificities in a subcorpus, support use case–specific extraction requirements, and enable performance fine-tuning, eg, by incorporating specific knowledge or researchers' expertise. SemEHR provides a study-based (use case–specific) learning engine that enables iterative learning and feedback. It collects user feedback and uses rule-based and machine learning techniques to tackle study-specific challenges and requirements in a continuous manner.
A few hurdles prevent the effective consumption of extracted data from general-purpose IE systems in scientific research and clinical studies. To fulfill requirements by various studies, developing general-purpose IE systems is inevitable in order to adopt large terminologies that users might not be familiar with. This poses challenges in (1) mapping look-up concepts to terminology terms, (2) translating clinical relations to term associations, and (3) exploiting terminology semantics to bring unexpected or unperceived new insights. At the consumption level, SemEHR implements an ontology-based semantic search component to tackle such challenges.
Last, and probably most important, EHRs represent a timeline of multiple patient interactions with services. As such, the document-level IE results should be integrated at the patient level to incorporate temporal and macrocontextual information (which reports, which visits, etc., as opposed to the sentence-based contextual information discussed above). Only after this integration is the EHR IE task complete. However, this requires a thorough understanding of the EHR system. SemEHR provides a multiperspective view of each patient by assembling NLP annotations at the patient level as longitudinal views and compiling structured medical profiles. Both the NLP results and the patient timeline are made available via an ontology-based search system, which effectively turns common IE tasks into semantic search queries. The interface provides a multiperspective view of each patient by assembling NLP annotations at the patient level as longitudinal views and compiling structured medical profiles.
METHOD
Data model and longitudinal patient views
As depicted in Figure 1 A, SemEHR is built upon 6 types of entities: patient, clinical note, concept, concept mention, medical profile, and profile aspect. Each patient is associated with a list of dated and typed clinical notes. From these notes, SemEHR identifies mentions of a wide range of biomedical concepts from the Unified Medical Language System (UMLS), 15 , 16 a compendium of many controlled vocabularies, including SNOMED CT, ICD-10, LOINC, Drug Ontology, and Gene Ontology. By analyzing the context of its appearance, each mention is associated with 3 pieces of dimensional contextual information: negation, temporality, and experiencer. Highlighted in green in Figure 1 A, the associations between concepts (eg, Steatohepatitis is a liver disease; Ribavirin is a drug for treating hepatitis C) are made available to conduct semantically enriched computations by incorporating the various biomedical ontologies and Linked Open Data ( https://en.wikipedia.org/wiki/Linked_data ) such as DBpedia 17 and Wikidata. 18 SemEHR derives periodical medical profiles from a patient's clinical notes, automatically generated medical summaries consisting of a set of profile aspects (sections describing different aspects of a medical profile, eg, past medical history, medications, etc.) for a defined period of time. Concept mentions are assigned to these aspects according to their appearance in the original clinical notes. As the rectangle boxes in Figure 1 A show, SemEHR entities are mapped to Fast Healthcare Interoperability Resources (FHIR) ( https://www.hl7.org/fhir/overview.html ) entities whenever possible.
Figure 1.
View large Download slide
(A) SemEHR data model: entities (patient, clinical note, concept, and concept mentions) and their associations. (B) SemEHR generates 2 longitudinal views for each patient: concept mentions grouped in typed and dated documents (upper part), and concept mentions grouped in structured (discharge) summaries (lower part).
Figure 1.
View large Download slide
(A) SemEHR data model: entities (patient, clinical note, concept, and concept mentions) and their associations. (B) SemEHR generates 2 longitudinal views for each patient: concept mentions grouped in typed and dated documents (upper part), and concept mentions grouped in structured (discharge) summaries (lower part).
Based on this data model, SemEHR populates 2 longitudinal views (shown in Figure 1 B) for each patient. As shown in the upper part of Figure 1 B, the first view is generated directly from the raw data. Concept mentions are organized in a list of clinical notes that are located on a timeline according to their date attributes (eg, the created date/time of the clinical notes). Wherever possible, types of clinical notes (such as GP Letter, Radiology, or Discharge Summary) are presented.
The second view (lower part of Figure 1 B) is designed to convey structured summaries for a patient, each of which summarizes the patient's medical history/conditions in a period of time (eg, an inpatient hospital stay). A summary is composed of groups of concept mentions, where each group is about one particular aspect of the patient's medical profile, eg, past medical history, medications, or physical exams. Preferably, such summaries are derived from discharge summaries. When discharge summaries are not available, an automated summarization approach is applied to generate the summaries based on the contextual information of the concept types and concept mentions. Automated summaries are differentiated from those generated from discharge summaries. Supplementary Material 2 describes the detailed process of automated medical profile generation.
Architecture: generic and adaptive information extraction and retrieval
As illustrated in Figure 2 , SemEHR is composed of 3 subsystems: the producing subsystem, the continuous learning subsystem, and the consuming subsystem.
Figure 2.
View large Download slide
The architecture of SemEHR is composed of 3 subsystems: (1) the producing subsystem (upper part of the figure), creation of SemEHR semantic index by harmonizing, natural language processing, and indexing EHR data; (2) the continuous learning subsystem, addressing study-specific requirements and supporting fine-tuning for separate studies; and (3) the consuming subsystem (lower part), supporting tailored care, patient recruitment, and clinical research by semantic searching and study-based continuous learning.
Figure 2.
View large Download slide
The architecture of SemEHR is composed of 3 subsystems: (1) the producing subsystem (upper part of the figure), creation of SemEHR semantic index by harmonizing, natural language processing, and indexing EHR data; (2) the continuous learning subsystem, addressing study-specific requirements and supporting fine-tuning for separate studies; and (3) the consuming subsystem (lower part), supporting tailored care, patient recruitment, and clinical research by semantic searching and study-based continuous learning.
The producing subsystem
Essentially, the producing subsystem extracts free-text clinical notes from heterogeneous underlying EHR systems, populating the data model described in the previous section. This task is performed in 3 main steps: data retrieval, information extraction, and semantic indexing. CogStack, 14 a data harmonization and enterprise search toolkit for EHRs, is adopted in the data retrieval step to provide a unified interface with unstructured EHR data, which is often very heterogeneous in format and distributed in storage. Each document that flows out from the data retrieval component is fed into the NLP pipeline, which embeds Bio-YODIE ( https://gate.ac.uk/applications/bio-yodie.html ), an NLP pipeline dedicated to annotating UMLS concepts in clinical notes ("documents" hereafter). (Bio-YODIE was developed as part of the EU KConnect project, in which GG, AR, HW, RS, and RD are involved.) Emerging from the NLP pipeline are the documents and concept mentions extracted from them, which are then analyzed by the Semantic Index component before being indexed. The analysis involves deriving document types (eg, Radiology, GP Letter, or Discharge Summary), parsing document structure (eg, identifying headed blocks from discharge summaries), and associating concept mentions with document structures. The analysis results, document content, and NLP outputs are finally indexed by an Elasticsearch ( https://www.elastic.co/products/elasticsearch ) cluster. Patient-level summaries are generated as described in the previous section. These summaries are updated as new documents are added to the index.
SemEHR aims to produce annotations with accurate contextual information. Three components work collectively to achieve this goal: the Bio-YODIE pipeline captures the sentence/paragraph-level contexts (eg, negation, hypothetical mentions); the semantic index's analyzer brings in section/document-level context (eg, past medical history, laboratory results); and the continuous learning subsystem (described in the next subsection) learns the contexts from user-assessed annotations (see Supplementary Material 1 for details).
The continuous learning subsystem
To accommodate the uniqueness of the IE requirements of different studies, SemEHR is designed with a continuous learning subsystem to iteratively address study-specific issues. The system collects and analyzes user feedback from an annotation component embedded within the user interface. Based on the analyzed feedback, 2 components are used to improve the IE results. The first is a rule engine, which generates and applies rules for filtering out unwanted results, eg, removing concept mentions based on their original string or surrounding text. The second component is a machine learning engine (a bidirectional recurrent neural network model), which takes user feedback as training data, applies the trained model on the study's corpus, and populates a confidence value for each concept mention. Confidence values are used as quantified indicators in analytic components for populating results. The user interface for collecting feedback and the continuous learning mechanisms are explained in detail in Supplementary Material 1 .
The consuming subsystem
This subsystem consists of a set of components that utilize IE results and clinical knowledge (accessed from biomedical ontology and Linked Open Data application programming interfaces) to support tasks such as patient characterization or trial recruitment. A consuming task is called a "study" in SemEHR. Each study will have its own storage within SemEHR's Study Knowledge Graph (KG) (bottom of the Storage section in Figure 2 ), which stores its study parameters (eg, cohort definition and metadata), search settings (eg, query concepts), study results (eg, selected cohort and exported features), and customized rules (eg, regular expressions to remove unwanted annotations). There is also a common KG (Common KG in Figure 2 ), where sharable knowledge or efforts (such as manually selected concepts of alcohol-related liver diseases or postprocessing rules for improving NLP results) are made available to other studies.
Key functionalities of the consuming subsystem include the following:
Translating search terms to query concepts. This translates the user's keyword searches (which are often ambiguous or incomplete) into semantically clear concepts (identified using UMLS Controlled Unclassified Information). The correct translation is essential to ensure the soundness and completeness of search and analytics results. Unfortunately, in the clinical scenario, it is often not a trivial task to compile an accurate and complete concept list even for a single clinical signal. For example, one SemEHR case study needs to look up patients with alcohol-related liver disease. Given a general clinical term such as "liver disease," it would be time-consuming to compile a list of all subtypes of liver disease that are also alcohol-related. As depicted in section A of Figure 3 , SemEHR provides 2 functions for supporting concept translation: (1) matching search terms to concepts, which is enhanced with logical reasoning to automatically include semantically related concepts and EHR-based exclusion to remove concepts that do not exist in EHRs of the study cohort; and (2) validating automatically populated lists, to allow manual assessment by the researchers.
Selecting and summarizing a cohort. Each query submitted to SemEHR will result in a cohort, a list of patients who match the query. As shown in Figure 3 B, a summary table is generated for the matching cohort. Each row summarizes a patient, and the first column shows the patient ID. The second one shows the total number of mentions of the search concepts within this patient's EHR, followed by numbers of 4 contextualized mentions: positive mentions, history/hypothetical mentions, negated mentions, and mentions associated with other experiencers. Clicking on the numbers brings the user to the clinical notes, where corresponding mentions are highlighted (lower part of Figure 3 B).
Generating patient views and structured medical profile. As a generic IE and retrieval platform, SemEHR processes all EHR records for patients and tries to identify a wide range of biomedical concepts from them. This enables it to produce a panorama for each patient. As shown in Figure 3 C, 3 different views are generated for each patient:
The first view is the longitudinal document view (upper part of Figure 3 C), which lists all patient documents in chronological order, labels documents using their types, and ticks those documents that match the query. This view delivers the abundance of a patient's records, the prevalence of matched documents, and their temporal distributions.
The second view is the structured medical profile (lower part of Figure 3 C), which is automatically derived from the patient's clinical notes and structured using extended FHIR discharge summary format (23 sections of the FHIR discharge summary [ http://hl7.org/fhir/us/ccda/2017Jan/StructureDefinition-CCDA-on-FHIR-Discharge-Summary.html ] are extended with an additional 8 headings). This structured summary enhances SemEHR's search and IE ability. For example, by constraining the search field to "Family History," one can get a cohort of patients with a family history of a certain disorder. In addition, knowing that a piece of text appeared in the "Hospital Discharge Physical," sophisticated rules can be applied to extract more structured data, such as vital signs.
The third view is the view of vital signs and other measurements (middle part of Figure 3 C). This is automatically generated by applying IE rules on the latest structured summary of a patient.
Figure 3.
View large Download slide
Screenshots of key functionalities provided by the consuming subsystem. (A) Identifying query concepts (UMLS CUIs): facilities to ensure the correct and complete concepts are used in the query to derive accurate clinical findings. (A1) Concept search for matching a user search term to one or more ontology (UMLS) concepts; logical reasoning is implemented to enable the automated inclusion of semantically related concepts (eg, hepatocellular damage is liver damage). (A2) Concept validation component for checking and approving the automated inferred concepts based on the aim and criteria of the clinical study (eg, only retain alcohol-related liver conditions for addiction analytics). (B) Selecting and summarizing cohort (the full text in the screenshot has been deliberately rewritten to avoid leaking sensitive patient data). A summary table is generated for a user query where each row summarizes the numbers of total mentions and contextualized mentions for one patient. (C) Patient timeline: longitudinal document view (upper), structured medical profile view (based on FHIR discharge summary format), and the view of latest vital signs and other measurements.
Figure 3.
View large Download slide
Screenshots of key functionalities provided by the consuming subsystem. (A) Identifying query concepts (UMLS CUIs): facilities to ensure the correct and complete concepts are used in the query to derive accurate clinical findings. (A1) Concept search for matching a user search term to one or more ontology (UMLS) concepts; logical reasoning is implemented to enable the automated inclusion of semantically related concepts (eg, hepatocellular damage is liver damage). (A2) Concept validation component for checking and approving the automated inferred concepts based on the aim and criteria of the clinical study (eg, only retain alcohol-related liver conditions for addiction analytics). (B) Selecting and summarizing cohort (the full text in the screenshot has been deliberately rewritten to avoid leaking sensitive patient data). A summary table is generated for a user query where each row summarizes the numbers of total mentions and contextualized mentions for one patient. (C) Patient timeline: longitudinal document view (upper), structured medical profile view (based on FHIR discharge summary format), and the view of latest vital signs and other measurements.
Based on these key functionalities, SemEHR provides a set of search interfaces to surface the clinical variables hidden in clinical notes. A typical query, such as "return all patients with a family history of hepatitis C," previously might have required the end user to have NLP expertise, eg, be able to do named entity recognition for "hepatitis C" that must be mentioned in the context of "family history." Using SemEHR, the end user can put in a simple keyword search: "hepatitis C." To fulfill this search, SemEHR will pull out the cohort of relevant patients, populate patient-level summaries (ie, numbers of contextualized concept mentions, such as patient has 16 total mentions of the disease, 15 of them positive and 1 about a family member), and provide a link to each mention in the original source clinical note (similar to the UI illustrated in Figure 3 B).
RESULTS
This section reports the experiments and results from 3 EHR systems focusing on evaluating SemEHR's capacities in semantic search, analytics, and clinical decision-making support. The evaluation on its natural language processing (Bio-YODIE) performance is available in Supplementary Material 3 .
Studies conducted on CRIS data of South London and Maudsley Hospital
SemEHR has been deployed on the anonymized psychiatric records database CRIS, 8 which contains a total of 18 million free-text documents from South London and Maudsley Hospital, one of Europe's largest mental health providers (serving 1.2 million residents). In the CRIS clinical notes, SemEHR identified 46 million mentions of concepts, the predominant ones being pharmacologic substances (16 million), mental or behavioral dysfunction (12 million), and sign or symptom (3.8 million). In a CRIS-based liver disease study, SemEHR identified (in the context of an information retrieval task) 94 instances out of 100 hepatitis C–positive patients that were manually annotated (based on structured blood test data). In an HIV study, a random 1000-patient cohort was selected, and SemEHR identified 21 out of 23 true positive (verifiable via structured blood test data) HIV patients using 2 search concepts, HIV Pos (UMLS code: C0019699) (20 true positives) and HIV diagnosis (UMLS code: C0920550) (8 true positives). SemEHR integrates document-level NLP annotations at the patient level to generate an integral view of patients. Table 1 presents the results of 2 experiments designed to evaluate the effectiveness of such integration on 2 case studies, hepatitis C and HIV. The results show that the number of positive mentions of diseases at the patient level is a good feature for supervised learning methods (naive Bayes or decision table) to classify whether a patient suffers from a disease or not. (The results reported in Table 1 are of a classification task, which is different from the previous information retrieval task.)
Table 1.
Given a disease (identified by one or more UMLS concepts, ie, search concepts), SemEHR can generate a summary table for a cohort of patients, which, for each patient, gives the number of positive mentions of the search concepts within all of his/her EHR documents. Using this number as the only feature, we classify whether a patient suffers from a disease or not.
 
Evaluation of a generalizable approach to clinical information retrieval using the automated retrieval console (ARC)
Leonard W D'Avolio
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
2
Center for Surgery and Public Health, Brigham and Women's Hospital, Boston, Massachusetts, USA
3
Division of Ageing, Department of Medicine, Brigham and Women's Hospital and Harvard Medical School, Boston, Massachusetts, USA
Correspondence to Dr Leonard W D'Avolio, 150 S Huntington Ave, MAVERIC (151 MAV), VA Boston Healthcare System, Jamaica Plain, MA 02130, USA; leonard.davolio@va.gov
Search for other works by this author on:
Thien M Nguyen
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
Search for other works by this author on:
Wildon R Farwell
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
3
Division of Ageing, Department of Medicine, Brigham and Women's Hospital and Harvard Medical School, Boston, Massachusetts, USA
4
Department of Medicine, VA Boston Healthcare System, Boston, Massachusetts, USA
Search for other works by this author on:
Yongming Chen
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
Search for other works by this author on:
Felicia Fitzmeyer
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
Search for other works by this author on:
Owen M Harris
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
Search for other works by this author on:
Louis D Fiore
1
Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) Cooperative Studies Coordinating Center, VA Boston Healthcare System, Jamaica Plain, Massachusetts, USA
5
Boston University School of Public Health, Boston, Massachusetts, USA
6
Boston University School of Medicine, Boston, Massachusetts, USA
Search for other works by this author on:
Journal of the American Medical Informatics Association, Volume 17, Issue 4, 1 July 2010, Pages 375–382, https://doi.org/10.1136/jamia.2009.001412
Published:
Cite
Citation
Leonard W D'Avolio, Thien M Nguyen, Wildon R Farwell, Yongming Chen, Felicia Fitzmeyer, Owen M Harris, Louis D Fiore;  Evaluation of a generalizable approach to clinical information retrieval using the automated retrieval console (ARC), Journal of the American Medical Informatics Association, Volume 17, Issue 4, 1 July 2010, Pages 375–382, https://doi.org/10.1136/jamia.2009.001412
Download citation file:
Advanced Search
Abstract
Reducing custom software development effort is an important goal in information retrieval (IR). This study evaluated a generalizable approach involving with no custom software or rules development. The study used documents "consistent with cancer" to evaluate system performance in the domains of colorectal (CRC), prostate (PC), and lung (LC) cancer. Using an end-user-supplied reference set, the automated retrieval console (ARC) iteratively calculated performance of combinations of natural language processing-derived features and supervised classification algorithms. Training and testing involved 10-fold cross-validation for three sets of 500 documents each. Performance metrics included recall, precision, and F-measure. Annotation time for five physicians was also measured. Top performing algorithms had recall, precision, and F-measure values as follows: for CRC, 0.90, 0.92, and 0.89, respectively; for PC, 0.97, 0.95, and 0.94; and for LC, 0.76, 0.80, and 0.75. In all but one case, conditional random fields outperformed maximum entropy-based classifiers. Algorithms had good performance without custom code or rules development, but performance varied by specific application.
Natural language processing , information retrieval , machine learning , prostate cancer , lung cancer , colorectal cancer
Introduction
Electronic medical record (EMR) data are becoming increasingly important for quality improvement, 1 comparative effectiveness research, 2 evidence-based medicine, 3 and establishing robust phenotypes for genomic analysis. 4 Unfortunately, most EMR implementations were designed to facilitate one-on-one interactions, not to support analysis of aggregated data as required by many secondary uses. 5 , 6 As a result, efforts to 'repurpose' clinical data must contend with few widely implemented data standards and large amounts of potentially useful information stored as unstructured free text.
Researchers have responded with the development and application of natural language processing (NLP), information extraction, and machine-learning algorithms—referred to here collectively as information retrieval (IR) technologies. Despite over 20 years of empirical demonstrations of capable IR performance, the complex nature of the challenge and technical barriers to entry have hindered widespread adoption and translation of clinical IR technologies. The Massachusetts Veterans Epidemiology Research and Information Center (MAVERIC) is addressing this challenge by attempting to deliver the benefits of IR technologies to non-technical end users. The automated retrieval console (ARC) is software designed to facilitate clinical IR translation by providing interfaces and workflows to automate many of the processes of clinical IR.
One process in particular that may be the most substantial barrier to adoption is the current reliance on custom software and rules or heuristic development for each individual application. In this study, we evaluate algorithms incorporated in ARC that were designed to be capable of achieving acceptable levels of performance without custom software development. We hypothesize that success in this regard will improve accessibility of IR technologies to non-technical users and afford system developers more time to focus on advancing the science and technologies of IR, rather than having to provide software as a service.
Background
The application motivating this study is the retrieval of relevant documents from EMR systems. The identification of relevant documents is a prerequisite to most secondary data uses, such as automated quality measurement, medical record-based research, cohort identification, and comparative effectiveness research. Unfortunately, queries of structured data fields such as ICD-9 codes and Current Procedural Terminology (CPT) codes for secondary data use have proven less than ideal. The questionable quality of administrative code assignments has been documented extensively since the rise of administrative code-based reimbursement, 7–10 and custom case-finding algorithms can be time consuming to develop and must be evaluated for each application. A solution to this dilemma may be provided by clinical IR technologies.
In the past two decades, clinical IR has evolved from a field with few researchers working on even fewer systems 11–13 to the release of open-source components and libraries. 14–16 More recently, researchers in the fields of computer science and linguistics have released open-source software frameworks upon which IR methods can be developed. 17 , 18 Clinical IR researchers have capitalized on these frameworks, producing modular pipelines for specific retrieval applications. 18 , 19 One such pipeline for clinical NLP is the Clinical Text Analysis and Knowledge Extraction System (cTAKES). 20 The cTAKES maps free text to SNOMED concepts and is based on the open-source Unstructured Information Management Architecture (UIMA). 17
Many approaches to clinical IR use open-source implementations of machine-learning classifiers to achieve high levels of performance. 21 , 22 Two supervised machine-learning classifiers used in this study are maximum entropy (MaxEnt) and conditional random fields (CRFs). MaxEnt is a framework for estimating probability distributions from a set of training data. 23 Maximum entropy models have been used in NLP to chunk phrases, 24 for part-of-speech tagging, 25 and in a number of biomedical applications. 26–28 A CRF is an undirected graphical model with edges representing dependencies between variables. 29 Peng and McCallum 30 showed that CRFs outperform the more commonly used support vector machines in extracting common fields from the headers and citations of literature. Wellner et al 21 showed the ability of CRFs to achieve high levels of performance in the deidentification of personal health identifiers, limiting customization to manual annotation of training sets.
Automated IR approaches have proven capable of high levels of performance across a number of applications, as evidenced by the results of 10 years of the Message Understanding Conferences (MUCs), 31 more than 15 years of the Text REtrieval Conference (TREC), 32 and in the clinical domain, three i2b2 'shared task' challenges. 33–35 Despite empirical evidence of its potential, widespread adoption of clinical IR remains elusive. A small number of systems have proven capable of migrating beyond empirical evaluation to actual implementation. Fewer have been adopted beyond the home institution of their developers, 36–40 and we know of no clinical IR systems that can be applied for different retrieval applications without custom software or rules development.
Methods
Design of ARC
Current use of clinical IR technologies is heavily dependent on the system developer. With ARC, we are attempting to either automate or shift to the end user as many of the processes of clinical IR as possible. Figure 1 shows the current processes of clinical IR versus the proposed shift in responsibilities we are attempting to achieve with ARC.
Figure 1
Current processes of clinical information retrieval (IR) versus those proposed in the design of the automated retrieval console (ARC).
Figure 1
View large Download slide
Current processes of clinical information retrieval (IR) versus those proposed in the design of the automated retrieval console (ARC).
The ARC design is based on the hypothesis that supervised machine learning with robust enough feature sets is capable of delivering acceptable performance across a number of clinical IR applications. This approach allows us to reduce end-user input to a reference set that can be used as both the training and test sets for any one application. Proceeding with this hypothesis, the challenge becomes how best to enable the end user to perform the remaining processes of clinical IR use, including annotation, training versus test set partitioning, performance calculation, storage of models and results, and deployment on the larger corpus.
Toward this end, ARC features several interfaces to enable greater end-user control over the processes of clinical IR. The ARC menu from which each of the interfaces is launched is shown in figure 2 .
Figure 2
Automated retrieval console (ARC) menu, showing the various ARC interfaces.
Figure 2
View large Download slide
Automated retrieval console (ARC) menu, showing the various ARC interfaces.
The 'Create New Project' interface is used to establish a workspace and import samples. This workspace is used to save the state of any project including models and performance results across the various interfaces. Annotation can be a bottleneck in applying IR technologies. The 'Judge' interface shown in figure 3 was therefore designed to be simple and fast, featuring one click and shortcut key labeling ('Y', 'N') and document advancement (left arrow, right arrow). The reference set created in the Judge interface is saved to the workspace and used for model creation and performance calculations. The 'Kappa' interface supports the calculation inter-rater reliability by presenting totals of agreement among judges that can be exported to statistical packages. The 'Feature Blast' interface iteratively calculates the performance (ie, recall, precision, F-measure) of different combinations of feature types and classifiers to determine appropriate combinations for a given application. The 'Laboratory' interface enables developers to explore and evaluate different approaches to IR. Developers can use the Laboratory interface to select which feature types and models to experiment with, tracking the performance of each combination. The 'Retrieve' interface shows the performance of all models created as part of a project and facilitates deployment of saved models on larger collections.
Figure 3
A screen shot of the Judge interface. The annotation instructions shown in the 'Help Information' window is populated as part of the creation of a new project.
Figure 3
View large Download slide
A screen shot of the Judge interface. The annotation instructions shown in the 'Help Information' window is populated as part of the creation of a new project.
The ARC was used to manage all of the processes involved in this study from sample creation to algorithm evaluation. It was developed in Java and is available as open-source software at http://research.maveric.org/mig/arc.html . Users can download ARC or, thanks to the generous cooperation of the National Library of Medicine and Dr Guergana Savova, users can download a 'full' version of ARC with cTAKES and its UMLS-based knowledge base installed. The site also features html and video tutorials designed to use a small collection of simulated radiology reports.
Approach
The focus of this study was the evaluation of the algorithms used within the Feature Blast interface to retrieve relevant documents across a number of different applications with no custom software development. Building on the collection of currently available open-source clinical IR software, ARC combines open-source NLP pipelines with machine learning.
The ARC uses UIMA-based pipelines for NLP. The UIMA pipelines can be launched to process text from within ARC, or complete UIMA project files can be loaded into ARC. Each pipeline created in UIMA has an XML-formatted configuration file that describes the structured output the pipeline produces. The ARC reads the XML configuration file and exposes NLP-structured output as feature types for machine learning classification. As a result, any UIMA-based pipeline can be used by ARC. However, the goal of this study is to design and evaluate the ability of our approach to perform well across different applications with no custom code or rules development. We therefore chose cTAKES, a general concept-mapping clinical pipeline. 20 The transforms performed on clinical data using cTAKES result in more than 90 different types of structured output (eg, noun phrases, tokens, sentences, SNOMED codes).
The version of cTAKES available for this study uses a section boundary detector that is based on the HL7 Clinical Document Architecture (CDA), which is not widely implemented by the VA Healthcare System. Therefore one minor modification made to cTAKES was the removal of the CDA-based section boundary detector and the addition of a regular expression-based section boundary detector. The ability to make such modifications easily is a function of the modular design of open-source NLP frameworks such as UIMA and GATE. An abbreviated list of some of the structured results produced by cTAKES is provided in table 1 .
Table 1
Abbreviated list of cTAKES structured output
UMLS concept (CUI) 
UMLS concept semantic type (TUI) 
Word token 
Canonical form of word token 
Range expression 
UMLS concept semantic type (TUI) 
Word token 
Canonical form of word token 
Range expression 
Abbreviated list of cTAKES structured output
UMLS concept (CUI) 
UMLS concept semantic type (TUI) 
Word token 
Canonical form of word token 
Range expression 
UMLS concept semantic type (TUI) 
Word token 
Canonical form of word token 
Range expression 
View Large
For supervised machine learning, ARC integrates the open-source Application Programming Initiative (API) exposed by the MAchine Learning for Language Toolkit (MALLET). 41 In this study, two particular classifiers from MALLET are used: a MaxEnt classifier and a classifier based on CRFs.
The ability of ARC to reduce developer involvement in the clinical IR process is predicated on the capacity of the system to 'learn' effective approaches to solving a given IR problem. After a user provides ARC with a reference set, ARC's Feature Blast algorithm uses the following steps to identify which types of NLP output and machine-learning classifiers to combine for a given application. Firstly, it processes text documents with the cTAKES NLP pipeline, exposing more than 90 NLP-derived feature types (eg, noun phrases, tokens, SNOMED concepts) for supervised classification. Using 10-fold cross-validation, the system partitions both the training and test sets and calculates the performance of each individual NLP-produced feature type using all available machine-learning classifiers. The performance of each of the individual feature types and classifier combinations is stored to the workspace.
The optimal combination of feature types and classification algorithms could be determined by calculating all possible variations. However, with greater than 90 different feature types and two classifiers, the cost in time would be prohibitive. Instead, we explored the performance of two different algorithms designed to identify favorable combinations more efficiently. The two algorithms used to determine those combinations are described below.
1. Algorithm 1: top scoring combinations
The first algorithm used by Feature Blast to determine optimal combinations evaluates all combinations of the five top scoring feature types or classes (eg, noun phrases, concepts) using either selected or all available classification algorithms. Algorithm 1 reduced the process to a manageable 52 iterations (26 combinations of feature types multiplied by two classifiers). The five top scoring feature types are defined as:
Configuration 
Highest recall not already included 
5 
Highest precision not already included 
Configuration 
Highest recall not already included 
5 
Highest recall not already included 
5 
Highest precision not already included 
Configuration 
Highest recall not already included 
5 
View Large
2. Algorithm 2: top score + negation
A limitation of the first algorithm is its exclusion of feature types that score poorly as the only feature types in consideration but may add value as part of a combination of feature types. The one feature type that most obviously falls into this category is negated concepts or phrases. For example, in classifying imaging reports consistent with cancer, evidence of negated concepts (eg, 'no evidence of cancer') may add value. The cTAKES assigns negation to both named entities and UMLS concept unique identifiers (CUIs). A named entity is an atomic element or 'thing' found in the text, usually mapped from a noun phrase (eg, 'heart attack'). Several different named entities can mean the same thing (eg, heart attack, myocardial infarction, MI), and therefore named entities are often mapped to unique concepts such as UMLS CUIs (eg, heart attack = CUI C0027051). The ARC supports the conversion of negated entities and concepts to features by allowing the user to specify a prefix or suffix to any feature type through the user interface. For example, by adding the prefix 'neg' to all negated named entities (eg, 'cancer'), ARC will pass 'neg-cancer' as a feature to the classifier. In each case, we chose the highest scoring configuration of negation, selecting either the negated named entity or the negated CUI based on the highest F-measure.
Our second algorithm, combining top scoring feature types and negation is defined as:
Configuration 
Highest recall + highest precision + negated text 
Configuration 
Highest recall + highest precision + negated text 
Configuration 
View Large
Data collection and sampling
In this study, we evaluate the ability of ARC to retrieve relevant documents from the collection of relevant and irrelevant documents returned from ICD-9 code-based queries. To test the ability of our approach to generalize across different applications, three samples and targets for retrieval were used: (1) imaging reports consistent with lung cancer; (2) pathology reports consistent with colorectal cancer (CRC); (3) pathology reports consistent with prostate cancer. For each sample, 500 documents were chosen at random from documents created between 1997 and 2007 at hospitals within the New England Veterans Integrated Service Network (VISN 1). Our original case finding queries for identifying the collections from which samples were selected were as follows.
For CRC:
Select all pathology reports within 60 days before and 60 days after the first appearance of ICD-9 codes 153.x, 154.x.
For prostate cancer:
Select all pathology reports within 60 days before and 60 days after the first appearance of ICD-9 codes 185.x.
For lung cancer:
Select all imaging reports within 60 days before and 60 days after the first appearance of ICD-9 codes 162.x.
We considered only the first appearance of a targeted ICD-9 code, regardless of assignment position (primary code, secondary code, etc). These samples were used to create 'gold standard' reference sets for both training and testing the algorithms.
Creation of reference sets
For each of the three samples, two physician judges assigned values of 'relevant' or 'irrelevant' to each of the 500 documents. A third physician judge served as final adjudicator for any disagreements. A total of five physicians participated in the creation of the three reference sets. Reviewers were instructed to base their assessment of relevance on whether each document was 'consistent with a diagnosis of cancer.' They were instructed to ignore any clinical history and instead focus on the immediate report of the pathologist or radiologist. In-situ cancers in the colon or rectum were counted as CRC, and prostate intraepithelial neoplasia was counted as prostate cancer. For CRC and prostate cancer, even if the subject of the report was tissue outside of the organ of interest, if the pathologist recorded CRC or prostate cancer, the reviewers were instructed to classify the document as consistent with the particular cancer of interest.
Whereas the pathology report is the primary document for recording a diagnosis of prostate cancer and CRC, imaging reports are less likely to contain conclusive evidence of a lung cancer diagnosis. Instead, lung cancer diagnoses may be determined by a combination of imaging studies, biopsies, and/or laboratory results. Despite the potential inconclusiveness of imaging reports for lung cancer, they are considered important documents for finding lung cancer cases and monitoring cancer progression. They also provide the opportunity to test the performance of our approach on a sample of documents with less structure and with less agreement between judges. The imaging reports in this study were generated from a number of study modalities including x-rays, CT scans, and MRI.
Study design
In order to evaluate the effectiveness of the proposed approach, we captured the performance of individual feature types and both classifiers for all three samples as well as the performance of algorithms 1 and 2 using both classifiers. In all experiments, performance was measured in terms of recall, precision, and F-measure using 10-fold cross-validation. The performance of the NLP system has a direct effect on the quality of the features produced for classification. However, the focus of this study does not include a specific evaluation of cTAKEs' performance on the samples used. Figure 4 illustrates the design of the study.
Figure 4
A graphical representation of the study design. CRC, colorectal cancer; CRF, conditional random field; MaxEnt, maximum entropy; NLP, natural language processing.
Figure 4
A graphical representation of the study design. CRC, colorectal cancer; CRF, conditional random field; MaxEnt, maximum entropy; NLP, natural language processing.
Results
The percentage of documents in the samples found to be consistent with CRC, prostate cancer, and lung cancer by the judges was 16.6%, 18.8%, and 28.6%, respectively. Reference set creation and distribution information, including annotation time, kappa scores, self-reported time to annotate 500 documents, and the number of documents adjudicated by a third judge is provided in table 2 .
Table 2
Reference sets for colorectal cancer (CRC), prostate cancer, and lung cancer samples
CRC (n=500) 
Estimated time to annotate (min) 
90 
Estimated time to annotate (min) 
60 
Estimated time to annotate (min) 
60 
Estimated time to annotate (min) 
90 
Estimated time to annotate (min) 
60 
Estimated time to annotate (min) 
60 
Reference sets for colorectal cancer (CRC), prostate cancer, and lung cancer samples
CRC (n=500) 
Estimated time to annotate (min) 
90 
Estimated time to annotate (min) 
60 
Estimated time to annotate (min) 
60 
Estimated time to annotate (min) 
90 
Estimated time to annotate (min) 
60 
Estimated time to annotate (min) 
60 
The top recall, precision, and F-measure for each sample and the classifier/feature type combinations with which they were achieved are shown in table 3 .
Table 3
Top scoring combinations for each sample
Cancer 
Algorithm 1 (token+named entity) 
   F-measure 
Algorithm 1 (token+named entity) 
 
Algorithm 2 (CUI+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 1 (canonical+token+negated named entity) 
   F-measure 
Algorithm 1 (token+named entity) 
   F-measure 
Algorithm 1 (token+named entity) 
 
Algorithm 2 (CUI+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 1 (canonical+token+negated named entity) 
   F-measure 
CRFs 
Algorithm 1 (canonical+measurement) 
CRF, conditional random field; MaxEnt, maximum entropy; CUI, concept unique identifier; neg. CUI, negated concept unique identifier; canonical, canonical form of a word.
Top scoring combinations for each sample
Cancer 
Algorithm 1 (token+named entity) 
   F-measure 
Algorithm 1 (token+named entity) 
 
Algorithm 2 (CUI+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 1 (canonical+token+negated named entity) 
   F-measure 
Algorithm 1 (token+named entity) 
   F-measure 
Algorithm 1 (token+named entity) 
 
Algorithm 2 (CUI+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 2 (token+neg. CUI)
 
Algorithm 1 (canonical+token+negated named entity) 
   F-measure 
CRFs 
Algorithm 1 (canonical+measurement) 
CRF, conditional random field; MaxEnt, maximum entropy; CUI, concept unique identifier; neg. CUI, negated concept unique identifier; canonical, canonical form of a word.
View Large
A total of 98 different types of structured output were produced by cTAKES. In all cases except in the precision of lung cancer document retrieval, CRFs outperformed MaxEnt. In most cases, the canonical form of word tokens, named entities, and CUIs were among the top scoring feature types. The top scoring feature types varied depending on the application and, in some cases, the classification algorithm used. For example, using MaxEnt to classify prostate cancer pathology reports, named entities were the top scoring feature type in recall, precision, and F-measure. However, named entities scored second in recall, fourth in precision, and fourth in F-measure for the same application using CRFs as a classifier. Certain feature types scored strongly in either recall or precision (eg, recall of CUIs for prostate cancer reports), suggesting that their inclusion in a model may be advantageous, depending on the clinical use-case.
Algorithm 1, which combined top scoring feature types (eg, CUI + noun phrases), matched or outperformed classification attempts using individual feature types (eg, CUIs) in all cases but one. For example, algorithm 1 achieved an improvement in F-measure of approximately three points compared with the top scoring individual feature in CRC classification (0.89 vs 0.86). The exception was the recall performance of the individual feature CUIs in classifying CRC (0.90 vs 0.88). Algorithm 1 also promoted negated named entities into consideration, resulting in the top precision score for all attempts at lung cancer document identification.
The addition of negation in algorithm 2 had an adverse effect on performance in some cases. For example, the recall of CRC reports using CRFs experienced a greater than two point drop when CUI was combined with negated CUIs. A three point drop in F-measure was experienced with the addition of negated CUIs to CUIs for the same CRC sample using MaxEnt (0.85 to 0.82). The few gains realized from the addition of negation were minimal.
Discussion
Overall performance in context
The assessment of what is considered acceptable performance is dependent on the intended secondary use of the data. That said, we see promise in the ability to create, evaluate, and deploy clinical IR across different applications at the performance levels achieved in a matter of hours rather than days or weeks. For the retrieval of CRC and prostate cancer reports consistent with cancer, the proposed approach was able to identify cases with F-measures of greater than 0.88 and 0.93, despite a collection with relatively few true positives to train on (83 for CRC; 94 for prostate). Classification of radiology reports consistent with lung cancer proved to be more challenging to both our algorithms and our physician judges, as indicated by the inter-rater reliability among the physician judges (κ=0.73). An F-measure of 0.75 is not unexpected in light of the disagreement among the physician judges. A more appropriate approach to imaging report classification may be the inclusion of a third class to represent 'not enough information.'
The performance degradation resulting from the inclusion of negated named entities and CUIs may indicate that negation is not a valuable contribution to such classification applications. It may also be due to poor performance of the NegEx-based negation detector included in cTAKES. The version of cTAKES used in this study uses an older version of NegEx, which has since been improved upon. A cursory review of cases did not indicate poor performance of the negation module. However, a thorough analysis of the performance of the individual feature types is an important topic for future investigation.
By focusing on streamlining processes through the development of generalizable algorithms, we do not anticipate best possible performance for all applications. Instead, we expect to sacrifice some performance that might otherwise be realized through code customization in exchange for the ability to move from one application to another with manual annotation as the only requisite input. While our focus in this study is the evaluation of solutions that require no custom code, ARC incorporates the structured output of NLP. Therefore the results of any custom code written for inclusion in a UIMA pipeline can be used as features for classification by ARC. For example, in a follow-up experiment, we incorporated a lymph node annotator component from IBM's open-source UIMA-based MedKAT pipeline 42 and realized an approximately 0.1 point improvement in recall, precision, and F-measure for classifying prostate cancer cases using MaxEnt.
The development of a one-click annotation interface helped keep annotation times for all five participating doctors between 60 and 90 min for 500 document samples. The reduction of annotation time from 90 to 60 min for the one physician annotating multiple samples indicates some benefit from familiarity. Total processing time per sample, including generating NLP-derived features and calculating iteration performance, was approximately 1.5 hours. All models created are serialized by ARC and can be deployed on other collections using the Retrieve interface. Maintaining short annotation times will be more challenging as we shift from document-level to concept-level IR.
No single 'best feature' or 'best model' for clinical IR
There was a trend toward strong performance of individual feature types such as tokens or their canonical form. This reinforces the findings of Salton and others decades ago who showed the power of simple tokens as features for document retrieval. 43 , 44 However, the results also showed that different feature types, different feature type combinations, and different classification algorithms performed best depending on the application. Some unexpected feature types proved valuable for achieving top classification scores. For example, canonical form + punctuation or measurement annotation was an unexpected combination that scored the highest precision for lung cancer retrieval. Also unexpected was the one case in which MaxEnt outperformed CRFs after consistently performing several percentage points lower in most other applications. This variation occurred despite the similar nature of the application and, in the case of prostate cancer and CRC classification, similar document types. These findings imply that there is no optimal configuration for all clinical IR applications and offers support for our attempt to learn favorable combinations from multiple feature types and classifiers.
Benefits of open-source clinical IR
The approach to clinical IR explored in this study capitalizes on the efforts of those that have previously developed and released open-source IR software. As a result of packages such as MALLET, UIMA, and cTAKES, we were able to focus on improving the processes involved in clinical IR and produce an open-source product in the relatively short span of six months. We expect that ARC will continue to benefit from the model of open-source software development. As new NLP components, pipelines, or machine-learning classifiers are released, they can be easily incorporated, extending their advantages to ARC users. Similarly, we hope that others will find ways to improve the processes currently exposed by ARC.
Quality of administrative code assignment
The focus of this study is the evaluation of algorithms that we hypothesize can be used as part of an effort to streamline the processes of clinical IR to lower the cost of adopting this important technology. The questionable quality of ICD-9 code assignment and the challenges it presents to secondary data use provoked the choice of this particular clinical IR use-case. While this study was not designed to answer questions pertaining to the quality of ICD-9 code assignment, we did not expect true positive rates of only 17–29% based on the case-finding technique used.
Concerned that we had an error in our ICD-9 code-based case-finding algorithm, we conducted reviews of 30 randomly selected false positives in each of the three samples for a total of 90 reports. The reviews showed that many of the false positives were reports related to the appropriate anatomy but without evidence of a cancer of interest (lung 43%, CRC 30%, prostate 1%). Dermatological analyses (skin lesions, biopsies, etc) comprised 30% of the total false-positive pathology reports. In many cases, the reports were focused on anatomy within close proximity of the anatomy of interest (eg, 23% of prostate assignments were for colorectal anatomy). In some false positives, the reports indicated a prior history of cancer. As a result, these numbers do not indicate that only 17–29% of the patients with the targeted ICD-9 codes ever had cancer. Instead, the numbers indicate that 17–29% of pathology or imaging reports appearing within 120 days of cancer-related ICD-9 code assignment were consistent with cancer. The low rates of true positives does emphasize the need for careful consideration of the quality of electronic medical data in light of the growing number of proposed secondary uses.
Conclusion
We theorize that greater adoption and translation of clinical IR can be achieved by reducing several of the dependencies of clinical IR on IR researchers and system developers. This study is a first step toward streamlining the processes of clinical IR in an effort to facilitate translation. In the process we achieved encouraging levels of performance with minimal time between applications and with no custom code or rules development. Our results show that the performance of various combinations of feature types and even classification algorithms is contingent on the application, supporting the potential of our approach.
There are limitations to this overall approach and the specific study conducted. Firstly, this study was an evaluation of technical feasibility, with performance measured in terms of recall, precision, and F-measure. Our goal of increased translation of clinical IR technology is not only dependent on performance in terms of system accuracy but also on usability. This study does not measure that critical aspect of system design. In addition, while document retrieval is an important prerequisite of most efforts at secondary data use, ARC will remain of limited utility until it is extended to perform concept-level IR (eg, retrieval of tumor stage from pathology reports).
Having explored the potential of an approach to document retrieval without custom code or rules development, we are in the process of extending ARC to address both concept-level and patient-level IR. This requires a rethinking of the document- and concept-oriented data structures and workflows of current IR to allow patient-level inference. A significant challenge will be providing such robust functionality while maintaining our emphasis on delivering the capabilities of IR to non-technical end users. Future work will also include the incorporation of alternative approaches for optimal feature type selection and the addition of other proven classifiers such as support vector machines.
Acknowledgments
We thank Guergana Savova, PhD and James Masanz of the Mayo Clinic as well as David Mimno and Fernando Pereira, PhD of the University of Massachusetts for their assistance in incorporating the open-source tools cTAKES and MALLET. We thank Jan Willis and the National Library of Medicine for working with us to make the UMLS available with ARC. We would also like to acknowledge the dedicated staff of MAVERIC for their assistance in this project.
Funding
This work was supported by VA Cooperative Studies Program as well as the Veterans Affairs Health Services Research and Development grant, Consortium for Health Informatics Research (CHIR), grant HIR 09-007. Other funders: VA Cooperative Studies Program; Veterans Affairs Health Services Research and Development; Consortium for Health Informatics Research. The views expressed here are those of the authors, and not necessarily those of the Department of Veterans Affairs.
Competing interests
Review and evaluation of electronic health records-driven phenotype algorithm authoring tools for clinical and translational research
Jie Xu
Department of Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA
Search for other works by this author on:
Department of Preventive Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA
Search for other works by this author on:
Pamela L Shaw
3
Galter Health Science Library, Clinical and Translational Sciences Institute (NUCATS), Feinberg School of Medicine, Northwestern University, Chicago, IL, USA
Search for other works by this author on:
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA
Search for other works by this author on:
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA
Search for other works by this author on:
Department of Biomedical Informatics, School of Medicine, Vanderbilt University, Nashville, TN, USA
Search for other works by this author on:
Center for Genetic Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA
Search for other works by this author on:
Department of Biomedical Informatics, School of Medicine, Vanderbilt University, Nashville, TN, USA
Search for other works by this author on:
Department of Information Systems, University of Maryland, Baltimore County (UMBC), Baltimore, MD, USA
Search for other works by this author on:
Department of Biomedical Informatics, School of Medicine, Vanderbilt University, Nashville, TN, USA
Search for other works by this author on:
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA
Search for other works by this author on:
Center for Biomedical Research Informatics, NorthShore University Health System, Evanston, IL, USA
Search for other works by this author on:
Department of Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA
Search for other works by this author on:
Journal of the American Medical Informatics Association, Volume 22, Issue 6, 1 November 2015, Pages 1251–1260, https://doi.org/10.1093/jamia/ocv070
Published:
Cite
Citation
Jie Xu, Luke V Rasmussen, Pamela L Shaw, Guoqian Jiang, Richard C Kiefer, Huan Mo, Jennifer A Pacheco, Peter Speltz, Qian Zhu, Joshua C Denny, Jyotishman Pathak, William K Thompson, Enid Montague;  Review and evaluation of electronic health records-driven phenotype algorithm authoring tools for clinical and translational research, Journal of the American Medical Informatics Association, Volume 22, Issue 6, 1 November 2015, Pages 1251–1260, https://doi.org/10.1093/jamia/ocv070
Download citation file:
Advanced Search
Abstract
Objective To review and evaluate available software tools for electronic health record–driven phenotype authoring in order to identify gaps and needs for future development.
Materials and Methods Candidate phenotype authoring tools were identified through (1) literature search in four publication databases (PubMed, Embase, Web of Science, and Scopus) and (2) a web search. A collection of tools was compiled and reviewed after the searches. A survey was designed and distributed to the developers of the reviewed tools to discover their functionalities and features.
Results Twenty-four different phenotype authoring tools were identified and reviewed. Developers of 16 of these identified tools completed the evaluation survey (67% response rate). The surveyed tools showed commonalities but also varied in their capabilities in algorithm representation, logic functions, data support and software extensibility, search functions, user interface, and data outputs.
Discussion Positive trends identified in the evaluation included: algorithms can be represented in both computable and human readable formats; and most tools offer a web interface for easy access. However, issues were also identified: many tools were lacking advanced logic functions for authoring complex algorithms; the ability to construct queries that leveraged un-structured data was not widely implemented; and many tools had limited support for plug-ins or external analytic software.
Conclusions Existing phenotype authoring tools could enable clinical researchers to work with electronic health record data more efficiently, but gaps still exist in terms of the functionalities of such tools. The present work can serve as a reference point for the future development of similar tools.
phenotyping , electronic health records , phenotype algorithm authoring tool , clinical research , review
INTRODUCTION
The widespread adoption of electronic health record (EHR) systems offers considerable potential for secondary use of clinical data, 1–3 especially for clinical research. 4 , 5 For example, as individual genetic variants often have weak correlations to complex diseases, 6 large sample sizes are needed for genome-wide association studies in order to obtain significant results. 7–9 The cost of assessing and identifying patients with a given disease or characteristic (a process referred to here as "phenotyping") in a large number of patients is very high. 4 , 10 However, the use of the data in EHR systems for such research could be a cost-effective solution. 4 , 11 As the EHR captures data in the delivery of care, researchers can use it to identify patient cohorts with conditions or events that are relevant to the study. 12 This can be achieved by defining study specific inclusion and exclusion criteria based on the EHR-based data fields—referred to as phenotype algorithms—and subsequently executing those algorithms on top of EHR systems. 13
However, several challenges exist when attempting to use EHRs for scalable phenotyping. First, the use of EHR data requires complex processing because EHR is a result of clinical practices and operations, so that it is in general multi-dimensional and temporal, 14 , 15 and contains different data types. 16 The different types of data in EHRs, such as diagnostic codes, laboratory results, and clinical notes, have varied availability; they may come in the form of structured data, semi-structured data, or un-structured data, and the same data may be collected in different formats across organizations, or even between different clinical specialties in the same organization. 16 Second, EHRs are typically optimized for data on single patients but not for the aggregation across cohorts of patients, thus the specification of queries can be challenging. 11 , 17 Finally, EHRs usually contain a high volume of data points, increasing their complexity. 15 , 18 As a result of these challenges, it is necessary to have knowledge of how the data are structured and represented in order to accurately formulate queries that define accurate phenotypes. 19 , 20 Furthermore, additional experience is needed to create a phenotype definition that is portable across multiple institutions with different EHR systems.
Clinical researchers often rely on expert database analysts to perform queries in order to identify patient cohorts according to their needs. This can be a time-consuming, error-prone, and inflexible process. 21 , 22 As discussed by Zhang et al., 23 the traditional model of phenotype extraction involves a data analyst who mediates between the clinical researcher and the clinical database. The clinical researcher has to communicate the phenotype algorithm—typically in human readable pseudo-code—to the data analyst who then translates it into a computable form. The clinical researcher and the data analyst may need to go through multiple cycles of communication in order for the request to be correctly translated. Miscommunication can lead to mismatch errors between the data analyst's computable algorithm and the researcher's desired algorithm. In addition, scarcity of data analyst resources may result in research bottlenecks as requests from clinical researchers grow in complexity and volume. 24 One of the solutions to this problem is to design an intuitive phenotype algorithm authoring tool so that clinical researchers can directly define the algorithm criteria unambiguously, preferably using the same data elements that are typically available within the EHR systems. Such an approach has the potential to significantly reduce the level of iterations and repeated interactions between researchers and data analysts.
The purpose of this study was to review available software tools for authoring EHR-driven phenotype algorithms and evaluate their functionalities using the current literature and feedback from the developers of these tools. By evaluating state-of-the-art tools, this study aimed to identify the gaps in phenotyping workflow support and provide insights to improve the throughput of this process. Identifying and rectifying these gaps will ultimately be necessary to facilitate wide-scale adoption of phenotyping authoring and execution tools, thus enabling clinical researchers to work more productively with data analysts, and also directly with EHR data.
METHOD
Phenotyping tools identification
The overall strategy involved two stages: (1) literature review to identify the existing tools and their features and (2) survey developers of existing tools to confirm our assessment of tool capabilities.
Literature search strategy
Online database searches were performed between April and May 2014, for relevant articles. In order to discover as many relevant tools as possible, a set of broad search terms was used. These search terms included: "electronic medical records," "electronic health records," "EHR," "Medical Records Systems, Computerized," "clinical research," "translational research," "graphical," "visual," "interface," "query," "platform," and combinations of these terms. The term "phenotyping" was not used because its vagueness may have limited the comprehensiveness of the search. 25 Initial searches were conducted using databases including PubMed, Embase, Web of Science, and Scopus. A follow-up search was conducted using Google Scholar to identify relevant papers in the reference lists. Google was also searched using the same terms, limiting to the website domains of.edu or.org.
Results of each academic literature database search were recorded and saved to an EndNote library. Search results were reviewed by one of the authors (PS) and the citation information of those results meeting inclusion criteria was entered into a spreadsheet. For every result entered into the spreadsheet, an additional Google search was conducted for the specific tool in attempt to discover if a user interface or project description of the tool were available. Figure 1 shows the literature search workflow.
Figure 1:
Flow diagram of the literature search.
Figure 1:
Flow diagram of the literature search.
Figure 2:
Percentages of phenotype authoring tools that support the corresponding algorithm representation features.
Figure 2:
Percentages of phenotype authoring tools that support the corresponding algorithm representation features.
Inclusion and exclusion criteria
The inclusion criteria included: (1) the tool provides a query function for users to identify patient cohorts; (2) the tool does not require users to use a programming language or a database-specific query language (e.g., SQL) to author a query; (3) the tool works with EHRs or a database that is fully or partially derived from EHRs; (4) the tool is an academic application rather than a commercially available software; and (5) the publication or documentation associated with the tool was written in English. The exclusion criteria included: (1) local extensions of an application (such as SHRINE, 26 Galaxy, 27 and FURTHeR, 28 since the difference among the infrastructures of these extensions and the Informatics for Integrating Biology and the Bedside platform is minimal) and (2) generic tools that provide query building capabilities beyond the healthcare domain, such as SAS 29 and KNIME. 30
Tool evaluation survey
Survey items
The research team opted to survey the developers of the tools rather than evaluate each tool individually because (1) it was not possible for the research team to obtain access to all the identified tools and conduct thorough evaluations and (2) an evaluation based on publications or documentations may not reflect the current state of the tools.
The survey was designed based on a previously conducted phenotyping tool review and evaluation studies. 31 , 32 The survey had 30 questions in total, which were grouped into nine sections for information regarding (1) algorithm representation, (2) Boolean operator functions, (3) temporal operator functions, (4) other operation functions, (5) data support and software extensibility, (6) search functions, (7) user interface, (8) data output features, and (9) other features. Most of the questions (28 out of 30) were "yes/no" questions with an open-ended comment field. The other two questions were a multiple-choice question and an open-ended question. A description or example was provided for each question, if applicable, to clarify the meaning of the question. Table 1 shows the sections, questions, and the corresponding description/examples provided in the survey.
Table 1:
Phenotyping tools capabilities survey questions
Section 
Clarification/examples 
Algorithm representation 
The algorithm can be represented in non-programming language, such as natural languages, charts, or diagrams. (Non-programming language) 
The graphical editor can generate a visual flowchart, or natural language representation of the algorithm. 
The algorithm can be represented as computable language. (Computable language) 
The underlying definition is something that a computer can understand and execute, to return results. 
The translation between the non-computable and computable language can be done automatically by the platform. (Automatic representation translation) 
 
The system represents algorithms as relational queries. (Relational queries) 
 
Boolean operators 
Algorithms can be written to exclude entities (patients, events, etc.) that have or do not have certain properties. (Boolean operations) 
"Exclude patients with an ICD9 code of 250.01." 
The system can perform Boolean operations, including negation, on properties and combinations of other logic. (Nested Boolean logic) 
"Find all patients that are not deceased, and were seen in the past two years." 
The system supports unlimited complexity of nested Boolean logic. (Exclusion) 
Nesting of Boolean operators can go down an infinite number of levels. 
Temporal operators 
Allow you to specify the reference date to use for temporal operations. (Temporal operations) 
Can relate something to the "first occurrence," or the "date documented," etc. 
Allow you to relate to a specific date. (Relate to a date) 
"Number of A&E admission due to fever on 25 December 2013." 
Allow you to relate to arbitrary time interval. (Relate to a time interval) 
"Number of A&E admission due to fever in the past 6 months." 
Allow you to relate items occurring at the same time. (Relate co-occur items) 
"Number of patients who had a diagnosis of diabetes and were on insulin at the time of diagnosis." 
Allow you to relate items occurring before/after each other. (Relate sequential items) 
"Patients who had a FNA biopsy and later underwent surgery" 
Temporal relationships/operators can be nested at any level of the definition. (Nested temporal operators) 
"Diagnosis X at least 6 months before (Procedure Y OR Procedure Z)." 
Other operation functions 
Algorithms may include arithmetic operations, which may be nested at any level of the definition. (Arithmetic operations) 
"Has at least 6 fasting glucose lab results OR at least 10 random glucose lab results." 
Can specify what level of information/entity to relate against (events, patients, etc.) (Specify entity to relate to) 
"Find all patients with age >30, with at least one event that occurred in the past month"—relates age to patient, and occurrence date to event. 
Data support and software extensibility 
Supports any type of structured data element, in any terminology (even ad hoc). (Structured data element) 
Lists of diagnoses can be in ICD-9 or ICD-10; age may be represented as ad-hoc categories of age (i.e., 0-9, 10-19); biobank status may be an institutional value based on type of sample(s) available (i.e., blood, saliva, tissue). 
Allows defining criteria for text/unstructured data sources as part of the algorithm definition. (Unstructured data sources) 
Apply regular expressions. Find a list of CUIs within a particular section of a clinical note. 
Supports data from any EHR system. (Any EHR) 
Some platforms are designed to work with specific EHR systems, while others can be used with different EHR systems. 
Support plug-ins or external software algorithms, such as machine learning, statistical computations, or natural language processing. (Extensibility) 
Allows you to specify that the algorithm should call out to an external system to perform some additional analysis. 
Search functions 
Support searching by codes. (Codes) 
"Number of patients who are diagnosed Cerebral Hemorrhage coded 430, 442.81, 421." 
Support searching by keywords. (Keywords) 
"Number of patients who are diagnosed with 'Cerebral Hemorrhage'." 
Support advanced search. (Advanced search) 
"Code ranges/find all codes 442.*-443.0, wildcards/find terms like 'cereb* hemm*'." 
User interface 
How is the system accessed and used? (Web-based/ desktop-based/ native mobile application) 
 
Supports drag-and-drop to build the algorithm. (Drag-and-drop operation) 
 
Can export an algorithm definition in a human-readable format. (Human-readable format) 
Create/save a PDF or HTML document containing the criteria for the algorithm. 
Can export an algorithm definition in a computable format. (Computable format) 
Allow exporting the definition of an algorithm so that it can be imported into another system (or another instance of the same system). 
What is returned from the search/query? (Return from the query) 
For example, patient counts, lists of patients, any clinical data (events, labs). 
Other features 
Is your system open source? (Open source) 
 
Please list any other features that your platform offers that have not already been discussed. 
 
Clarification/examples 
Algorithm representation 
The algorithm can be represented in non-programming language, such as natural languages, charts, or diagrams. (Non-programming language) 
The graphical editor can generate a visual flowchart, or natural language representation of the algorithm. 
The algorithm can be represented as computable language. (Computable language) 
The underlying definition is something that a computer can understand and execute, to return results. 
The translation between the non-computable and computable language can be done automatically by the platform. (Automatic representation translation) 
 
The system represents algorithms as relational queries. (Relational queries) 
 
Boolean operators 
Algorithms can be written to exclude entities (patients, events, etc.) that have or do not have certain properties. (Boolean operations) 
"Exclude patients with an ICD9 code of 250.01." 
The system can perform Boolean operations, including negation, on properties and combinations of other logic. (Nested Boolean logic) 
"Find all patients that are not deceased, and were seen in the past two years." 
The system supports unlimited complexity of nested Boolean logic. (Exclusion) 
Nesting of Boolean operators can go down an infinite number of levels. 
Temporal operators 
Allow you to specify the reference date to use for temporal operations. (Temporal operations) 
Can relate something to the "first occurrence," or the "date documented," etc. 
Allow you to relate to a specific date. (Relate to a date) 
"Number of A&E admission due to fever on 25 December 2013." 
Allow you to relate to arbitrary time interval. (Relate to a time interval) 
"Number of A&E admission due to fever in the past 6 months." 
Allow you to relate items occurring at the same time. (Relate co-occur items) 
"Number of patients who had a diagnosis of diabetes and were on insulin at the time of diagnosis." 
Allow you to relate items occurring before/after each other. (Relate sequential items) 
"Patients who had a FNA biopsy and later underwent surgery" 
Temporal relationships/operators can be nested at any level of the definition. (Nested temporal operators) 
"Diagnosis X at least 6 months before (Procedure Y OR Procedure Z)." 
Other operation functions 
Algorithms may include arithmetic operations, which may be nested at any level of the definition. (Arithmetic operations) 
"Has at least 6 fasting glucose lab results OR at least 10 random glucose lab results." 
Can specify what level of information/entity to relate against (events, patients, etc.) (Specify entity to relate to) 
"Find all patients with age >30, with at least one event that occurred in the past month"—relates age to patient, and occurrence date to event. 
Data support and software extensibility 
Supports any type of structured data element, in any terminology (even ad hoc). (Structured data element) 
Lists of diagnoses can be in ICD-9 or ICD-10; age may be represented as ad-hoc categories of age (i.e., 0-9, 10-19); biobank status may be an institutional value based on type of sample(s) available (i.e., blood, saliva, tissue). 
Allows defining criteria for text/unstructured data sources as part of the algorithm definition. (Unstructured data sources) 
Apply regular expressions. Find a list of CUIs within a particular section of a clinical note. 
Supports data from any EHR system. (Any EHR) 
Some platforms are designed to work with specific EHR systems, while others can be used with different EHR systems. 
Support plug-ins or external software algorithms, such as machine learning, statistical computations, or natural language processing. (Extensibility) 
Allows you to specify that the algorithm should call out to an external system to perform some additional analysis. 
Search functions 
Support searching by codes. (Codes) 
"Number of patients who are diagnosed Cerebral Hemorrhage coded 430, 442.81, 421." 
Support searching by keywords. (Keywords) 
"Number of patients who are diagnosed with 'Cerebral Hemorrhage'." 
Support advanced search. (Advanced search) 
"Code ranges/find all codes 442.*-443.0, wildcards/find terms like 'cereb* hemm*'." 
User interface 
How is the system accessed and used? (Web-based/ desktop-based/ native mobile application) 
 
Supports drag-and-drop to build the algorithm. (Drag-and-drop operation) 
 
Can export an algorithm definition in a human-readable format. (Human-readable format) 
Create/save a PDF or HTML document containing the criteria for the algorithm. 
Can export an algorithm definition in a computable format. (Computable format) 
Allow exporting the definition of an algorithm so that it can be imported into another system (or another instance of the same system). 
What is returned from the search/query? (Return from the query) 
For example, patient counts, lists of patients, any clinical data (events, labs). 
Other features 
Is your system open source? (Open source) 
 
Please list any other features that your platform offers that have not already been discussed. 
 
Phenotyping tools capabilities survey questions
Section 
Clarification/examples 
Algorithm representation 
The algorithm can be represented in non-programming language, such as natural languages, charts, or diagrams. (Non-programming language) 
The graphical editor can generate a visual flowchart, or natural language representation of the algorithm. 
The algorithm can be represented as computable language. (Computable language) 
The underlying definition is something that a computer can understand and execute, to return results. 
The translation between the non-computable and computable language can be done automatically by the platform. (Automatic representation translation) 
 
The system represents algorithms as relational queries. (Relational queries) 
 
Boolean operators 
Algorithms can be written to exclude entities (patients, events, etc.) that have or do not have certain properties. (Boolean operations) 
"Exclude patients with an ICD9 code of 250.01." 
The system can perform Boolean operations, including negation, on properties and combinations of other logic. (Nested Boolean logic) 
"Find all patients that are not deceased, and were seen in the past two years." 
The system supports unlimited complexity of nested Boolean logic. (Exclusion) 
Nesting of Boolean operators can go down an infinite number of levels. 
Temporal operators 
Allow you to specify the reference date to use for temporal operations. (Temporal operations) 
Can relate something to the "first occurrence," or the "date documented," etc. 
Allow you to relate to a specific date. (Relate to a date) 
"Number of A&E admission due to fever on 25 December 2013." 
Allow you to relate to arbitrary time interval. (Relate to a time interval) 
"Number of A&E admission due to fever in the past 6 months." 
Allow you to relate items occurring at the same time. (Relate co-occur items) 
"Number of patients who had a diagnosis of diabetes and were on insulin at the time of diagnosis." 
Allow you to relate items occurring before/after each other. (Relate sequential items) 
"Patients who had a FNA biopsy and later underwent surgery" 
Temporal relationships/operators can be nested at any level of the definition. (Nested temporal operators) 
"Diagnosis X at least 6 months before (Procedure Y OR Procedure Z)." 
Other operation functions 
Algorithms may include arithmetic operations, which may be nested at any level of the definition. (Arithmetic operations) 
"Has at least 6 fasting glucose lab results OR at least 10 random glucose lab results." 
Can specify what level of information/entity to relate against (events, patients, etc.) (Specify entity to relate to) 
"Find all patients with age >30, with at least one event that occurred in the past month"—relates age to patient, and occurrence date to event. 
Data support and software extensibility 
Supports any type of structured data element, in any terminology (even ad hoc). (Structured data element) 
Lists of diagnoses can be in ICD-9 or ICD-10; age may be represented as ad-hoc categories of age (i.e., 0-9, 10-19); biobank status may be an institutional value based on type of sample(s) available (i.e., blood, saliva, tissue). 
Allows defining criteria for text/unstructured data sources as part of the algorithm definition. (Unstructured data sources) 
Apply regular expressions. Find a list of CUIs within a particular section of a clinical note. 
Supports data from any EHR system. (Any EHR) 
Some platforms are designed to work with specific EHR systems, while others can be used with different EHR systems. 
Support plug-ins or external software algorithms, such as machine learning, statistical computations, or natural language processing. (Extensibility) 
Allows you to specify that the algorithm should call out to an external system to perform some additional analysis. 
Search functions 
Support searching by codes. (Codes) 
"Number of patients who are diagnosed Cerebral Hemorrhage coded 430, 442.81, 421." 
Support searching by keywords. (Keywords) 
"Number of patients who are diagnosed with 'Cerebral Hemorrhage'." 
Support advanced search. (Advanced search) 
"Code ranges/find all codes 442.*-443.0, wildcards/find terms like 'cereb* hemm*'." 
User interface 
How is the system accessed and used? (Web-based/ desktop-based/ native mobile application) 
 
Supports drag-and-drop to build the algorithm. (Drag-and-drop operation) 
 
Can export an algorithm definition in a human-readable format. (Human-readable format) 
Create/save a PDF or HTML document containing the criteria for the algorithm. 
Can export an algorithm definition in a computable format. (Computable format) 
Allow exporting the definition of an algorithm so that it can be imported into another system (or another instance of the same system). 
What is returned from the search/query? (Return from the query) 
For example, patient counts, lists of patients, any clinical data (events, labs). 
Other features 
Is your system open source? (Open source) 
 
Please list any other features that your platform offers that have not already been discussed. 
 
Clarification/examples 
Algorithm representation 
The algorithm can be represented in non-programming language, such as natural languages, charts, or diagrams. (Non-programming language) 
The graphical editor can generate a visual flowchart, or natural language representation of the algorithm. 
The algorithm can be represented as computable language. (Computable language) 
The underlying definition is something that a computer can understand and execute, to return results. 
The translation between the non-computable and computable language can be done automatically by the platform. (Automatic representation translation) 
 
The system represents algorithms as relational queries. (Relational queries) 
 
Boolean operators 
Algorithms can be written to exclude entities (patients, events, etc.) that have or do not have certain properties. (Boolean operations) 
"Exclude patients with an ICD9 code of 250.01." 
The system can perform Boolean operations, including negation, on properties and combinations of other logic. (Nested Boolean logic) 
"Find all patients that are not deceased, and were seen in the past two years." 
The system supports unlimited complexity of nested Boolean logic. (Exclusion) 
Nesting of Boolean operators can go down an infinite number of levels. 
Temporal operators 
Allow you to specify the reference date to use for temporal operations. (Temporal operations) 
Can relate something to the "first occurrence," or the "date documented," etc. 
Allow you to relate to a specific date. (Relate to a date) 
"Number of A&E admission due to fever on 25 December 2013." 
Allow you to relate to arbitrary time interval. (Relate to a time interval) 
"Number of A&E admission due to fever in the past 6 months." 
Allow you to relate items occurring at the same time. (Relate co-occur items) 
"Number of patients who had a diagnosis of diabetes and were on insulin at the time of diagnosis." 
Allow you to relate items occurring before/after each other. (Relate sequential items) 
"Patients who had a FNA biopsy and later underwent surgery" 
Temporal relationships/operators can be nested at any level of the definition. (Nested temporal operators) 
"Diagnosis X at least 6 months before (Procedure Y OR Procedure Z)." 
Other operation functions 
Algorithms may include arithmetic operations, which may be nested at any level of the definition. (Arithmetic operations) 
"Has at least 6 fasting glucose lab results OR at least 10 random glucose lab results." 
Can specify what level of information/entity to relate against (events, patients, etc.) (Specify entity to relate to) 
"Find all patients with age >30, with at least one event that occurred in the past month"—relates age to patient, and occurrence date to event. 
Data support and software extensibility 
Supports any type of structured data element, in any terminology (even ad hoc). (Structured data element) 
Lists of diagnoses can be in ICD-9 or ICD-10; age may be represented as ad-hoc categories of age (i.e., 0-9, 10-19); biobank status may be an institutional value based on type of sample(s) available (i.e., blood, saliva, tissue). 
Allows defining criteria for text/unstructured data sources as part of the algorithm definition. (Unstructured data sources) 
Apply regular expressions. Find a list of CUIs within a particular section of a clinical note. 
Supports data from any EHR system. (Any EHR) 
Some platforms are designed to work with specific EHR systems, while others can be used with different EHR systems. 
Support plug-ins or external software algorithms, such as machine learning, statistical computations, or natural language processing. (Extensibility) 
Allows you to specify that the algorithm should call out to an external system to perform some additional analysis. 
Search functions 
Support searching by codes. (Codes) 
"Number of patients who are diagnosed Cerebral Hemorrhage coded 430, 442.81, 421." 
Support searching by keywords. (Keywords) 
"Number of patients who are diagnosed with 'Cerebral Hemorrhage'." 
Support advanced search. (Advanced search) 
"Code ranges/find all codes 442.*-443.0, wildcards/find terms like 'cereb* hemm*'." 
User interface 
How is the system accessed and used? (Web-based/ desktop-based/ native mobile application) 
 
Supports drag-and-drop to build the algorithm. (Drag-and-drop operation) 
 
Can export an algorithm definition in a human-readable format. (Human-readable format) 
Create/save a PDF or HTML document containing the criteria for the algorithm. 
Can export an algorithm definition in a computable format. (Computable format) 
Allow exporting the definition of an algorithm so that it can be imported into another system (or another instance of the same system). 
What is returned from the search/query? (Return from the query) 
For example, patient counts, lists of patients, any clinical data (events, labs). 
Other features 
Is your system open source? (Open source) 
 
Please list any other features that your platform offers that have not already been discussed. 
 
View Large
Survey administration
The protocol of this survey study was reviewed and approved by Northwestern University's Institutional Review Board. Potential developers of the phenotyping tools were identified through publications or official websites associated with the tools. Emails were sent to these individuals to ask if they were the developers of the tools and if they were willing to participate in the study, or if they could recommend someone else to fill out the survey if they were not the developer or not available. The survey data were collected and managed using REDCap electronic data capture tools. 33
Survey result verification
After receiving the survey results from the developers of the phenotyping tools, the research team conducted additional verifications of the survey answers using the following information associated with each of the corresponding tool: journal publications, conference proceedings, help documentations, written or video tutorials, and software trials or demos.
RESULTS
List of phenotype algorithm authoring tools
A total number of 24 phenotype algorithm authoring tools were included in this review and evaluation. Please refer to Table 2 for the list of the tools and the brief descriptions for each tool.
Table 2:
The list of the phenotyping tools identified from the literature and web search and their brief descriptions
Phenotyping tool 
Description 
Advanced Screening for Active Protocols (ASAP) 34  
ASAP is a tool designed for identifying and pre-screening patients for clinical trial eligibility based on the Ohio State University Medical Center's Information Warehouse (IW) that contains data from electronic health record (EHR) systems and billing and administrative systems. 
Biomedical Translational Research Information System and its de-identified query tool 35–37  
Biomedical Translational Research Information System is a clinical research data repository developed at the US National Institutes of Health to consolidate data from multiple databases, including EHR systems, and provide query functions for data retrieval in the dataset. 
DANBIO 38  
DANBIO is Denmark's nation-wide research database and EHR system for rheumatoid arthritis, ankylosing spondylitis, and psoriatic arthritis patients. The user can perform queries in the system to derive subset of data for research. 
DBMap 39 
DBMap is a data visualization and knowledge discovery framework implemented in the University of California, San Francisco's Brain Research Registry. Its user interface allows the users to query the database and returns the results visualized as a color-coded map. 
Duke Enterprise Data Unified Content Explorer (DEDUCE) 21 , 40  
DEDUCE is a query platform developed to support data exploration, cohort identification, and data extraction from Duke University's enterprise data warehouse, which stores clinical data from a number of hospitals and clinics of the university's health care system. 
Electronic Health Records for Clinical Research (EHR4CR) 41 , 42  
EHR4CR is a European platform aims to improve clinical research with EHRs by supporting clinical protocol feasibility, patient identification and recruitment, clinical trial execution, and adverse event reporting. For patient cohort identification, a formal query language is developed to enable queries to be executed to heterogeneous EHRs. 
electronic Primary Care Research Network (ePCRN) Research Workbench 43–45  
ePCRN is an electronic infrastructure that offers a database of clinical information and a research portal to support the conduction of randomized control trials. Its Research Workbench enables users to create queries on the EHR data to identify eligible patient cohorts for research. 
Electronic Medical Records and Genomics (eMERGE) Network and its Record Counter (eRC) 46  
eRC is a research tool designed for research planning purpose and feasibility assessment for the genotyped patients in the eMERGE subject pool. This tool supports functions for users to construct queries base on diagnosis codes. 
Eureka! Clinical Analytics 47  
Eureka! Clinical Analytics is part of the Analytic IW software system developed at Emory University that enables users to upload a data source, specify patient cohort definitions as temporal patterns, and derive the cohort matches into an instance of i2b2. 
Feasibility Assessment and Recruitment System for Improving Trial Efficiency (FARSITE) 48  
FARSITE aims to support clinical trial feasibility assessment and recruitment in the UK. Its query interface provides assessments of the size of patient cohorts returned from the user specified search criteria to assist the evaluation of clinical trial feasibility. 
Harvest 15  
Harvest is a software toolkit designed for building web-based application to perform custom query of a dataset for data discovery and reporting purpose. This toolkit is developed by the Children's Hospital of Philadelphia Research Institute and optimized for biomedical research use. 
Informatics for Integrating Biology and the Bedside (i2b2) 11 , 26  
The i2b2 platform is based on Research Patient Data Registry, which is developed in Partners HealthCare. The software allows users to perform queries on an EHR system and identify patient cohorts that fit the research criteria. A project data mart can be created for the selected patient cohorts for further processing and analysis. 
Integrative Platform for Translational Research (IPTrans) and its query tool 49  
IPTrans is the user interface level of Chado, which is a modular ontology-oriented database model and it supports the management of clinical and socio-demographic data, project management, and microarray assays and biomaterials management. It enables users to author a set of clinical or socio-demographic characteristics criteria to identify patient cohorts. 
Measure Authoring Tool (MAT) 50  
The MAT is designed to author electronic Clinical Quality Measures using the Quality Data Model (QDM), which is aligned with Meaningful Use standards. It is possible to author phenotype algorithms that are compliant with QDM. 
Multi-Modality, Multi-Resource Information Integration environment (Physio-MIMI) and Visual Aggregator and Explorer (VISAGE) 23  
VISAGE is a query interface and a component of Physio-MIMI. It provides query building, managing and exploring functionalities to assist users with hypothesis generation and patient cohort identification activities. 
Phenotype Builder 51  
Phenotype Builder is prototype software tool designed for the users to author phenotype algorithms by manipulating data elements in a graphical user interface. 
RetroGuide is an EHR query authoring system that utilizes a flowchart analytical paradigm for clinical researchers to perform queries. HealthFlow is a package that integrates RetroGuide with FlowGuide, which is a prospective version that communicates with EHR in real time. 
SemanticDB and Semantic Research Assistant (SRA) 54  
SemanticDB is a clinical research platform developed in Cleveland Clinic and it consists of three main components: a clinical data repository, a query interface, and a data analysis and reporting interface. The SRA is the query interface that allows the users to use natural language to construct queries for patient cohort identification. 
Stanford Translational Research Integrated Database Environment (STRIDE) and Anonymous Patient Cohort Discovery Tool 55  
STRIDE is an informatics platform that consists of a clinical data warehouse, a data management application development framework, and a biospecimen data management system, developed in Stanford University. The users can use the query tool called the Anonymous Patient Cohort Discovery Tool to identify potential research patient cohorts. 
Synthetic Derivative (SD) and its query tool 9  
SD is a de-identified clinical information database derived from Vanderbilt University Medical Center's EHR system and has a link to the corresponding DNA biobank (BioVU). Its query interface allows users to create queries to identify patient cohorts. 
Translational Research Platform for colorectal cancer (crcTRP) 56  
crcTRP is a software platform designed to support colorectal cancer research. It provides a solution to collect data in multiple sources, including EHRs and integrate clinical and omics data, and a web portal for data query and data visualization. 
TrialViz 57  
TrialViz is a query system that works with Clinical Practice Research Datalink database in the UK. This tool enables the users to author queries for selecting patient cohorts, examine the quality of the extracted data, and visualize the results of the queries. 
University of Virginia's (UVa) Clinical Data Repository (CDR) 58 , 59  
CDR is a data warehouse that contains data derived from multiple UVa clinical and administrative patient information systems and Virginia Department of Health. It provides a web interface for the users to conduct queries for patient cohort identification. 
Utah Population Database Limited (UPDBL) and its query tool 60  
UPDBL is a research platform at the University of Utah that includes data from multiple sources such as EHRs, vital records, driver license records, voter registration, etc. Its query tool allows users to build and run queries and view aggregated results. 
Phenotyping tool 
Description 
Advanced Screening for Active Protocols (ASAP) 34  
ASAP is a tool designed for identifying and pre-screening patients for clinical trial eligibility based on the Ohio State University Medical Center's Information Warehouse (IW) that contains data from electronic health record (EHR) systems and billing and administrative systems. 
Biomedical Translational Research Information System and its de-identified query tool 35–37  
Biomedical Translational Research Information System is a clinical research data repository developed at the US National Institutes of Health to consolidate data from multiple databases, including EHR systems, and provide query functions for data retrieval in the dataset. 
DANBIO 38  
DANBIO is Denmark's nation-wide research database and EHR system for rheumatoid arthritis, ankylosing spondylitis, and psoriatic arthritis patients. The user can perform queries in the system to derive subset of data for research. 
DBMap 39 
DBMap is a data visualization and knowledge discovery framework implemented in the University of California, San Francisco's Brain Research Registry. Its user interface allows the users to query the database and returns the results visualized as a color-coded map. 
Duke Enterprise Data Unified Content Explorer (DEDUCE) 21 , 40  
DEDUCE is a query platform developed to support data exploration, cohort identification, and data extraction from Duke University's enterprise data warehouse, which stores clinical data from a number of hospitals and clinics of the university's health care system. 
Electronic Health Records for Clinical Research (EHR4CR) 41 , 42  
EHR4CR is a European platform aims to improve clinical research with EHRs by supporting clinical protocol feasibility, patient identification and recruitment, clinical trial execution, and adverse event reporting. For patient cohort identification, a formal query language is developed to enable queries to be executed to heterogeneous EHRs. 
electronic Primary Care Research Network (ePCRN) Research Workbench 43–45  
ePCRN is an electronic infrastructure that offers a database of clinical information and a research portal to support the conduction of randomized control trials. Its Research Workbench enables users to create queries on the EHR data to identify eligible patient cohorts for research. 
Electronic Medical Records and Genomics (eMERGE) Network and its Record Counter (eRC) 46  
eRC is a research tool designed for research planning purpose and feasibility assessment for the genotyped patients in the eMERGE subject pool. This tool supports functions for users to construct queries base on diagnosis codes. 
Eureka! Clinical Analytics 47  
Eureka! Clinical Analytics is part of the Analytic IW software system developed at Emory University that enables users to upload a data source, specify patient cohort definitions as temporal patterns, and derive the cohort matches into an instance of i2b2. 
Feasibility Assessment and Recruitment System for Improving Trial Efficiency (FARSITE) 48  
FARSITE aims to support clinical trial feasibility assessment and recruitment in the UK. Its query interface provides assessments of the size of patient cohorts returned from the user specified search criteria to assist the evaluation of clinical trial feasibility. 
Harvest 15  
Harvest is a software toolkit designed for building web-based application to perform custom query of a dataset for data discovery and reporting purpose. This toolkit is developed by the Children's Hospital of Philadelphia Research Institute and optimized for biomedical research use. 
Informatics for Integrating Biology and the Bedside (i2b2) 11 , 26  
The i2b2 platform is based on Research Patient Data Registry, which is developed in Partners HealthCare. The software allows users to perform queries on an EHR system and identify patient cohorts that fit the research criteria. A project data mart can be created for the selected patient cohorts for further processing and analysis. 
Integrative Platform for Translational Research (IPTrans) and its query tool 49  
IPTrans is the user interface level of Chado, which is a modular ontology-oriented database model and it supports the management of clinical and socio-demographic data, project management, and microarray assays and biomaterials management. It enables users to author a set of clinical or socio-demographic characteristics criteria to identify patient cohorts. 
Measure Authoring Tool (MAT) 50  
The MAT is designed to author electronic Clinical Quality Measures using the Quality Data Model (QDM), which is aligned with Meaningful Use standards. It is possible to author phenotype algorithms that are compliant with QDM. 
Multi-Modality, Multi-Resource Information Integration environment (Physio-MIMI) and Visual Aggregator and Explorer (VISAGE) 23  
VISAGE is a query interface and a component of Physio-MIMI. It provides query building, managing and exploring functionalities to assist users with hypothesis generation and patient cohort identification activities. 
Phenotype Builder 51  
Phenotype Builder is prototype software tool designed for the users to author phenotype algorithms by manipulating data elements in a graphical user interface. 
RetroGuide is an EHR query authoring system that utilizes a flowchart analytical paradigm for clinical researchers to perform queries. HealthFlow is a package that integrates RetroGuide with FlowGuide, which is a prospective version that communicates with EHR in real time. 
SemanticDB and Semantic Research Assistant (SRA) 54  
SemanticDB is a clinical research platform developed in Cleveland Clinic and it consists of three main components: a clinical data repository, a query interface, and a data analysis and reporting interface. The SRA is the query interface that allows the users to use natural language to construct queries for patient cohort identification. 
Stanford Translational Research Integrated Database Environment (STRIDE) and Anonymous Patient Cohort Discovery Tool 55  
STRIDE is an informatics platform that consists of a clinical data warehouse, a data management application development framework, and a biospecimen data management system, developed in Stanford University. The users can use the query tool called the Anonymous Patient Cohort Discovery Tool to identify potential research patient cohorts. 
Synthetic Derivative (SD) and its query tool 9  
SD is a de-identified clinical information database derived from Vanderbilt University Medical Center's EHR system and has a link to the corresponding DNA biobank (BioVU). Its query interface allows users to create queries to identify patient cohorts. 
Translational Research Platform for colorectal cancer (crcTRP) 56  
crcTRP is a software platform designed to support colorectal cancer research. It provides a solution to collect data in multiple sources, including EHRs and integrate clinical and omics data, and a web portal for data query and data visualization. 
TrialViz 57  
TrialViz is a query system that works with Clinical Practice Research Datalink database in the UK. This tool enables the users to author queries for selecting patient cohorts, examine the quality of the extracted data, and visualize the results of the queries. 
University of Virginia's (UVa) Clinical Data Repository (CDR) 58 , 59  
CDR is a data warehouse that contains data derived from multiple UVa clinical and administrative patient information systems and Virginia Department of Health. It provides a web interface for the users to conduct queries for patient cohort identification. 
Utah Population Database Limited (UPDBL) and its query tool 60  
UPDBL is a research platform at the University of Utah that includes data from multiple sources such as EHRs, vital records, driver license records, voter registration, etc. Its query tool allows users to build and run queries and view aggregated results. 
Table 2:
The list of the phenotyping tools identified from the literature and web search and their brief descriptions
Phenotyping tool 
Description 
Advanced Screening for Active Protocols (ASAP) 34  
ASAP is a tool designed for identifying and pre-screening patients for clinical trial eligibility based on the Ohio State University Medical Center's Information Warehouse (IW) that contains data from electronic health record (EHR) systems and billing and administrative systems. 
Biomedical Translational Research Information System and its de-identified query tool 35–37  
Biomedical Translational Research Information System is a clinical research data repository developed at the US National Institutes of Health to consolidate data from multiple databases, including EHR systems, and provide query functions for data retrieval in the dataset. 
DANBIO 38  
DANBIO is Denmark's nation-wide research database and EHR system for rheumatoid arthritis, ankylosing spondylitis, and psoriatic arthritis patients. The user can perform queries in the system to derive subset of data for research. 
DBMap 39 
DBMap is a data visualization and knowledge discovery framework implemented in the University of California, San Francisco's Brain Research Registry. Its user interface allows the users to query the database and returns the results visualized as a color-coded map. 
Duke Enterprise Data Unified Content Explorer (DEDUCE) 21 , 40  
DEDUCE is a query platform developed to support data exploration, cohort identification, and data extraction from Duke University's enterprise data warehouse, which stores clinical data from a number of hospitals and clinics of the university's health care system. 
Electronic Health Records for Clinical Research (EHR4CR) 41 , 42  
EHR4CR is a European platform aims to improve clinical research with EHRs by supporting clinical protocol feasibility, patient identification and recruitment, clinical trial execution, and adverse event reporting. For patient cohort identification, a formal query language is developed to enable queries to be executed to heterogeneous EHRs. 
electronic Primary Care Research Network (ePCRN) Research Workbench 43–45  
ePCRN is an electronic infrastructure that offers a database of clinical information and a research portal to support the conduction of randomized control trials. Its Research Workbench enables users to create queries on the EHR data to identify eligible patient cohorts for research. 
Electronic Medical Records and Genomics (eMERGE) Network and its Record Counter (eRC) 46  
eRC is a research tool designed for research planning purpose and feasibility assessment for the genotyped patients in the eMERGE subject pool. This tool supports functions for users to construct queries base on diagnosis codes. 
Eureka! Clinical Analytics 47  
Eureka! Clinical Analytics is part of the Analytic IW software system developed at Emory University that enables users to upload a data source, specify patient cohort definitions as temporal patterns, and derive the cohort matches into an instance of i2b2. 
Feasibility Assessment and Recruitment System for Improving Trial Efficiency (FARSITE) 48  
FARSITE aims to support clinical trial feasibility assessment and recruitment in the UK. Its query interface provides assessments of the size of patient cohorts returned from the user specified search criteria to assist the evaluation of clinical trial feasibility. 
Harvest 15  
Harvest is a software toolkit designed for building web-based application to perform custom query of a dataset for data discovery and reporting purpose. This toolkit is developed by the Children's Hospital of Philadelphia Research Institute and optimized for biomedical research use. 
Informatics for Integrating Biology and the Bedside (i2b2) 11 , 26  
The i2b2 platform is based on Research Patient Data Registry, which is developed in Partners HealthCare. The software allows users to perform queries on an EHR system and identify patient cohorts that fit the research criteria. A project data mart can be created for the selected patient cohorts for further processing and analysis. 
Integrative Platform for Translational Research (IPTrans) and its query tool 49  
IPTrans is the user interface level of Chado, which is a modular ontology-oriented database model and it supports the management of clinical and socio-demographic data, project management, and microarray assays and biomaterials management. It enables users to author a set of clinical or socio-demographic characteristics criteria to identify patient cohorts. 
Measure Authoring Tool (MAT) 50  
The MAT is designed to author electronic Clinical Quality Measures using the Quality Data Model (QDM), which is aligned with Meaningful Use standards. It is possible to author phenotype algorithms that are compliant with QDM. 
Multi-Modality, Multi-Resource Information Integration environment (Physio-MIMI) and Visual Aggregator and Explorer (VISAGE) 23  
VISAGE is a query interface and a component of Physio-MIMI. It provides query building, managing and exploring functionalities to assist users with hypothesis generation and patient cohort identification activities. 
Phenotype Builder 51  
Phenotype Builder is prototype software tool designed for the users to author phenotype algorithms by manipulating data elements in a graphical user interface. 
RetroGuide is an EHR query authoring system that utilizes a flowchart analytical paradigm for clinical researchers to perform queries. HealthFlow is a package that integrates RetroGuide with FlowGuide, which is a prospective version that communicates with EHR in real time. 
SemanticDB and Semantic Research Assistant (SRA) 54  
SemanticDB is a clinical research platform developed in Cleveland Clinic and it consists of three main components: a clinical data repository, a query interface, and a data analysis and reporting interface. The SRA is the query interface that allows the users to use natural language to construct queries for patient cohort identification. 
Stanford Translational Research Integrated Database Environment (STRIDE) and Anonymous Patient Cohort Discovery Tool 55  
STRIDE is an informatics platform that consists of a clinical data warehouse, a data management application development framework, and a biospecimen data management system, developed in Stanford University. The users can use the query tool called the Anonymous Patient Cohort Discovery Tool to identify potential research patient cohorts. 
Synthetic Derivative (SD) and its query tool 9  
SD is a de-identified clinical information database derived from Vanderbilt University Medical Center's EHR system and has a link to the corresponding DNA biobank (BioVU). Its query interface allows users to create queries to identify patient cohorts. 
Translational Research Platform for colorectal cancer (crcTRP) 56  
crcTRP is a software platform designed to support colorectal cancer research. It provides a solution to collect data in multiple sources, including EHRs and integrate clinical and omics data, and a web portal for data query and data visualization. 
TrialViz 57  
TrialViz is a query system that works with Clinical Practice Research Datalink database in the UK. This tool enables the users to author queries for selecting patient cohorts, examine the quality of the extracted data, and visualize the results of the queries. 
University of Virginia's (UVa) Clinical Data Repository (CDR) 58 , 59  
CDR is a data warehouse that contains data derived from multiple UVa clinical and administrative patient information systems and Virginia Department of Health. It provides a web interface for the users to conduct queries for patient cohort identification. 
Utah Population Database Limited (UPDBL) and its query tool 60  
UPDBL is a research platform at the University of Utah that includes data from multiple sources such as EHRs, vital records, driver license records, voter registration, etc. Its query tool allows users to build and run queries and view aggregated results. 
Phenotyping tool 
Description 
Advanced Screening for Active Protocols (ASAP) 34  
ASAP is a tool designed for identifying and pre-screening patients for clinical trial eligibility based on the Ohio State University Medical Center's Information Warehouse (IW) that contains data from electronic health record (EHR) systems and billing and administrative systems. 
Biomedical Translational Research Information System and its de-identified query tool 35–37  
Biomedical Translational Research Information System is a clinical research data repository developed at the US National Institutes of Health to consolidate data from multiple databases, including EHR systems, and provide query functions for data retrieval in the dataset. 
DANBIO 38  
DANBIO is Denmark's nation-wide research database and EHR system for rheumatoid arthritis, ankylosing spondylitis, and psoriatic arthritis patients. The user can perform queries in the system to derive subset of data for research. 
DBMap 39 
DBMap is a data visualization and knowledge discovery framework implemented in the University of California, San Francisco's Brain Research Registry. Its user interface allows the users to query the database and returns the results visualized as a color-coded map. 
Duke Enterprise Data Unified Content Explorer (DEDUCE) 21 , 40  
DEDUCE is a query platform developed to support data exploration, cohort identification, and data extraction from Duke University's enterprise data warehouse, which stores clinical data from a number of hospitals and clinics of the university's health care system. 
Electronic Health Records for Clinical Research (EHR4CR) 41 , 42  
EHR4CR is a European platform aims to improve clinical research with EHRs by supporting clinical protocol feasibility, patient identification and recruitment, clinical trial execution, and adverse event reporting. For patient cohort identification, a formal query language is developed to enable queries to be executed to heterogeneous EHRs. 
electronic Primary Care Research Network (ePCRN) Research Workbench 43–45  
ePCRN is an electronic infrastructure that offers a database of clinical information and a research portal to support the conduction of randomized control trials. Its Research Workbench enables users to create queries on the EHR data to identify eligible patient cohorts for research. 
Electronic Medical Records and Genomics (eMERGE) Network and its Record Counter (eRC) 46  
eRC is a research tool designed for research planning purpose and feasibility assessment for the genotyped patients in the eMERGE subject pool. This tool supports functions for users to construct queries base on diagnosis codes. 
Eureka! Clinical Analytics 47  
Eureka! Clinical Analytics is part of the Analytic IW software system developed at Emory University that enables users to upload a data source, specify patient cohort definitions as temporal patterns, and derive the cohort matches into an instance of i2b2. 
Feasibility Assessment and Recruitment System for Improving Trial Efficiency (FARSITE) 48  
FARSITE aims to support clinical trial feasibility assessment and recruitment in the UK. Its query interface provides assessments of the size of patient cohorts returned from the user specified search criteria to assist the evaluation of clinical trial feasibility. 
Harvest 15  
Harvest is a software toolkit designed for building web-based application to perform custom query of a dataset for data discovery and reporting purpose. This toolkit is developed by the Children's Hospital of Philadelphia Research Institute and optimized for biomedical research use. 
Informatics for Integrating Biology and the Bedside (i2b2) 11 , 26  
The i2b2 platform is based on Research Patient Data Registry, which is developed in Partners HealthCare. The software allows users to perform queries on an EHR system and identify patient cohorts that fit the research criteria. A project data mart can be created for the selected patient cohorts for further processing and analysis. 
Integrative Platform for Translational Research (IPTrans) and its query tool 49  
IPTrans is the user interface level of Chado, which is a modular ontology-oriented database model and it supports the management of clinical and socio-demographic data, project management, and microarray assays and biomaterials management. It enables users to author a set of clinical or socio-demographic characteristics criteria to identify patient cohorts. 
Measure Authoring Tool (MAT) 50  
The MAT is designed to author electronic Clinical Quality Measures using the Quality Data Model (QDM), which is aligned with Meaningful Use standards. It is possible to author phenotype algorithms that are compliant with QDM. 
Multi-Modality, Multi-Resource Information Integration environment (Physio-MIMI) and Visual Aggregator and Explorer (VISAGE) 23  
VISAGE is a query interface and a component of Physio-MIMI. It provides query building, managing and exploring functionalities to assist users with hypothesis generation and patient cohort identification activities. 
Phenotype Builder 51  
Phenotype Builder is prototype software tool designed for the users to author phenotype algorithms by manipulating data elements in a graphical user interface. 
RetroGuide is an EHR query authoring system that utilizes a flowchart analytical paradigm for clinical researchers to perform queries. HealthFlow is a package that integrates RetroGuide with FlowGuide, which is a prospective version that communicates with EHR in real time. 
SemanticDB and Semantic Research Assistant (SRA) 54  
SemanticDB is a clinical research platform developed in Cleveland Clinic and it consists of three main components: a clinical data repository, a query interface, and a data analysis and reporting interface. The SRA is the query interface that allows the users to use natural language to construct queries for patient cohort identification. 
Stanford Translational Research Integrated Database Environment (STRIDE) and Anonymous Patient Cohort Discovery Tool 55  
STRIDE is an informatics platform that consists of a clinical data warehouse, a data management application development framework, and a biospecimen data management system, developed in Stanford University. The users can use the query tool called the Anonymous Patient Cohort Discovery Tool to identify potential research patient cohorts. 
Synthetic Derivative (SD) and its query tool 9  
SD is a de-identified clinical information database derived from Vanderbilt University Medical Center's EHR system and has a link to the corresponding DNA biobank (BioVU). Its query interface allows users to create queries to identify patient cohorts. 
Translational Research Platform for colorectal cancer (crcTRP) 56  
crcTRP is a software platform designed to support colorectal cancer research. It provides a solution to collect data in multiple sources, including EHRs and integrate clinical and omics data, and a web portal for data query and data visualization. 
TrialViz 57  
TrialViz is a query system that works with Clinical Practice Research Datalink database in the UK. This tool enables the users to author queries for selecting patient cohorts, examine the quality of the extracted data, and visualize the results of the queries. 
University of Virginia's (UVa) Clinical Data Repository (CDR) 58 , 59  
CDR is a data warehouse that contains data derived from multiple UVa clinical and administrative patient information systems and Virginia Department of Health. It provides a web interface for the users to conduct queries for patient cohort identification. 
Utah Population Database Limited (UPDBL) and its query tool 60  
UPDBL is a research platform at the University of Utah that includes data from multiple sources such as EHRs, vital records, driver license records, voter registration, etc. Its query tool allows users to build and run queries and view aggregated results. 
View Large
Tool evaluation survey results
In total, we received the responses from the developers of 16 out of 24 tools, with one survey response collected for each tool. The response rate accounted for 67%. The 16 tools included in the evaluation included: Advanced Screening for Active Protocols, Biomedical Translational Research Information System and its de-identified query tool, DANBIO, Duke Enterprise Data Unified Content Explorer, Electronic Medical Records and Genomics Network and its Record Counter, Harvest, Informatics for Integrating Biology and the Bedside, Integrative Platform for Translational Research and its query tool, Measure Authoring Tool, Phenotype Builder, Multi-Modality Multi-Resource Information Integration environment and Visual Aggregator and Explorer, RetroGuide/HealthFlow, Synthetic Derivative and its query tool, Stanford Translational Research Integrated Database Environment and Anonymous Patient Cohort Discovery Tool, TrialViz, and Utah Population Database Limited and its query tool. There was no missing data presented in the "yes/no" questions. Please refer to the online supplementary document ( Supplementary Data and Supplementary Data ) for the details of the evaluation for each tool against the survey items. Please refer to Figure 2–5 for the results from the "yes/no" questions. According to the responses, in the 16 tools that were evaluated, seven (44%) only support defining criteria from the EHR systems that the tool was designed to use with ("specific EHR" group in the figures); the rest of them (56%) were designed to support any EHR systems ("any EHR" group in the figures).
For the algorithm representation features, 11 out of 16 tools (69%) were reported to be able to represent the algorithm in both noncomputable (defined as a format that is optimized for review by a human, that a computer is not able to also understand) and computable formats (those that can be interpreted and executed by a computer). The rest of the tools were reported to be only able to represent the algorithm to be either noncomputable or computable.
For the data support features, five of out the 16 tools (31%) were reported to support both structured data and un-structured data.
In terms of the results returned from the execution of the algorithm, 11 tools (69%) were able to report patient counts and seven tools (44%) were able to report patient/encounter list; 9 tools (56%) were able to report some sort of summary statistics of the patient cohort, and 4 tools (25%) were able to generate detailed report according to user specification.
DISCUSSION
Algorithm representation
It was found that a high percentage (88% overall) of the surveyed tools are able to represent the algorithm in a noncomputable format. The survey results also indicated that for the tools that are able to represent algorithms in both formats, all of them can translate the algorithm between these two formats automatically. Representing algorithms in a human-readable format is very useful in phenotyping. Creating phenotype algorithm involves knowledge level authoring (e.g., inclusion/exclusion criteria) and data level authoring (e.g., specific value ranges of a data field). 61 For clinical researchers, usually the knowledge level is the first step of the authoring process, and it can be better represented in noncomputable formats such as flow charts or natural language. In addition, the noncomputable representation is also more likely to be used in communication between clinical researchers themselves, between clinical researchers and data analysts, or even between institutions where computable algorithms may be represented differently due to the difference in software platforms. As such, the use of noncomputable, human-readable algorithm representation can potentially increase the portability of the algorithm among these entities. However, the lack of a standardized representation of the algorithms still remains a challenge. Although quality standards such as the National Quality Forum Quality Data Model 62 and HL7 Health Quality Measures Format 63 can be used, they are not comprehensive for complex phenotype algorithms. 64 There is a need for developing a standard mechanism for phenotype algorithm representation.
Boolean operators, temporal operators, and other operation features
Intuitive query authoring tools with graphical interface can make clinical researchers who are not expert data analysts able to perform data queries themselves; however, those tools may not provide sufficient capability in authoring complex queries that completely satisfies the requirements of clinical research. 19 In the evaluation, it was found that some operational features had low implementation rate among the surveyed tools: nested Boolean logic (50% overall, and 29% for specific EHR group), relate co-occur items (50% overall), nested temporal operators (38% overall), and arithmetic operations (44% overall).
It may still be possible to author simple algorithms based on basic Boolean operators and temporal operators. However, many algorithms require complex operations. An analysis of the phenotype algorithms on the electronic Medical Records and Genomics Network 65 indicated that most of the algorithms require complex Boolean and temporal logic. 66 For example, the algorithms for diabetic retinopathy, hypothyroidism, resistant hypertension, and type 2 diabetes all require nested Boolean logic and complex temporal logic. 66
The implementations of various temporal operation are not very high (see Figure 3 ) among the surveyed tools. However, as noted by Albers et al., 67 human phenotypes are inherently time-dependent and dynamic (e.g., the probability of acquiring a disease and physical characteristic can change over time); but this is somewhat neglected in many phenotype algorithms. This situation may be caused by the inherent difficulty in authoring dynamic phenotype algorithms with temporal components or lack of required tools to finish the task.
Figure 3:
Percentages of phenotype authoring tools that support the corresponding Boolean operators, temporal operators, and other operation features.
Figure 3:
Percentages of phenotype authoring tools that support the corresponding Boolean operators, temporal operators, and other operation features.
Figure 4:
Percentages of phenotype authoring tools that support the corresponding search functions, user interface, and other features.
Figure 4:
Percentages of phenotype authoring tools that support the corresponding search functions, user interface, and other features.
Figure 5:
Percentages of phenotype authoring tools that support the corresponding data support and software extensibility and data output features.
Figure 5:
Percentages of phenotype authoring tools that support the corresponding data support and software extensibility and data output features.
Data support and software extensibility
Many of the surveyed tools (75% overall) support defining queries that utilize structured data while less than half of them (44% overall) provide such support for un-structured data. Structured data may be an accurate way of storing and extracting data; 68 for example, the positive predictive value of using billing codes to identify acute myocardial infarction was reported to be higher than 90%. 69 However, using only structured data may negatively influence the accuracy of cohort definition in some cases. 34 , 70 For example, a study comparing using structured International Classification Of Diseases - 9 (ICD-9) code and natural language processing (NLP) processed un-structured data for clinical trials pre-screening concluded that using a combination of both types of data would yield the best results. 71 In an another study, the researchers found that NLP-based techniques showed higher sensitivity and positive predictive value than ICD9 code-based techniques in identifying individuals in need of testing for celiac disease. 72
A solution to the lack of the processing capability for un-structured data is to allow defining a phenotype algorithm that (when executed) may invoke external software, such as a NLP plug-in, to be able to be used in the system. Unfortunately, most of the tools that did not support un-structured data also did not support the use of plug-ins. Another problem associated with not supporting plug-ins is that novel approaches in phenotyping may not be used without modifying the core system. While a review article indicated that there were machine learning and statistical analysis approaches being using as phenotype algorithms in research, 25 these functions are rarely supported by existing phenotyping tools.
It was found that only about half of the evaluated tools could be used with different EHR systems to facilitate portability of phenotyping algorithms between institutions. This is a significant design challenge as not all of the EHR databases provide mappings to standardized terminology systems, such as ICD-9/10, RxNorm, or LOINC, as recommended. 73 In designing a phenotyping tool that supports portability, the algorithm should be able to employ standardized terminologies and at the same time accommodate non-standardized when possible.
User interface and other features
Almost all the evaluated tools (94% overall) provide a web interface for convenient access. Also most tools offer basic search functions for codes in medical terminologies. However, only about half of the tools (50%) provided advanced search functions (e.g., wildcard matching), and this may reduce the usability for certain search scenarios.
In terms of data return from queries, there were variations among the tools. Some tools only provide a simple patient count, while others provide a detailed patient list, an encounter list, and detailed reports depending on the specifications of the user. This may relate to the different scenarios for which the tools were designed. For example, a query tool may be designed for assessing the size of the patient cohort given the algorithm before starting a clinical trial, and due to patient privacy issues, no further information will be shown other than a simple patient count; another system may be designed for a de-identified database and able to provide all the data fields for the user to conduct complex data analysis. For these different cases, the designers may need to study the specific user needs in order to design the best form of data output.
Limitations
The main limitation of this study was that the evaluation survey was only filled out by developers of 67% of the tools. It is therefore possible that the results are biased towards the tools for which we were able to obtain developers' feedback.
CONCLUSION
This review and evaluation of existing EHR-driven phenotype algorithm authoring tools provided an overview of the current state of the available tools. Overall, these phenotyping tools can provide interfaces that are relatively accessible for the clinical researchers who may not have high expertise in database and query coding. Most of the evaluated tools can enable users to author simple algorithms. However, important gaps also exist: many of the evaluated tools do not support the complex logic specifications, un-structured data processing, and external analytic software. These problems are obstacles for the users to author more complex algorithms. Future development of phenotyping tools should focus on improving capabilities in these areas.
FUNDING
This work was funded primarily by R01-GM105688 from the National Institute of General Medical Sciences. Additional contribution came from the eMERGE Network sites funded by the National Human Genome Research Institute through the following grants: U01-HG004610 and U01-HG006375 (Group Health Cooperative/University of Washington); U01-HG004608 (Marshfield Clinic); U01-HG04599 and U01-HG06379 (Mayo Clinic); U01-HG004609 and U01-HG006388 (Northwestern University); U01-HG006389 (Essentia Institute of Rural Health); U01-HG04603 and U01-HG006378 (Vanderbilt University); and U01-HG006385 (Vanderbilt University serving as the Coordinating Center). LVR received additional support from NCATS grant UL1TR000150.
COMPETING INTERESTS
The authors have no competing interests to declare.
CONTRIBUTORS
JCD, EM, JP, and WKT provided leadership for the project; P.S. conducted systematic literature review for the candidate tools; LVR, JX, RCK, JP, and WKT refined and reviewed the list of tools; LVR and JX designed and administered the survey; LVR, JX, JCD, and JP recruited participants for the survey; JX analyzed the survey data; LVR, JX, and HM performed verification for the survey responses. JX and LVR drafted the manuscript; all authors contributed expertise and edits.
SUPPLEMENTARY MATERIAL
Supplementary material is available online at http://jamia.oxfordjournals.org/ .
REFERENCES
Advanced Search
Abstract
Objective Health care generated data have become an important source for clinical and genomic research. Often, investigators create and iteratively refine phenotype algorithms to achieve high positive predictive values (PPVs) or sensitivity, thereby identifying valid cases and controls. These algorithms achieve the greatest utility when validated and shared by multiple health care systems.
Materials and Methods We report the current status and impact of the Phenotype KnowledgeBase (PheKB, http://phekb.org ), an online environment supporting the workflow of building, sharing, and validating electronic phenotype algorithms. We analyze the most frequent components used in algorithms and their performance at authoring institutions and secondary implementation sites.
Results As of June 2015, PheKB contained 30 finalized phenotype algorithms and 62 algorithms in development spanning a range of traits and diseases. Phenotypes have had over 3500 unique views in a 6-month period and have been reused by other institutions. International Classification of Disease codes were the most frequently used component, followed by medications and natural language processing. Among algorithms with published performance data, the median PPV was nearly identical when evaluated at the authoring institutions (n = 44; case 96.0%, control 100%) compared to implementation sites (n = 40; case 97.5%, control 100%).
Discussion These results demonstrate that a broad range of algorithms to mine electronic health record data from different health systems can be developed with high PPV, and algorithms developed at one site are generally transportable to others.
Conclusion By providing a central repository, PheKB enables improved development, transportability, and validity of algorithms for research-grade phenotypes using health care generated data.
electronic health records , electronic phenotyping , natural language processing , genomic research , clinical research
Objective
The Electronic MEdical Records and GEnomics (eMERGE) Network, 1 as well as other efforts, 2–7 have demonstrated that electronic health record (EHR) data can be used to identify research-grade disease phenotypes with sufficient positive and negative predictive values for use in identifying traits and diseases for biomedical research and clinical care, 6–11 recruitment for clinical trials, 3 , 4 quality improvement studies, 12 population-based health outcomes research, 9 disease or drug safety surveillance, 13 , 14 and genetic research. 15–21 These studies have required accurate, sharable, and quickly adaptable phenotype algorithms implemented in varying clinical data repositories. 22 Moreover, research has shown that once created, these algorithms can be ported from one institution to another, often with comparable performance. 15 , 23–26 The Phenotype Knowledgebase (PheKB; available at http://phekb.org ) was created as a workflow management system and learning center supporting the creation, validation, and dissemination of computable algorithms. PheKB has built-in tools specifically designed to enhance knowledge sharing and collaboration across sites. With feedback mechanisms and structured performance measures for implementations, PheKB facilitates the transportability of algorithms across different institutions, health care systems, and clinical data repositories and into multiple research applications.
Background and Significance
eMERGE is a national network organized and funded by the National Human Genome Research Institute that combines DNA biorepositories with EHRs for large-scale, high-throughput genetic research in support of implementing genomic medicine. During Phases I and II (2007–2015), the network developed and deployed more than 47 electronic phenotype algorithms across more than 55 000 subjects with dense genomic data. The eMERGE network found that phenotype development is both highly iterative and time consuming, 16 , 22 often taking up to 6–8 months to develop and validate a single algorithm and requiring coordination and collaboration among informaticians and clinical experts. Phenotype algorithms typically use heuristic or machine learning algorithms, combining multiple data sources to achieve high positive predictive values (PPVs) in identifying cases and controls for phenotypes of interest. 27 Critically, sharing phenotypic and genotypic data across sites allowed increased sample size and thus power for discovery and has been able to advance genomic discoveries based on this data. 15 , 19 , 20 , 28 , 29 For example, in the study of ventricular conduction in normal hearts, eMERGE leveraged existing genotype data from multiple sites to increase the statistical power of the analysis, resulting in a significant result where single site results had been nonsignificant. 24
The initial approach adopted in eMERGE was to store versions of phenotype algorithms on shared wiki pages. Although these postings provided a way to share the final pseudocode product, the approach lacked critical historical information collected, as each site worked through implementation within the specific data available in their clinical data repository. Access to versions with an understanding of the changes became critical for efficient workflow. Communicating changes in algorithms as they occur along with associated work documents, included flow charts, site adaptations, implementation notes, and validation tools enables validating sites to efficiently implement the lead site's algorithm. PheKB was created to meet this need by offering a collaborative environment to support all stages of development, validation, implementation, data sharing, and dissemination. More recent collaborations with the Patient-Centered Outcomes Research Network (PCORnet), the NIH Collaboratory, and the Pharmacogenomics Research Network have yielded a broader engaged community providing a greater number of use cases, resulting in important evolution of the site. While PheKB typically supports rule-based phenotype algorithms, emerging statistical approaches to phenotyping 5 , 30 are yielding data-driven approaches to phenotyping; one such example is currently available on PheKB. 31 Lessons learned in multisite collaboration for phenotype algorithm development include early assessment of feasibility, appropriate versioning, standardizing data elements, data quality and validation checks, and methods for disseminating the results. 22
Materials and Methods
PheKB was developed iteratively within the eMERGE Network starting from 2012 and is continuously enhanced as needs are identified and the field advances. The site leverages the Drupal content management system 32 and includes custom-developed extensions to facilitate aspects of the phenotype algorithm workflow (see Results ). Drupal was chosen largely due to local familiarity, its flexibility to support custom content types with arbitrary metadata tags, the ability to integrate custom code with off-the-shelf modules, and its longevity, though many other content management systems would support the needed functionality. PheKB required the ability to collect custom metadata for phenotypes and their implementations and support different views and searching based on these fields, implemented with standard Drupal modules. Nonstandard features including customized access controls, phenotype workflow, and integrated external code for data validation features, described in more detail below, were implemented with custom programming using Drupal's standard application programming interface. Over the years, we have migrated websites across major revisions from Drupal 5 to Drupal 7 with relative ease and with each upgrade had major improvements.
The goal of PheKB is to enable a workflow with purposefully integrated tools and standards that guide the user in efficiently navigating each of these stages from initial development to public sharing and reuse. As the eMERGE Network Coordinating Center, Vanderbilt University led development of PheKB and solicited feedback from eMERGE members during face-to-face and online conference meetings. On-boarding new users has led to additional suggestions to improve the usability of the site. For additional support, the eMERGE Coordinating Center has provided "How To" documentation and held one-on-one help sessions to assist new users, less than 5 sessions for the over 250 registered users. The design objectives for PheKB were to: (1) be a reference site enabling asynchronous communication between collaborators, (2) manage levels of sharing through the iterative stages of development and implementation, (3) facilitate sharing results, (4) provide an archive of published algorithms and implementation results (with customizations) that can serve as a resource for others to reuse the algorithms with confidence, and (5) provide efficient search and navigation mechanisms.
PheKB accomplishes these functions by (1) extending user permissions with a user group system to apply the custom access rules for viewing, creation, and data access; (2) having user profiles and authentication with institution as a metadata field; (3) defining content types to collect phenotype metadata and files as well as supporting commenting on discussion boards around phenotypes; (4) creating a metadata element that flags phenotype status to varied levels of visibility; and (5) allowing search by keywords as well as metadata. The concept of metadata-driven application development is not new and is well established; 33 a key component in development was to create a simple workflow methodology and lightweight interface to allow research teams to autonomously develop and share phenotype algorithms efficiently.
A Data Dictionary/Data Validation tool written as a Ruby web service was integrated via a custom Drupal module to identify errors and warnings in data dictionaries and data files associated with a given phenotype. As files are uploaded, PheKB is able to display errors and warnings about the structure and content of the files. Users will upload data dictionaries for a phenotype, and data files containing implementation results can be associated to one of those dictionaries. In this way, the system ensures that the data structure and values fit the rules established in the data dictionary. A key feature in the data validation tool and sharing is the role-based security to restrict viewing of datasets to only specific groups. While most commonly this is the access group created for the phenotype algorithm authoring institution, it is flexible enough to include other groups (such as a secondary analysis site). Other groups can be given read-only access and can view the phenotype artifacts and uploaded data, but only the algorithm owners and the data owners can download data. In this way, PheKB becomes a secure environment to centralize asynchronous data collection and validation.
Results
The current version of PheKB supports the following workflow and features.
Computable phenotype creation, validation, and implementation
To facilitate electronic algorithm creation within the eMERGE Network, a site typically leads the study by assessing the feasibility, then develops a phenotype algorithm that is implemented and tested within their system. The algorithm is shared on PheKB for iterative feedback and revision as it is implemented ( Figure 1 ). In addition to posting associated documents, researchers are encouraged to catalog their uploaded phenotype with metadata labels based on multidimensional representations, such as the methods and modalities used in the phenotype criteria (e.g., International Classification of Disease [ICD] codes, medications, natural language processing [NLP]), age, network, or institution affiliation. Uploaded documents typically include full descriptions of the computable algorithms including data types used, execution logic and variable dependencies, data definitions, and flow charts or other descriptive graphics. Although most files currently in PheKB are descriptive documents, such as human-readable portable document format files, users can also upload executable logic such as Konstanz Information Miner (KNIME) 34 files and customized modules (e.g., for NLP tools). KNIME is a graphical workbench that includes extract, transform, and load functions and provides a framework to embed a logical workflow of steps for a phenotype algorithm. Sites within the eMERGE Network have used KNIME to create transportable phenotypes, and at least 7 of these have been shared on PheKB by at least 2 different sites, and have been implemented by at least 4 sites. 25 , 35–37
Figure 1:
A Novel Framework to Expedite Systematic Reviews by Automatically Building Information Extraction Training Corpora Tanmay Basu1, Shraman Kumar2, Abhishek Kalyan2, Priyanka Jayaswal2, Pawan Goyal2, Stephen Pettifer3 and Siddhartha R. Jonnalagadda1 1 Division of Biomedical Informatics, Feinberg School of Medicine, Northwestern University, USA 2 Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India 3 School of Computer Science, University of Manchester, Manchester, UK Abstract Background: A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. Objective: The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. Method: The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data. A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. Results: We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). Conclusions: The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews Keywords: systematic review; evidence synthesis; machine learning; natural language processing; Introduction A systematic review identifies and collates various clinical studies, and compares data elements and results in order to provide an evidence-based answer for a particular clinical question [1]. Despite their widely acknowledged usefulness, the data extraction phase of the systematic review process is time-consuming [2]. Some reviews on the automation of systematic review process describe technologies needed for automating the overall process or individual steps. Tsafnat et al. [3] described four main tasks in systematic review: identifying the relevant studies, evaluating risk of bias in selected trials, synthesis of the evidence, and publishing the systematic reviews by generating human-readable text from trial reports. The use of text mining to reduce screening workload in systematic reviews has been discussed by Shemilt et al. [4]. Miwa et al. [5] proposed an active learning framework to reduce the workload in citation screening for inclusion in the systematic reviews. A classification system for screening articles for systematic review has been presented by Adeva et al. [6]. Choong et al. developed a system for automatic citation snowballing that recursively pursues relevant literature for helping in evidence retrieval for systematic reviews [7]. Kiritchenko et al. [8] developed ExaCT, a tool that assists users with locating and extracting key trial characteristics such as eligibility criteria, sample size, drug dosage, and primary outcomes from full-text journal articles. The major limitation of ExaCT is that it uses manually labelled training set for information extraction from a new article. Unfortunately, none of these existing reviews and articles focus on automatic data extraction from a full text article [2]. In a recent study, Tsafnat et al. [1] have described the informatics systems that automate some of the tasks of systematic review and report systems for each stage of systematic review and while data extraction has been described as a task in their review, they only highlighted three studies as an acknowledgement of the ongoing work. Moreover, as we have noted in our review on this topic [2], there is a dearth in gold standards for information extraction from literature for the purpose of systematic reviews. It has been discussed that only three out of 26 studies included in that systematic review, use a common corpus, namely 1000 medical abstracts from the PIBOSO corpus [2]. Unfortunately, even that corpus facilitates only classification of sentences into whether they contain one of the data elements corresponding to the PIBOSO categories. No two other studies shared the same gold standard or dataset for evaluation. State of the art text mining techniques use gold standards of documents with concepts annotated manually and then builds machine-learning systems. However, it will be expensive to build gold standards even for 50 standard data elements and practically impossible to build gold standards for hundreds of domain-specific data elements. We, therefore, present a framework to leverage the wealth of human curated information already present in the existing systematic reviews. We propose to develop our training set by retrieving sentences from the full text articles that correspond to the manually extracted data elements present in the systematic reviews. Instead of having humans to go through the content of the full PDF article, we expedite this process by first developing a heuristics based system for automatically annotating included studies to assist the human annotator or directly use the outputs of this system for data elements where the accuracy is high enough. It is significantly lesser effort on the part of the humans. The aim of this work is two-fold. The first objective is to develop a framework to generate gold standard using existing systematic reviews by applying natural language processing (NLP) techniques such as sentence similarity algorithms to automatically create training corpora from the references mentioned in the systematic reviews. The other objective is to use these gold standards to train machine learning classifiers for extracting data elements from a new study. A SVM classifier has been used towards this learning task. Methods The proposed system is developed in two stages – training corpus (or gold standard) generation and data extraction using this training corpus. A graphical abstract of the proposed framework is presented in Figure 1. Gold Standard Generation: Initially, different data elements (e.g., the inclusion criteria of a health care study) and their references are extracted from each systematic review. The synopsis of different data elements are generally mentioned in systematic reviews. Therefore the entire information about the data elements are extracted from the associated studies included in the systematic reviews. A modified Jaccard similarity measure is used to identify specific sentences from an included reference that represent individual data elements. The modified Jaccard similarity measure (Jac_Mod) finds the similarity between the sentence (Sx) that represents a data element (D) in a systematic review and each sentence (Sy) in an included reference as follows: Jac_Mod (Sx, Sy) = (number of common terms between Sx and Sy)/ (number of terms in Sx) Note that the modified Jaccard similarity measure ranges between [0, 1]. The sentences are sorted according to their similarity values and the top few sentences are considered as the positive instance for D. Note that a specific data element may be represented in the references by more than one sentence. If the difference of Jac_Mod values between the top-most sentence and the next few sentences lie between 0 and α (say), then all of these sentences are considered as positive instance for D. The sentences that have very low similarity values are considered as negative instance for D. A range of [0, β] of Jac_Mod scores has been considered for creating negative instances. Thus, the gold standard for each data element is developed by combining the positive and negative instances from all the included studies in two different groups. Here α, β are two different thresholds on Jac_Mod score and their value should lie in range [0, 1]. The values of α and β are data dependent and could be fixed at runtime based on the characteristics of gold standard. Automatic Data Extraction: In the second phase, a machine learning model has been developed, where this gold standard is used to train a classifier to determine whether a sentence contains the data element of interest (in our evaluation case – inclusion criteria) from a new clinical article. Unigrams, Bigrams and Trigrams generated from sentences are used as features with SVM classifier in the experiments. In its simplest form, SVM is a hyperplane that separates a set of positive examples from a set of negative examples with maximum margin [9]. Given a set of training documents in a vector space, SVM finds the best decision hyperplane that separates two categories. The quality of a decision hyperplane is determined by a margin between two hyperplanes that are parallel to the decision hyperplane and touch the closest documents of each category. Therefore best decision hyperplane is the one with the maximum margin, and is used to categorize the new test documents. Results Datasets: 31 Cochrane Database systematic reviews (CDSR) related to congestive heart failure are Systematic reviews repository A systematic review article Retrieve One by one Extract All tables of different studies referred Retrieve One by one A table of a study Extract Data element of interest (A sentence) Reference Retrieve Each sentence Calculate Jac_Mod score Jac_mod score of all sentences Sort If Positive instances Negative instances the Jac-Mod score of a sentence ϵ [0, β] difference of Jac-Mod score of the top sentence and next few sentences is < α Train a machine learning classifier New reference article Retrieve Each sentence G old Standard G eneration D ata Extraction Classification ? Data Element Not desired Positive Negative Label Figure 1: Graphical abstract of the proposed system randomly identified from PubMed. PDFs of the corresponding full-texts of the systematic reviews and the PDFs of the associated clinical studies are obtained automatically through APIs accessible from Northwestern University's Galter library for health sciences [10]. For obtaining the PDFs of the included references, the titles of these references are automatically extracted from the webpage of the corresponding systematic reviews in online Cochrane Library [11]. Therefore these titles are passed as individual queries to PubMed using eUtils webservice [12] to automatically retrieve the EndNotes of those references. These EndNotes are used to download the corresponding full-texts (in PDF format) through EndNote API of Galter library. The systematic reviews contain a section (typically called 'Characteristics of Studies' or a variation thereof) in which the properties of each clinical study are summarized by means of a table. A snapshot of a section of such table is shown in Figure 2 from the systematic review by Martí-Carvajal et al. [13]. In this figure, the keys, e.g., eligibility, inclusion criteria, exclusion criteria etc. along with their values are the data elements. Either the exact sentences or some significant information about these data elements are manually extracted from the referred articles and noted in such tables to develop systematic reviews. Study Design: The PDF files of each of these 31 systematic reviews are converted to JSON format using a version of the PDFx tool [14] customized for the structure of Cochrane Systematic Reviews. The JSON file format represents a machine readable version of these tables from which data elements can easily be retrieved programmatically. We have tested the performance of the proposed system on a commonly used data element (inclusion criteria of Randomized Control Trials (RCT)). Therefore the tables that contain inclusion criteria as data element are considered in the experiments. Eventually 69 such references have been considered to generate the gold standard. The PDF version of each of these references is converted to text format using a PDF to text converter tool freely available on internet. The proposed system is implemented in scikit-learn, a machine learning platform developed in Python [15]. The gold standard is automatically developed following the proposed algorithm using these 69 references in scikit-learn. The values of α and β of Jac_Mod similarity measure have been fixed as 0.2 and 0.005 respectively. The inclusion criteria are extracted from each reference, which is considered as positive instance. After applying the method for gold standard generation described above for these 69 references, the gold standard contains 122 such positive instances and 12651 negative instances for inclusion criteria. Each instance represents a sentence. The test set contains 24 new RCT publications that are chosen randomly from the Cochrane Database [2] and are not included in the gold standard. These publications have been cited in some of the Cochrane Database Systematic Reviews. It has been confirmed that every reference contains some inclusion criteria based on them being explicitly mentioned in the corresponding systematic review. Therefore the inclusion criteria have been manually identified from these references. Methods Multicentre study: yes Country: USA Intention-to-treat: yes (an intention-to-treat analysis was performed for all participants who had carotid ultrasonography at baseline and at least 1 follow-up visit, page 731) Unit of randomization: patients Follow-up period (years): B vitamins group (3.14 (0.48 to 4.56) versus placebo group (3.07 (0.46 to 5.0)) Participants Eligibility: 5309 Randomised: 506 (254 vitamins versus 252 placebo) Age (years) • Overall: 61.4 • B vitamins group: 61.7 ± 0.1) • Placebo group: 61.1 (±9.6) Gender (men): • Overall: 61% • B vitamins group: 61% • Placebo group: 61% Inclusion criteria: 1. Men and postmenopausal women 40 years old 2. Fasting tHcy 8.5 mol/L 3. No clinical signs/symptoms of cardiovascular disease (CVD) Exclusion criteria: 1. Fasting triglycerides > 5.64 mmol/L (500 mg/dL) 2. Diabetes mellitus or fasting serum glucose > 6.99 mmol/L (126 mg/dL) 3. Systolic blood pressure ≥ 160 mm Hg and/or diastolic blood pressure ≥ 100 mm Hg 4. Untreated thyroid disease 5. Creatinine clearance < 70 mL/min 6. Life-threatening illness with prognosis 5 years 7. 5 alcoholic drinks daily Interventions HLI-intervention: folic acid (5 mg), vitamin B12 (0.4 mg) and vitamin B6 (50 mg, daily supplementation) Control: placebo Treatment duration: initial 2.5-year treatment period was extended on average 1 to 2 years Figure 2: Snapshot of a part of systematic review table The aim is to automatically identify the inclusion criteria in each of these new references using the proposed tool and evaluate the performance using the manually labeled information from systematic reviews. Each sentence of a new test reference is categorized to either positive (i.e., inclusion criteria) or negative instance by the system. Experimental Analysis: The performance of the proposed tool has been evaluated by precision and recall measure [7]. The precision and recall can be calculated using the standard formulae as follow: Precision = (True positives) / (True positives + False positives) Recall = (True positives) / (True positives + False negatives) Here, true positive stands for the number of sentences that are correctly retrieved as positive instances. False positive is the number of sentences that the tool marked as positive, but actually belong to the negative category. False negative is the number of sentences that are retrieved by the machine as negative instances, but actually belong to the positive category. Table 1: Recall and precision score of all new test references Note that the objective of the proposed tool is to achieve high accuracy in terms of identifying inclusion criteria (or other data elements in general) from new articles. Therefore, a high recall is desirable for the system. The recall and precision for each new test article is measured using the manually labeled inclusion criteria described above. The individual true positive, false positive, false negative, recall and precision values of each of the 24 new articles are presented in Table1. Article Id True Positive False Negative False Positive Recall Precision Test Sample 1 1 0 1 1 0.50 Test Sample 2 1 0 6 1 0.14 Test Sample 3 1 0 2 1 0.33 Test Sample 4 1 0 2 1 0.33 Test Sample 5 1 0 3 1 0.25 Test Sample 6 1 0 2 1 0.33 Test Sample 7 1 0 7 1 0.12 Test Sample 8 1 0 7 1 0.12 Test Sample 9 2 0 6 1 0.25 Test Sample 10 1 0 2 1 0.33 Test Sample 11 1 0 5 1 0.17 Test Sample 12 1 0 5 1 0.17 Test Sample 13 1 0 7 1 0.12 Test Sample 14 1 0 3 1 0.25 Test Sample 15 1 0 3 1 0.25 Test Sample 16 2 0 0 1 1 Test Sample 17 2 2 4 0.5 0.33 Test Sample 18 1 0 4 1 0.20 Test Sample 19 1 0 5 1 0.17 Test Sample 20 1 1 3 0.5 0.25 Test Sample 21 1 1 4 0.5 0.20 Test Sample 22 1 0 7 1 0.12 Test Sample 23 1 0 2 1 0.33 Test Sample 24 1 0 4 1 0.20 It may be observed from Table 1 that the number of false negatives is zero for most of the test samples. It indicates that the proposed system is able to retrieve information of interest from new references with the framework being trained by automatically generated corpora. The aggregate recall and precision of all the test references are 0.93 and 0.27 respectively. A precision of 0.27 is still efficient because the reviewers now need to review only 1/0.27=3.7 sentences per article for the data elements as opposed to reading the entire article or training a corpus using completely new annotations. Thus the performance shows the effectiveness of the proposed tool. Discussion Principal Observations: It has been suggested in earlier studies that SVM performs well for text classification [16, 17]. Hence, SVM is chosen as the classifier in the proposed system. Moreover, we have done extensive experiments using several other classifiers, such as Random Forests, Naïve Bayes using 10 fold cross validation on the training corpus and eventually SVM outperforms the other classifiers in terms of recall. The training corpus is imbalanced, since it contains many negative instances (12651) and few positive instances (122). Hence we have used balanced class weights to avoid the proportional influence of the negative class. It has been recommended in the study by Hsu et al. [17] that the linear kernel is useful, when the number of features is very large. It should be noted that the natural language texts generally produce large number of features [18]. Therefore linear kernel is used in the experiments. In SVM, C is a regularization parameter that controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights [19]. For large values of C, the optimization algorithm of SVM chooses a small margin separating hyperplane, if that hyperplane could correctly classify all the training instances. Conversely, if the value of C is too small then the optimization algorithm chooses a large margin separating hyperplane, resulting a high misclassification rate. It may be noted that the value of C lies in (0, ∞]. The goal is to identify a good C value, so that SVM can accurately predict unknown test samples i.e., to minimize the misclassification rate. Usually the value of C is determined by grid search on different class intervals of C values [17]. In grid search, SVM is implemented by performing k-fold cross validation on the training set using every C value in a given range. A finer grid search is performed on a narrow range on the C value that has achieved best accuracy. The process continues to refine C value up to third or fourth decimal place, which is predefined. For example, if the initial range is considered as [1, 10] then the subsequent finer ranges may be (4.1, 4.2... 4.9), (4.21, 422... 4.29), (4.261, 4.262... 4.269) etc. and a particular value from the last interval say, 4.263 could fixed as C. Eventually, this value of C is used for classifying the test samples. In k-fold cross-validation, the training set is randomly divided into k subsets of equal size. Sequentially one subset is tested using the classifier trained on the remaining k-1 subsets. Thus, each instance of the whole training set is predicted once and therefore the cross-validation accuracy is the percentage of data that are correctly classified. In our experiments, we have performed 10-fold cross validation on the training set to determine the value of C ranges in (0, 1000] and refined it up to fourth decimal place. It has been observed from various publications included in the systematic reviews that the sentences which represent the inclusion criteria generally contain the keywords or phrases like, eligibility, entry conditions, eligible patients, etc. Sometimes the phrase inclusion criteria is explicitly mentioned before its description. Therefore these keywords/ phrases help to reduce the manual load. But, it is difficult to identify an inclusion criteria where it is not represented by such regular keywords. The following inclusion criterion has been extracted from test sample 7 [20] that is used in the experimental analysis. • This prospective cohort is composed by patients who are 18 years old or older, diagnosed as having chronic heart failure of ischemic or non-ischemic etiology, at least 3 months before inclusion into the registry. Although the sentence does not contain any discriminating keyword for inclusion criteria, the proposed system extracted this sentence from the reference. The most interesting aspect of the proposed framework, however, is that no knowledge about the meaning of "inclusion criteria" is used in generating the training corpus or building machine learning features. The system is retrieving the data element of interest as well as some sentences similar to that data element from a new article, which results to a low precision score. The following sentences have been extracted by the proposed tool from test sample 18 [21] to identify inclusion criteria. • Inclusion criteria were diagnosed heart failure, either by echocardiography, radiographic evidence of pulmonary congestion or typical symptoms and signs of heart failure. • Since the majority of patients hospitalized with heart failure were over 75 years of age, other limitations of recruitment were that patients were in an end-stage of heart failure or other severe disease or had cognitive dysfunction. • A total of 161 patients met the inclusion criteria, 55 patients declined to participate mainly due to fatigue. • According to age, sex and co-morbidities the patients included in our study are representative for the heart failure population. It should be noted that only the first sentence represents the inclusion criteria. The other sentences appear either in the results section or discussion section in test sample 18 [21], but they have some relevance with the first sentence. Hence the tool has extracted all of them as data elements, which results in a low precision score. The same scenario is observed in almost all the test samples. Note that the aggregate precision score of the system is reasonable (0.27), since the reviewers have to read only 3.7 sentences in a new article on average. Limitations: The first limitation of the tool is that the study cannot guarantee that SVM will be useful in every case. It may be noted here that a particular classifier cannot provide an optimum solution in every situation and for any type of data [9]. Hence, the tool may show good results using other classifiers in machine learning for various other types of articles. Moreover, the values of the thresholds α and β of Jac_Mod similarity measure should be chosen carefully to create the training corpus. It is recommended that both of the values should be close to 0. Otherwise many irrelevant sentences could be selected in the training corpus, which may degrade the performance of the machine learning classifier. The performance of the tool is tested on some randomly selected articles from PubMed, mostly related to heart disease. The performance should be tested on studies from different topics of medical research e.g., internal medicine, oncology, neuroscience etc. Conclusions: The literature survey has shown that a tool is lacking for automatic data extraction from a full text article. Therefore, a framework has been proposed in this study to overcome this limitation. It has been observed from the experiments that the tool is retrieving valuable information from some references, even when it is difficult to identify them manually. Thus, we hope that the tool will be useful for automatic data extraction from biomedical literature. The future scope of this work is to generalize this information extraction framework for all topics and to validate it for all frequently used data elements. References 1. Tsafnat G, Glasziou P, Choong MK, Dunn A, Galgani F, Coiera E. Systematic review automation technologies. Systematic Reviews; 2014; 3:74. DOI: 10.1186/2046-4053-3-74. 2. S. R. Jonnalagadda, P. Goyal and M. D. Huffman. Automating data extraction in systematic reviews: a systematic review. Systematic Reviews; 2015; 4:78. DOI: 10.1186/s13643-015-0066-7 3. Tsafnat G, Dunn A, Glasziou P, Coiera E. The automation of systematic reviews. BMJ; 2013; 346:f139. DOI: 10.1136/bmj.f139 4. Shemilt I, Simon A, Hollands GJ, Marteau TM, Ogilvie D, O'Mara‐Eves A, et al. Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews. Research Synthesis Methods; 2014; 5(1):31-49. DOI: 10.1002/jrsm.1093. 5. Miwa MTJ, O'Mara-Eves A, Ananiadou S. Reducing systematic review workload through certainty-based screening. Journal of Biomedical Informatics; 2014; 51:242-253 DOI:org/10.1016/j.jbi.2014.06.005 6. Adeva JJG, Pikatza Atxa JM, Ubeda Carrillo M, Ansuategi ZE. Automatic text classification to support systematic reviews in medicine. Expert Systems and Applications; 2014; 41(4):1498-1508. 7. Choong MK, Galgani F, Dunn AG, Tsafnat G. Automatic Evidence Retrieval for Systematic Reviews. JMIR; 2014; 16:10. DOI: 10.2196/jmir.3369. 8. Kiritchenko S, de Bruijn B, Carini S, Martin J, Sim I. ExaCT: Automatic Extraction of Clinical Trial Characteristics from Journal Publications. BMC Med Inform Decis Mak. 2010;10:56. 9. Witten IH, frank E and Hall MA. Data Mining: Practical Machine Learning Tools and Techniques; 2011. ISBN-13: 978-0123748560. 10. Galter Health Sciences Library, Northwestern University, Chicago USA. https://galter.northwestern.edu/. Archived at: http://www.webcitation.org/6ghFMkbUL 11. Cochrane Library. http://www.cochranelibrary.com/. Archived at: http://www.webcitation.org/6gh0sjtMu 12. Sayers E, "A General Introduction to the E-utilities," 2009. http://www.ncbi.nlm.nih.gov/books/NBK25497/#_ncbi_dlg_citbx_NBK25497. 13. Martí-Carvajal AJ, Sola I, Lathyris D, Salanti G. Homocysteine lowering interventions for preventing cardiovascular events. Cochrane Database Systematic Review, 2015. DOI: 10.1002/14651858.CD006612.pub2. 14. Hager C. PDFx: Extract references (PDF, url, doi, arxiv) and metadata from a PDF. https://github.com/metachris/PDFx. Archived at: http://www.webcitation.org/6gFnv66VI 15. Pedregosa F, Varoquaux G et al. Scikit-learn: Machine Learning in Python, The Journal of Machine Learning Research; 2011; 12:2825-2830. 16. Joachims T. Text categorization with support vector machines: Learning with many relevant features. Proceedings of the European Conference on Machine Learning; 1998; p. 137-142. 17. Hsu CW, Chang CC, and Lin CJ. A Practical Guide to Support Vector Classification, 2010. http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf. Archived at: http://www.webcitation.org/6gFnWKILv 18. Basu T, Murthy CA. A supervised term selection technique for effective text categorization. International Journal of Machine Learning and Cybernetics; 2015. DOI: 10.1007/s13042-015-0421-y 19. A user's guide to support vector machines, data mining techniques for the life sciences, Springer; 2009; p. 223-239. DOI: 10.1007/978-1-60327-241-4_13 20. Grancelli H, Varini S, Ferrante D, Schwartzman R, Zambrano C, Soifer S, Nul D, Doval H; GESICA Investigators. Randomized Trial of Telephone Intervention in Chronic Heart Failure (DIAL): study design and preliminary observations. Journal of Cardiac Failure; 2003; 9(3):172-179. 21. Stromberg A, Martensson J, Fridlund B, Levin LA, Karlsson JE, Dahlstrom U; Nurse-led heart failure clinics improve survival and self-care behaviour in patients with heart failure: results from a prospective, randomised trial. European Heart Journal; 2003; (11):1014-1023. A Novel Framework to Expedite Systematic Reviews by Automatically Building Information Extraction Training Corpora Abstract Background: A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A ... Objective: The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expens... Method: The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modi... Results: We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences c... Conclusions: The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction ... Keywords: systematic review; evidence synthesis; machine learning; natural language processing; Introduction Methods Results Discussion References 1     AN INFORMATION EXTRACTION APPROACH TO PRESCREEN HEART FAILURE PATIENTS FOR CLINICAL TRIALS ABHISHEK KALYAN ADUPA1, RAVI PRAKASH GARG1, JESSICA CORONA-COX2, SANJIV J. SHAH2, SIDDHARTHA JONNALAGADDA1 1Division of Health and Biomedical Informatics, Department of Preventive Medicine, Northwestern University Feinberg School of Medicine, Chicago, IL 60611, USA Email: sid@northwestern.edu 2Division of Cardiology, Department of Medicine, Northwestern University Feinberg School of Medicine, Chicago, IL 60611, USA To reduce the large amount of time spent screening, identifying, and recruiting patients into clinical trials, we need prescreening systems that are able to automate the data extraction and decision-making tasks that are typically relegated to clinical research study coordinators. However, a major obstacle is the vast amount of patient data available as unstructured free-form text in electronic health records. Here we propose an information extraction-based approach that first automatically converts unstructured text into a structured form. The structured data are then compared against a list of eligibility criteria using a rule-based system to determine which patients qualify for enrollment in a heart failure clinical trial. We show that we can achieve highly accurate results, with recall and precision values of 0.95 and 0.86, respectively. Our system allowed us to significantly reduce the time needed for prescreening patients from a few weeks to a few minutes. Our open-source information extraction modules are available for researchers and could be tested and validated in other cardiovascular trials. An approach such as the one we demonstrate here may decrease costs and expedite clinical trials, and could enhance the reproducibility of trials across institutions and populations. 1. Introduction The creation and acceptance of electronic health records (EHRs) has ignited widespread interest in the use of clinical data for secondary purposes and research [1]. One such application that can greatly benefit from an EHR-based approach is clinical trial screening and recruitment. Clinical trial screening is a process that helps medical practitioners and researchers determine whether a particular patient is suitable for trial based on certain eligibility criteria. The eligibility criteria are generally divided into two parts: inclusion criteria and exclusion criteria. Inclusion criteria are characteristics that the prospective subjects must have if they are to be included in the study, while exclusion criteria are those characteristics that disqualify prospective subjects from inclusion in the study. In general, screening for clinical trial recruitment is done manually. Clinicians and study coordinators go through each of the eligibility criteria, determine data elements relevant to the clinical trial, extract the data elements from structured and unstructured EHR of each patient, and match the data elements with the eligibility criteria to decide whether the patient qualifies for the trial. Not only this process is slow, it is also prone to errors. It typically takes approximately 15 to 20 minutes for a study coordinator to examine each patient's data. Because of the subjectivity involved in human decision-making, domain knowledge, which patients are considered for initial search and other factors [2], there is always a possibility of type-1 and type-2 errors in the 2     prescreening process and biases in the overall recruitment. Furthermore, clinicians and study coordinators typically rely on patients identified in their own specialty clinics or in certain defined patient care settings, thereby missing out on the advantage of screening an entire healthcare system. We hypothesize that an automated process for prescreening would be quicker and serve as an independent judge of inclusion/exclusion criteria free of human bias. If the prescreening algorithm also has a high recall (sensitivity), it would potentially reduce recruitment bias because it would be possible to consider patients from a larger pool. Thus, an algorithm that can prescreen eligible patients efficiently could provide a proficient and robust approach to clinical trial recruitment. Therefore, we sought to develop a high recall (sensitivity) prescreening algorithm for recruiting patients into a multicenter, randomized, double-blind, parallel group, active-controlled study to evaluate the efficacy and safety of LCZ696 compared to valsartan, on morbidity and mortality in heart failure patients with preserved ejection fraction (PARAGON). Our approach involves development of information extraction modules that can be reused not only for other EHRs but also for other trials using similar data elements. 2. Background Heart failure (HF) occurs when the heart muscle is no longer able to meet the demands of the body either due to reduced cardiac output or increased ventricular filling pressures. It is one of the most common reasons for hospital admissions among those aged 65 years and older. In 2010 alone, HF affected 6.6 million Americans at a cost of $34.4 billion [3, 4]. Many clinical trials have been undertaken to find efficient solutions to the condition. However, it has been found that 86% of all clinical trials are delayed in patient recruitment from 1 to 6 months, and 13% are delayed by longer than 6 months [2]. A major cause of delay in HF clinical trials is the inability to efficiently screen for and identify eligible patients. An automated system is therefore needed to accelerate the process of prescreening patients for clinical trials. The surge of the use of EHRs in the United States has created abundant opportunities for clinical and translational research. As Friedman et al noted, the extensive use of clinical data provides great potential to transform our healthcare system into a "Self-learning Health System" [5, 6]. In addition to its primary purpose of providing improved clinical practice, the use of EHRs offers means for the identification of participants who satisfy predefined criteria. This can be used for a variety of applications, including clinical trial recruitment, outcome prediction, survival analysis, and other retrospective studies [7-10]. EHRs contain patient data in both structured and unstructured formats. The structured data generally encompass a patient's demographic data, physical characteristics (e.g., body mass index [BMI], blood pressure), laboratory data, and diagnoses. Structured data are not only the best representation of knowledge but also easier to process. However, there is a vast amount of medical knowledge that is locked in the unstructured format. The unstructured data are typically text clinical narratives present in progress notes, imaging reports (e.g., echocardiographic reports), and discharge summaries, for example. Thus, a module that can automatically and efficiently extract information from unstructured clinical text and convert it into a structured form is needed. 3     The syndromic nature of HF presents unique challenges for the identification of patients from EHR data for research [11]. HF with preserved ejection fraction, in which the global pumping function of the heart is normal, is particularly challenging to identify during prescreening activities. The presence of large amounts of unstructured data in patient medical reports aggravates the challenges. Previous studies have shown that clinicians often prefer free text entry to coded options, in order to fully explain the health conditions of each patients [12-14]. It has also been noted that unstructured data are essential because of the information they contain [15]; therefore, unstructured data are likely to persist in the future. There is an immediate need for an automated data extraction system to transform unstructured clinical reports into a structured form, which is much easier to process and handle [16-18]. There has been considerable research in identifying patient cohorts from EHRs [19]. These approaches can be categorized into three general types: (1) rule-based approaches [20-24], (2) machine learning–based approaches [25-28], and (3) information retrieval–based approaches [29-32]. All these approaches use either pattern matching (regular expressions) or language modeling–based methods [33-36] to extract features for their system to work on. Rule-based systems are stringent and binary (either yes or no) in nature. On the other hand, machine learning– and information retrieval–based methods provide output as probability or a score. Machine learning techniques, however, require a large amount of training data to give accurate results. Our proposed system is different from these approaches in various ways. A majority of the reported systems aim to identify whether a patient shows a certain phenotype. Therefore, the number of criteria required is less than that which are necessary for clinical trial screening. For example, a majority of the systems only use a variation of disease names, medications used, or treatments taken as their eligibility criteria [21, 23]. Ours is a more diverse application. Our goal is to check whether a particular patient qualifies for a certain clinical trial. Clinical trials usually have a large number of eligibility criteria that need to be checked. Therefore, a large amount of information related to the eligibility criteria needs to be extracted. Our study goal is similar to that of the plethora of approaches proposed in computer-based clinical trial recruitment systems [37]. However, a majority of these approaches either lack EHRs as data source or are not equipped to handle unstructured data. We, on the other hand, obtain patient data from EHRs and handle unstructured data through information extraction methods, as opposed to the "bag of terms" or "bag of concepts" suggested in other methods Input   Patient   Record   Pattern  Matching   (Regular  Expressions)   Language  Modeling   Based  Methods   Features   Rule-­‐Based  Techniques   Machine  Learning–Based   Techniques   Information  Retrieval–Based   Techniques   Figure 1. Summary of techniques for patient cohort identification. 4     [38]. The main contributions of our study are to (1) show that automated recruitment systems can only serve as prescreening tools and to (2) develop and validate a clinical trial screening system based on information extracted from EHRs. Here, we demonstrate how our system processes a set of eligibility criteria, extracts information from patient records automatically into a structured format, and finally prescreens the patients who could qualify for the trial by matching the structured patient document with the eligibility criteria. Section 3 describes the data and the algorithm used to convert the data into a structured form. We present our results in Section 4, discuss our experience and the challenges faced in Section 5, and then conclude in Section 6. 3. Methods 3.1. Patient Records and Eligibility Criteria – Data Description The patient records used in this study come from the EPIC EHR used by Northwestern Memorial Group. The initial cohort of patients we have considered for our experiments was very broad to ensure we were not missing any patients that could be included – patients that currently have been documented to have HF with the ICD-9-CM Diagnosis Code 428.0 or had an echocardiogram within the past year. We selected 198 of these patients for development and 3002 patients for validation. Each patient's data consists of five types of reports: encounters, problem list, echocardiography reports, lab reports, and current medication list. Encounters contain two types of files: encounter diagnosis name and encounter progress notes. The characteristics of the patient records for both datasets are summarized in Table 1. We have 40 eligibility criteria – 7 for inclusion and 33 for exclusion – for the PARAGON clinical trial [39]. However, we currently evaluate our approach based on a subset of these criteria (Figure 3). Patient  Before   Demographics,  Lab   Values,  etc.   (Structured  Data)   Pathology  Reports,   Echo  Report,   Encounter  Notes,   Discharge   Summaries,  etc.   (Unstructured  Data)   Information   Extraction   Modules   Demographics          Patient  After   Lab  Values   Pathology  Report   Information   Echo  Report   Information   Encounter  Notes   Information   (All  Structured   Form)   Eligibility   Criteria   OUTPUT   Figure 2. Overview of our clinical trial recruitment system architecture.  We have analyzed different HF-related patient medical reports and derived pattern-based Information extraction modules that provide output of structured data to compare against eligibility criteria for clinical trial recruitment 5     Table 1. Characteristics of the development and validation patient datasets.       Development  set     Validation  set   Total  number  of  patients   198   3002   Encounters   54,173   393,482   Echocardiography  reports   96,281   883,385   Lab  reports   52,393   371,879   Current  medication  entries   4490   41,947   Problem  lists   3521   33,089   LVEF   Encounter   Progress  Notes   Encounter   Diagnosis  Names   Problem  List   Lab  Report   Echo  Report   Medication  List   IC:  Age  ≥  55  y,  EC:  BMI  >  40  kg/m2   IC:  Number  of  HF  terms  ≥1   IC:  LVEF  value  ≥45%   EC:  Number  of  angioedema  or   pancreatitis  or  bilateral  renal  artery   stenosis  terms  ≥1   EC:  Number  of  "transplant"  or  "ICD"   terms  ≥1   EC:  Hb  <  10  g/dl,  GFR  <  30   EC:  Sentences  with  "malignant"  term   and  absence  of  "basal"  and   "prostrate"  terms  ≥1   EC:  SBP  value  >  150  mm  Hg  and   number  of  antihypertensive  drugs  >  3   EC:  Number  of  "pericardial   constriction"  or  "genetic   hypertrophic  cardiomyopathy"  or   "infiltrative  cardiomyopathy"  terms  ≥   1   Patient  Records   Data   Structured   data   normalizer   Structured   data   extractor   Unstructured   data  extractor   Age   BMI   Hb   GFR   BP   a.  Heart  failure   b.  Angioedema   c.  Pancreatitis   d.  Organ  transplant   e.  ICD   f.  Malignant  cancer   g.  Valvular  heart   disease   h.  Pericardial   constriction.   Cardiomyopathy   Figure 3. Algorithm. Each patient's data parsed through three types of extraction module. The modules extract the appropriate information and create a patient profile. This profile is then checked against the clinical trial eligibility criteria to check the patient's qualification. (BMI=Body Mass Index, Hb = Hemoglobin, GFR=Glomerular Filtration Rate, BP = Blood Pressure, LVEF=  left ventricular ejection fraction, ICD=  implantable cardioverter defibrillators, IC=Inclusion criteria, EC=Exclusion criteria) 6     3.2 Algorithm The information from the patient data is extracted as part of separate modules. These modules are designed to extract the data elements relevant to PARAGON but are reusable individually for other clinical trials. After extraction, a rule-based system matches the eligibility criteria and discards patients who do not satisfy any of the inclusion criteria or satisfies one of the exclusion criteria. Figure 3 describes the system's architecture in finer detail. We broadly categorize the modules as: (1) structured data normalizer, (2) unstructured data extractor, and (3) unstructured data classifier. Structured data normalizer is used for the extraction of data elements whose values are already present in the structured form. This module is further divided into two submodules. In submodule 1, we extract the values for age, BMI, hemoglobin, GFR and blood pressure from structured fields. In submodule 2, we extract the number of medications a patient belong to different drug classes. The reports are in structured form with mapping of the medication to the patient. This submodule requires external information resources, which we provide as databases to our system. Table 2 and Table 3 list the drug classes that we incorporated for the PARAGON clinical trial. Table 2. Types of antihypertensive drugs Beta  (β)-­‐   blockers     Dihydropyridines     Nondihydropyridines:  class  IV   antiarrhythmics   Antihypertensives   (others)   Acebutolol   Amlodipine   Diltiazem   Aliskiren   Atenolol   Felodipine   Verapamil   Fenoldopam   Betaxolol   Nicardipine     Hydralazine   Bisoprolol   Nifedipine     Hydralazine/HCTZ   Carvedilol   Nisoldipine     Methyldopa/HCTZ   Esmolol       Minoxidil   Table 3. Common angiotensin-converting enzyme (ACE) inhibitors and angiotensin II receptor blockers (ARBs) Common  ACE  Inhibitors   Common  ARBs   Benazepril   Candesartan   Captopril   Eprosartan   Enalapril   Irbesartan   Fosinopril   Losartan   Lisinopril   Olmesartan   Unstructured data extractor is used for the extraction of values of data elements present in unstructured text. This module also accepts input and provides output just as the previous module but uses a complex set of regular expressions to extract the exact value. For example, we currently extract the left ventricular ejection fraction (LVEF) value for our clinical trial. For this, we first use a set of regular expressions to extract sentences where the LVEF value may be present and then another set of regular expressions to extract the definite values as shown in Table 4. Regular expressions 1 through 4 extract the sentences that can contain LVEF values. Then, the sentences are parsed through regular expressions 5 and 6. Regular expression 5 extracts the values present in 7     range format: for example, "40% to/- 45%." Regular expression 6 extracts the freely available values: for example, "40%." Table 4. Regular expressions for extracting LVEF-containing sentences and values. S/N   Regular  Expression   1   (left  ventricular  ejection   fraction|lvef|lv  ejection   fraction|left  ventricle  ejection   fraction|ejection   fraction|   ef  |ejection  fraction)[^_%\\.]*?([\\d-­‐\\.]+)\\s*'?%   2   (left   ventricular   systolic   function|left   ventricular   function|systolic   function   of   the   left   ventricle|lv   systolic   function|left   ventricular   ejection   fraction|ejection   fraction|left   ventricle)(normal|normal   global|low   normal|well   preserved|severely   reduced|moderately   decreased|moderately   depressed|severely   decreased|severe|markedly   decreased|markedly   reduced|severely   globally   reduced|mildly   decreased|mildly  depressed|severely  depressed)   3   (normal|normal   global|low   normal|well   preserved|severely   reduced|moderately   decreased|moderately   depressed|severely   decreased|severe|markedly   decreased|markedly   reduced|severely   globally   reduced|mildly  decreased|mildly  depressed|severely  depressed)   4   .*(moderate|marked|severe)   (lv   systolic   dysfunction|left   ventricular   dysfunction|left   ventricular   systolic   dysfunction).*   5   ((\\d+\\s*(\\-­‐|to)\\s*\\d+)|(\\d*\\.\\d*\\s*(\\-­‐|to)\\s*\\d*\\.\\d*)|(\\d*\\.\\d+)|(\\d+))(?=(\\s*(\\%)))   6   \\d+(\\.\\d+)?   Unstructured data classifier is used for classifying whether certain data elements are present or absent in relation to the context of the patient. Currently in this module, we extract all the instances of a given data element (diagnosis, medication, treatment, or tests) and its synonyms in the input report(s). For this, the module first checks for synonyms of the input term using UMLS Metathesaurus [40], builds automatically a set of regular expressions, and then applies them to the input report text to extract all the instances. For example, to extract HF-related terms the module compiles a list of synonyms: "heart failure," "HF," "diastolic dysfunction," and "cardiomyopathy." Next, a set of regular expressions are automatically generated (Table 5) and used to extract all the instances of HF-related terms. For PARAGON, the other data elements processed in this category are "angioedema", "pancreatitis", "valvular heart disease," etc. We adapt existing rule-based systems to make sure the data elements are not in their negated form using a rule-based negation detection algorithm, the data elements refer to the current status (as opposed to historical condition or a hypothesis for conducting a test) and the data elements correspond to the patient (as opposed to a family member or relative) [41, 42]. Table 5. Regular expressions to extract HF-related terms. Regular  Expression   [^\w]+(h|H)eart\s+(f|F)ailure[^\w]+   [^\w]+(d|D)iastolic\s+(d|D)ysfunction[^\w]+   [^\w]+(c|C)ardiomyopathy[^\w]+   8     [^\w]+HF[^\w]+   4. Evaluation and Results We first evaluated our methods iteratively using the development set of 198 patient reports. A study coordinator read each patient record, extracted data elements of relevance to PARAGON, and matched against the eligibility criteria. For the 198 patient reports, our experienced research coordinator took two weeks (80 hours) to generate the gold standard data. Finally, we had 40 of the 198 patients (20%) prescreened for further analysis according to the eligibility criteria. After consulting a cardiologist, the number of patients finally found eligible was 12. The sheer size of the data that the clinical investigator or research coordinator has to read through is time consuming as well as tedious (Table 1). The number of patients finally qualifying for any clinical trial is always small. This is mostly due to the large number of stringent eligibility criteria. Therefore, it becomes important for an automated system to give more importance to retrieving nearly all the qualifying patients; in other words, the recall of the system should be close to 100%. We tuned our system in order to achieve a high recall (i.e., high sensitivity) so as not to have too many false negatives (which would result in missing potentially eligible patients). On the experiments run on the development dataset, we achieved close to 95% recall with a precision of 86% (F-score of 90%). Table 6 presents further details. Table 6. Outputs for the development dataset. On the validation dataset, we prescreened 113 (3.7%) patients for the PARAGON clinical trial. Our clinical trial study coordinator went through these records and found that 21 of the patients fully qualify for the clinical trial. Twenty-five of the patients require consultation with a cardiologist. However, 67 of the patients do not qualify for the trial. In most cases, this is not because of errors in the prescreening system but due to certain other criteria that have either been not included in the algorithm (for example, certain specific allergies to medication, pregnancy, patient not present in the country, etc.) or are beyond the scope of any system to check due to lack of data (for example, type of cancer or cancer is malignant or benign when the details are not present). We detail some these issues in the Discussion section.   Prescreening  Gold  Standard   (Manual)   Patients  Included     Patients  Excluded   Classification     outcome   (algorithmic)   Patients  included   38   6   Patients  excluded   2   152   9     Table 7 lists the number of patients we discard based on each criteria for both the development and validation dataset. It can be seen that each information extraction module played a major role in screening out large proportions of patients without human involvement. For example, module 2, which extracts LVEF values, discarded 90 patients from the 198-patient development dataset and 672 patients from the 3002-patient validation dataset. This would not have been captured by any methods that aim to prioritize patients using information retrieval approaches without first extracting the values of the relevant data elements from unstructured reports. Table 7. Number of patients discarded at each step for the 198- and 3002-patient datasets Criterion  based  on   Report  Type   Number  of  Patients   Discarded   Age  +  BMI   Encounter  report   22   1071   HF  related  term   Encounter  report/problem  list   3   1597   LVEF   Echo  report   90   672   Angioedema,  Pancreatitis  or  Bilateral   Renal  Artery  stenosis   Encounter  report   44   218   Organ  Transplant  or  ICD   Encounter  report/problem  list   50   806   Malignant  organ  system   Encounter  report/problem  list   49   600   Hb  +  GFR   Lab  value  report   42   507   Blood  Pressure  and  Hypertensive  drugs   Encounter  report   6   522   Pericardial  constriction  or  genetic   hypertrophic  cardiomyopathy  or   infiltrative  cardiomyopathy   Echo  report   23   314   The time taken by our system to successfully parse and extract the required information from different data reports is just 2 minutes (for the whole 198-patient dataset). For 3002 patients, we are able to do so in approximately 20 minutes. Our clinical trial coordinator took almost two weeks to go through each of 198 patients' reports. Thus, the time required for her to go through 3002 patients would have been several months. Instead, she only had to examine the 113 prescreened patients from our system, which only took one week. This demonstrates the usefulness of our system in practical application. However, from these results and observations, we also understand that the system can only be used for prescreening, and further validation by the clinical trial study coordinator or clinical investigator is still required. 5. Discussion We achieved high recall with reasonable precision on our development dataset and were able to replicate the performance on a larger dataset. As with any automated system, there are certain limitations to our proposed architecture, which can be broadly categorized into (1) data processing and (2) data-handling issues. We briefly describe some of these issues. The precision of the system suffers from the complexity of text data. In some cases current unstructured data extractor module is unable to extract terms correctly. For example, the module fails to identify certain HF or ICD related terms. This is due to large number of synonyms and spelling mistakes for the relevant data elements. As mentioned earlier, there are some cases where a patient has certain allergies or may show a certain adverse reaction to a medication, both of which are difficult to extract from unstructured 10     notes because they are not always reported in a standard format within the EHR. There are also cases where the patient has moved out of the hospital's geographic area and therefore cannot provide consent for the clinical trial. These are details that are too patient-specific for automated extraction and can only be checked manually. In some cases, the LVEF value (which is an important factor for inclusion in HF clinical trials) is present in the form of a range or qualitative description. This created a problem while checking for eligibility according to the criterion given. For example, in our clinical trial, we have set the lower limit of LVEF at 45% based on the inclusion criteria. This creates a problem when the value is contained within the range extracted (40% to 45% or 30% to 50%, for example). Our initial approach was to take the average value and compare it with the threshold. However, after consultation with the cardiologist, our approach was deemed inappropriate. Therefore, we subsequently modified our algorithm to include these patients but with a warning regarding their LVEF value. This then served as an indication to the study coordinators to recheck the echocardiogram report (and review the echocardiographic images with the clinical investigator) in order to make further decisions about the patient's eligibility for the clinical trial. There are also some cases where the clinicians are just screening the patient for a particular diagnosis but the patient may not actually have the disease, such as a "malignancy of organ system" check of exclusion criteria. To handle this, we do not discard those patients if we find the "screening" term in the sentences extracted for eligibility check. In similar cases, we also see the term "cancer" instead of "malignancy." However, we cannot discard all patients with the "cancer" term present since some can have a benign diagnosis and not be malignant, and it is impossible for our system to decide if the cancer is malignant or relatively benign. To mitigate these issues, we currently just display a warning in these cases, as we did for LVEF. The coordinator can then perform further checks and decide the classification. In other exclusion criterion where we have to check the B-type natriuretic peptide and glomerular filtration rate values, we face the issues of non-availability and potential outliers in the data. For such cases too, we currently report them as a warning to coordinators for further checking. We also had to deal with data-handling issues in some cases. For example, in criteria where we have to perform a check for recent hemoglobin values, we found that the value may also be present in reports other than just blood reports. To mitigate this issue, we check for hemoglobin values in all reports and then extract the most recent one. Similarly, there were also cases where "end-date" of medication and "department-name" for encounter reports were missing or misplaced. We handled such cases following discussions with the data warehouse coordinator. To summarize, we can deduce that the patient data records are noisy due to various reasons and a preprocessing module is required to handle these issues. 6. Conclusions and Future Work We have presented here a new method for automated clinical trial recruitment system. We have shown, through our results and discussion, that any automated recruitment system suffices as a prescreening process that significantly reduces the workload in recruiting patients, even if it cannot completely replace manual intervention. Our system works on the hypothesis that the performance can be greatly enhanced by converting unstructured free clinical text into a structured 11     form. To validate our hypothesis, we built modules that extract key data elements from the unstructured text on the basis of given eligibility criteria. We evaluated our system on two datasets: one of 198 patients and one of 3002 patients. Our experiments show highly favorable results and affirm our hypothesis. For future research, we aim to evaluate the reproducibility of our system for PARAGON trial at other institutions. We also intend to build further modules to use the framework for other clinical trials. Acknowledgments This work was made possible by funding from the National Library of Medicine: R00LM011389 and R01LM011416 and Novartis. Dr. Sanjiv Shah is supported by grants from the National Institutes of Health (R01 HL107577 and R01 HL127028). The authors acknowledge Prasanth Nannapaneni for his valuable ideas on extracting information from EHR. Disclosures: Dr. Shah reports receiving consulting fees from Novartis, Bayer, AstraZeneca, and Alnylam. References 1.   Jensen,  P.B.,  L.J.  Jensen,  and  S.  Brunak,  Mining  electronic  health  records:  towards  better  research   applications  and  clinical  care.  Nat  Rev  Genet,  2012.  13(6):  p.  395-­‐405.   2.   Sullivan,   J.   Subject   Recruitment   and   Retention:   Barrier   to   Success.   2004     [cited   2015   27   July];   Available   from:   http://www.appliedclinicaltrialsonline.com/subject-­‐recruitment-­‐and-­‐retention-­‐ barriers-­‐success.   3.   Heidenreich,  P.A.,  et  al.,  Forecasting   the   future  of  cardiovascular  disease   in   the  United  States:  a   policy  statement  from  the  American  Heart  Association.  Circulation,  2011.  123(8):  p.  933-­‐44.   4.   Mozaffarian,   D.,   et   al.,   Heart   disease   and   stroke   statistics-­‐-­‐2015   update:   a   report   from   the   American  Heart  Association.  Circulation,  2015.  131(4):  p.  e29-­‐322.   5.   Friedman,  C.P.,  A.K.  Wong,  and  D.  Blumenthal,  Achieving  a  Nationwide  Learning  Health  System.   Science  Translational  Medicine,  2010.  2(57):  p.  57cm29-­‐57cm29.   6.   Friedman,   C.   and  M.   Rigby,  Conceptualising   and   creating   a   global   learning   health   system.   Int   J   Med  Inform,  2013.  82(4):  p.  e63-­‐71.   7.   Ma,   X.-­‐J.,   et   al.,  A   two-­‐gene  expression   ratio  predicts   clinical   outcome   in  breast   cancer   patients   treated  with  tamoxifen.  Cancer  Cell,  2004.  5(6):  p.  607-­‐616.   8.   Strom,  B.L.,  et  al.,  Detecting  pregnancy  use  of  non-­‐hormonal  category  X  medications  in  electronic   medical  records.  Vol.  18.  2011.  i81-­‐i86.   9.   Mathias,  J.S.,  D.  Gossett,  and  D.W.  Baker,  Use  of  electronic  health  record  data  to  evaluate  overuse   of  cervical  cancer  screening.  Vol.  19.  2012.  e96-­‐e101.   10.   De  Pauw,  R.,  et  al.,  Identifying  prognostic  factors  predicting  outcome  in  patients  with  chronic  neck   pain  after  multimodal  treatment:  A  retrospective  study.  Man  Ther,  2015.  20(4):  p.  592-­‐7.   11.   Onofrei,   M.,   et   al.,   A   first   step   towards   translating   evidence   into   practice:   heart   failure   in   a   community  practice-­‐based  research  network.  Inform  Prim  Care,  2004.  12(3):  p.  139-­‐45.   12.   Johnson,   S.B.,   et   al.,  An   Electronic  Health   Record   Based   on   Structured  Narrative.   Journal   of   the   American  Medical  Informatics  Association  :  JAMIA,  2008.  15(1):  p.  54-­‐64.   13.   Zhou,  L.,  et  al.,  How  many  medication  orders  are  entered  through  free-­‐text   in  EHRs?-­‐-­‐a  study  on   hypoglycemic  agents.  AMIA  Annu  Symp  Proc,  2012.  2012:  p.  1079-­‐88.   14.   Zheng,  K.,  et  al.,  Handling  anticipated  exceptions  in  clinical  care:  investigating  clinician  use  of  'exit   strategies'  in  an  electronic  health  records  system.  Vol.  18.  2011.  883-­‐889.   12     15.   Raghavan,  P.,  et  al.,  How  essential  are  unstructured  clinical  narratives  and   information  fusion  to   clinical   trial   recruitment?   AMIA   Summits   on   Translational   Science   Proceedings,   2014.   2014:   p.   218-­‐223.   16.   Stanfill,  M.H.,  et  al.,  A  systematic  literature  review  of  automated  clinical  coding  and  classification   systems.  Vol.  17.  2010.  646-­‐651.   17.   Jha,  A.K.,  The  promise  of   electronic   records:   around   the   corner   or   down   the   road?   JAMA,  2011.   306(8):  p.  880-­‐1.   18.   Friedman,   C.,   T.C.   Rindflesch,   and  M.   Corn,  Natural   language   processing:   State   of   the   art   and   prospects   for   significant   progress,   a   workshop   sponsored   by   the   National   Library   of   Medicine.   Journal  of  Biomedical  Informatics,  2013.  46(5):  p.  765-­‐773.   19.   Shivade,   C.,   et   al.,   A   review   of   approaches   to   identifying   patient   phenotype   cohorts   using   electronic  health  records.  Vol.  21.  2014.  221-­‐230.   20.   Nguyen,   A.N.,   et   al.,   Symbolic   rule-­‐based   classification   of   lung   cancer   stages   from   free-­‐text   pathology  reports.  Vol.  17.  2010.  440-­‐445.   21.   Mia  Schmiedeskamp,   P.P.,   et   al.,  Use   of   International   Classification   of   Diseases,   Ninth   Revision,   Clinical  Modification  Codes  and  Medication  Use  Data   to   Identify  Nosocomial  Clostridium  difficile   Infection  •   Infection  Control  and  Hospital  Epidemiology,  2009.  30(11):  p.  1070-­‐1076.   22.   Penberthy,   L.,   et   al.,   Automated   matching   software   for   clinical   trials   eligibility:   Measuring   efficiency  and  flexibility.  Contemporary  Clinical  Trials,  2010.  31(3):  p.  207-­‐217.   23.   Kho,  A.N.,  et  al.,  Use  of  diverse  electronic  medical  record  systems  to  identify  genetic  risk  for  type  2   diabetes  within  a  genome-­‐wide  association  study.  J  Am  Med  Inform  Assoc,  2012.  19(2):  p.  212-­‐8.   24.   Klompas,  M.,  et  al.,  Automated  identification  of  acute  hepatitis  B  using  electronic  medical  record   data  to  facilitate  public  health  surveillance.  PLoS  One,  2008.  3(7):  p.  e2626.   25.   Mani,  S.,  et  al.,  Early  prediction  of   the   response  of  breast   tumors   to  neoadjuvant  chemotherapy   using  quantitative  MRI  and  machine  learning.  AMIA  Annu  Symp  Proc,  2011.  2011:  p.  868-­‐77.   26.   Van   den   Bulcke,   T.,   et   al.,   Data   mining   methods   for   classification   of   Medium-­‐Chain   Acyl-­‐CoA   dehydrogenase  deficiency  (MCADD)  using  non-­‐derivatized  tandem  MS  neonatal  screening  data.  J   Biomed  Inform,  2011.  44(2):  p.  319-­‐25.   27.   Zhao,   D.   and   C.   Weng,   Combining   PubMed   knowledge   and   EHR   data   to   develop   a   weighted   bayesian  network  for  pancreatic  cancer  prediction.  J  Biomed  Inform,  2011.  44(5):  p.  859-­‐68.   28.   Kawaler,   E.,   et   al.,   Learning   to   predict   post-­‐hospitalization  VTE   risk   from  EHR   data.   AMIA  Annu   Symp  Proc,  2012.  2012:  p.  436-­‐45.   29.   Lowe,   H.J.,   et   al.,   STRIDE-­‐-­‐An   integrated   standards-­‐based   translational   research   informatics   platform.  AMIA  ...  Annual  Symposium  proceedings  /  AMIA  Symposium.  AMIA  Symposium,  2009.   2009:  p.  391-­‐395.   30.   Gregg,  W.,   et   al.,  StarTracker:   an   integrated,  web-­‐based   clinical   search   engine.   AMIA   ...   Annual   Symposium  proceedings  /  AMIA  Symposium.  AMIA  Symposium,  2003:  p.  855.   31.   Hanauer,  D.A.,  et  al.,  Supporting  information  retrieval  from  electronic  health  records:  A  report  of   University   of   Michigan's   nine-­‐year   experience   in   developing   and   using   the   Electronic   Medical   Record  Search  Engine  (EMERSE).  Journal  of  Biomedical  Informatics,  2015.  55:  p.  290-­‐300.   32.   Zalis,  M.  and  M.  Harris,  Advanced  Search  of  the  Electronic  Medical  Record:  Augmenting  Safety  and   Efficiency  in  Radiology.  Journal  of  the  American  College  of  Radiology,  2010.  7(8):  p.  625-­‐633.   33.   Lehman,   L.W.,   et   al.,   Risk   stratification   of   ICU   patients   using   topic   models   inferred   from   unstructured  progress  notes.  AMIA  Annu  Symp  Proc,  2012.  2012:  p.  505-­‐11.   34.   Carroll,  R.J.,  A.E.  Eyler,  and  J.C.  Denny,  Naive  Electronic  Health  Record  phenotype  identification  for   Rheumatoid  arthritis.  AMIA  Annu  Symp  Proc,  2011.  2011:  p.  189-­‐96.   35.   Liao,  K.P.,  et  al.,  Electronic  medical  records  for  discovery  research  in  rheumatoid  arthritis.  Arthritis   Care  &  Research,  2010.  62(8):  p.  1120-­‐1127.   36.   Bejan,  C.A.,  et  al.,  Pneumonia  identification  using  statistical  feature  selection.  Vol.  19.  2012.  817-­‐ 823.   37.   Kopcke,   F.   and   H.U.   Prokosch,   Employing   computers   for   the   recruitment   into   clinical   trials:   a   comprehensive  systematic  review.  J  Med  Internet  Res,  2014.  16(7):  p.  e161.   13     38.   Ni,   Y.,   et   al.,  Automated   clinical   trial   eligibility   prescreening:   increasing   the   efficiency   of   patient   identification  for  clinical  trials  in  the  emergency  department.  J  Am  Med  Inform  Assoc,  2015.  22(1):   p.  166-­‐78.   39.   PARAGON   Inclusion/Exclusion   Criteria.   2015     [cited   2015   10th   August];   Available   from:   https://sjonnalagadda.files.wordpress.com/2015/08/paragon_ie-­‐criteria_10-­‐01-­‐2014.pdf.   40.   Bodenreider,   O.,   The   Unified   Medical   Language   System   (UMLS):   integrating   biomedical   terminology.  Nucleic  Acids  Res,  2004.  32(Database  issue):  p.  D267-­‐70.   41.   Harkema,  H.,   et  al.,  ConText:  an  algorithm   for  determining  negation,   experiencer,  and   temporal   status  from  clinical  reports.  J  Biomed  Inform,  2009.  42(5):  p.  839-­‐51.   42.   Mitchell,   K.J.,   et   al.,   Implementation   and   evaluation   of   a   negation   tagger   in   a   pipeline-­‐based   system  for  information  extract  from  pathology  reports.  Stud  Health  Technol  Inform,  2004.  107(Pt   1):  p.  663-­‐7. Improving average ranking precision in user searches for biomedical research datasets Douglas Teodoro1,2, Luc Mottin1,2, Julien Gobeill1,2, Arnaud Gaudinat2, Thérèse Vachon3, Patrick Ruch1,2 1Text Mining Group, SIB Swiss Institute of Bioinformatics, 1227 Geneva, Switzerland 2Department of Information Science, HEG Geneva, HES-SO, 1227 Geneva, Switzerland 3Novartis Institutes for BioMedical Research – Text Mining Services (NIBR Informatics/TMS), Novartis Pharma AG, Postfach, 4002 Basel, Switzerland Abstract Availability of research datasets is keystone for health and life science study reproducibility and scientific progress. Due to the heterogeneity and complexity of these data, a main challenge to be overcome by research data management systems is to provide users with the best answers for their search queries. In the context of the 2016 bioCADDIE Dataset Retrieval Challenge, we investigate a novel ranking pipeline to improve the search of datasets used in biomedical experiments. Our system comprises a query expansion model based on word embeddings, a similarity measure algorithm that takes into consideration the relevance of the query terms, and a dataset categorisation method that boosts the rank of datasets matching query constraints. The system was evaluated using a corpus with 800k datasets and 21 annotated user queries. Our system provides competitive results when compared to the other challenge participants. In the official run, it achieved the highest infAP among the participants, being +22.3% higher than the median infAP of the participant's best submissions. Overall, it is ranked at top 2 if an aggregated metric using the best official measures per participant is considered. The query expansion method showed positive impact on the system's performance increasing our baseline up to +5.0% and +3.4% for the infAP and infNDCG metrics, respectively. Our similarity measure algorithm seems to be robust, in particular compared to Divergence From Randomness framework, having smaller performance variations under different training conditions. Finally, the result categorization did not have significant impact on the system's performance. We believe that our solution could be used to enhance biomedical dataset management systems. The use of data driven expansion methods, such as those based on word embeddings, could be an alternative to the complexity of biomedical terminologies. Nevertheless, due to the limited size of the assessment set, further experiments need to be performed to draw conclusive results. Introduction The use of search engines in the Web as content indexing but also as data providers, via e.g., hyperlinks and text snippets, such as in Google and PubMed, has changed the way digital libraries are managed from static catalogues and databases to dynamically changing collections in a highly distributed environment. Systems, such as PubMed, PubMed Central (PMC) and Europe PMC, successfully provide platforms for retrieving and accessing information in the scientific literature, an essential step for the progress of biomedical sciences. Biomedical research produces enormous amount of digital data, which is stored in a variety of formats and hosted in a multitude of different sites [1]. Datasets, such as individual-level genotype, protein sequence, pathology imaging and clinical trials, are a few common examples. To guarantee the quality of scientific research, and maximize societal investments, it is key that, in addition to manuscript content, supporting material and datasets used and produced are also accessible and easily searchable, enabling thus the reproduction of key research findings and, additionally, the generation of novel insights [2]. In particular, transparent and integrated access to these datasets is paramount for asserting reproducibility and reliability of research outcomes [2,3]. Consequently, more and more journals request authors to make datasets used and produced in their studies publicly available [4]. Furthermore, providing integrated access to research datasets fosters collaborations and speeds up scientific progress, and, with the advance in data analytics methods, allows the discovery of new knowledge from connected data [5]. Aware of the needs for data sharing in scientific research, several systems are being investigated and implemented to provide flexible and scalable information management that meets the scale and variety of data produced by the biomedical community [6]. For example, dbGaP provides public access to large-scale genetic and phenotypic datasets required for wide association study designs [7]. PhenDisco brings standardization of phenotype variables and of study metadata, and result ranking to dbGaP to improve search performance of phenotypes [8]. GigaDB not only hosts research datasets but also tools, such as executable workflows, and assigns a Digital Object Identifier to datasets, which can be then used and cited by other researchers [9]. OpenAIRE, a large-scale initiative funded by the European Commission, provides an open data infrastructure service that enables collection, interlink and access to research publications and datasets, and to projects of the European Commission and other national funding schemes [10]. Finally, the biomedical and healthCAre Data Discovery Index Ecosystem (bioCADDIE) consortium, funded by the US National Institute of Health Big Data to Knowledge program [11], aims at building a data discovery index that makes data findable, accessible, interoperable and reusable allowing thus biomedical researchers to more easily find, reanalyse, and reuse data [12]. A common characteristic of these systems is that they are all powered by an information retrieval engine that enables indexing of the dataset metadata and content, and allows end users to locate the appropriate research data from the set of indexed repositories. In particular, bioCADDIE implements a search engine prototype, called DataMed, using ElasticSearch [13]. DataMed catalogues research datasets based on a standard core metadata model, called Data Tag Suite (DATS), which is designed to be generic, applicable to any type of dataset, and extendable to specialized data types [14] so that it can accommodate the diversity of research data types and formats. Research datasets are fed into the bioCADDIE architecture through an ingestion repository. Then, an indexing pipeline maps the disparate metadata from the diverse indexed repositories into the unified DATS model [13]. Due to the nature of bioCADDIE datasets and the system's intended use, DataMed faces several challenges to provide end users with a relevant ranking list of datasets for their queries. First, as largely studied in TREC challenges [15-17], in daily usage casual users tend to create small queries, varying usually from one to ten words in length. In general, top-ranked retrieval systems, such as those based on BM25-Okapi similarity measures, underperform in these scenarios in comparison with long queries [15]. Second, the heterogeneity of the research data corpora brings new challenges to search engines and similarity measure algorithms. Differently from the scientific literature, which is composed basically of text data types, research datasets are available in a myriad of multimodal formats, varying from gene expression and protein sequence data to results of bioassays and exhibiting contents generated over several years of development. Finally, constraints in query specifications posed by casual users, such as the dataset type of interest, makes the ranking task closer to the more sophisticated question-answering search and requires additional work in the original ranked list [18-21]. To improve search of biomedical research datasets, in this work we investigate a novel dataset ranking pipeline that goes beyond the use of metadata available in DataMed. In the context of the 2016 bioCADDIE Dataset Retrieval Challenge [22], which aimed at enhancing the indexing and searching features of bioCADDIE's DataMed, we have developed a few strategies to deal with short queries, disparate dataset corpora and user query constraints. Our approach includes a query expansion module that computes the word embedding for query terms and performs their expansion using the k-nearest embedded word vectors [23,24]. Query expansion based on similarity of embedded words has been successfully applied to enrich queries [25,26]. The word embedding algorithm is trained locally using different biomedical corpora. We have also developed an original similarity measure algorithm, named RTRL, which takes into account the relevance of the query terms to boost the ranking of datasets. The algorithm divides query terms into non-relevant, relevant and key-relevant. Then, it attempts to rank datasets containing most of key relevant terms on top. Finally, we have created a query and dataset classifier module that boosts datasets matching the query class in the ranking list [27,28]. The classifier uses the Universal Protein Resource1 (UniProt) topics to constrain query and datasets to a set of biomedical topics [29]. Our system was evaluated using a corpus of around 800k datasets and 21 queries, and achieved competitive results in comparison with other participants. In the next sections, we describe in detail our approach and the results obtained. 1 www.uniprot.org Materials and methods In Figure 1, the architecture of our dataset information retrieval system is described: given as input the bioCADDIE dataset corpus and a set of user queries, we have (a) a pre-processing phase, where both the dataset corpus and the query are cleaned, and the query terms are enriched, (b) a ranking phase, where the query terms are compared to the dataset corpus, and a ranked list of datasets, which are likely answers to the query, is obtained, and (c) a post-processing phase, where the results are refined based on the categorization of the top-ranked datasets in relation to the input query. Processes (a), (b) and (c) are run online. In the background, we have three processes that run on a batch mode: (1) training of a word embedding model based on neural network, (2) bioCADDIE corpus indexing and (3) training of a dataset classification model. The word embedding model is used on the query pre-processing phase to expand query terms. The index created is used to actually provide the query answers, via a ranking model that computes the similarity between the indexed datasets and the query terms. Finally, the dataset classification model is used to constrain (or improve the ranking of) the results matching the input query constraints (e.g., gene expression dataset, protein sequence data, etc.). Input data bioCADDIE dataset corpus In Table 1, an example of bioCADDIE's dataset representation following the DATS data model is presented. The DOCNO tag is a unique identifier for the dataset. The TITLE tag provides the dataset title, which is usually a meaningful and concise description of the dataset but that can be also sometimes just a code string identifying the dataset. The REPOSITORY tag indicates the repository hosting the original dataset content. The repository name is appended with a snapshot date, indicating the dataset version date. Finally, the METADATA tag is a JSON object with many attributes describing the dataset but also containing parts of the dataset itself. The METADATA tag is the most informative part of the dataset, containing altogether 96 attributes that vary from dataset description and organism to chemical formula. The first prototype of DataMed available at datamed.org integrates an initial set of 23 repositories covering 10 data types. In the challenge, a subset of this indexed collection was provided to the participants to build their information retrieval strategies. Table 2Error! Reference source not found. shows the repositories and the dataset distribution for the subset available. In total, there were 794,992 datasets distributed among 20 repositories. As we can see, there is a concentration of datasets in some repositories, in particular ClinicalTrials, BioProject, PDB and GEO, each of which contains more than 100k datasets. These top four repositories constitute 71.4% of the datasets and the top eight repositories constitute 99.5% of the total corpus. Training and assessment queries Among others, bioCADDIE framework aims to answer user queries such as i) disease-based search across scales; ii) molecular-based search across organisms and scales, iii) molecular data/phenotype associations; and iv) behavioural and environmental data [13]. The challenge organizers provided 21 annotated queries covering these use cases. Six queries were made available at the beginning of the challenge to train the information retrieval system and 15 queries were used to do the official assessment. Table 3 shows some query examples as provided during the challenge. The organisers generated the initial query answers combining the top 1000 datasets retrieved by 4 information retrieval engines: Apache Lucene2 , Lemur Indri3, Terrier4 , and Semantic Vectors5. Then, the documents retrieved were classified as not relevant, if they had less than 50% of the key query concepts; partially relevant, if they had all key query concepts but did not answer the question, or if they had the majority of the concepts but not all; and relevant, if they had all key query concepts and were an answer to the question. Key query concepts were defined according to the annotator's expertise. Finally, the query results were annotated by two annotators and corrected whenever necessary by a domain expert. For the test queries, a post-submission judgment was performed by pooling the participant's results and enriching the original set obtained with the 4 original information retrieval engines. The full query list is available in the Train and test queries section (Table S1) as supplementary material. 2 https://lucene.apache.org/core 3 https://www.lemurproject.org/indri 4 http://terrier.org 5 https://github.com/semanticvectors/semanticvectors Pre-processing phase Data treatment In the pre-processing phase (Figure 1a), we treat the input data corpus and query. Greek letters are substituted by their literal names, stopwords are removed and terms are stemmed using Porter stemmer. Then, all non-alphanumeric characters are removed and terms are truncated to 20 characters. Finally, numerical sequences are replaced with the literal _number_. Query expansion Recent results demonstrate the effectiveness of continuous space word embedding methods for analogy and word similarity tasks [25,26]. Continuous space embedding models project terms from a vocabulary into real-number vectors in a low-dimensional space [23]. Word representations in continuous space embeddings are modelled by assuming that if two words tend to occur in similar contexts, they are likely to play similar syntactic and semantic roles. We propose to use the word2vec algorithm [24] for word embedding although other continuous space methods behave similarly for query expansion [30]. In one of its model, so called continuous bag of words (CBOW), the word2vec algorithm implements a neural network that predicts a word given a small set of context words. Our query expansion model uses the k-nearest neighbours of the query terms in the embedding space. Formally, let ! be a given user query consisting of the words "#, "%, ... , "'. Let (#), (%) , ... , (*) be the k-nearest neighbours of ") in the embedding space. Then, the vectors (+) constitute the set of term expansion candidates. We have created two expansion models based on this algorithm. The first model considers all query terms equally relevant for expansion. Original query terms are assigned to the unitary weight and the weights of the expanded terms are determined by their cosine similarity to the original query term in the embedding space. The second model implements a similar algorithm but with the difference that it reduces the weight of a query term and of its expansions by a factor c if the original term is not a key-relevant query term. We consider key-relevant terms as those with lower document frequency in the collection. For a given query, terms with document frequency lower or equal than 50% compared to the other query term document frequencies are considered key-relevant query terms. For example, in the query "transgenic mice", if the term transgenic has lower document frequency in the collection compared to the term mice, then transgenic will be considered a key-relevant query term and mice as a relevant query term. Thus, the weight of the term mice and of its expansions will be reduced by a loss factor l. We use the Gensim word2vec library [31] to create the word embeddings with different input corpora: bioCADDIE (800k), PMC (36k) and Medline (200k). These collections were selected due to their relevance to biomedicine. It has been shown that locally trained word embeddings might provide superior word representation [25]. Therefore, we only considered external resources that have similar context to bioCADDIE's datasets. In particular, only articles annotated with UniProt terms were selected for the PMC and Medline collections. The word2vec neural network was trained using the continuous bag-of-words model with the dimensionality of the word embeddings set to 200 and the window size set to 5. The loss factor l for non key-relevant terms in the second query expansion model was empirically set to 1%. Ranking phase For the dataset ranking (Figure 1b), we have investigated a similarity measure to robustly cope with the small query sizes expected from bioCADDIE users. As it has been shown in previous TREC competitions, the performance of more sophisticate ranking models, such as BM25-Okapi and Divergence from Randomness (DFR), tends to degrade when short queries are posed [15,32]. Simpler models often outperform these models. The main issue with more complex models for short queries is the tuning of the term frequency normalization parameter [32]. While for long queries this effect is smoothed due the abundance of context terms, for short queries the fine-tuning of the normalization parameters becomes much more relevant. In this context, we propose a simpler similarity model that we expect to be more robust to the changes in the training sets. Our model, called robust term relevance logic (RTRL), considers that a user query contains three types of terms: non-relevant, relevant and key-relevant. The rationale behind our similarity model is that if a document contains all key-relevant terms of a query, it should be ranked at the top. The remaining relevant terms are used to improve the ranking of these documents in the ranking list. For example, for the query "Find data on T-cell homeostasis related to multiple sclerosis across all databases", after removing non-relevant query terms (i.e., query stopwords), we remain with the terms T-cell, homeostasis, multiple and sclerosis. Let us consider that the terms T-cell and multiple are relevant terms, and the terms homeostasis and sclerosis are the key-relevant terms for the query. Hence, documents containing the terms homeostasis and sclerosis should be at least as relevant to the query as documents containing T-cell, homeostasis, and multiple terms or as documents containing T-cell, multiple and sclerosis terms. More formally, for a query ! = "#, "%, ... , "- the weight of a key-relevant term is given by the equation .(′′ ≥ . − 1 (′′ + 4 − . (′, (1) where L is the number of query terms, k is the number of key-relevant query terms, and (′ and (′′ are the weights of the relevant and key-relevant query terms respectively. Additionally, to take into account the relevance of a query term to a document, the weight parameter of a term can be expanded into two components, ( = (5 + (6, where the component (5 captures the relevance of a query term for a document and the component (6 captures the term relevance within the query. Substituting this expansion in Eq. 1 and considering the worst case scenario, where top ranked documents contain only key-relevant terms with low document relevance, we have . (5′ + (6′′ ≥ . − 1 (5′′ + (6′′ + (4 − .) (5′′ + (6′ , (2) where (5′ and (5′′ are the weights of a term with low and high document relevance respectively, and (6′ and (6′′ are the weights of a term with low and high query relevance respectively (i.e., relevant and key-relevant terms). To solve Eq. 2, we can arbitrarily assign values to the weight variables (′ and (′′ in such a way that the inequality constraint is respected. In our experiments, we fixed the values for the low document and query relevance weights to (5′ = (6′ = 1 and considered the weight of a high relevant document term twice the weight of a low relevant document term, i.e., (5′′ ≡ 2(5′ . Furthermore, we assume that the number of key and non-key relevant is roughly the same, i.e., . ≈ 4/2. Thus, we end up with a single free variable, the weight of a key-relevant term (6==, which is a function of the query length and can be given by (6′′ = ? 24 − 2 , (3) where ? ≥ 1 determines the gain in the key-relevant terms in relation to the other query terms. Then, the score of a document @ ∈ B will be given by the equation score @, ! = ( H, @I∈J = ((5 + (6)I∈J . (4) In our experiments, we use the same definition for relevant and key-relevant as presented in the Query expansion section, i.e., key-relevant terms have a lower collection occurrence KI,L when compared to relevant terms. For the document relevance, we consider high document relevance when the term document occurrence KI,5is more than one. Otherwise, the document relevance is low. Hence, (5 = (5 = = 1 if KI,5 = 1 (5′′ = 2(5′ if KI,5 > 1 , (5) and (6 = (6= = 1 if KI,L > median(KJ,L) (6== = ?(24 − 2) if KI,L ≤ median(KJ,L) . (6) Non-relevant terms are basically stopwords and are neglected during the similarity computation. When applying the weighting model described in Eq. 4 to the query terms, documents will be ranked into relevance bins. For example, for a query with 4 terms, being q1 and q2 relevant terms and q3 and q4 key-relevant, if a document d1 contains only one occurrence of q3 and d2 only one occurrence of q4, and none of the other terms, then using Eq. 4, Eq. 5 and Eq. 6 these documents will be ranked in the same ranking bin (U?VWX5# = U?VWX5% = 1). However, the discrimination power of q3 and q4 might be different. Then, to rank these documents within their bin, we add a HK ∙ Z@K component to Eq. 4, i.e., score @, ! [ = (5 + (6 [I∈J + HK ∙ Z@K[, (7) where 0 < HK ∙ Z@K[ ≤ 1 and is computed for each score bin score @, ! [. The HK and Z@Kcomponents are defined as: HK H, @ = 0.5 + 0.5 `a,b max{`ae,b:I=∈5} , (8) Z@K H, B = log |L| |{5∈L:I∈5}| . (9) Post-processing phase In the post-processing phase (Figure 1c), the initial results obtained during the ranking phase are modified so that the rank of documents matching the query constraints is boosted. For example, for queries searching specifically for gene expression datasets, the datasets retrieved containing gene expression information shall have higher ranks than those that do not contain gene expression information, regardless of the appearance of the terms gene and expression in the dataset body. Our query and result categorization model is based on the UniProt categories. UniProt has been developing a categorization model for many years now, where UniProt curators classify and annotate biomedical datasets into 11 categories: Expression, Family & Domains, Function, Interaction, Names, Pathology & Biotech, PTM/processing, Sequences, Structure, Subcellular location, Unclassified (or miscellaneous classes). UniProt curators use these categories to annotate scientific articles available in PubMed. Each annotation provides a category, such as Sequences, and a qualifier to justify the citation, e.g., cited for 'NUCLEOTIDE SEQUENCE [MRNA]'. These classes cover relatively well the domain of bioCADDIE datasets and queries. A dataset class that is largely present in bioCADDIE but is not in UniProt is Clinical trial. Thus, we add this class to the other 11 UniProt classes to increase our classification coverage. We perform the re-ranking for the datasets originally ranked in the top 50% positions having the highest rank as reference. The datasets matching the query constraint classes are then multiplied by a gain factor g, which was set empirically to 10% for the official results. Query classification model The query categorization is performed using simple string matching algorithm. 3-skip-2-grams, 2-skip-2-grams (2,2) and 1-gram are extracted from the query and matched against the UniProt categories and qualifiers. For the Clinical trial class, we defined a set of clinical trial keywords, namely inclusion, exclusion, criteria, patients, subjects, stage, duration and study. Then, we search for these keywords in the input query and classify them into the Clinical trial class in case a match is found. Dataset classification model We use a multilayer perceptron (MLP) classifier to classify automatically bioCADDIE datasets against the UniProt classes mentioned above [29]. The input layer of the MLP is fed by documents (datasets) and the output layer is mapped to the UniProt classes (i.e., a multi-label multi-class classification task). The input documents (or datasets) are embedded into a vector space using the paragraph2vec model [33]. Paragraph2vec is a variation of the word2vec model that takes into account the document identifier in the context. The classifier is trained using 200k PubMed abstracts annotated with UniProt classes. We use the paragraph2vec implementation from Gensim (doc2vec) [31] to embed the documents/datasets. The classification model achieves a F1 score of 0.80. For the Clinical trial class, we use the keywords clinical and trial and clinicaltrial to categorize a document as clinical trial. If a bioCADDIE dataset contains one of these keywords they are classified into the Clinical trial class. In the online phase, new unclassified documents coming from the result set produced in the ranking phase are embedded in the paragraph2vec space and are fed into the MLP classifier, which produces the UniProt classes. In parallel, a string matching algorithm searches for the Clinical trial class keywords into the dataset content. Evaluation criteria Performance results are reported using the inferred Average Precision (infAP) and inferred Normalized Discounted Cumulative Gain (infNDGC) metrics [34]. Both measures are designed to deal with incomplete judgements, as it is often the case for very large corpora, such as the collection indexed by DataMed. Hence, infAP and infNDCG deploy a randomly sampling approach to infer the mean AP (MAP) and NDCG performances, respectively. Additionally, we report the precision at 10 (P@10) considering partially relevant answers as relevant (+partial) but also as not relevant (-partial). These metrics compose the official measures of the 2016 bioCADDIE Dataset Retrieval Challenge. The organisers associated weight 2 to relevant, weight 1 to partially relevant, weight 0 to not relevant and weight -1 to unjudged documents in the gold standard query relevance file (see [35] for detailed information). We provide the results of the training and official assessments. Furthermore, we performed a 5-fold cross-validation using the post-judgement official gold standard. We compare the performance of our methods to a baseline approach based on the DFR model implemented by Terrier 4.1 [36,37]. Statistic hypothesis testing is performed using paired t-test with a two-tailed distribution and results with P-value smaller than .05 are considered statistically significant. Results We assessed 5 information retrieval models using the bioCADDIE corpus and the annotated queries. The first model, sibtex-1, is a baseline model based on the DFR algorithm. For this model, in the pre-processing phase, the query and input corpus are treated and the similarity is computed using Terrier. The other 4 models use the methods described in the previous section. For the second model, sibtex-2, in the pre-processing phase, in addition to the query and corpus treatment, the query terms are expanded using the word2vec algorithm trained on the bioCADDIE and PMC collections, and in the post-processing phase the results are categorized using our classifier. The third model, sibtex-3, implements a similar algorithm to sibtex-2. The difference is that the weight of expanded terms derived from non key-relevant terms are reduced. For both models sibtex-2 and sibtex-3 the similarity score is obtained using Terrier's DFR algorithm. The fourth model, sibtex-4, uses the RTRL similarity model, which takes into account the relevance of the query terms in the ranking phase, and in the post-processing phase the results are categorized. Finally, the fifth model, sibtex-5, combines linearly the results of ranking model sibtex-3 and sibtex-4, i.e., sibtex-5 = nsibtex-3 + 1 − n sibtex-4, where α = 0.5 for the official submission. In resume, we assess the following method and model combinations: • sibtex-1: DFR-based; • sibtex-2: sibtex-1 + query expansion + results categorisation; • sibtex-3: sibtex-1 + query expansion with penalized terms + results categorisation; • sibtex-4: RTRL similarity measure + results categorisation; • sibtex-5: linear combination of sibtex-3 and sibtex-4. As an example of how the data processing works during an online search, let us consider the input query "T1 – Find protein sequencing data related to bacterial chemotaxis across all databases" submitted to model sibtex-2 (see Train and test queries section, provided as supplementary material, for the whole list of queries). In the pre-processing phase (Figure 1a), the cleaning process removes the non-relevant query terms (query stopwords) and reduces the remaining terms to their stem, leaving only the terms "protein", "sequenc", "bacteri" and "chemotaxi". These terms are then expanded, resulting in an array with weights proportionally to their similarity to the original term. For example, the expansion of the term chemotaxis with k=3 results into the following weighted array: [chemotactic^0.089, chemoattractant^0.083, motility^0.080]. These terms are then stemmed and, together with the other original (with unitary weight) and expanded terms, processed by the ranking algorithm (Figure 1b). The ranking algorithm computes the query-to-dataset similarity and provides the ranked list of most similar datasets to the expanded query. Finally, in the post-processing phase (Figure 1c), the original query is classified into the UniProt/clinical trial classes using string matching against the UniProt class descriptors or clinical trial keywords. In this example, the query T1 is classified as searching for "Sequences" datasets since the bigram "protein sequence" matches one of the UniProt class qualifiers for the Sequences class. In parallel, the result set containing the dataset retrieved is classified using the UniProt classifier. For example, the content of the dataset with identifier 719124, which appears in the top 1 for this query, is submit to the classifier and as output the classifier categorizes it as belonging to the Sequences class. Thus, its ranking score is boosted by 10%. This process is repeated for the top ranked datasets (we use a 50% threshold) and the final result set is obtained by rearranging the new dataset scores. Performance results on the training set Table 4 shows the performance of 5 information retrieval models for the 6 training queries. As we can see, the model sibtex-3 outperforms the other models for the infAP and infNDCG metrics, with a marginal improvement over the baseline of +0.9% for the infAP metric and +0.4% for the infNDCG metric. For the P@10 metric, the models sibtex-4 and sibtex-5 outperform the other models, improving the baseline by +20%, with the latter having enhanced infAP and infNDCG outcomes with respect to the first. The optimized parameters for the different models are shown as supplementary material in Table S2 of the Training parameters section. Performance results on the official test set The official performance results of the bioCADDIE challenge for our 5 models are displayed in Table 5. As we can notice, the sibtex-5 model outperforms all the other models for all metrics apart from infNDCG, for which the sibtex-4 model achieves the best performance. In particular, the sibtex-5 model outperforms the inferred average precision of the baseline by +21.89%. Apart from model sibtex-2, the other 3 models outperform the baseline for the infAP metric. There was a decrease in performance for the models sibtex-2 and sibtex-3 for the metric infNDCG with respect to the baseline, suggesting that the use of query expansion have a negative impact for this metric. The models using our similarity measure were able though to outperform the baseline for this metric. Specifically, only model sibtex-5 was able to increase performance for the infNDCG@10 results with respect to the baseline. The linear combination of results sibtex-3 and sibtex-4 to obtain sibtex-5 improved a -0.3% and -8.7% decrease in performance for sibtex-3 and sibtex-4 models, respectively, to a positive outcome of +9.32% for the infNDCG@10 metric. Similarly, for the P@10 (+partial) metric, i.e., considering partial answers as relevant, only model sibtex-5 outperforms the baseline. Again, surprisingly, the linear combination of two negative performances (or neutral, in this case for sibtex-3) with respect to the baseline resulted in an increase in performance of +6.6% for the P@10 (+partial) metric. Finally, all the models outperformed the baseline for the P@10 (-partial) metric, i.e., considering partial answers as not relevant, with emphasis on the model sibtex-5, which improved the baseline by +8.3%. Despite the high difference in relative performance between the models, there is not statistically significant difference between them. Table 6 presents the overall bioCADDIE challenge performance results aggregated over the participant's top systems for each metric in addition to our individual best results. In particular, the model sibtex-5 achieved the highest inferred average precision among the 10 participants. If we take the overall median result as baseline, our best system would improve the baseline by +22.4% for the infAP metric. Our results for the infNDCG and P@10 (+partial) metrics are not as expressive, for which there is only a marginal improvement over the median score of +0.2% and +0.4%, respectively. For these two metrics, our best system was ranked into 5 out of 10. For the NDCG@10 and P@10 (-partial) metrics, our best system was ranked 3/10 and 2/10, and improved the overall median score by +8.0% and +11.8%, respectively. To be able to compare the overall systems, we computed the Unanimous Improvement Ratio (UIR) metric [38] using the participant's best results achieve in the official run. While this metric is meaningless from the information retrieval viewpoint, it is useful to understand how our overall strategies behave with respect to the other participants. According to this metric, our system ranks overall on top 2 with a URI score of 0.51. Finally, using the results of all participants, we computed the correlation matrix for the official metrics. The correlation coefficients are displayed in Table 7. Metrics NDCG@10, P@10 (+partial) and P@10 (-partial) present among them moderately strong to strong correlations 0.6 < ρ ≤ 1.0 while the other metrics show modest to moderate correlations (0.2 < ρ ≤ 0.6). Particularly, there is a high correlation (ρ = 0.86) between the P@10 (+partial) and NDCG@10 metrics, which can be indeed verified on our system's results (see Table 5). Nonetheless, while our system ranks at top 5 for the P@10 (+partial) metric, it ranks at top 3 for the NDCG@10 metric. Post-official assessment Due to the small size of the original training dataset (6 queries), it was a challenge to tune all model parameters for the official run. Hence, as described in the Methods section many of them were empirically set. With the disclosure of the challenge results and the gold standard for the test queries, we performed a new experiment where we split the gold standard results into 5-folds, each fold containing 12 training and 3 test queries, and assess the models using cross-validation (see Table S3 in the supplementary materials for parameter setting values). We consider only the results of the test queries because their quality is very different from the training set, due to the pooling method performed after the participants submission. Additionally, it allows us to compare the new results with the official results. The results obtained are displayed in Table 8. First, we can notice a considerable improvement of the baseline model in comparison with the official results (infAP: +20.0%; infNDCG: +7.0%; and P@10 (+partial): +17.9%). Indeed, the performance of the models based only the DFR algorithm have on average improvements of +18.7% and +9.2% for the infAP and infNDCG metrics, respectively. On the other hand, the performance of the model based only on the RTRL similarity measure (i.e., sibtex-4) was more robust for these metrics, varying only -0.03% and +0.29%, respectively. Differently from the official results, the best infAP and infNDCG outcomes are now achieved using sibtex-3 and sibtex-2 models, respectively. Sibtex-5 is still the highest performer for the P@10 (+partial) metric. However, sibtex-4 and sibtex-5 models no longer outperform the baseline for the infAP and infNDCG metrics. Furthermore, in this new setting, the increase in performance of models sibtex-2 and sibtex-3 for the metric infAP is statistically significant (P=.049 and P=.022). Table 8 also shows the results for the models sibtex-2, sibtex-3 and sibtex-4 without the post-processing phase (result categorization). We can see that a significant categorization gain is verified only for metric P@10 (+partial) of the sibtex-4 model (+1.3%). For the other metrics and models, the change in performance is marginal. Thus, the actual gain in performance for models sibtex-2 and sibtex-3 in comparison with the baseline derives from the query expansion in the pre-processing phase. Query-wise analyses To understand how the different models perform at the query level, we computed the individual query infAP, infNDCG and P@10 (+partial) metrics as showed in Figure 2, Figure 3 and Figure 4, respectively. We use the results of the cross validated experiment since we believe they are better tuned compared to the official run. For infAP metric (Figure 2), we can notice that best performing model, sibtex-3, outperforms all the other models for 4 queries (T1, T6, T8 and T14) while worst performing model, sibtex-1, outperforms all other models for only one query (T3). Models sibtex-2, sibtex-4 and sibtex-5 completely outperform the other models for 3 queries each. For infNDCG metric (Figure 3), model sibtex-1 and sibtex-4 completely outperform the other models for 5 queries each while model sibtex-3, the worst performer for this metric, does not outperform all the other models for any query (conversely, note that on average sibtex-3 has the second best infNDCG). Lastly, for P@10 (+partial) metric (Figure 4), model sibtex-4 outperforms the other models for queries T1, T2, T7 and T9 while models sibtex-2 and sibtex-3 do not outperform all the other models for any query. These results are opposed to the average P@10 (+partial) marks, for which sibtex-3 and sibtex-2 models are, together with sibtex-5, the best performers and sibtex-4 has the poorest results. Thus, from the individual queries view for the tested set, the enhancing characteristics of the models seem to be query specific, without a particular dominant feature that improves the overall results. Indeed, only query T9, Search for data of all types related to the ob gene in obese M. musculus across all databases, has consistent highest performance for model sibtex-3, outperforming the other models for infAP, infNDCG and P@10 (+partial) metrics. On the other side, query T1, Find protein sequencing data related to bacterial chemotaxis across all databases, has highest performance marks for different models (sibtex-3 for infAP; sibtex-1 for infNDCG; sibtex-4 for P@10 (+partial)). Indeed, query T1 shows peculiar results. While it has high precision for the top 10 retrieved datasets (Figure 4), it has the lowest recall among the 15 official queries for the 5 models (mean recall=0.25; SD=0.01), likely leading to the low infAP (Figure 2). Moreover, for infNDCG metric (Figure 3), it presents average (RTRL model) to high (DFR models) marks depending on the base similarity model. At the query level, we can also note that queries T3, T4 and T15 obtain consistently good results, having their performance marks higher than the 3rd quartile threshold for the different metrics for at least one model. On the other hand, queries T5 and T8 have overall worst performances, being below the 1st quartile threshold for all the models for at least two metrics. In the example of T5, Search for gene expression and genetic deletion data that mention CD69 in memory augmentation studies across all databases, the key relevant terms are CD69 and memory. However, they are not present simultaneously in any of the datasets judged as relevant. Additionally, as showed in Figure 2 and Figure 3, for query T5 the expansion step did not succeed in bringing additional equivalent terms that could enrich the original query. Indeed, it is the combination of the DFR and RTLR similarity models that boosts relevant datasets to the top ranks as showed in Figure 4 (note that models sibtex-2 and sibtex-3 are the low performers). On the other hand, for query T8, Search for proteomic data related to regulation of calcium in blind D. melanogaster, the query expansion process is able to effectively enrich the query, resulting in a 17% higher recall (from 42 to 49) for models sibtex-2 and sibtex-3 in comparison with sibtex-1. Nevertheless, as showed in Figure 2, the overall results for query T8 is still poor. Query expansion In this section, we analyse the effect of the different collections for query expansion on the information retrieval performance. Table 9 shows some examples of terms expanded using the bioCADDIE, PMC and Medline corpora. The terms were expanded using the 10 nearest vectors in the embedding space. The similarity score between the original term and the expanded term is also provided. As we can notice, the terms expanded can be syntactic variations of the original term, e.g., cancer and cancers; semantic synonyms as in cancer and tumour; term subclasses, e.g., cancer and carcinoma; term superclasses, e.g., human and vertebrate; but also, just a simple co-occurrence term that has neither syntactic nor semantic relations to the original term, e.g., human and also and repair and mus7. In Table 10, we present the performance results of the information retrieval system using the different collections in the pre-processing phase to expand the terms. Using the bioCADDIE collection for training the word vectors marginally improves the baseline (sibtex-1) for the infNDCG metric while it degrades the results for the other metrics. On the other hand, using the PMC collection marginally improves the infAP metric while also degrading the other two metrics. Finally, the Medline collection improves all the baseline metrics, increasing the infAP metric by +4.1%, the infNDCG metric by +3.4%, and the P@10 (+partial) metric by +3.4%. Discussion In this manuscript, we described our methods to improve ranking of biomedical datasets using an information retrieval engine. The biomedical community is increasingly developing data management systems based on advanced text analytics to cope with extremely large size and variety of datasets involved in scientific research [6-10,39]. Indeed, this trend is not specific to publicly available data but also directly affects large pharma and biotech companies, which are struggling to transform silo-based drug development models into more integrated platforms powered with advanced text analytics [40]. Here the dataset retrieval model is especially adequate if we consider that user queries in such large companies are likely to involve simultaneously a large set of orthogonal entities (a chemical compound, a therapeutic indication, a genetic profile, ...) from a large set of perspective (efficacy, toxicity, mode of action...) and modalities (structured laboratory results, sequences, chemical structures, textual reports, images...). Our work was developed in the context of the 2016 bioCADDIE Dataset Retrieval Challenge, which aimed at developing innovative methods to retrieve biomedical datasets relevant to researcher's needs. As strategies to enhance retrieval performance, our system proposes a query expansion model based on word embeddings, a similarity model (RTRL) that discriminates key-relevant terms from the user query, and a query and dataset categorizer that boosts the ranking of matching query-dataset results. Our retrieval system was evaluated using a corpus of 800k datasets and 15 official assessment queries. It achieved the highest inferred average precision among the 10 challenge participants. If the aggregation of the best official metrics per participant is taken into account, it also achieved very competitive results, figuring on an overall top-2 position. Nevertheless, due to the limited size of the assessment set, i.e., 15 queries, it is hard to extract statistically significant comparisons among different systems and methods. The approach developed in this manuscript is data driven rather than model driven. As such, it could with some effort be transposed to other domains. In particular, its application could be interesting in the health big data context. One of the main challenges for health and clinical research in large and distributed environments is to index and search for existing datasets, in particular for cohort identification in privacy-preserving frameworks. There are many systems being investigated and developed that tackles the issue of preserving individual and patient privacy and confidentiality so that personal datasets can be ethically shared within trusted networks [41,42,43]. While these systems are very good in keeping data safe and private, they usually lack functionalities that would allow the value of healthcare datasets to be unlocked. If inserted in such secure frameworks, the methodology described here could be a first step for indexing and effectively searching for shareable datasets to enable, for example, posterior patient counting and individual consent requests. A main requirement for employing our approach to related use-cases would be the availability of a corpus composed of dataset metadata extracts pointing to encoded dataset object identifiers, which would roughly describe the content of the dataset, as provided for example by the European Genome-phenome Archive [43]. Query expansion approach The analysis of the results show that use of the query expansion algorithm based on continuous embedding space enhanced the average performance of the ranking system. Exploiting a Medline collection with 200k abstracts as the input for the word2vec algorithm resulted in a query expansion model that provides statistically significant improvements in comparison with the baseline (sibtex-3: +5.0% for the infAP metric). Given the diversity and richness of bioCADDIE dataset, we argue that it is impractical and unnecessary to implement syntactic and semantic query expansion services using the myriad of biomedical information resources scattered in the web if the goal is to produce richer user queries. Instead, we propose to use either the bioCADDIE dataset itself or a comprehensive biomedical corpus resource, such as Medline or PMC, fed to a continuous embedding space algorithm to extract syntactically and semantically closest concepts to a query term. This approach has two advantages: i) instead of having a query expansion service connected to external resources that are prone to changes over time, we need only a single resource that can be stored and managed locally; ii) it might be able to provide more up-to-date terms, since new terms are likely to appear first in the scientific literature than on curated terminologies. The main drawback of such approach is that, as showed in Table 9, there is little control on the type of terms resulted from the expansion, i.e., the algorithm can produce synonyms, syntactic variations, super classes, sub classes, etc. Thus, this approach should fit more with a user supervised expansion than as deployed in our system. One of the main challenges for query expansion is to define which expanded terms are actually relevant for the querying task. Syntactically and semantically close terms, as those provided by word2vec, are natural candidates. However, the degree of proximity can vary based on several factors, such as the type of query, the input corpus, the expansion model, etc. In our setup, to select and weight the candidate terms from the embedding vector space, we combine a term neighbourhood (trained for k=10 in the official run) with a similarity based threshold, by taking into account the cosine similarity of the expanded term as a query weight variable. There are many other possibilities for query expansion though, such as training a word similarity cut-off, creating word clusters in the embedding space (e.g., based on k-means) or even performing the expansion per query rather than per term. During the training phase, we have indeed assessed a different method, based purely on the term similarity. In our experiments, the neighbourhood based method described in the manuscript yielded better performance (e.g. for sibtex-2: sim=0.80 -> infAP=0.0505; sim=0.85 -> infAP=0.0530; sim=0.90 -> infAP=0.0542; sim=0.95 -> infAP=0.0556; k=10 -> infAP=0.0570). Nevertheless, we were more interested at exploring different biomedical corpora as expansion sources and consider the assessment of the various query expansion threshold algorithms as a subject for another work. RTRL similarity measure algorithm Despite the simplicity of our similarity model, it provided competitive results in comparison with more complex and powerful models, such as the DFR framework. Specifically, our model seemed to be more robust to different short query and training scenarios. While there was a large performance variance between the official run and the cross-validated run for the DFR-based system (infAP: 18.3%; infNDCG: 8.6%), the RTRL similarity measure model showed a small performance difference under these distinctive tuning scenarios (infAP: 2.9%; infNDCG: 0.6%). Indeed, using Eq. 5 and Eq. 6 to represent if-then fuzzy rule-based forms and Eq. 7 to represent if-then fuzzy rule-based form with multiple conjunctive antecedents, our similarity measure approach can be reduced to a fuzzy logic based method. As has been showed by Gupta et al. [44], these systems outperform ranking models, such as BM25-Okapi, in certain ranking scenarios. One of main assumptions of the RTRL similarity measure algorithm is that the system shall deal with small user queries, i.e., with 10 or less terms. Therefore, logic components, such as query term frequency, can be removed from the equation as the number of word occurrences in the query is unlikely to be relevant in these cases (most likely one occurrence per query). In addition, the algorithm implements only a concise set of key information retrieval ranking rules. Thus, even compared to other fuzzy logic models, our approach is still simpler. We believe that the robustness of the results may derive from this characteristic, as the system might be less prone to overfitting. However, a larger training and assessment sets are still necessary to generalize such observations. Query and dataset categorization Our strategy for boosting performance using result classification according to the query constraints did not achieve significant improvements for most of the metrics and systems. As we can see from Table 8, apart from the +1.3% increase in the P@10 (+partial) metric for model sibtex-4, all the other models and metrics have marginally significant performance increase or decrease when post-processing classification is included in the pipeline. This could be due to the fact that the similarity measure model is already correctly capturing the query constraints and there is little room for improvement. It could be also due to the failure of our categorisation system to capture correctly the query constraint, the result dataset class or both. First, the extraction of the query constraints is performed using an ad-hoc algorithm that searches for keyword combinations in the query and match them against the UniProt classification definition. Second, since our categorization algorithm was validated using scientific abstracts [29], it may be behaving differently for the bioCADDIE datasets. We have no gold standard to explicitly validate both classification approaches and the implicitly validation used via changes in the information retrieval performance did not show significant results. However, we believe that a larger assessment set together with more investigation on the classifier tuning are still needed to draw final conclusions about this approach. Train and test set drifting Looking at the results of the training and official assessment phases, we notice a large variation between the model's performances. As expected, the official judgement, which was initially obtained following the same methodology of the training set, was enriched by pooling the participant's methods. Indeed, while the training set contains on average 49 relevant (including partially) datasets judged per query, the official set has on average 259, i.e., a 5-fold increase. We believe that the enrichment of the assessment set, with new complementary datasets to those retrieved by the official engines, resulted in a significant performance shift between the training and official phases by better capturing the relevance of the results provided by our models. This performance drifting is seeing on both intra- and inter-model comparisons. When we make a model-wise performance comparison between training (Table 4) and official (Table 5) phases, on average, our models achieved during the official phase superior performance levels of 520%, 61% and 692% for the infAP, infNDCG and P@10 (+partial) metrics, respectively. Also, for inter-model changes, the performance of the sibtex-4 model in the infAP metric, for example, degraded by -28.6% compared to the baseline model in the training phase (Table 4). For this same metric, the sibtex-4 model outperformed the baseline by +15.0% in the official results (Table 5). Thus, it seems that the quality of the training set was too poor to allow realistic parameter tuning. Furthermore, it seems that our original similarity measure model, RTRL, was able to bring new relevant documents to the top ranking, which were not initially captured by the 4 information retrieval engines used to generate the official gold standard (training and test). These new relevant documents were then deemed as relevant in the post-judgement phase. To overcome the issues with the training set, we performed a re-tuning of the system's parameters using the official assessment gold standard in a 5-fold cross-validation fashion. With the enhanced gold standard, we managed to better optimize the model parameters, in particular, for the systems using the DFR algorithm, for which the term frequency normalization changed from 1 in the training phase to 33 in the cross-validation assessment (see Training parameters section - Table S2 and Table S3 - in the supplementary materials). Indeed, we think that the size and depth of the training/test set are the main limitations of the results. Assessment methodology An important point to notice is that the official gold standard results contain unjudged datasets (weight -1) obtained during the pooling phase, in addition to the relevant (weight 2), partially relevant (weight 1) and not relevant (weight 0) judgments. Having unjudged results in the gold standard is not an issue per se, as long as this factor is properly propagated to the evaluation tool, in particular, when unjudged datasets compose a significant part of the relevance judgement, as is the case of bioCADDIE's official results (86% of the 142805 total results are unjudged datasets). Nevertheless, it seems not to be case of the official trec_eval assessment tool provided by the challenge organisers. We made few experiments with our official results, where we i) changed the weight of the unjudged results to 0, i.e., we deem them as not relevant, and ii) removed the unjudged results from the relevance file. In both cases, there were average improvements of +6% for the infAP metric and of +47% for the infNDCG metric. Therefore, the unjudged results have a higher negative impact on the official results than results judged as not relevant, a highly unexpected observation. This issue shall be also verified in the other participants results and shall be taken into account in future comparisons using bioCADDIE's benchmark. Online deployment Our system was designed to work on real time both for data ingestion and for querying. For indexing, we use Terrier, which allows incremental document indexing. Nevertheless, we are not tightly coupled to an information retrieval engine. We could change easily to ElasticSearch for example. For the similarity model, the main parameter is the query length (number of terms). Thus, we expect it to provide robust performance in different scenarios without the need of constant fine-tuning, as it was showed for the official and cross-validated results. After the challenge, we have implemented the document classifier and query expansion modules as software as a service for Novartis (NIBR TX TMS). The document classification service takes on average 30ms to classify a document on a non-dedicated server with 40 Intel® Xeon® CPU E5-2690 v2 @ 3.00GHz and 757GB of memory. In the challenge, we post-processed 50% of the top returned datasets. If we fix this number to the top 100 retrieved results, it will take 3s per user query. For the query classification step, the time is negligible since we perform only string matching (skip-gram, bigram, unigram). Finally, for the query expansion service, it takes on average 68ms to expand a term (online phase). In the challenge, each query had on average 5.6 terms (excluding stopwords). Hence, it should take only 0.4s to expand an average user query. Conclusion Data management systems are increasingly being developed to integrate, store and provide scientists with easy access to health and life science research data. In this work, we introduce a ranking pipeline to improve search of biomedical research datasets. Our system was assessed in the context of the 2016 bioCADDIE Dataset Retrieval Challenge. Our approach achieved competitive results in comparison with the challenge participants, improving by +22.3% the median inferred average precision of the overall participant's best submissions. Based on these preliminary results, we believe that the dataset retrieval engine solution proposed in this work can be an alternative to research data management frameworks that need enhanced dataset search. Nevertheless, further experiments with larger assessment sets shall still be performed to achieve more conclusive results. Funding These experiments were supported by Novartis (NIBR NX TMS). Conflict of interest. None declared. References 1. Mervis, J., 2012. Agencies rally to tackle big data. Science, 336(6077), pp.22-22. 2. Alsheikh-Ali, A.A., Qureshi, W., Al-Mallah, M.H. and Ioannidis, J.P., 2011. Public availability of published research data in high-impact journals. PloS one, 6(9), p.e24357. 3. Anagnostou, P., Capocasa, M., Milia, N., Sanna, E., Battaggia, C., Luzi, D. and Bisol, G.D., 2015. When data sharing gets close to 100%: what human paleogenetics can teach the open science movement. PloS one, 10(3), p.e0121409. 4. Bishop, D., 2015, October. Reproducibility and reliability of biomedical research: improving research practise. In The Academy of Medical Sciences, Symposium report. 5. Yang, H., 2011. Support the Manchester Manifesto: a case study of the free sharing of human genome data. Prometheus, 29(3), pp.337-341. 6. Teodoro, D., Choquet, R., Pasche, E., Gobeill, J., Daniel, C., Ruch, P. and Lovis, C., 2009. Biomedical data management: a proposal framework. In MIE (pp. 175-179). 7. Mailman, M.D., Feolo, M., Jin, Y., Kimura, M., Tryka, K., Bagoutdinov, R., Hao, L., Kiang, A., Paschall, J., Phan, L. and Popova, N., 2007. The NCBI dbGaP database of genotypes and phenotypes. Nature genetics, 39(10), pp.1181-1186. 8. Doan, S., Lin, K.W., Conway, M., Ohno-Machado, L., Hsieh, A., Feupe, S.F., Garland, A., Ross, M.K., Jiang, X., Farzaneh, S. and Walker, R., 2014. PhenDisco: phenotype discovery system for the database of genotypes and phenotypes. Journal of the American Medical Informatics Association, 21(1), pp.31-36. 9. Edmunds, S.C., Li, P., Hunter, C.I., Xiao, S.Z., Davidson, R.L., Nogoy, N. and Goodman, L., 2016. Experiences in integrated data and research object publishing using GigaDB. International Journal on Digital Libraries, pp.1-13. 10. Manghi, P., Bolikowski, L., Manola, N., Schirrwagen, J. and Smith, T., 2012. Openaireplus: the european scholarly communication data infrastructure. D-Lib Magazine, 18(9-10). 11. Bourne, P.E., Bonazzi, V., Dunn, M., Green, E.D., Guyer, M., Komatsoulis, G., Larkin, J. and Russell, B., 2015. The NIH Big Data to Knowledge (BD2K) initiative. Journal of the American Medical Informatics Association, 22(6), pp.1114-1114. 12. Wilkinson, M.D., Dumontier, M., Aalbersberg, I.J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.W., da Silva Santos, L.B., Bourne, P.E. and Bouwman, J., 2016. The FAIR Guiding Principles for scientific data management and stewardship. Scientific data, 3. 13. Ohno-Machado, L., Sansone, S.A., Alter, G., Fore, I., Grethe, J., Xu, H., Gonzalez-Beltran, A., Rocca-Serra, P., Soysal, E., Zong, N. and Kim, H.E., 2016. DataMed: Finding useful data across multiple biomedical data repositories. bioRxiv, p.094888. 14. Sansone, S.A., Gonzalez-Beltran, A., Rocca-Serra, P., Alter, G., Grethe, J.S., Xu, H., Fore, I., Lyle, J., Gururaj, A.E., Chen, X. and Kim, H., 2017. DATS: the data tag suite to enable discoverability of datasets. bioRxiv, p.103143. 15. Wilkinson, R., Zobel, J. and Sacks-Davis, R., 1995, November. Similarity measures for short queries. In TREC. 16. Hearst, M.A., 1996, April. Improving full-text precision on short queries using simple constraints. In Proceedings of the Symposium on Document analysis and Information Retrieval (pp. 217-228). 17. Crouch, C.J., Crouch, D.B., Chen, Q. and Holtz, S.J., 2002. Improving the retrieval effectiveness of very short queries. Information processing & management, 38(1), pp.1-36. 18. Gobeill, J., Pasche, E., Teodoro, D., Veuthey, A.L., Lovis, C. and Ruch, P., 2009, November. Question answering for biology and medicine. In Information Technology and Applications in Biomedicine, 2009. ITAB 2009. 9th International Conference on (pp. 1-5). IEEE. 19. Pasche, E., Teodoro, D., Gobeill, J., Ruch, P. and Lovis, C., 2009, November. QA-driven guidelines generation for bacteriotherapy. In AMIA. 20. Gobeill, J., Pasche, E., Teodoro, D., Veuthey, A.L. and Ruch, P., 2012. Answering gene ontology terms to proteomics questions by supervised macro reading in Medline. EMBnet. journal, 18(B), pp.pp-29. 21. Gobeill, J., Gaudinat, A., Pasche, E., Vishnyakova, D., Gaudet, P., Bairoch, A. and Ruch, P., 2015. Deep question answering for protein annotation. Database (Oxford). 22. Roberts, K., Gururaj, A., Chen, X., Pournejati, S., Hersh, W.R., Demner-Fushman, D., Ohno-Machado, L., Cohen, T., Xu, H., 2017. Information Retrieval for Biomedical Datasets: The 2016 bioCADDIE Dataset Retrieval Challenge. Database (Oxford). 23. Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C., 2003. A neural probabilistic language model. Journal of machine learning research, 3(Feb), pp.1137-1155. 24. Mikolov, T., Chen, K., Corrado, G. and Dean, J., 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 25. Diaz, F., Mitra, B. and Craswell, N., 2016. Query expansion with locally-trained word embeddings. arXiv preprint arXiv:1605.07891. 26. Aydın, F., Hüsünbeyi, Z.M. and Özgür, A., 2017. Automatic query generation using word embeddings for retrieving passages describing experimental methods. Database: The Journal of Biological Databases and Curation, 2017. 27. Teodoro, D., Gobeill, J., Pasche, E., Vishnyakova, D., Ruch, P. and Lovis, C., 2010. Automatic Prior Art Searching and Patent Encoding at CLEF-IP'10. In CLEF (Notebook Papers/LABs/Workshops). 28. Teodoro, D., Gobeill, J. and Pasche, E., 2010. Automatic IPC encoding and novelty tracking for effective patent mining. PAJ, 104(420), p.4738. 29. Teodoro, D., Mottin, L., Gobeill, J., Arighi C. and Ruch P., 2017. Assessing Text Embedding Models for Assigning UniProt Classes to Scientific Literature. In Proceedings of Biocuration 2017. 30. Levy, O., Goldberg, Y. and Dagan, I., 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3, pp.211-225. 31. Rehurek, R. and Sojka, P., 2010. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. 32. He, B. and Ounis, I., 2005, March. Term frequency normalisation tuning for BM25 and DFR models. In European Conference on Information Retrieval (pp. 200-214). Springer Berlin Heidelberg. 33. Le, Q.V. and Mikolov, T., 2014, June. Distributed Representations of Sentences and Documents. In ICML (Vol. 14, pp. 1188-1196). 34. Cohen, T., Roberts, K., Gururaj, A., Chen, X., Pournejati, S., Hersh, W.R., Demner-Fushman, D., Ohno-Machado, L., Xu, H., 2017. A Publicly Available Benchmark for Biomedical Dataset Retrieval: The Reference Standard for the 2016 bioCADDIE Dataset Retrieval Challenge. Database (Oxford). 35. Yilmaz, E., Kanoulas, E. and Aslam, J.A., 2008, July. A simple and efficient sampling method for estimating AP and NDCG. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (pp. 603-610). ACM. 36. Amati, G. and Van Rijsbergen, C.J., 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS), 20(4), pp.357-389. 37. Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, C. and Johnson, D., 2005, March. Terrier information retrieval platform. In European Conference on Information Retrieval (pp. 517-519). Springer Berlin Heidelberg.Amigó, E., Gonzalo, J., Artiles, J. and Verdejo, F., 2011. Combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks. Journal of Artificial Intelligence Research, 42, pp.689-718. 38. Teodoro, D., Pasche, E., Gobeill, J., Emonet, S., Ruch, P. and Lovis, C., 2012. Building a transnational biosurveillance network using semantic web technologies: requirements, design, and preliminary evaluation. Journal of medical Internet research, 14(3). 39. Pasche, E., Gobeill, J., Teodoro, D., Gaudinat, A., Vishnyakova, D., Lovis, C. and Ruch, P., 2012, August. An advanced search engine for patent analytics in medicinal chemistry. In MIE (pp. 204-209). 40. Azaria, A., Ekblaw, A., Vieira, T. and Lippman, A., 2016, August. Medrec: Using blockchain for medical data access and permission management. In Open and Big Data (OBD), International Conference on (pp. 25-30). IEEE. 41. digi.me - www.digi.me. Last visited: 05/07/2017 42. Lappalainen, I., Almeida-King, J., Kumanduri, V., Senf, A., Spalding, J.D., Saunders, G., Kandasamy, J., Caccamo, M., Leinonen, R., Vaughan, B. and Laurent, T., 2015. The European Genome-phenome Archive of human data consented for biomedical research. Nature genetics, 47(7), pp.692-695. 43. Gupta, Y., Saini, A. and Saxena, A.K., 2015. A new fuzzy logic based ranking function for efficient information retrieval system. Expert Systems with Applications, 42(3), pp.1223-1234. Figures and Tables Figure 1 – Architecture of the SIB Text Mining dataset retrieval system. Table 1 – Example of a bioCADDIE dataset. <DOC> <DOCNO>215676</DOCNO> <TITLE>VGlut-F-800286</TITLE> <REPOSITORY>neuromorpho_030116</REPOSITORY> <METADATA> { "dataItem": { "dataTypes": ["dataset", "organism", "anatomicalPart", "treatment", "cell", "studyGroup", "dimension", "dataRepository", "organization"] }, "studyGroup": { "name": "Control" }, "anatomicalPart": { "name": ["Left Antennal Lobe", "Not reported"] }, "dataRepository": { "abbreviation": "NeuroMorpho", "homePage": "http://neuromorpho.org/", "name": "NeuroMorpho.Org", "ID": "SCR:002145" }, "dataset": { "downloadURL": "http://neuromorpho.org/neuron_info.jsp?neuron_name=VGlut-F-800286", "note": "Cell types and Brain regions were assigned with a <a href=\"techDocFlyData.jsp?code=1\">heuristic process</a> based on available metadata. This dataset was processed with a <a href=\"techDocFlyData.jsp?code=2\">streamlined automated variant</a> of the standardization procedure, additional details of which are published <a href=\http://www.ncbi.nlm.nih.gov/pubmed/?term=25576225\ target=\"_blank\">here</a>. Digital reconstruction used a <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/?term=23028271\" target=\"_blank\">custom method</a> after image segmentation by Amira.", "ID": "27187", "title": "VGlut-F-800286" }, "cell": { "name": ["Principal cell", "Glutamatergic neuron", "day8 Born"] }, "treatment": { "title": "Green fluorescent protein (GFP)" }, "organization": { "abbreviation": "GMU", "homePage": "http://www.gmu.edu/", "name": "George Mason University", "ID": "SCR:011213" }, "organism": { "strain": "VGlut-Gal4", "scientificName": "", "name": "Drosophila melanogaster", "gender": "Female" }, "dimension": [{"name": "age"}, {"name": "weight"}, {"name": "soma surface area"}, {"name": "surface area"}, {"name": "volume"}] } </METADATA> </DOC> Table 2 – Repositories and the dataset distribution used in the 2016 bioCADDIE Dataset Retrieval Challenge. Repository Datasets Avg dataset size Avg number of attributes (#) (%) (KB) (#) ClinicalTrials 192500 24.257 4.0 45 BioProject 155850 19.638 1.1 11 PDB 113493 14.301 4.0 147 GEO 105033 13.235 0.4 14 Dryad 67455 8.500 2.1 38 ArrayExpress 60881 7.672 1.6 12 Dataverse 60303 7.599 1.9 20 NeuroMorpho 34082 4.295 1.3 38 Gemma 2285 0.288 1.6 9 ProteomeXchange 1716 0.216 1.1 32 PhenDisco 429 0.054 67.2 36 NursaDatasets 389 0.049 1.6 34 MPD 235 0.030 2.2 36 PeptideAtlas 76 0.010 3.2 24 PhysioBank 70 0.009 1.2 18 CIA 63 0.008 1.0 32 CTN 46 0.006 1.4 17 OpenfMRI 36 0.005 1.5 20 CVRG 29 0.004 2.0 20 YPED 21 0.003 1.7 25 Table 3 – Example of user queries used in the bioCADDIE challenge. Table 4 – Performance results obtained using the training queries. Model infAP infNDCG P@10 (+partial) sibtex-1 0.0570 0.2714 0.0833 sibtex-2 0.0573 0.2717 0.0833 sibtex-3 0.0575 0.2724 0.0833 sibtex-4 0.0407 0.2016 0.1000 sibtex-5 0.0539 0.2520 0.1000 Table 5 - Official performance results for the SIB Text Mining models. Model infAP infNDCG P@10 (+partial) NDCG@10 P@10 (-partial) sibtex-1 0.3006 0.3898 0.7067 0.5736 0.3200 sibtex-2 0.2997 0.3864 0.7067 0.5726 0.3267 sibtex-3 0.3008 0.3875 0.7067 0.5718 0.3267 sibtex-4 0.3458 0.4258 0.6600 0.5237 0.3267 i) Search for data on neural brain tissue in transgenic mice related to Huntington's disease ii) Search for gene expression datasets on photo transduction and regulation of calcium in blind D. melanogaster iii) Find data of all types on the regulation of DNA repair related to the estrogen signaling pathway in breast cancer patients across all databases iv) Search for protein aggregation and gene expression data regarding aging across all databases sibtex-5 0.3664 0.4188 0.7533 0.6271 0.3467 Table 6 – bioCADDIE official results: SIB Text Mining individual best score and relative rank, and aggregated participant's best score stats. UIR: Unanimous Improvement Ratio. Stats infAP infNDCG P@10 (+partial) NDCG@10 P@10 (-partial) UIR SI B T ex t M in in g rank 1/10 5/10 5/10 3/10 2/10 2/10 score 0.3664 0.4258 0.7533 0.6271 0.3467 0.51 A ll pa rt ic ip an ts median 0.2994 0.4250 0.7500 0.5806 0.3100 0.13 min 0.0876 0.3580 0.5333 0.4265 0.1600 -1.00 1st quartile 0.2570 0.3954 0.7150 0.5546 0.2700 -0.43 3rd quartile 0.3219 0.4433 0.7600 0.6234 0.3333 0.40 max 0.3664 0.5132 0.8267 0.6861 0.4267 0.82 Table 7 – Metrics correlation matrix – Kendall method. infAP infNDCG P@10 (+partial) NDCG@10 P@10 (-partial) infAP 1 infNDCG 0.50 1 P@10 (+partial) 0.43 0.28 1 NDCG@10 0.42 0.29 0.86 1 P@10 (-partial) 0.60 0.39 0.63 0.69 1 Table 8 - Performance results obtained using 5-fold cross-validation with the post-judgment gold standard. *Results skipping original post-processing phase. Model infAP infNDCG P@10 (+partial) sibtex-1 0.3557 0.4235 0.7327 sibtex-2* 0.3704 0.4377 0.7511 sibtex-2 0.3704 0.4378 0.7511 sibtex-3* 0.3735 0.4367 0.7544 sibtex-3 0.3734 0.4365 0.7544 sibtex-4* 0.3454 0.4216 0.7067 sibtex-4 0.3441 0.4197 0.7156 sibtex-5 0.3514 0.4199 0.7578 Figure 2 – infAP performance at the query level for SIB Text Mining models. Results obtained using 5-fold cross-validation. Lowest horizontal line: 1st quartile computed for all results. Highest horizontal line: 3rd quartile computed for all results. min avg horizontal line: minimum infAP among the 5 models. max avg horizontal line: maximum infAP among the 5 models. Figure 3 – infNDCG performance at the query level for SIB Text Mining models. Results obtained using 5-fold cross-validation. Lowest horizontal line: 1st quartile computed for all results. Highest horizontal line: 3rd quartile computed for all results. min avg horizontal line: minimum infNDCG among the 5 models. max avg horizontal line: maximum infNDCG among the 5 models. Figure 4 – P@10 (+partial) performance at the query level for SIB Text Mining models. Results obtained using 5-fold cross-validation. Lowest horizontal line: 1st quartile computed for all results. Highest horizontal line: 3rd quartile computed for all results. min avg horizontal line: minimum P@10 (+partial) among the 5 models. max avg horizontal line: maximum P@10 (+partial) among the 5 models. Table 9 - Example of term expansion with embedding vectors trained on different corpora. Term bioCADDIE PMC Medline expansion score expansion score expansion score cancer carcinoma 0.737 carcinoma 0.674 breast 0.889 cancers 0.720 tumor 0.616 cancers 0.855 adenocarcinoma 0.669 cancers 0.585 prostate 0.801 malignancies 0.626 tumour 0.583 colorectal 0.794 lymphoma 0.621 glioma 0.559 tumor 0.779 tumor 0.616 carcinomas 0.545 carcinoma 0.773 transplantation 0.613 melanoma 0.534 tumorigenesis 0.761 transplant 0.570 tumorigenesis 0.532 tumors 0.754 myeloma 0.563 carcinogenesis 0.530 metastasis 0.745 carcinogenesis 0.559 glioblastoma 0.520 tumour 0.733 human bovine 0.553 mammalian 0.582 mouse 0.756 porcine 0.542 murine 0.441 mammalian 0.711 murine 0.526 rat 0.428 also 0.661 mouse 0.518 vertebrate 0.417 humans 0.661 humans 0.486 preeclamptic 0.400 murine 0.656 mammalian 0.483 pompe 0.396 present 0.650 rat 0.483 hepa1 0.392 report 0.644 chicken 0.471 chicken 0.388 function 0.641 tissue 0.456 gpr84 0.380 functional 0.632 cellular 0.456 cyprinid 0.373 well 0.625 repair closure 0.515 repairthe 0.597 damage 0.794 metabolism 0.510 replication 0.570 excision 0.764 formation 0.509 ssbr 0.543 double-strand 0.727 grafting 0.504 repairing 0.540 nucleotide-excision 0.723 implantation 0.502 damage 0.516 damaged 0.717 reconstruction 0.498 regeneration 0.499 breaks 0.715 autologous 0.489 healing 0.498 mus7 0.699 testing 0.474 detoxification 0.493 dsb 0.697 mobilization 0.469 resection 0.489 helix-distorting 0.681 remodeling 0.468 processing 0.479 post-replication 0.679 Table 10 – Retrieval performance for different collections as query expansion source. The baseline results use no query expansion. Collection infAP infNDCG P@10 (+partial) - (baseline) 0.3557 0.4235 0.7267 bioCADDIE 0.3545 0.4243 0.7178 PMC 0.3571 0.4216 0.7178 Medline 0.3704 0.4377 0.7511 Supplementary material Train and test queries Training parameters Official results Table S2 – Optimized parameters for official submission. Model Term frequency normalizati on (c) Query expansion (k) Classificatio n gain (g) Non key-relevant term expansion loss (l) Key-relevant term weight boost (c) Linear combinatio n coefficient (α) sibtex-1 1.00 sibtex-2 0.95 10 0.10 sibtex-3 0.95 10 0.10 0.01 sibtex-4 0.10 1 sibtex-5 0.5 Post-official cross-validation Table S3 – Optimized parameters for cross-validation experiment. Model Term Query Classificatio Non key- Key- Linear EA1 – Find data of all types related to TGF-β signaling pathway across all databases EA2 – Find data of all types on synaptic growth and remodeling related to glycolysis in the human brain across all databases EA3 – Search for data on BRCA gene mutations and the estrogen signaling pathway in women with stage I breast cancer EA4 – Find data of all types on the regulation of DNA repair related to the estrogen signaling pathway in breast cancer patients across all databases EA5 – Search for data of all types on multiple sclerosis of all types across all databases EA6 – Find data on T-cell homeostasis related to multiple sclerosis across all databases T1 – Find protein sequencing data related to bacterial chemotaxis across all databases T2 – Search for data of all types related to MIP-2 gene related to biliary atresia across all databases T3 – Search for all data types related to gene TP53INP1 in relation to p53 activation across all databases T4 – Find all data types related to inflammation during oxidative stress in human hepatic cells across all databases T5 – Search for gene expression and genetic deletion data that mention CD69 in memory augmentation studies across all databases T6 – Search for data of all types related to the LDLR gene related to cardiovascular disease across all databases T7 – Search for gene expression datasets on photo transduction and regulation of calcium in blind D. melanogaster T8 – Search for proteomic data related to regulation of calcium in blind D. melanogaster T9 – Search for data of all types related to the ob gene in obese M. musculus across all databases T10 – Search for data of all types related to energy metabolism in obese M. musculus T11 – Search for all data for the HTT gene related to Huntington's disease across all databases T12 – Search for data on neural brain tissue in transgenic mice related to Huntington's disease T13 – Search for all data on the SNCA gene related to Parkinson's disease across all databases T14 – Search for data on nerve cells in the substantia nigra in mice across all databases T15 – Find data on the NF-κB signaling pathway in MG (Myasthenia gravis) patients Table S1 – Train queries (EA prefix) and test queries (T prefix). frequency normalizati on (c) expansion (k) n gain (g) relevant term expansion loss (l) relevant term weight boost (c) combinatio n coefficient (α) sibtex-1 33 sibtex-2 33 25 0.01 sibtex-3 33 25 0.01 0.05 sibtex-4 0.02 1.85 sibtex-5 0.7 Abstract
Background
Healthcare providers generate a huge amount of biomedical data stored in either legacy system (paper-based) format or electronic medical records (EMR) around the world, which are collectively referred to as big biomedical data (BBD). To realize the promise of BBD for clinical use and research, it is an essential step to extract key data elements from unstructured medical records into patient-centered electronic health records with computable data elements. Our objective is to introduce a novel solution, known as a double-reading/entry system (DRESS), for extracting clinical data from unstructured medical records (MR) and creating a semi-structured electronic health record database, as well as to demonstrate its reproducibility empirically.
Methods
Utilizing the modern cloud-based technologies, we have developed a comprehensive system that includes multiple subsystems, from capturing MRs in clinics, to securely transferring MRs, storing and managing cloud-based MRs, to facilitating both machine learning and manual reading, and to performing iterative quality control before committing the semi-structured data into the desired database. To evaluate the reproducibility of extracted medical data elements by DRESS, we conduct a blinded reproducibility study, with 100 MRs from patients who have undergone surgical treatment of lung cancer in China. The study uses Kappa statistic to measure concordance of discrete variables, and uses correlation coefficient to measure reproducibility of continuous variables.
Results
Using the DRESS, we have demonstrated the feasibility of extracting clinical data from unstructured MRs to create semi-structured and patient-centered electronic health record database. The reproducibility study with 100 patientâs MRs has shown an overall high reproducibility of 98 %, and varies across six modules (pathology, Radio/chemo therapy, clinical examination, surgery information, medical image and general patient information).
Conclusions
DRESS uses a double-reading, double-entry, and an independent adjudication, to manually curate structured data elements from unstructured clinical data. Further, through distributed computing strategies, DRESS protects data privacy by dividing MR data into de-identified modules. Finally, through internet-based computing cloud, DRESS enables many data specialists to work in a virtual environment to achieve the necessary scale of processing thousands MRs within days. This hybrid system represents probably a workable solution to solve the big medical data challenge.
Keywords
Big dataBig medical dataClinical researchClinical decision support systemCloud-based systemDouble data entryElectronic medical recordHealth service researchStructured data and unstructured data
Background
In medical practice, healthcare workers, including physicians, nurses and supporting staff, produce a great amount of clinical and administrative data, from general check-up information, drug prescription information, treatment records, physiciansâ notes, laboratory results/images, surgical information, in addition to financial or administrative information [ 1 ]. Most of clinical data are traditionally stored in legacy systems, including paper-based filing system [ 2 , 3 ], and are now increasingly stored in electronic medical record system (EMR) on computers [ 1 , 4 , 5 ]. Accumulations of clinical data have produced âbig biomedical dataâ (BBD). Successful exploration of big data sets in industries and ecommerce [ 6 â 9 ] prompts healthcare professionals to seek research opportunities within medical big data as well. For example, one may be interested in research questions such as which treatments are more effective than others, whether or not new or existing therapies are safe in real world practice, what is the cost-effectiveness of many equivalent treatments, how we learn about and streamline clinical practices, and how we develop clinical decision support to improve clinical diagnosis and management [ 10 â 12 ]. Collectively, there is a high hope for the secondary use of BBD for clinical and healthcare research [ 13 , 14 ]. Conceptually, it seems straightforward to organize all available clinical data into a database, after linking different pieces of data sets via patient identification numbers.
In practice, there are many challenges facing the big data research in healthcare. Obviously, many challenges are associated with data privacy, data ownership, data security, interdisciplinary collaborations and conflicts of research and commercial interests. Here we focus on one specific technical challenge on how to extract âcomputable data elementsâ from BBD and to store them into one database. Since most BBD data sets are created for operational purposes, they are largely unstructured, i.e., many texts and drawings document symptoms, diagnoses, laboratory test results, or physiciansâ notes. In some cases, BBD include anatomic hand-drawings with informal annotations. Such MRs are highly informative for trained physicians, but unfortunately not machine-readable, nor computable. Hence, such BBD, regardless how large they are, are not readily available to fulfill the promises of the big data.
In the biomedical community, researchers and physicians have long realized that BBD provides valuable clinical information for research, and have been using BBD for various research projects [ 15 ]. Unless working with clinical research database with structured data elements, clinical researchers typically have to review all medical charts, and to extract manually pertinent clinical data to address specific clinical research questions, i.e., manually converting unstructured MRs into semi-structured data that are appropriate for specific research questions on hand. While it is probably the practical method in routine use to extract purpose-driven information from selected MRs, this manual extraction is unfortunately not scalable to deal with general BBD, because of unattainable highly skilled human resources, i.e., physicians. An alternative approach, commonly adopted in clinical trials, is to design specific forms, such as Case Report Form (CRF) as a way of identifying cases [ 16 , 17 ]. Typically, CRF is simple, and includes only limited structured variables so that nurses can easily fill in CRF forms, as a task to their already busy routines. After receiving filled CRFs, research staff enter structured CRF data elements into database via a double-entry system [ 18 ]. Even though CRFs are useful for identifying and recruiting patients, they are of limited use for general clinical researches, beyond the tailored clinical trial. Hence, the data extraction method used in clinical trials is not readily applicable to process BBD either.
To solve this data extraction challenge, some computer scientists have placed high hope on machine-learning algorithms, data mining or artificial intelligence, based on recently developed Natural Language Processing (NLP) and Text Predictics [ 19 â 29 ]. Conceptually, if these novel machine-learning algorithms could process natural spoken language, one would hope to utilize these technologies to read unstructured data in BBD, to comprehend the intent of physicians, to quantify research information, and to create a structured database [ 28 ]. In practice, numerous complications may distort machine-reading of unstructured MRs at this time. For example, algorithms and predictive models depend on the availability of well annotated large training data sets, with methods such as neural networks, cluster analysis, genetic algorithm, decision trees and support vector machines to find hidden patterns. For these methods to perform well, one needs to have a manual curated âtraining setâ with gold standard, which is not easily attainable. On the other hand, NLP based on machine learning algorithms intends to interpret natural spoken language based on established ontologies, and is making steady but slow progresses towards high accuracy. At this time, the accuracy is not high enough for medical uses. Nevertheless, it remains a hope that one day, we can combine machine-learning algorithms, statistical learning methods, and linguistic knowledge, to predict structured data elements from unstructured data, as evidenced by some pioneering studies [ 29 ]. When such methodologies will become available, we will be able to extract accurate structured data automatically from MRs in the future.
Waiting for a better solution to exploring MR is not an option, in light of urgent needs for better-personalized treatment and improved patient-centered outcomes at affordable costs, especially for many developing countries. Here we propose an interim solution to extract structured data from unstructured MRs, based on our intimate understanding of EMRs and needs of clinical research. Specifically, this manuscript describes a double-reading/entry system (DRESS) for extracting structured elements from unstructured MRs, using power of both information technologies and human intelligence. Utilizing the âdivide and conquerâ strategy, DRESS separates a target MRs into relatively homogeneous modules with varying levels of required medical knowledge. On each module, trained and certified personnel with sufficient background for the corresponding module and is known as âData Specialistâ (DS), reads corresponding module in MRs, and extracts pertinent data into the database system. To control quality of reading comprehension and data entry, DRESS requires two independent entries by DS. Discrepancies in entered information are then subject to the adjudication by third and senior DS. In essence, DRESS represents a hybrid system of manual curation and computer automation.
Methods
DRESS overview
We develop DRESS as a general methodology for extracting structured data elements from MRs. For simplicity in description, we focus the presentation on a DRESS application to processing MRs created in surgical departments dealing with cancer patients. Suppose that we are building a BBD for multiple oncology departments in hospitals located in diverse geographic areas (using one common language, i.e., Chinese in DRESS). As expected, these oncologic departments in different hospitals may use diverse systems for storing clinical data, from legacy system to EMR. For a coherent discussion, we suppose that all hospitals use legacy systems with diverse paper formats. Clinical data recorded on papers include the following structured data: patientâs demographics (birth date, birth place, ethnicity, blood type), admission/discharge information (dates, attending physicians, costs), medical history, surgical operational details, pathology, image data, clinical examination data, laboratory test data, uses of medications, chemo/radio therapies, genetic testing results, clinical assessments at discharge, and follow-up information.
Clinical data collected from individual hospitals and stored in our database cannot be used for clinical research, without official approval of respective Institutional Review Board (IRB). Just as in United States, each major healthcare organization or medical university has IRB, and their rules and regulations are locally developed and enforced. Hence, when launching a multi-center study with clinical data from our database, investigators need to apply an approval for accessing and using their own clinical data to participate in the multi-center study.
Recognizing that cancer treatment is specific to anatomic sites with specialized procedures and that recorded information in MRs varies between hospitals, we devote a substantial amount of time and effort to create a common data model (CDM) for each cancer site, e.g., lung cancer is focused in this discussion. For this development, we organized an interdisciplinary team of oncologists with pertinent specialty, experienced staff with clinical data collection and experienced computer scientists familiar with medical ontologies. Through iterative trial-error process, the team comes up a general CDM for collecting structured data elements for patients who undergo treatment of lung cancer. The current version of CDM is LinkDoc-CDM (version2.1.12), created on November 27, 2015. Then, working with individual hospitals, we identify all pertinent data elements from hospital-specific MRs, and convert them to necessary data elements by CDM. Hence, after all MRs are processed and organized into our system, the resulting data elements in the database by the same CDM.
DRESS fundamentally represents a hybrid system with manual curations and with information technologies. A pre-requisite to developing DRESS is for us a database developer to recognize that healthcare workers generate MRs for delivering healthcare, and their priority is to ensure patient-centered services and outcome, rather than data collection and quality. Often, healthcare workers are too busy to devote much attention to collecting data, even though all of them recognize the importance having MRs for downstream clinical research. As part of DRESS, we assign one college-educated personnel, typically, graduates from nursing schools, to each oncologic department, to work side-by-side with healthcare workers, and call him/her a health-information assistant (HIA). The primary responsibility is for HIA to gather all pertinent MR, both from clinics and from hospital EMR. To enable the HIA to efficiently work with healthcare workers, we develop a LinkMR (abbreviated from Linking Medical Records), a subsystem with specific functionalities, i.e., to assign an internal identification number for the patient, to remove patient-specific and doctor-specific identification information, to scan paper-based MR into images, and to upload images to a cloud-based data storage, representing the first step of DRESS (the first column in Fig.  1 ). The second subsystem in DRESS is LinkCore (an abbreviation from Linkage Core), to divide uploaded MR images into different modules through subsystem (the second column in Fig.  1 ), and to create data entry tasks for DS and to coordinate quality control with data specialist for quality control (DS-QC). The third subsystem is LinkQC (abbreviated from Linking Quality Control) (the fifth column in Fig.  1 ), to compare double entries by two DSs for quality control, to resolve conflicts through quality control specialists, and to carry out other quality control activities.
Fig. 1
An overview of DRESS from accessing clinical data in legacy system, to de-sensitizing all clinical data, to create double reading-entry tasks, to perform quality control, and ultimately to store the structured data into BBD
Subsystem: LinkMR
As noted above, HIA is working side-by-side with healthcare workers (physicians, nurses and staff) in the clinical setting. Meanwhile, all clinical data, collected from clinics, are strictly confidential. To balance the effectiveness of gathering clinical data and the necessity of protecting patient/physicianâs privacy, we develop the LinkMR system of DRESS on iOS system so that HIA can run LinkMR on iPad or iPhone (Fig.  2 ). The mobile devices scan MRs, images and other clinical data into digital images. After receiving data, LinkMR carries out âoptical character recognitionâ (OCR), identifying personal information, such as name, social security ID, address, etc., and assigns internal study ID. Note that internal identification number is chosen to be, if available, the official âsocial security IDâ issued by government and is commonly used on MR in China. In the event that the social security ID is not available, the internal identification number is constructed as a composite of âpatientâs nameâ, âhospitalization record numberâ, and âhospital identification numberâ. Automatically, LinkMR creates a separate linkage file for study ID and personal information. After stripping away personal and identification data and creating a de-identified data file, LinkMR uploads these images to a centrally controlled LinkCore system (see next section) via a secured private cloud. Such images will be organized (see the following) ready for manual reading. While mobile-based LinkMR offers the maximum flexibility for HIA to work in computer-unfriendly environments, we have also implemented LinkMR in Window environment, so that HIA can process clinical data more efficiently on personal computers than mobile devices. Further, LinkMR operates the designated scanner that uptakes more clinical data with better quality. Indeed, uploading scanned MR images to cloud is much more efficient via PC than via mobile devices. Once all scanned clinical data are in cloud storage, LinkCore system takes on all database management, data security protocol and protection of personal privacy.
Fig. 2
LinkMR, operated on either iPAD or PC, scans in medical records, processes them by OCR, divides them into modules, and submits encrypted MR images to cloud storage
When designing this system, we were confronted with a key choice between private cloud storage and private server farm, from the perspective of the data security. Indeed, if financial resources were of no concern, one could argue that private server farm would offer better data security than cloud storage, since the latter involves the third party. However, recent technological improvement in clouding computing and storage has gradually chipped away the superiority of private server farm with respect to data security. Increasingly, IT community in China, following the trend of United States in adopting cloud computing, begins to coalesce around cloud computing for storing governmental data as well as for clinical data. In the current implementation, we use cloud storage and computing capacity on Alibaba cloud ( https://intl.aliyun.com ). Through this arrangement, we were able to devote more resources to create data security protocols and policies, while spending limited resources on data storage hardware. Because of our direct experiences working on healthcare data in cloud, LinkDoc, teamed up with Alibaba, has been commissioned to create China standard on data security policies for National Health and Family Planning Commission of China.
Subsystem: LinkCore
LinkCore manages all de-identified clinical data in the computing cloud storage. By storing all de-identified clinical data in the cloud, LinkCore provides a cost-effective management of sensitive clinical data, and is a scalable solution for fast-growing data without large investment into hardware infrastructure and associated delay. Utilizing PHP [ 30 ] and running on CentOS Linux system [ 31 ], LinkCore takes in unstructured and de-identified clinical data, processes all data, and stores the structured data in MongoDB ( https://www.mongodb.org ). The primary reason for choosing
MongoDB is that it is flexible for complex data of any structures without schema-restrictions, and for rapid iterations to improve data business models.
Now from the perspective of LinkCore, it places a central role of interacting with HIA, mobile-based or PC-based App equipment, cloud storage of all clinical data, DS, quality controller, and, cloud storage of all structured clinical data (Fig.  3 ). The administrator of LinkCore distributes data capture templates to HIA in each hospital. Once clinics authorize use of specific batch of clinical data, HIA uploads de-identified clinical data to LinkCore via iOS-based or Window-based apps.
Fig. 3
LinkCore takes in unstructured clinical data from HIA, divides clinical data into multiple modules, assigns data to DS A and B, performs LinkQC to identify conflicts, and allows DS QC to make final adjudicated values on those conflicts
After receiving de-identified clinical data in the cloud, LinkCore divides clinical data into different modules. Each module has relatively homogeneous contents within certain subject matters. Some modules require collection of data elements that can be directly extracted from MR, and other modules may require âsearchingâ and âhuman interpretationsâ. All modules are thus assigned varying degrees of complexity, based on the required human interpretations. LinkCore makes these modules available for eligible DS to select data-entry tasks appropriate for their certified levels (Fig.  3 ). In addition, LinkCore identifies the priorities for various tasks and potential order of task completions.
As noted above, many hospitals have their own forms and organizations of MRs. As part of the certification process, an experienced staff with clinical background and with hospital-specific MRs will develop hospital-specific guidelines, and will provide training to DS prior to exposing those DS to associated MRs. Training and certification are continuing processes as DRESS takes MRs from newly participating hospitals.
It is also worth noting that DS plays a special role in DRESS. As we know, reading and extracting information from typical MRs is complex and generally requires knowledges of multiple disciplines, possessed by physicians who receive years of training and clinical practice. Through dividing entire MR into multiple relatively homogeneous modules, DRESS presents modules appropriate to DS who have varying degrees of experiences and knowledges. Some modules require DS to be knowledgeable about specific medical procedures, while other modules require DS to have good judgement or to be detailed. Based on experiences, we have found that college graduates, preferably health-related education, are ideal candidates for DS positions. Currently, 25 % of DS working on DRESS are graduates from nursing schools.
Typically, we have two DS, call them DS A and B, to perform data-reading and data-entry tasks and to submit structured data via LinkCore.. Afterwards, LinkCore calls on LinkQC system (see next section) to detect possible conflicts between the two independent entries. When quality issues are present, LinkCore archives them to a pool of quality control tasks and prompts them to DS-QC (Fig.  3 ). The responsible DS-QC reads the unstructured clinical data (in the scanned images of MR), determines post-quality control values for those in conflict. Following the initial adjudication, DS-QC sends this query back to DS A and B without disclosing post-quality control values, and ask the DS in question to repeat the data entry, as another quality control on the DS-QC. Once the consensus is reached, DS-QC will commit adjudicated values into the final BBD database system. However, if a DS questions the validity of values entered by DS-QC, the DS can escalate this case to higher level of quality control by a senior DS-QC who will then be making a final decision on the correct value.
Subsystem: LinkQC
The quality of structured data extracted from complex MRs is of great importance for downstream applications. We design LinkQC to perform quality control (Fig.  4 ). After receiving entry values from DS A and B, LinkQC executes the comparison function, to detect differences between two entries. Differences, if any, would prompt LinkQC to queue the QC task for DS-QC. Following the manual investigation, DS-QC determines the final values, with iterative feedbacks from DS A and B, and commits them back to LinkQC. The system takes in the final values with specific annotations on QC process. Additionally, LinkQC performs automatic checks on ranges of entered values and inconsistencies of entered values among themselves and with values in the system.
Fig. 4
LinkQC system is designed to ensure the quality and integrity of clinical data extracted from medical records
Fig. 5
Estimated reproducibility parameters (Kappa statistics for categorical variable and Correlation for continuous variable, denoted by blue square next each row) computed for 127 variables (rows) in six modules (Pathology, Radio/Chemo therapy, Clinical examination, Surgery, Medical image and General patient information). The line denotes 95 % confidence interval. A visual absence of the line implies that very narrow confidence interval. The difficulty level of extracting variable is categorized as easy (black), medium (green) and difficult (red). Within each dot-line plot, a string of texts and numbers provides the range of reproducibility parameters, median and mean, for all variables within the module
LinkQC still needs to cover another gap in quality control. While LinkQC promptly addresses quality control issues due to inconsistent entries, it remains possible that some entry values could be consistently âincorrectâ. If extracted data elements are continuous values, or texts, or categorical values with many levels, this concern is minor, since the chance for consistent entries by two independent DS is quite low. However, for categorical variables with only few possible values, e.g., male or female for gender, yes or no for specific procedure, chance consistency can be appreciable. To address this quality concern, LinkQC applies the data range and logic check to detect some obvious errors. Further, LinkQC schedules a sampling-based protocol to ensure the data quality. Specifically, LinkQC takes one random sample of entered data each week in first three weeks of each month, and takes, randomly, \( \sqrt{n+1} \) samples (n represents the total number of submitted MRs) for the fourth week. On each sample, the current LinkQC protocol repeats the data entry on those error prone variables, to detect possible errors. Finally, to monitor the overall status of quality control, LinkQC documents data quality issues from all sources over time, and assesses if there any temporal unexpected trends in data quality.
Protection of personal privacy
We design the DRESS system with the following stakeholders in mind: medical directors, physicians, and patients. In this development, we consider personal privacy issues, specifically, medical directorâs privacy across different departments, physicianâs privacy among physicians, and patientâs privacy from other people who may or may not know the patient. In general, each stakeholder has a password-protected account and each account authorizes access to specific data sets consistent to their assigned roles. Specifically, a medical director has authority to access data created by his/her subordinates on their patients. Each physician has access to his/her patients only, and cannot access patients who receive care by other physicians unless receiving explicit permission. Each patient has access to his/her clinical data only. LinkDoc internally develops this authorization system. It has a dual control on the authorization process: one dimension is on authorized functionalities and the second dimension is on authorized access to data types. Within the first dimension, the system manages users, roles, and scope of authorities. With respect to data access, the system manages which hospitalâs data, which departmentâs data, which data modules within MR and if patient identifier data are involved.
Data security
Besides protecting personal privacy, DRESS has instituted strict protection of data security throughout data collection and processing. In the front end, LinkMR takes in MRs from clinics. The first task is to extract personal information from MRs, to assign an internal identification numbers, and to store personal information with identification numbers into a separate âlinkage dataâ file, which is known as a LinkData. The second task is to de-identify MR with respect to personal data. The third task is to divide MR into multiple modules. The last task is to encrypt MR images before submitting to the cloud-based data storage managed by LinkCore. Once modulated clinical data enter LinkCore, no one can identify which MR images associate with the corresponding patients, without LinkData, while LinkData is stored in a physically separated storage.
Through implementing above protection strategy, we achieve the following protection goal. First, LinkMR is password-protected, and no one can access the system in the event of losing iPad or PC equipment by HIA. Second, all captured MR images are encryption- protected in the event that unintended parties intercept uploaded data. Third, DRESS has identified multiple roles within the organization with appropriate authorization levels. For example, a junior DS can accesses only his or her data. DS-QC has authority to access those data files where conflicted data values exist. Fourthly, every DS is assigned only one module a time without any reference to other modules or any specific MR, so that it is nearly impossible for any DS to re-identify patients, physicians or hospitals. At the system-wide level, DRESS adopts the state of art technologies to protect the local system with LinkData as well as the database in the cloud storage.
A reproducibility study
As an enterprise solution, we have implemented the DRESS with over 100 DSs and over 20 DS-QCs. The implemented DRESS is currently processing thousands MRs a day. To assess the actual performance of DRESS, we have randomly sampled 100 MRs from lung cancer patients, whose MRs were entered into the database between October 1, 2015 and December 3, 2015. The only selection criterion is that pathology and surgery modules are not empty. We then manually re-designate these patientsâ MRs as new MRs, and repeat the entire DRESS process following the exactly same protocol, finishing data entry within 2 days (December 10 â December 11, 2015). Table  1 shows basic characteristics of these 100 patients, on selected variables. Note that these patients are from multiple hospitals, and their data are used only for evaluating reproducibility of extracted data elements without evaluating any clinical outcomes or addressing any clinical hypotheses. This is deemed as a technology-evaluation study, rather than a clinical study.
Table 1
Basic characteristics of 100 patients included in the reproducibility study
Variables
aIndicator 5 and 6 were to be combined when extracted from EHR
b+âEHR data is more complete; - EHR data is less complete
c+âQI scores are the same (relative changeâ<â5 %); - QI scores are not the same (relative changeâ>â5 %)
dn.a. not applicable
Discussion
Our study showed that changes in data collection methods from survey data to data extracted from EHRs had a major impact on the comparability of the content. Survey data had to be recalculated to fit the redefined quality indicators from the EHR data. Further, only four out of eight indicators could be compared as three indicators were discarded in the transition from survey to EHR data and a fourth was not comparable due to errors in the software of two of the largest EHR suppliers, which blocked extraction of the correct data. The data quality of the indicators that we could compare showed that the EHR data was more complete than survey data on three out of four indicators and indicators based on EHR data seemed to be as accurate or correct as the indicators based on survey data on three out of four indicators.
Explanations for the findings
Chan et al. concluded that comparability, both of EHR data to other data sources, as well as comparability between EHRs was of importance to valid care quality comparisons and outcome research [ 3 ]. Differences in what data elements needed for a measure are present in the data sources, but also variation in EHR content, structure and data format or extraction procedures can significantly affect data comparability. Due to major changes needed for the extraction of data from EHRs and time pressure, it was decided that not all quality indicators could be extracted from the EHRs, limiting the comparison of the entire quality indicator set. Ambiguous and inconsistent operationalisations of two of the largest EHR software providers caused errors in data extraction that further limited comparability. A pilot phase was initially planned in the project to test the extraction procedure with similar patient cases. However, due to enormous pressure from different stakeholders for a rapid implementation, the pilot phase was skipped with all its consequences. Despite the presence of a uniform extraction algorithm, two of the largest software providers deviated from this algorithm, blocking extraction of the correct data. An important indicator for patient outcomes (the combination of indicator 5 (result administrated) and indicator 6 (subjective result)) could therefore not be compared. Not only is the outcome of a treatment an important measurement of the quality of care, this indicator also showed the largest variation in an earlier study evaluating the psychometrical properties of the quality indicators in the survey data [ 11 ]. It could therefore have been of great interest for comparisons. In the end, four out of the original eight indicators were comparable.
One of the benefits of using EHRs is that it serves as a tool to facilitate completeness of administering the medical process. We found evidence for this because there were less missing values in the EHR data than in the survey data for three out of four quality indicators. Indicator 2 (screening and diagnosis for referred patients) had slightly more missing values in the EHR data. This could be caused by the fact that as the patients are referred, some of the steps in the clinical reasoning process were already performed by the referring physician and simply not administrated properly in the EHR by the treating physical therapist. Further, indicator 8 (information shared with and agreed upon by patients) showed the largest improvement on completeness. This major improvement could be the result of the difference between retrospective reporting in the survey data collection method and prospective reporting in EHR data. In the surveys, physical therapists were asked to reflect on patient cases that were already closed. The communication process with the patient might be harder to remember for the physical therapist than the steps in clinical reasoning, resulting in more missing values on this indicator. However, EHRs are normally completed during or right after the consult with the patient, making it easier to answer questions on the communication process with the patient.
EHRs can serve as a technological checklist for clinical reasoning, as Salazar et al. also states [ 16 ]. Although this might not improve the outcomes on clinical conditions, [ 17 ] it might help prevent mistakes in the clinical process and increase safety as a relation was found between clinical incidents and poor reporting [ 18 ]. At the least it can help mistakes be more easily retrieved, increasing transparency and accountability.
One of the reasons behind the relatively small differences between indicator scores in survey data and EHR data could be the presence of ceiling effects. Ceiling effects are represented by the percentage of therapists that have the maximum indicator score. An earlier study into the psychometric properties of the survey data revealed a high level of ceiling effects [ 11 ]. This posed a problem as it was thus more difficult to distinguish between different physical therapists on the level of their quality, but also that it would be difficult to establish relevant change over time within the same therapist. This would probably explain the small changes in values when the survey data is compared to the EHR data.
Using the EHRs for quality measurements saves valuable time as clinicians do not have to complete additional surveys for quality assessments next to the regular administration of their patients. The administration is done electronically in the EHRs during or right after the therapy session and the data can be directly extracted without further action from the therapist. That is time better spent on patient care and may potentially lead to an indirect positive effect on the quality of care. EHRs present a possibility for continuous and automated data extraction for real time monitoring of the quality of care and for providing direct feedback to patients, medical professionals and health insurance companies. If software differences between the various EHRs are overcome to deliver standardized output and the process of extracting the data from the EHRs is automated, quality managers can use the quality information when it suits them instead of waiting for the results of a study or a report to act on them more promptly. Research has indicated that feedback can be used to improve the quality of care, showing improvements after feedback initiatives on process or outcome of care, although the effectiveness of feedback initiatives does depend on the (perceived) quality of the data and the willingness of the recipients [ 15 ].
Limitations
A limitation in using EHR software was the difference in designs of the EHRs. Some of the smaller software suppliers included visual cues into their EHR design to signal the physical therapist whether or not information was missing in the patient file. Research suggests that visualization tools could have a positive effect on the number of missing values in EHRs [ 19 ]. These differences within EHR data must be overcome to ensure that physiotherapy practices can be compared on quality of care by standardizing the design, or the EHR software supplier must be controlled for in comparative research. In our study, the number of practices that used an EHR with visual clues were too small to allow for comparative analysis between EHRs.
A limitation regarding the generalizability of the findings was the overrepresentation of the group of acute care patients in the EHR data. Only data on patient intervention episodes were included that were actively closed by the therapist in the data extraction period. Given the relative short timeframe of data extraction, there is a higher chance of closing a patient intervention episode in which the patient has acute symptoms than a patient case of a patient with chronic symptoms. In the survey data, physical therapists selected from cases that were already closed, so they could select any case, including chronic cases that was closed in the last year. An earlier study confirmed that the steps in the process of clinical reasoning were significantly better followed in acute cases on half of the quality indicators, when controlling for other patient characteristics [ 11 ]. With the overrepresentation of acute patient cases, the indicator scores could be overestimated in the EHR data. However, on the quality indicators that were compared in this paper, no significant differences in scores between acute and chronic disease patients were found in the survey data for three indicators (indicators 1,2 and 3) [ 11 ] whereas on indicator 8 chronic disease patients were significantly better informed than acute patients [ 11 ]. To ensure that the differences found in this study were not the result of differences in any of the patient, therapist or practice characteristics, we performed additional multilevel regression analyses (see Additional file 2 ). This showed that even when controlling for these characteristics, the differences between indicator scores in the EHR data and the survey data were similar to the results presented in Table  5 . We are therefore confident that the generalizability of the results in this paper is not limited by differences in the sample.
Another limitation is the absence of a gold standard on indicator scores for physical therapy. We assessed correctness in this paper by comparing the EHR data to the survey data, which we used as a benchmark. That is not to say that the survey data was âcorrectâ. However, in the development process of the quality indicators, consensus rounds were held with all stakeholders; physical therapists, patients, insurers and the inspectorate to ensure content validity. Further, construct validity was positively assessed as well as reproducibility and interpretability [ 11 ]. The biggest problem in the indicators were the high ceiling effects, as mentioned before [ 11 , 20 ]. It is also possible that since the data sources are of different time frames, but with the same physical therapists, changes in indicator scores are caused by a time effect. Physical therapists might have learned from participation in the first cohort and adapted their practice accordingly to improve the quality of care. However, since the differences between indicator scores calculated from survey data and EHR data are relatively small (or in the case of indicator 1 even negative), we consider the time effect not to be a major influence on the results.
Implications for research and quality policy
Although data completeness [ 3 ] and correctness [ 3 , 4 ] are important proxies of data quality, there are other important data properties left untouched in this study due to restraints in time or funding. Weiskopf and Weng for example argue that three dimensions of data quality are of fundamental value; that is correctness, completeness and currency, or timeliness [ 7 ]. The data in the EHRs must be representative of a patient state at the time of recording. Other properties are different aspects of reliability, validity and reproducibility of the data, as Terwee et al. [ 20 ] for example proposed. Although our study focused on the comparison of data quality, quality of care research would benefit from a closer look at other data properties to assess the added value of using the EHR as a data source for research purposes.
Another implication of this study is the need for effective natural language programming (NLP) tools. With these tools, text boxes in the EHR, e.g. for describing patient goals for example, can be analysed and used for a more content-based quality measurement. Although these tools can be successful, it requires a considerable user involvement [ 1 ]. Clinicians should collaborate in developing such tools to ensure that the right âdictionaryâ is being used, i.e. the right professional lexicon. In a trade-off between a deeper linguistic understanding and computational efficiency, Jung et al. [ 21 ] advocates the use of simpler NLP tools to advance adoption of NLP in practice. A simpler, dictionary-based term recognition tool can be used, as these are easier to use and with more speed than more advanced NPL tools [ 21 ]. The use of text-mining tools allows clinicians to maintain a level of narrative information so that he can use his own words to describe his patient to administrate the treatment, while researchers can encode this information to measure the quality of care.
At the same time, we advice the use of standardized coding with a search function as another possibility for a more content-based quality measurement. For example, for encoding complaints the International Classification of Functioning, Disability and Health (ICF) could be used [ 22 ]. The ICF is an extensive effort of the World Health Organization (WHO) to standardize terminology and to classify problems in the human functioning. The classification covers multiple dimensions of human functioning with underlying categories in body functions, activities and participation, including internal and external factors that may influence human functioning. The use of the ICF in coding problems in functioning of patients would assist in establishing reliable comparisons for estimating the quality of care, although preliminary training is required to use the ICF correctly [ 23 ]. Complaints in physiotherapy are usually situational and activity related, and by this specific set of coding, it can help the physical therapist in deciding which intervention is needed and what outcome is achievable, and at the same time it can also help the quality research to step up to more content-based quality measurements [ 24 ]. The use of classification systems will transform the EHR from an ad-hoc extraction system for quality research, into a proactive documentation support system to improve the administration of health data upfront, as proposed by Botsis et al [ 1 ]. It should be studied if these standardizations of documentation and terminology [ 25 , 26 ] will be effective in enhancing comparability and decreasing variation between EHR suppliers. Indicators 4 (defining and administrating intervention) and 7 (objectified result, by use of measurement instrument) for example could not be extracted from the EHRs because there was not enough time within the project to classify the possible interventions and the measurement instruments that could, or should be used in each case. With a classification system implemented in the EHRs, the correct data to calculate these two quality indicators from could be extracted from the EHRs.
Conclusion
The main challenges are ensuring comparability between survey data and EHR data, as well as comparability between different EHR-systems. Collecting data from surveys is more costly both in time and money, and data quality of both methods was roughly the same so future efforts should be aimed at streamlining the use of EHR data for quality of care research. Standardization of the format of EHRs, the use of a standardized coding and exploring text mining tools require a considerable effort from the physiotherapy community, researchers and EHR developers. A standardized EHR can be used for continuous measurement of the quality of care, and for providing real-time feedback to all stakeholders. More research and testing is needed to bridge the needs of clinicians for using the EHR in practice and the needs of researchers and health insurers for using the EHR as a database for quality research.
Abbreviations
Barriers to Retrieving Patient Information from Electronic Health Record Data: Failure Analysis from the TREC Medical Records Track Tracy Edinger, ND, Aaron M. Cohen, MD, MS, Steven Bedrick, PhD, Kyle Ambert, BA, William Hersh, MD Department of Medical Informatics and Clinical Epidemiology, Oregon Health & Science University, Portland, OR, USA Abstract Objective: Secondary use of electronic health record (EHR) data relies on the ability to retrieve accurate and complete information about desired patient populations. The Text Retrieval Conference (TREC) 2011 Medical Records Track was a challenge evaluation allowing comparison of systems and algorithms to retrieve patients eligible for clinical studies from a corpus of de-identified medical records, grouped by patient visit. Participants retrieved cohorts of patients relevant to 35 different clinical topics, and visits were judged for relevance to each topic. This study identified the most common barriers to identifying specific clinic populations in the test collection. Methods: Using the runs from track participants and judged visits, we analyzed the five non-relevant visits most often retrieved and the five relevant visits most often overlooked. Categories were developed iteratively to group the reasons for incorrect retrieval for each of the 35 topics. Results: Reasons fell into nine categories for non-relevant visits and five categories for relevant visits. Non-relevant visits were most often retrieved because they contained a non-relevant reference to the topic terms. Relevant visits were most often infrequently retrieved because they used a synonym for a topic term. Conclusions: This failure analysis provides insight into areas for future improvement in EHR-based retrieval with techniques such as more widespread and complete use of standardized terminology in retrieval and data entry systems. Introduction The use of clinical data for research is a widely anticipated benefit of the electronic health record (EHR) [1]. Clinical data stored in structured fields is relatively straightforward to retrieve and use; however, a large proportion of EHR data is "locked" in textual documents [2]. EHR chart notes are typically stored in text files, which include the medical history, physical exam findings, lab reports, radiology reports, operative reports, and discharge summaries. These records contain valuable information about the patient, treatment, and clinical course. This "free text" data is much more difficult to access for secondary purposes. In order to use this data, we must be able to retrieve records accurately and reliably for a desired patient population, usually through the use of natural language processing (NLP). While NLP has been applied to EHR data for decades, the performance of these systems has been variable across the techniques used, as well as the clinical task [3]. Historically, the field of information retrieval (IR) has studied the retrieval of documents and other content [4]. However, IR has tended to place a greater focus on presenting content to users for human interpretation, rather than on extracting the specific information they contain. This task is typically referred to as information extraction or text mining [5]. IR also has a long tradition of system evaluation, especially involving the use of test collections that contain fixed assemblies of content, query topics, and relevance judgments, a "gold standard" defining which content items are relevant to which topics. Such test collections are important, because they allow direct comparison of results obtained by different IR systems. The field of IR also has a tradition of advancing knowledge by hosting challenge evaluations, in which the same test collection is used by many groups to compare the efficacy of different approaches. One of the best-known is the Text Retrieval Conference (TREC), an annual challenge evaluation hosted by the US National Institute for Standards & Technology (NIST) [6]. TREC is a long-standing event that allows different tasks and approaches to be assessed in an open, collegial, and comparable manner. Each year, TREC holds a number of "tracks" devoted to different aspects of IR, such as Web searching or cross-language IR. While TREC is focused on general-purpose IR, there have been some tracks dedicated to specific domains, including genomics [7]. In 2011, TREC launched a Medical Records Track (TRECMed) [8] to develop an IR challenge task pertinent to real-world clinical medicine. The track was made possible by access to a large corpus of de-identified medical text from 180 the University of Pittsburgh Medical Center (http://www.dbmi.pitt.edu/blulab). De-identified clinical documents in the collection are organized according to patient visits. The task in the first year of TRECMed was to retrieve cohorts of patients fitting criteria similar to those specified for participation in clinical studies. Retrieval topics were derived from an Institute of Medicine list prioritizing conditions for comparative effectiveness research [9] and modified to be unambiguous and to generate an appropriate quantity of visits relevant to the tasks. Funding from NIST allowed organization of the topic development and relevance assessment processes of the track. The documents for the task come from the University of Pittsburgh NLP Repository, a repository of 95,702 de-identified clinical reports available for NLP research purposes. The reports were generated from multiple hospitals during 2007 and are grouped into visits consisting of one or more reports from the patient's hospital stay. Reports for each visit are stored as text files and may include medical history, physical exam findings, radiology reports, operative reports, or discharge summaries. Each document is formatted in Extensible Markup Language (XML), with a table that maps one or more reports to a visit. An admission diagnosis, discharge diagnoses, and ICD-9 code(s) are recorded for each visit. These codes are stored at the visit level, rather than at the encounter level, so they must be searched separately from the report documents. The data set contains a total of 17,199 visits. Based on resources available for relevance judging, a decision was made to develop and judge 35 topics for the test collection, consistent with known observations that 25-50 topics are the minimum required for statistical stability of results in IR test collections [10]. As is common practice in TREC and other challenge evaluations, research groups submitted runs consisting of a ranked list of visits for all 35 topics. Each of the 29 participating research groups was allowed to submit up to eight runs. A total of 127 runs were submitted, with the results pooled to allow representative sampling for relevance judging. The pooled visits for each topic were judged for relevance by physicians enrolled in Oregon Health & Science University's graduate program in Biomedical Informatics; topics and number of relevant visits are listed in Table 1. Because resources did not allow for exhaustive judging, many of the documents that were retrieved in various runs had not been judged. This led to a decision to use the bpref measure [11] to compare retrieval performance, which only measures recall and precision on documents that have had relevance judgments. Bpref is defined as the inverse of the fraction of judged irrelevant documents that are retrieved before judged relevant documents, and varies from 0 (no retrieval) to 1 (retrieval of all known relevant documents). The best bpref performance in TRECMed was in the range 0.5-0.6, reasonably good performance for a general IR task but not satisfactory for identifying patients in a set of clinical records. Further details of the test collection, including the methods we used for topic development and relevance judging, are in the track overview paper from the TREC 2011 conference proceedings [8]. The goal of the study presented here was to perform a failure analysis on two types of retrieved visits. The first type was those visits that were frequently retrieved but judged to be non-relevant (i.e., false-positive retrievals). The second type was those visits that were infrequently retrieved yet judged to be relevant (false-negative retrievals). For both categories of retrieved visits, we created a list of reasons for the inappropriate frequent or infrequent retrieval, with the goal of determining the reasons for these errors and guiding future system development. Methods Using the 127 runs submitted by the participating research groups, we combined each run's top 100 retrieved visits for each topic to produce a list of visits ranked by the number of systems retrieving that visit within the top 100. At the top of this list were visits that were commonly ranked in the top 100 by many systems for a given topic, i.e., commonly highly ranked visits. At the bottom of this list were those rarely ranked within the top 100 by any participating system, the rarely highly ranked visits. At least one system had to rank a visit within its top 100 results for the visit to be considered rarely highly ranked. Often-retrieved non-relevant visits: Among the commonly highly ranked visits were visits that were incorrectly retrieved and marked as non-relevant by the judges. These visits were problematic for many of the retrieval systems, in that they were often ranked much more highly than they should have been, and can be considered to be common precision errors. 181 Table 1. TRECMed topic number, description of criteria for visit retrieval, and number of visits judged relevant for each. Topic Number Description Relevant Visits 101 Patients with hearing loss 69 102 Patients with complicated GERD who receive endoscopy 89 103 Hospitalized patients treated for methicillin-resistant Staphylococcus aureus (MRSA) endocarditis 7 104 Patients diagnosed with localized prostate cancer and treated with robotic surgery 8 105 Patients with dementia 143 106 Patients who had positron emission tomography (PET), magnetic resonance imaging (MRI), or computed tomography (CT) for staging or monitoring of cancer 85 107 Patients with ductal carcinoma in situ (DCIS) 9 108 Patients treated for vascular claudication surgically 12 109 Women with osteopenia 119 110 Patients being discharged from the hospital on hemodialysis 95 111 Patients with chronic back pain who receive an intraspinal pain-medicine pump 19 112 Female patients with breast cancer with mastectomies during admission 66 113 Adult patients who received colonoscopies during admission which revealed adenocarcinoma 10 114 Adult patients discharged home with palliative care / home hospice 53 115 Adult patients who are admitted with an asthma exacerbation 33 116 Patients who received methotrexate for cancer treatment while in the hospital 10 117 Patients with Post-traumatic Stress Disorder 22 118 Adults who are received a coronary stent during an admission 50 119 Adult patients who presented to the emergency room with anion gap acidosis secondary to insulin dependent diabetes 37 120 Patients admitted for treatment of CHF exacerbation 104 121 Patients with CAD who presented to the Emergency Department with Acute Coronary Syndrome and were given Plavix 32 122 Patients who received total parenteral nutrition while in the hospital 24 123 Diabetic patients who received diabetic education in the hospital 33 124 Patients who present to the hospital with episodes of acute loss of vision secondary to glaucoma 2 125 Patients co-infected with Hepatitis C and HIV 14 126 Patients admitted with a diagnosis of multiple sclerosis 5 127 Patients admitted with morbid obesity and secondary diseases of diabetes and or hypertension 85 128 Patients admitted for hip or knee surgery who were treated with anti-coagulant medications post-op 75 129 Patients admitted with chest pain and assessed with CT angiography 45 130 Children admitted with cerebral palsy who received physical therapy 1 131 Patients who underwent minimally invasive abdominal surgery 96 132 Patients admitted for surgery of the cervical spine for fusion or discectomy 88 133 Patients admitted for care who take herbal products for osteoarthritis 4 134 Patients admitted with chronic seizure disorder to control seizure activity 25 135 Cancer patients with liver metastasis treated in the hospital who underwent a procedure 55 182 Rarely-retrieved relevant visits: Similarly, within the rarely highly ranked visits, there were records that were correctly retrieved and marked relevant by the TRECMed relevance judges,. These correctly retrieved, rarely highly ranked visits were problematic for the retrieval systems, in that most systems failed to rank them as highly as they should have been, and can be considered to be common recall errors. For each topic, we examined the top five ranked common precision errors, or false positives, and the bottom five ranked common recall errors, or false negatives. For topics that did not include five relevant documents, we reviewed all relevant visits. If one or more visits were assigned tied system counts, we included all visits having the tied count. Our sample contained 359 visits, with 182 common precision errors and 177 common recall errors. A review of these two sets of visits per topic enabled us to perform a failure analysis of a sample of the TRECMed results. One author (TE) manually examined the chart notes from each visit to determine why it may have been retrieved incorrectly or missed, iteratively developing a set of codes to categorize the reasons. If our existing code set did not describe the reason for a given error, we developed a new code to describe the reason. Because we did not have access to the actual queries used by the systems in their runs, the codes reflect our best judgment as to why the visit was difficult to find accurately for each topic, based on the topic statements themselves, the concepts contained in the topics, and the content of the visit notes. The dataset contains documents generated from hospital stays. Each stay, or visit, includes one or more reports. These reports, from each clinical encounter during a visit, are stored as separate text files and may include medical history, physical exam findings, radiology reports, operative reports, or discharge summaries. Each report, or document, is formatted in Extensible Markup Language (XML), with a table that maps one or more reports to a visit. An admission diagnosis, discharge diagnoses, and ICD-9 code(s) are recorded for each visit and are stored at the visit level, rather than at the encounter level. Each document displays the diagnoses in the visit header and a list of all encounters in the visit. For this analysis, the encounter documents were opened one at a time and searched for the topic terms until the reviewer identified a satisfactory explanation for that visit's anomalous retrieval. Once an explanation was found, the reviewer moved on to the next visit. Results Our analysis of the common precision or "false positive" errors revealed a wider variety of reasons for incorrect retrieval than we observed for the common recall or "false negatives" errors. The final list contained nine codes for precision errors, and five codes for recall errors (see Figure 1). Several visits appeared to have been judged incorrectly by the TRECMed relevance assessors, accounting for almost 20% of the sample. This is a common occurrence in IR evaluations, and has usually been shown to affect the absolute, but not the relative, comparison of different runs [12]. Figure 1 depicts the distribution of visits and topics for each code category, showing the total number of incorrectly retrieved visits in each category, as well as each category's distinct topics. Irrelevant references were observed to cause the greatest number of incorrect retrievals of non-relevant visits, and use of alternative terms was the most common cause of low retrieval rates of relevant visits. The number of relevant visits with topic terms present but overlooked in the search is most likely due to our error sampling strategy; this is explained in greater detail below. The specific reasons for relevant visits being infrequently retrieved and non-relevant visits being frequently retrieved are described next, with examples given. Relevant Visits with Low Retrieval Counts Topic not present—record is not relevant—disagree with relevance judge A number of visits judged relevant were found to be irrelevant upon closer examination. There were a variety of reasons for this, including confusion of terms for different body regions and procedures mentioned as future possibilities. These reasons covered the same issues as seen in the often retrieved non-relevant visits discussed in the next section. 183 Visits Judged Relevant Visits Judged Not Relevant 0 20 40 60 80 100 Irrelevant reference in record to topic terms Topic present— record is relevant Topic symptom/condition/ procedure done in the past All criteria present but not in the order specified Topic terms denied or ruled out Most, but not all, criteria present Topic terms mentioned as future possibility Topic terms not present Notes contain very similar term confused with topic Topic present in record but overlooked in search Topic not present— record is not relevant Visit notes used a synonym for topic terms Topic terms present in diagnosis list but not visit notes Topic terms not named and must be inferred Number of Retrieval Errors Visits Topics Figure 1. Number of visits and number of topics for each type of incorrect retrieval. Topic present in record but overlooked in search For visits that were relevant to the topic, the topic terms were present in the majority of the low-count visits, and the reason for not retrieving it could not be readily determined. We believe most of these had low counts because they were not ranked highly in the original runs, and they were not included in our lists of the top 100 visits. The system "error" in this case may be that other non-relevant visits were retrieved and ranked higher, rather than an actual failure to retrieve the visit. In other words, non-relevant visits pushed the relevant ones down in the ranking, and therefore the error is in retrieving the other non-relevant visits and ranking them highly. This may be due to topic terms commonly occurring in non-relevant documents. Visit notes used a synonym or lexical variant for topic terms The most common feature in under-retrieved relevant visits was the use of different terminology in the notes. A search for patients with hearing loss tended to overlook visits mentioning deafness or hearing aids. Spelling variations and typographical errors also fell into this category. 184 Topic terms not named in notes and must be inferred Chart notes for two topics contained information that was equivalent to the desired topic, but did not use the specific term or diagnosis. In searching for patients with anion gap acidosis, it was common to miss visits listing lab values for sodium, potassium, chloride, and bicarbonate that arithmetically indicated anion gap acidosis. Some of the topic terms present in diagnosis list but not visit notes Each document displayed a header listing the admission diagnosis, discharge diagnoses, and ICD-9 codes. Although they appear at the top of each document, these headers are stored separately from the encounter notes and must be searched separately from the notes. Visits for patients co-infected with hepatitis C and HIV contained both topic conditions in the diagnosis list but not in the notes. Many encounters were for treatment of one of the conditions, a concomitant condition, or something else entirely, and the notes mentioned only one (or none) of the topic conditions. Systems that did not search both the encounter notes and the diagnosis lists would miss relevant visits. Non-relevant Visits with High Retrieval Counts Topic present—record is relevant—disagree with relevance judge A number of visits judged non-relevant were found to be relevant upon closer examination. The topic terms were present in the notes and appeared to have been overlooked during judging. Topic terms not present—cannot determine why record was retrieved A number of visits did not appear to contain any reference to the topic. It is unclear why these would be retrieved by the systems. It is likely that systems were performing query expansion, or some other statistical IR approach that introduced semantic error. Irrelevant reference in record to some topic terms The largest number of retrieved non-relevant visits made irrelevant references to topic terms. In the search for patients who underwent minimally invasive abdominal surgery, non-relevant visits were retrieved that mentioned minimally invasive surgery of other parts of the body. Notes contain very similar term confused with topic Some visits contained terms very similar to the topic terms. For example, searching for ductal carcinoma in situ (DCIS) often retrieved patients with invasive, rather than in situ, ductal carcinoma. Topic terms denied or ruled out Several non-relevant visits appeared to be retrieved because the notes mentioned ruling out a diagnosis or denial of a symptom, a common component of medical records. Visits retrieved for patients with hearing loss included many that noted the "patient denies hearing loss." Most, but not all, topic criteria present In some cases the visit contained almost all of the desired criteria. One example was the search for adult patients who presented to the emergency room with anion gap acidosis secondary to insulin dependent diabetes, which also retrieved patients with non-insulin dependent diabetes. All criteria present but not in the order specified by the topic description Visits in this category contained all the terms present in the topic but in a different time frame or sequence. In many cases, these were patients who already had a condition or procedure specified by the topic but who were being seen for other conditions. Other errors in this category included visits containing all elements of the topic, but that were still not quite right. For example, one visit retrieved for "patients who present to the hospital with episodes of acute loss of vision secondary to glaucoma" referred to a patient with glaucoma and "blurred vision of unclear etiology in right eye." Topic symptom/condition/procedure done in the past Procedures done previously, or pre-existing conditions, also increased the likelihood of retrieving non-relevant visits. Topic terms mentioned as future possibility Mentioning a procedure to be done at a later date or a condition to be considered in the future increased the likelihood a visit would be retrieved incorrectly. Several non-relevant visits for patients who received physical therapy (topic 130) or diabetic education (topic 123) mentioned they would be done at a later time: "we are going to consult diabetic education." 185 Discussion Several themes emerged from our analysis, with terminology differences, negation, and time aspects having the most impact on retrieval inaccuracy. These areas reflect well-known issues in the general IR field and in the application of IR to medical records [13, 14]. The current analysis demonstrates that these problems also arise during cohort identification in medical text. Overlap in terminology between conditions or procedures Irrelevant chart references to a term in the topic were associated with retrieval of the greatest number of non-relevant visits. Knowledge and use of the correct medical terminology in the search may increase retrieval accuracy. This could be accomplished by including all the different ways of referring to a condition or procedure and eliminating conditions or procedures that are almost the same as the one desired. For example, a search for patients with hearing loss should include "deaf," "deafness," "hearing aids," and "cochlear implant" in the search criteria. Some terms are commonly used to refer to specific organs or body parts without actually naming them. For example, a search for ductal carcinoma in situ should distinguish between mammary ductal carcinoma and biliary ductal carcinoma. This search should also distinguish between cancer that has not spread to surrounding tissues (in situ) and cancer that has. Distinguishing between conditions that share terms in their description will require the intelligent use of medical terminology to distinguish relevant from irrelevant phrases. Simply assigning weights to overlapping words regardless of their context was seen to lead to incorrect results. Negation detection Successful searches must also incorporate a way of distinguishing between a symptom or condition and the denial or ruling out of that symptom or condition. Our analysis found 19 visits, or 5% of our sample, that were incorrectly retrieved because of this. Negation statements are very common components of medical records: the physical exam will contain references to symptoms denied by the patient, the medical history may include conditions not present in the patient or patient's family, and procedure and chart notes may refer to conditions that have been considered and ruled out. This is a known issue in retrieving information from medical texts, and tools are available to overcome this barrier [15]. Data integrity and lexical variants of terms Ensuring that clinical data is entered accurately and consistently is necessary. The ability to retrieve data accurately is dependent in large part on the quality of the data. Variations in terminology, spelling errors, and the completeness of the clinical record can all be improved by emphasizing the importance of quality data entry in the clinic and by incorporating spell checking in the EHR. Encouraging consistency in term use and descriptions would further enhance usability. Time factors or sequencing of terms Identifying the desired time frame presents another critical factor in correctly retrieving records. Symptoms or procedures done in the past, or being considered for the future treatment, were often confused with those in the present. Our analysis found several instances in which the patient's condition changed dramatically during the visit, resulting in visits that did not correspond to the admission or pre-existing diagnoses. In a few cases, the patient died, rendering orders for post-discharge treatment irrelevant. Incorporating temporal information into query and document analysis may prevent certain classes of retrieval error. Allowing the search to be constructed to exclude past or future conditions or procedures would limit the search results to current issues. Age proved to be a difficult component for a number of topics. One topic specified "children admitted with cerebral palsy who received physical therapy." All of the non-relevant visits reviewed for this topic referred to adults matching the rest of the topic specifications. The de-identification process used for this data set changed age to a range of ages. This is seen in one record that reports that "the patient is a **AGE[in 20s]-year-old..." This may not be an issue when searching clinical records that have not been de-identified. For this particular case, age could be handled by including terms such as "infant", "month-old," "child," or "teenager," or by searching for the phrases produced through the de-identification process. 186 Incorporating knowledge of logical data constructs Systems must be designed to search both free-text chart notes as well as more structured data such as diagnosis and medication lists. Searching both areas will enhance retrieval for those conditions listed in only one place. This may be especially relevant for visits where patients have chronic conditions, as in the Hepatitis C and HIV topic, but are seeking treatment for acute illness. This is particularly challenging in the case of the TRECMed data set, since that level of structure is not explicitly encoded in the encounter documents, which are provided as flat text files. Limitations There were several limitations in the design and execution of this study. The method of reviewing visits to determine the reason for incorrect retrieval produced one source of potential error. The classification schema for error types was generated using an iterative process that included the feedback and consensus of all the authors. However, only one author reviewed the system retrieval data in detail, and therefore some visits may be misclassified either due to operator error or ambiguity in the source documents of schema itself. Furthermore, once the most likely reason for incorrect retrieval was identified, the reviewer recorded the reason and moved on to the next visit, not evaluating the possibility that there may be more than one reason for incorrect retrieval for an individual document. Another limitation of this study resulted from the fact that we did not have access to the search strategy or implementation details of each IR system from which runs were submitted. It would not be possible to determine with certainty the reason an individual system correctly or incorrectly retrieved a document without access to these internal system details. It is likely that in some cases our assigned error types do not correspond precisely to the reason that an individual system incorrectly retrieved or missed a document. As mentioned in the results section, a number of non-relevant visits were retrieved although they did not contain any obvious relevance to the topic. We presume that many of these were retrieved incorrectly as a result of the query expansion used. In these visits, the reason for incorrect retrieval cannot be confirmed without access to the code. These limitations would be unlikely to have a significant effect on the conclusions of the study because the goal of this analysis was to identify general challenges and trends in the retrieval task across the set of submitted systems rather than perform a precise error analysis for any individual system. The goal of this study was to determine challenges in the task based on the characteristics of the data. The data examples representing challenging situations were identified by looking at common failures across the submitted systems. For these reasons, in spite of the limitations, we remain confident that our analysis identifies valid areas to address that will improve the ability to more effectively retrieve and utilize clinical records for secondary purposes such as research and surveillance. Conclusions The EHR is a valuable source of data for enhanced patient care, research, and quality improvement. The usefulness of this data depends on the ability to locate information as accurately as possible. This error analysis provides insight into areas for future improvement in EHR-based retrieval systems. In our work, the most common sources of retrieval error included irrelevant chart references to a term, variation in terminology and spelling, lack of distinction between different conditions with similar names, lack of distinction between past, present, and future conditions or procedures; and failure on the part of IR systems to distinguish between the presence or denial of a symptom or condition. All of these sources of error will have to be addressed by future improvements in EHR systems and retrieval capabilities for them. This analysis also highlights areas where changing EHR data entry could have a beneficial impact on retrieval. The EHR is an important resource for biomedical research and healthcare improvement. Improvement in patient record information retrieval is only one area where clinical text processing has the potential to advance medical research and improve healthcare. Additional work is necessary to realize the full potential of the valuable information stored in the EHR. References 1. Safran C, Bloomrosen M, Hammond WE, Labkoff SE, Markel-Fox S, Tang P, et al., Toward a national framework for the secondary use of health data: an American Medical Informatics Association white paper. Journal of the American Medical Informatics Association, 2007. 14: 1-9. 187 2. Hripcsak G, Friedman C, Anderson PO, DuMouchel W, Johnson SB, and Clayton PD, Unlocking clinical data from narrative reports: a study of natural language processing. Annals of Internal Medicine, 1995. 122: 681-688. 3. Stanfill MH, Williams M, Fenton SH, Jenders RA, and Hersh WR, A systematic literature review of automated clinical coding and classification systems. Journal of the American Medical Informatics Association, 2010. 17: 646-651. 4. Hersh WR, Information Retrieval: A Health and Biomedical Perspective (3rd Edition). 2009, New York, NY: Springer. 5. Cohen AM and Hersh WR, A survey of current work in biomedical text mining. Briefings in Bioinformatics, 2005. 6: 57-71. 6. Voorhees EM and Harman DK, eds. TREC: Experiment and Evaluation in Information Retrieval. 2005, MIT Press: Cambridge, MA. 7. Hersh W and Voorhees E, TREC genomics special issue overview. Information Retrieval, 2009. 12: 1-15. 8. Voorhees EM and Tong RM. Overview of the TREC 2011 Medical Records Track. The Twentieth Text REtrieval Conference Proceedings (TREC 2011). 2011. Gaithersburg, MD: National Institute for Standards and Technology. 9. Anonymous, Initial National Priorities for Comparative Effectiveness Research. 2009, Institute of Medicine: Washington, DC. 10. Buckley C and Voorhees E. Evaluating evaluation measure stability. Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 2000. Athens, Greece: ACM Press. 33-40. 11. Buckley C and Voorhees EM. Retrieval evaluation with incomplete information. Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 2004. Sheffield, England: ACM Press. 25-32. 12. Voorhees EM. Variations in relevance judgments and the measurement of retrieval effectiveness. Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 1998. Melbourne, Australia: ACM Press. 315-323. 13. Nadkarni PM, Ohno-Machado L, and Chapman WW, Natural language processing: an introduction. Journal of the American Medical Informatics Association, 2011. 18: 544-551. 14. Chapman WW, Nadkarni PM, Hirschman L, D'Avolio LW, Savova GK, and Uzuner O, Overcoming barriers to NLP for clinical text: the role of shared tasks and the need for additional creative solutions. Journal of the American Medical Informatics Association, 2011. 18: 540-543. 15. Chapman WW, Bridewell W, Hanbury P, Cooper GF, and Buchanan BG, A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 2001. 34: 301-310. 188 Information Retrieval in Medicine: The SAPHIRE Experience William R. Hersh Biomedical information Communication Center, Oregon Health Sciences University, 3 781 S. W. Sam Jackson Park Rd., Portland, OR 97201. Phone: 503-494-4563; Fax: 503-494-4551; E-mail: hersh@ohsu.edu David Hickam VA Medical Center, Health Services Research and Development (152), P.O. Box 1034, Portland, OR 97207. Phone: 503-273-5305; Fax: 503-273-5367; E-mail: hickam&hsrd.gov Information retrieval systems are being used increasingly in biomedical settings, but many problems still exist in in-dexing, retrieval, and evaluation. The SAPHIRE Project was undertaken to seek solutions for these problems. This article summarizes the evaluation studies that have been done with SAPHIRE, highlighting the lessons learned and laying out the challenges ahead to all medical information retrieval efforts. Once confined mainly to medical libraries and com-puter pioneers with modems, medical information re-trieval (IR) systems have become widespread. Few med-ical schools or large medical centers lack access to MED-LINE, which is often subsidized for students and staff. Other IR databases in areas such as nursing and drug in-formation are also widely available, and there is increas-ing access to multimedia materials and the Internet. Despite their prevalence, however, many impedi-ments to effective use of medical IR systems remain. It is still not known, for example, how to index databases most effectively. Likewise, there are disagreements over the best way to phrase search statements, whether with traditional Boolean operators, "natural language" input, or more complex representations. Finally, there are problems with evaluation approaches that evolved in li-brary settings but are less suitable for the end-user envi-ronment. For the past 5 years, the SAPHIRE (Semantic and Probabilistic Heuristic Information Retrieval Environ-ment) Project has been devoted to identifying the opti-mal approaches to indexing, retrieval, and evaluation of IR resources in the biomedical domain. The scope of the research project has gone beyond just the development and testing of the SAPHIRE software itself. The purpose of this article is to review the evaluation studies of the project, which provide insight into the problems that ex-0 1995 John Wiley&Sons, Inc. ist in all IR systems and how they may guide future re-search. We begin by describing the basic issues t.hat mo-tivate the SAPHIRE Project, followed by a summary of six evaluation studies. We then summarize the conclu-sions drawn from this work and lay out the challenges ahead. IR Problems and SAPHIRE Solutions IR databases are generally of two types: bibliographic and full-text. The former typically consist of references to the original medical literature, while the latter contain the complete text of documents from journals, text-books, and other print sources. Bibliographic databases usually contain indexing terms assigned by a human in-dexer from a controlled vocabulary, although the other content words in the reference (e.g., those from the title and abstract) are usually searchable. In contrast, full-text databases are usually indexed based on the words present in the entire document. Although human indexing of bibliographic databases is considered state-of-the-art, it is still unclear how much benefit this indexing provides. While humans are good at discerning the focus of a document, manual indexing is known to be inconsistent (Funk & Reid, 1983), and the vocabulary terms can be difficult for novices to mas-ter (Kirby & Miller, 1986). The usual alternative to hu-man indexing is to index all words in the document. But this too presents problems. For example, medical lan-guage is known to have much synonymy (different words meaning the same thing) and polysemy (the same words meaning different things). Furthermore, extract-ing single words for indexing removes the context in which they occur (i.e., the phrase high bloodpressure has a different meaning from any of the three words used individually). Computational linguistic approaches to discerning concepts and relationships have been advo-cated, but constructing knowledge resources to recognize these elements has been difficult (Salton, Buckley, & Smith, 1990). On the retrieval side, search terms combined by the Boolean operators AND and OR have been used in most systems, but they have been shown to be difficult for nov-ices (Sewell & Teitelbaum, 1986) and their benefit is also unknown. Another retrieval problem is that most sys-tems return documents in arbitrary order. A potential solution is to rank documents based on the frequency of terms similar to the query and document, called rele-vance ranking (Salton, 199 1). The only way to determine which methods of index-ing and retrieval work best has been to evaluate them empirically. There is a large literature on IR system eval-uation, and much has been learned about the perfor-mance of various techniques. However, there is also dis-agreement over the value of the measures used to assess performance and how to apply them in different settings (Hersh, 1994). The SAPHIRE Project was implemented to address the problems in current IR systems. SAPHIRE provides potential solutions to the problems of inconsistent hu-man indexing, clinician difficulty with controlled vocab-ularies and Boolean searching, and the rich synonymy of medical language. The details of the system have been described elsewhere (Hersh, 199 1)) but the basic ap-proach is to extend word-based automated methods by indexing on concepts found in text instead of individual words. The vocabulary for identifying concepts and their synonyms is based on the Metathesaurus from the Na-tional Library of Medicine's (NLM) Unified Medical Language System (UMLS) Project (Lindberg, Hum-phreys, & McCray, 1993). In order to compare SAPH-IRE with existing approaches to IR, two additional sys-tems have been implemented: One combining word-based indexing with Boolean searching (BOOLEAN), and another combining word-based indexing with natu-ral language searching (SWORD, Statistical Word-Ori-ented Retrieval from Databases) (Hersh & Hickam, 1995). Evaluation Studies Over the past 4 years, there have been six evaluation studies in the SAPHIRE Project. Each of these studies has provided incremental information about the benefits and limitations of SAPHIRE as well as adding to general knowledge about the indexing, retrieval, and evaluation of IR systems in the biomedical setting. All of these stud-ies used the measures of relative recall (proportion of known relevant references retrieved from the database, hereafter referred to as recall ) and precision (proportion of relevant references retrieved by the search). These measures are often considered the gold standard of re-trieval system evaluation, yet have limitations in mea-suring the true effectiveness of retrieval systems. SAPHIRE vs. Human Inde.x-ing ofAIDSLINE Documents (Hersh & Hickam, 1992) The first evaluation study of SAPHIRE used a 200-document subset from the AIDSLINE database, along with 12 queries generated by library users at the NLM and Oregon Health Sciences University (OHSU). The initial component of the study was to compare SAPH-IRE indexing with both human indexing of MEDLINE and simple word-based indexing of the title and abstract fields in a command-line searching environment. SAPH-IRE performed inferiorly as an indexing replacement in this environment. An unexpected finding was that the search results of librarians were as good using text words only as with the full MEDLINE feature set, while physi-cians actually had better results with text words. Because SAPHIRE was not designed to run in a com-mand-line Boolean searching environment, another component of the study looked at SAPHIRE's perfor-mance by entering the free-text query statement directly in the natural language interface. SAPHIRE performed better in this latter mode, and, in fact, achieved better recall and precision than physicians using Boolean search statements, though not as good as librarians. This study also looked at the frequency and type of concept matching errors made by SAPHIRE. An average of 2.7 (out of an average of 18) inappropriate indexing assignments per abstract were made, with problems aris-ing due to syntax (the verb lead matched as the chemical element, a noun), abbreviations (PCP meant P. car&ii pneumonia in this domain but matched to the drug phencyc/idine), and stemming (the state Maine was stemmed to the word main). SAPHIRE vs. Conventional MEDLINE Searching of MEDLINE Documents (Her.& Hickam, Haynes, & McKibbon, 1994~) The second study of SAPHIRE used previously searched topics and judgments of relevance from a clini-cal evaluation of Grateful Med at McMaster University (Haynes et al., 1990). A test collection of 2,344 MED-LINE references was created, consisting of all references that were retrieved (and contained abstracts) for 75 que-ries generated by clinicians. In this study, SAPHIRE's recall and precision appeared intermediate between ex-pert and novice clinician searchers, although none of the differences among the groups was statistically significant. A failure analysis identified some recurring patterns for false-negative and false-positive retrievals. The most common causes of relevant documents failing to be re-trieved were the presence of synonyms not recognized by SAPHIRE (i.e., the form in the document was not in the Metathesaurus) or terms being present but at a different level of granularity (i.e., the query might have had the term antibiotic while the document contained the actual antibiotic name). The most common reason for retrieval of nonrelevant documents was the presence of most or all query terms in the document, but with a different fo-cus or relationship between the terms. SAPHIRE vs. Word-Based Boolean Searching of Yearbook Series Extended Abstracts (Hersh & Hickam, 1993) One problem with the first two studies was the lack of interactive searching by real users. The next study cor-rected that problem by using a group of I6 senior medi-cal students to search on 10 questions generated on med-ical rounds at the University of Pittsburgh. The database used was six volumes from the Yearbook Series, a publi-cation which provides abstracts and commentaries for all the major articles in a given field published each year. For indexing and retreival purposes, each document consists of the title and text. Each student searched half of the questions with SAPHIRE and the other half with BOOLEAN. No statistically significant difference in re-call or precision was found between the two systems. SAPHIRE vs. Word-Based Natural Language Searching ofAIDSLINE, MEDLINE, and Yearbook Series Documents (Hersh, Hickam, dt Leone, 1992) The next study took advantage of the existence of the test collections from the previous three studies to assess various approaches to word-based and concept-based automated systems. Like the first two studies, this study was conducted in a non-interactive setting, with queries entered in batch mode. This study compared SAPHIRE with SWORD alone, SWORD and SAPHIRE com-bined, and a version of SAPHIRE with a different con-cept-matching algorithm. The latter eliminated the exact word order requirement of SAPHIRE's original concept-matching algorithm and instead only required that words in a concept be adjacent. It also allowed partial matching as long as more than half of the words were present, aiming to overcome the problem of synonyms not matching the exact form in the Metathesaurus. The results showed that SWORD had the best overall perfor-mance. The combination of SWORD and SAPHIRE (weighting both individual words from SWORD and concepts from SAPHIRE) performed intermediately be-tween the two programs alone, while the version of SAPHIRE with the new concept-matching algorithm performed worst, due to excess inappropriate concept matching. This study also assessed a simple form of relevance feedback for SAPHIRE and SWORD, using the entire top-ranking relevant document to replace the original query. This enhanced performance for both systems, al-though SWORD still outperformed SAPHIRE. SAPHIRE vs. Word-Based Boolean Searching and Word-Based Natural Language Searching of Scientific American Medicine (Hersh & Hickam, in press) This study compared SAPHIRE, SWORD, and BOOLEAN using a different type of database, which was the internal medicine textbook, Scientific American Medicine (SAM). The textbook was subdivided into 6,623 "documents." In the study, 2 1 senior medical stu-dents searched on 18 queries each, half with one of the three systems and half with another. These queries were generated by the internal medicine faculty and the house staff in the OHSU General Medicine Clinic. Each query was searched using each of the three systems. As with the Yearbook Series study above, there was no statistically significant difference in recall or precision among the three systems. Boolean vs. Free- Text Searching in MEDLINE (Hersh & Hickam, 1994) A final study did not involve the SAPHIRE software, but rather compared the commercial product Knowl-edge Finder (KF) (Aries Systems, Inc., North Andover, MA), which uses word-based natural language searching similar to SWORD, with conventional command-line use of MEDLINE on the NLM's ELHILL system. In this study, KF was placed on a Macintosh workstation in the OHSU General Medicine Clinic. Before each search, us-ers entered a brief statement about their patient and in-formation need. These statements were used by librari-ans and experienced clinician searchers to replicate the searches. Each search was repeated by two librarians and two clinicians, with one librarian and one clinician using the full MEDLINE feature set, and the other librarian and clinician using Boolean combinations of text words. (On ELHILL, text words are defined as all words that appear in the title, abstract, and MeSH fields.) The results of this study showed that the KF searchers had significantly higher recall and lower precision than all of the other searchers. Thus they found many more relevant references but also many more nonrelevant ones. This was due to the much larger retrieval sets that they obtained, an average of 88 references for the KF group and 15 for the others. KF (and word-based natural language systems in general) tend to have larger retrieval sets, since they retrieve and rank all of the documents that contain as little as one word from the query. How-ever, their relevance ranking techniques lead to relevant documents tending to be ranked nearer the top of the retrieval set. In an attempt to control for the larger re-trieval set size, additional recall and precision values were calculated for KF with the default retrieval size set at 15 (the average size of the non-KF retrieval set). With the reduced KF set, the KF searching results were very close to those of the other searchers. This study showed that word-based natural language searching in the hands of clinicians was as effective as searching by index term. This study also verified the ob-servation from the AIDSLINE study above that simple text word searching is just as effective as using all of the advanced MEDLINE techniques, especially for non-li-brarians. In fact, while librarians obtained statistically significant improvement in recall over clinicians using the full MEDLINE feature set, they did not obtain sig-nificant improvement over clinicians using just text words, suggesting that advanced MEDLINE features are beneficial mainly to librarians. Conclusions The various experiments performed in the SAPHIRE Project have provided insight into the performance of a number of different indexing and retrieval techniques with a wide variety of resources (bibliographic databases, extended abstract collections, and textbooks). A number ofconclusions can be drawn from these studies, although the results also serve to show the limitations of current evaluation methods and the need for better ones. Indexing The project has addressed two questions in indexing. First, for bibliographic databases. do human-assigned in-dexing terms offer benefit over machine-assigned words or concepts? Our studies suggest that the incremental benefit of human indexing as measured by retrieval per-formance is small. A follow-up on the KF study above using the SMART system found that the presence of the words in the MeSH term field conferred about a 10% per-formance benefit (Hersh, Buckley, Leone, & Hickam, 1994a). The second question is whether concept-based auto-mated indexing offers any benefit over the use of single words. In the aggregate, it appears it does not. Salton, the foremost advocate of word-based automated indexing. has argued that no methods of automated indexing have improved upon the use of words alone (Salton, 199 1). Nonetheless, in the failure analysis of SAPHIRE, in-stances occurred when the synonyms present in the Metathesaurus led to superior retrieval performance (Hersh et al., 1994~). It is possible that more complex systems that utilize computational linguistic approaches (i.e., CLARIT [Evans, Hersh, Monarch, Lefferts, & Handerson, 19911) may show a benefit for concept-based indexing, but such studies have not yet been pub-lished. Retrieval Three issues have been assessed in retrieval. The first is whether searching with MeSH terms offers a benefit over the use of text words alone, which may include the words of those terms. In the KF study, the benefit of us-ing MeSH terms was seen mainly for librarians, who are well-trained in the use of those terms. Clinicians are less experienced in using MeSH terms, and as such do not show improved searching performance. The second question is the comparison of natural lan-guage vs. Boolean searching. In studies using both types of approaches, comparable results were achieved. In the SAM and Yearbook Series studies using SAPHIRE, SWORD, and/or BOOLEAN, there were minimal differences in recall or precision. Likewise, for physicians in the KF study, there was little difference between KF and Boolean searching with either the full MEDLINE feature set or text words alone. The third issue is the benefit of relevance feedback. With small databases, relevance feedback was definitely seen to offer benefit, while in a follow-on to the KF study, the gains were minimal (Hersh et al., 1994a). As with SAPHIRE's situation-specific benefit of synonyms, there may be only intermittent benefit for relevance feedback as well. Evaluation Our research has also provided insight into IR evalu-ation measures themselves. Although the measures of re-call and precision have enhanced our understanding of IR systems in general and allowed assessment of individ-ual features within and across different systems, they do not provide all of the insight we might like to have in assessing the use ofthese systems. While few would argue against retrieving more relevant and fewer nonrelevant documents, it has not been shown that the quantity of relevant documents necessarily correlates with the over-all quality of a search. Even if recall and precision did correlate with value of information obtained, there is another problem that arises when comparing systems, which is: What consti-tutes a significant difference? With a large enough sample size we can, of course, show that a difference in recall or precision is statistically significant. But it is less clear what a "clinically" significant difference would be. For example, while we know that a drop in diastolic blood pressure from 110 to 90 mm Hg would lead to signifi-cantly different medical outcomes in a patient popula-tion, it is less certain what level of difference in recall and/or precision is necessary to show that a given index-ing or retrieval method is superior to another (i.e., would achieve a better quality search). An additional problem in the use of recall and precision is how to define a relevant document. It has been argued that the relevance of a document to an in-formation need cannot be assigned objectively, espe-cially by a third party, and that the user and his/ her sit-uation must be taken into account (Schamber, Eisen-berg, & Nilan, 1990). There is no data to support or re-fute that statement. It has also been argued that relevance judgments are unreliable. The level of interobserver vari-ability in our relevance judgments for the test collections we built were moderate, with kappa scores on duplicated relevance judgments ranging from 0.35 to 0.59. Future Challenges Like many research efforts, the SAPHIRE Project has answered some questions but uncovered many new ones. This work has shown clearly that easy-to-use sys-tems featuring automated indexing, natural language queries, and relevance ranking perform comparably to traditional Boolean systems. Whether more sophisti-cated indexing procedures-such as the concept map-ping and synonym substitution used in SAPHIRE-are of benefit is less clear. It is certain, however, that better measures of evaluat-ing systems are needed. Recall and precision may not be adequate for comparing the benefit of systems. Not only must better measures of evaluation be developed, but they must also be applied in realistic settings. System as-sessment via batch input of queries may provide useful preliminary information, but evaluation of searches by real users with realistic databases (even if in a simulated setting) is necessary. Our research has begun to address these issues. We are currently porting our systems to run as client-server applications on the World Wide Web, making them available in numerous clinical sites (as well as a labora-tory for simulation). We are also assembling a suite of realistic databases that would likely benefit clinicians. Fi-nally, we have also begun to experiment with new mea-sures of performance, including those that measure in-formation obtained and not just number of relevant doc-uments (Hersh et al., 1994b). Acknowledgment The majority of the SAPHIRE Project has been funded by grant LM 05307 of the NLM. Additional funding has been provided by Contract 467-MZ-00 1022 of the NLM and Grant 9040 of the Medical Research Foundation of Oregon. References Evans. D.. Hersh. W., Monarch, 1.. Lefferts. R., & Handerson, S. ( I99 I ). Automatic indexing of abstracts via natural language pro-cessing using a simple thesaurus. hfdit~al Decision Making, II, Slog-S115. Funk, M., & Reid, C. ( 1983). Indexing consistency in MEDLINE. Bztl-kin g/'l'rhr Medical Lihrap A.swciation, 71, I Xi- 183. Haynes. R., McKibbon. K.. Walker. C., Ryan, N.. Fitzgerald, D.. & Ramsden. M. ( 1990). Online access to MEDLINE in clinical set-tings. Annds o/Inrc,rnal Medicrtw, 112, 78-84. Hersh, W. ( 199 I ). Evaluation of Meta- I for a concept-based approach to the automated indexing and retrieval of bibliographic and full-text databases. Medical Dwi.tion Muking, I I, S 120-S 124. Hersh, W. ( 1994). Relevance and retrieval evaluation: Perspectives from medicine. Jortrnd r~ffheilmcritun Socic'ty./br Ir$~rmutron Sci-once. 45, 20 I-206. Hersh. W., Buckley, C.. Leone, T., & Hickam. D. ( 1994a). OH-SUMED: An interactive retrieval evaluation and new large test col-lection for research. Procwdin,y.s of the 17th Annual Inirrnaliona~ AChI Special 1nlcw.s~ Grozfp in Ir7fi)rmation RetriCvuf (pp. l92-201). Hersh, W.. Elliot, D.. Hickam, D.. Wolf. S.. Molnar, A.. & Leich-tenstein, C. ( 1994b). Towards new measures of information re-trieval evaluation. Procrd/ngs of rhc 18th Annlrul S~wpo.sium an C'omp~rtc~rA/7p/ica/ion.s in hfdical c'aw (pp. 895-899). Hersh, W., & Hickam, D. ( 1992). A comparison ofretrieval effective-ness for three methods of indexing medical literature. Americun Jownal o$lhc !2lrdical Scicwrs, 303. 292-300. Hersh. W., & Hickam, D. ( 1993). A comparison of two methods for indexing and retrieval from a full-text medical database. hfdica/ De-crsion Making. 13. 220-226. Henh. W., & Hickam. D. ( 1994). The use ofa multi-application com-puterworkstation in a clinical setting. &r//c/in oj'~h~Mcdicul Library Assoc~iation, 8-7. 382-389. Hersh. W., & Hickam. D. ( 1995). An evaluation ofinteractive Boolean and natural language searching with an on-line medical textbook. Journal cf/hc American Socim~~,fi)r In/i)rmation Science, 46. 478-489. Hersh, W., Hickam, D.. Haynes, R.. & McKibbon. K. ( 1994~). A per-formance and failure analysis of SAPHIRE with a MEDLINE test collection. Journal of'[lwAm~rivic.an h~c~~lit~alInformatic..s Asxociation, I. 51-60. Hersh. W.. Hickam. D.. & Leone. T. ( 1992). Word, concepts, or both: Optimal indexing units for automated information retrieval. Pro-ceedings o/ Ihc 16th dnnrral .S)wpo.silrm on C'o/npzrier Applications in MdicaI (hro (pp. 644-648). Baltimore: McGraw-Hill. Kirby, M., & Miller, N. ( 1986). MEDLINE searching on Colleague: Reasons for failure or success of untrained users. Medieval R&rencc Scwkes Qzfar!er/j: 5. 17-34. Lindberg. D.. Humphreys, B.. & McCray. A. ( 1993). The unified med-ical language system project. Merhods o/'lfl/iwmufion in Medicine, 32.281-291. Salton, G. ( I99 I ). Developments in automatic text retrieval. Scienw, 253.974-980. Salton, G., Buckley, C.. & Smith, M. ( 1990). On the application of syntactic methodologies in automatic text analysis. I&wnurion Pro-ccwing and Managemcw. 26, 73-92. Schamber. L., Eisenberg. M.. & Nilan, M. ( 1990). A re-examination of relevance: Toward a dynamic, situational definition. Infiwmafion ProcewinR and Mana~~mcnt. 26, 755-776. Sewell. W., & Teitelbaum, S. ( 1986). Observations ofend-user online searching behavior over eleven years. Jownal of'thc Amrrican Soci-c'i~~{ix Irzfiwmafron Sciencr, 37, 234-245. Introduction IR Problems and SAPHIRE Solutions Evaluation Studies Conclusions Future Challenges Acknowledgment References Challenge  Evaluations  in  Biomedical  Information  Retrieval     William  Hersh,  MD    Professor  and  Chair    Department  of  Medical  Informatics  &  Clinical  Epidemiology    School  of  Medicine    Oregon  Health  &  Science  University    Email:  hersh@ohsu.edu    Web:  www.billhersh.info    Blog:  http://informaticsprofessor.blogspot.com   Twitter:  @williamhersh       References     Amini,  I,  Martinez,  D,  et  al.  (2016).  Improving  patient  record  search:  a  meta-­‐data  based  approach.   Information  Processing  &  Management.  52:  258-­‐272.   Anonymous  (2012).  From  Screen  to  Script:  The  Doctor's  Digital  Path  to  Treatment.  New  York,  NY,   Manhattan  Research;  Google.  http://www.thinkwithgoogle.com/insights/library/studies/the-­‐ doctors-­‐digital-­‐path-­‐to-­‐treatment/   Baker,  M  (2016).  1,500  scientists  lift  the  lid  on  reproducibility.  Nature.  533:  452-­‐454.   Bastian,  H,  Glasziou,  P,  et  al.  (2010).  Seventy-­‐five  trials  and  eleven  systematic  reviews  a  day:  how   will  we  ever  keep  up?  PLoS  Medicine.  7(9):  e1000326.   http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.1000326   Blumenthal,  D  (2011).  Implementation  of  the  federal  health  information  technology  initiative.  New   England  Journal  of  Medicine.  365:  2426-­‐2431.   Blumenthal,  D  (2011).  Wiring  the  health  system-­‐-­‐origins  and  provisions  of  a  new  federal  program.   New  England  Journal  of  Medicine.  365:  2323-­‐2329.   Buckley,  C  and  Voorhees,  E  (2000).  Evaluating  evaluation  measure  stability.  Proceedings  of  the  23rd   Annual  International  ACM  SIGIR  Conference  on  Research  and  Development  in  Information  Retrieval,   Athens,  Greece.  ACM  Press.  33-­‐40.   Buckley,  C  and  Voorhees,  EM  (2004).  Retrieval  evaluation  with  incomplete  information.  Proceedings   of  the  27th  Annual  International  ACM  SIGIR  Conference  on  Research  and  Development  in  Information   Retrieval,  Sheffield,  England.  ACM  Press.  25-­‐32.   Demner-­‐Fushman,  D,  Abhyankar,  S,  et  al.  (2012).  NLM  at  TREC  2012  Medical  Records  Track.  The   Twenty-­‐First  Text  REtrieval  Conference  Proceedings  (TREC  2012),  Gaithersburg,  MD.  National   Institute  for  Standards  and  Technology   http://trec.nist.gov/pubs/trec21/papers/NLM.medical.final.pdf   Demner-­‐Fushman,  D,  Abhyankar,  S,  et  al.  (2011).  A  knowledge-­‐based  approach  to  medical  records   retrieval.  The  Twentieth  Text  REtrieval  Conference  Proceedings  (TREC  2011),  Gaithersburg,  MD.   National  Institute  for  Standards  and  Technology   Edinger,  T,  Cohen,  AM,  et  al.  (2012).  Barriers  to  retrieving  patient  information  from  electronic   health  record  data:  failure  analysis  from  the  TREC  Medical  Records  Track.  AMIA  2012  Annual   Symposium,  Chicago,  IL.  180-­‐188.   Egan,  DE,  Remde,  JR,  et  al.  (1989).  Formative  design-­‐evaluation  of  Superbook.  ACM  Transactions  on   Information  Systems.  7:  30-­‐57.   Fidel,  R  and  Soergel,  D  (1983).  Factors  affecting  online  bibliographic  retrieval:  a  conceptual   framework  for  research.  Journal  of  the  American  Society  for  Information  Science.  34:  163-­‐180.   Fox,  S  (2011).  Health  Topics.  Washington,  DC,  Pew  Internet  &  American  Life  Project.   http://www.pewinternet.org/Reports/2011/HealthTopics.aspx   Harman,  DK  (2005).  The  TREC  Ad  Hoc  Experiments.  TREC:  Experiment  and  Evaluation  in   Information  Retrieval.  E.  Voorhees  and  D.  Harman.  Cambridge,  MA,  MIT  Press:  79-­‐98.   Hersh,  W,  Müller,  H,  et  al.  (2009).  The  ImageCLEFmed  medical  image  retrieval  task  test  collection.   Journal  of  Digital  Imaging.  22:  648-­‐655.   Hersh,  W,  Turpin,  A,  et  al.  (2001).  Challenging  conventional  assumptions  of  automated  information   retrieval  with  real  users:    Boolean  searching  and  batch  retrieval  evaluations.  Information  Processing   and  Management.  37:  383-­‐402.   Hersh,  W  and  Voorhees,  E  (2009).  TREC  genomics  special  issue  overview.  Information  Retrieval.  12:   1-­‐15.   Hersh,  WR  (1994).  Relevance  and  retrieval  evaluation:  perspectives  from  medicine.  Journal  of  the   American  Society  for  Information  Science.  45:  201-­‐206.   Hersh,  WR  (2001).  Interactivity  at  the  Text  Retrieval  Conference  (TREC).  Information  Processing   and  Management.  37:  365-­‐366.   Hersh,  WR  (2009).  Information  Retrieval:  A  Health  and  Biomedical  Perspective  (3rd  Edition).  New   York,  NY,  Springer.   Hersh,  WR,  Crabtree,  MK,  et  al.  (2002).  Factors  associated  with  success  for  searching  MEDLINE  and   applying  evidence  to  answer  clinical  questions.  Journal  of  the  American  Medical  Informatics   Association.  9:  283-­‐293.   Hersh,  WR  and  Greenes,  RA  (1990).  SAPHIRE:  an  information  retrieval  environment  featuring   concept-­‐matching,  automatic  indexing,  and  probabilistic  retrieval.  Computers  and  Biomedical   Research.  23:  405-­‐420.   Hersh,  WR  and  Hickam,  DH  (1995).  An  evaluation  of  interactive  Boolean  and  natural  language   searching  with  an  on-­‐line  medical  textbook.  Journal  of  the  American  Society  for  Information  Science.   46:  478-­‐489.   Hersh,  WR,  Hickam,  DH,  et  al.  (1994).  A  performance  and  failure  analysis  of  SAPHIRE  with  a   MEDLINE  test  collection.  Journal  of  the  American  Medical  Informatics  Association.  1:  51-­‐60.   Hersh,  WR,  Müller,  H,  et  al.  (2006).  Advancing  biomedical  image  retrieval:  development  and   analysis  of  a  test  collection.  Journal  of  the  American  Medical  Informatics  Association.  13:  488-­‐496.   Hersh,  WR,  Pentecost,  J,  et  al.  (1996).  A  task-­‐oriented  approach  to  information  retrieval  evaluation.   Journal  of  the  American  Society  for  Information  Science.  47:  50-­‐56.   Ide,  NC,  Loane,  RF,  et  al.  (2007).  Essie:  a  concept-­‐based  search  engine  for  structured  biomedical  text.   Journal  of  the  American  Medical  Informatics  Association.  14:  253-­‐263.   Jarvelin,  K  and  Kekalainen,  J  (2002).  Cumulated  gain-­‐based  evaluation  of  IR  techniques.  ACM   Transactions  on  Information  Systems.  20:  422-­‐446.   King,  B,  Wang,  L,  et  al.  (2011).  Cengage  Learning  at  TREC  2011  Medical  Track.  The  Twentieth  Text   REtrieval  Conference  Proceedings  (TREC  2011),  Gaithersburg,  MD.  National  Institute  for  Standards   and  Technology   Martinez,  D,  Otegi,  A,  et  al.  (2014).  Improving  search  over  electronic  health  records  using  UMLS-­‐ based  query  expansion  through  random  walks.  Journal  of  Biomedical  Informatics.  51:  100-­‐106.   Müller,  H,  Clough,  P,  et  al.,  Eds.  (2010).  ImageCLEF:  Experimental  Evaluation  in  Visual  Information   Retrieval.  Heidelberg,  Germany,  Springer.   Mynatt,  BT,  Leventhal,  LM,  et  al.  (1992).  Hypertext  or  book:  which  is  better  for  answering   questions?  Proceedings  of  Computer-­‐Human  Interface  92.  19-­‐25.   Roberts,  K,  Simpson,  M,  et  al.  (2016).  State-­‐of-­‐the-­‐art  in  biomedical  literature  retrieval  for  clinical   cases:  a  survey  of  the  TREC  2014  CDS  track.  Information  Retrieval  Journal.  19:  113-­‐148.   Safran,  C,  Bloomrosen,  M,  et  al.  (2007).  Toward  a  national  framework  for  the  secondary  use  of   health  data:  an  American  Medical  Informatics  Association  white  paper.  Journal  of  the  American   Medical  Informatics  Association.  14:  1-­‐9.   Stead,  WW,  Searle,  JR,  et  al.  (2011).  Biomedical  informatics:  changing  what  physicians  need  to  know   and  how  they  learn.  Academic  Medicine.  86:  429-­‐434.   Tenenbaum,  JD,  Avillach,  P,  et  al.  (2016).  An  informatics  research  agenda  to  support  precision   medicine:  seven  key  areas.  Journal  of  the  American  Medical  Informatics  Association:  Epub  ahead  of   print.   Voorhees,  E  and  Hersh,  W  (2012).  Overview  of  the  TREC  2012  Medical  Records  Track.  The  Twenty-­‐ First  Text  REtrieval  Conference  Proceedings  (TREC  2012),  Gaithersburg,  MD.  National  Institute  of   Standards  and  Technology  http://trec.nist.gov/pubs/trec21/papers/MED12OVERVIEW.pdf   Voorhees,  EM  and  Harman,  DK,  Eds.  (2005).  TREC:  Experiment  and  Evaluation  in  Information   Retrieval.  Cambridge,  MA,  MIT  Press.   Yilmaz,  E,  Kanoulas,  E,  et  al.  (2008).  A  simple  and  efficient  sampling  method  for  estimating  AP  and   NDCG.  Proceedings  of  the  31st  Annual  International  ACM  SIGIR  Conference  on  Research  and   Development  in  Information  Retrieval,  Singapore.  603-­‐610.     1 Challenge  Evalua,ons  in  Biomedical   Informa,on  Retrieval   William  Hersh   Professor  and  Chair   Department  of  Medical  Informa,cs  &  Clinical  Epidemiology   Oregon  Health  &  Science  University   Portland,  OR,  USA   Email:  hersh@ohsu.edu     Web:  www.billhersh.info     Blog:  hKp://informa,csprofessor.blogspot.com     TwiKer:  @williamhersh   1   Two  talks  today   •  Primer  on  informa,on  retrieval  and  challenge   evalua,ons   •  TREC  challenge  evalua,ons  –  prac,ce  talk  for   TREC  25th  anniversary  event   2   2 Learning  objec,ves   •  Define  the  evalua,on  measures  used  in   informa,on  retrieval  system  evalua,on  and   how  they  are  used  in  challenge  evalua,ons   •  Describe  the  biomedical  "tracks"  in  the  Text   Retrieval  Conference  (TREC)  challenge   evalua,ons   •  Discuss  the  major  results  and  findings  of  the   TREC  biomedical  tracks   3   Informa,on  retrieval  (IR,  aka  search)   •  Focus  on  indexing  and   retrieval  of  (predominantly)   knowledge-­‐based   informa,on   •  Historically  centered  on  text   in  knowledge-­‐based   documents,  but  increasingly   associated  with  many  types   of  content   •  www.irbook.info     4   (Hersh,  2009)   3 Basics  of  IR   5   Metadata   Queries   Content   Search   engine   Retrieval   Indexing   Use  cases  for  IR   •  Historically,  retrieval  of  knowledge   – Documents,  especially  journal  ar,cles  (originally   abstracts)   – Mul,media  –  images,  sounds,  video,  etc.   – Hypermedia  –  Web-­‐based  content   •  Newer  foci   – Clinical  data  –  e.g.,  cohort  discovery  from   electronic  health  records   – Data  –  e.g.,  finding  data  sets   6   4 Evalua,on  of  IR  systems  has  always   been  important   •  System-­‐oriented  –  how  well  system  performs   –  Historically  focused  on  relevance-­‐based  measures   •  Recall  and  precision  –  propor,ons  of  relevant  documents   retrieved   – When  documents  ranked,  can  combine  both  in  a  single   measure   •  Mean  average  precision  (MAP)   •  Normal  discounted  cumula,ve  gain  (NDCG)     •  Binary  preference  (Bpref)   •  User-­‐oriented  –  how  well  user  performs  with  system   –  e.g.,  performing  task,  user  sa,sfac,on,  etc.   7   System-­‐oriented  IR  evalua,on   •  Historically  assessed  with  test  collec(ons,  which   consist  of   –  Content  –  fixed  yet  realis,c  collec,ons  of  documents,   images,  etc.   –  Topics  –  statements  of  informa,on  need  that  can  be   fashioned  into  queries  entered  into  retrieval  systems   –  Relevance  judgments  –  by  expert  humans  for  which   content  items  should  be  retrieved  for  which  topics   •  Evalua,on  consists  of  runs  using  a  specific  IR   approach  with  output  for  each  topic  measured  and   averaged  across  topics   8   5 Recall  and  precision   •  Recall   – Usually  use  rela(ve  recall  when  not  all  relevant   documents  known,  where  denominator  is  number   of  known  relevant  documents  in  collec,on   •  Precision   collectionindocumentsrelevant documentsrelevantandretrievedR # # = documentsretrieved documentsrelevantandretrievedP # # = 9   Some  measures  can  be  combined  into   a  single  aggregated  measure   •  Mean  average  precision  (MAP)  is  mean  of  average   precision  for  each  topic  (Harman,  2005)   –  Average  precision  is  average  of  precision  at  each  point  of   recall  (relevant  document  retrieved)   –  Despite  name,  emphasizes  recall   •  Bpref  accounts  for  when  relevance  informa,on  is   significantly  incomplete  (Buckley,  2004)   •  Normal  discounted  cumula(ve  gain  (NDCG)  allows  for   graded  relevance  judgments  (Jarvelin,  2002)   •  MAP  and  NCDG  can  be  "inferred"  when  there  are   incomplete  judgments  (Yilmaz,  2008)   10   6 Challenge  evalua,ons   •  A  common  approach  in  computer  science,  not  limited  to  IR   •  Develop  a  common  task,  data  set,  evalua,on  metrics,  etc.,   ideally  aiming  for  real-­‐world  size  and  representa,on  for   data,  tasks,  etc.   •  In  case  of  IR,  this  usually  means   –  Test  collec,on  of  content  items   –  Topics  of  items  to  be  retrieved  –  usually  want  25-­‐30  for   "stability"  (Buckley,  2000)   –  Runs  from  par,cipa,ng  groups  with  retrieval  for  each  topic   –  Relevance  judgments  of  which  content  items  are  relevant  to   which  topics  –  judged  items  derived  from  submiKed  runs   11   Challenge  evalua,ons  (cont.)   •  Typical  flow  of  events  in  an  IR  challenge  evalua,on   •  In  IR,  challenge  evalua,on  results  usually  show  wide   varia,on  between  topics  and  between  systems   –  Should  be  viewed  as  rela,ve,  not  absolute  performance   –  Averages  can  obscure  varia,ons   12   Release  of   document   collec,on  to   par,cipa,ng   groups   Experimental   runs  and   submission   of  results   Relevance   judgments   Analysis  of   results   7 Some  well-­‐known  challenge  evalua,ons   in  IR   •  Text  Retrieval  Conference  (TREC,  hKp://trec.nist.gov;  Voorhees,   2005)  –  sponsored  by  Na,onal  Ins,tute  for  Standards  and   Technology  (NIST),  started  in  1992   –  Many  "tracks"  of  interest,  such  as  rou,ng/filtering,  Web  searching,   ques,on-­‐answering,  etc.   –  Mostly  non-­‐biomedical,  but  some  tracks  focused  on  genomics,  EHRs,  etc.   •  Conferences  and  Labs  of  the  Evalua,on  Forum  (CLEF,   www.clef-­‐ini,a,ve.eu)     –  Started  as  track  in  TREC  in  1996,  spun  off  in  2000  to  Cross-­‐Language   Evalua,on  Forum   –  Focus  on  retrieval  across  languages,  European-­‐based   –  Addi,onal  focus  on  image  retrieval,  which  includes  medical  image   retrieval  tasks  –  www.imageclef.org  (Hersh,  2009;  Müller,  2010)   •  TREC  has  inspired  other  challenge  evalua,ons,  e.g.,   –  i2b2  NLP  Shared  Task,  hKps://www.i2b2.org/NLP/     –  bioCADDIE  Dataset  Retrieval  Challenge  –   hKps://biocaddie.org/biocaddie-­‐2016-­‐dataset-­‐retrieval-­‐challenge-­‐ registra,on   13   IR  and  text  mining  in  context  of   biomedical  knowledge  management   All  literature     Possibly  relevant   literature  (abstracts)     Definitely  relevant   literature  (full  text)     Ac,onable   knowledge   Informa,on   retrieval   Informa,on   extrac,on,   text  mining   14   (Hersh,  2009) 8 The  TREC  Bio/Medical  Tracks   William  Hersh   Professor  and  Chair   Department  of  Medical  Informa,cs  &  Clinical  Epidemiology   Oregon  Health  &  Science  University   Portland,  OR,  USA   Email:  hersh@ohsu.edu     Web:  www.billhersh.info     Blog:  hKp://informa,csprofessor.blogspot.com     TwiKer:  @williamhersh   15 The  TREC  Bio/Medical  Tracks   •  Why  is  informa,on  retrieval  (IR)  important  in   biomedicine   •  TREC  Genomics  Track   •  ImageCLEFmed   •  TREC  Medical  Records  Track   •  TREC  Clinical  Decision  Support  Track   •  TREC  Precision  Medicine  Track   •  Beyond  system-­‐oriented  evalua,on   16   9 Why  is  IR  important  in  health  and   biomedical  domain?   •  Clinicians  cannot  keep  up  –  average  of   75  clinical  trials  and  11  systema,c   reviews  published  each  day  (Bas,an,   2010)   •  Data  points  per  clinical  decision   increasing  (Stead,  2011)   •  Search  for  health  informa,on  by   clinicians,  researchers,  and  pa,ents/ consumers  is  ubiquitous  (Fox,  2011;   Google/ManhaKan  Research,  2012)   •  Concerns  about  reproducibility  of   science  (Baker,  2012)   •  "Precision  medicine"  will  increase   quan,ty  and  complexity  of  data   (Tenenbaum,  2016)   17   TREC  Genomics  Track  (Hersh,  2009)   •  Mo,vated  by  exploding  research  in  genomics  and   inability  to  biologists  to  know  all  that  might  impact   work   •  First  TREC  track  devoted  to  "domain-­‐specific"  retrieval,   with  focus  on  IR  systems  for  genomics  researchers   –  Supported  by  NSF  Informa,on  Technology  Research  (ITR)   grant   •  History   –  2004-­‐2005  –  focus  on  ad  hoc  retrieval  and  document   categoriza,on   –  2006-­‐2007  –  focus  on  passage  retrieval  and  ques,on-­‐ answering  as  means  to  improve  document  retrieval   18   10 Lessons  learned  (Hersh,  2009)   •  Ad  hoc  retrieval   – Modest  benefit  for  techniques  known  to  work  well  in   general  IR,  e.g.,  stop  word  removal,  stemming,  weigh,ng   –  Query  term  expansion,  especially  domain-­‐specific  and/or   done  by  humans,  helped  most   •  QA   – Most  consistent  benefit  from  query  expansion  and   paragraph-­‐length  passage  retrieval   •  For  all  experiments  (and  papers  describing  them),   major  problems  were   –  Lack  of  detailed  descrip,on  of  systems   –  Use  of  low-­‐performing  baselines   19   Image  retrieval  –  ImageCLEF  medical  image   retrieval  task   •  Biomedical  professionals  increasingly  use  images  for   research,  clinical  care,  and  educa,on,  yet  we  know   very  liKle  about  how  to  best  retrieve  them   •  Developed  test  collec,on  and  explora,on  of   informa,on  needs  mo,va,ng  use  of  image  retrieval   systems  (Hersh,  2006;  Hersh,  2009;  Müller,  2010)   •  Started  with  ad  hoc  retrieval  and  added  tasks   – Modality  detec,on   –  Case  finding   •  Overall  conclusions:  text  yielded  most  consistent   results  with  image  features  providing  variable  value   •  Con,nues  on  with  highly  defined  tasks   20   11 TREC  Medical  Records  Track   (Voorhees,  2012)   •  Adap,ng  IR  techniques  to  electronic  health   records  (EHRs)   •  Use  case  somewhat  different  –  want  to  retrieve   records  and  data  within  them  to  iden,fy  pa,ents   who  might  be  candidates  for  clinical  studies   •  Mo,vated  by  larger  desire  for  "re-­‐use"  of  clinical   data  (Safran,  2007)   •  Opportuni,es  facilitated  by  incen,ves  for   "meaningful  use"  of  EHRs  in  the  HITECH  Act   (Blumenthal,  2011;  Blumenthal,  2011)   21   Challenges  for  informa,cs  research   with  medical  records   •  Has  always  been  easier  with  knowledge-­‐based   content  than  pa,ent-­‐specific  data  due  to  a   variety  of  reasons   – Privacy  issues   – Task  issues   •  Facilitated  with  development  of  large-­‐scale,   de-­‐iden,fied  data  set  from  University  of   PiKsburgh  Medical  Center  (UPMC)   •  Launched  in  2011,  repeated  in  2012   22   12 Test  collec,on   (Courtesy,  Ellen  Voorhees,  NIST)   23   Results  for  2012   24   13 Which  approaches  did  (and  did  not)   work?   •  Best  results  in  2011  and  2012  obtained  from  NLM  group   (Demner-­‐Fushman,  2011;  Demner-­‐Fushman,  2012)   –  Top  results  from  manually  constructed  queries  using  Essie   domain-­‐specific  search  engine  (Ide,  2007)   •  Many  approaches  known  to  work  in  general  IR  fared  less   well,  e.g.,  term  expansion,  document  focusing,  etc.   –  Other  domain-­‐specific  approaches  also  did  not  show  benefit,   e.g.,  crea,on  of  PICO  frames,  nega,on   •  Some  success  with   –  Results  filtered  by  age,  race,  gender,  admission  status;  terms   expanded  by  UMLS  Metathesaurus  (King,  2011)   –  Expansion  by  concepts  and  rela,onships  in  UMLS   Metathesaurus  (Mar,nez,  2014)   –  Pseudorelevance  feedback  using  ICD-­‐9  codes  (Amini,  2016)   25   Failure  analysis  for  2011  topics   (Edinger,  2012)   26   14 TREC  Clinical  Decision  Support  Track   (Roberts,  2016)   •  www.trec-­‐cds.org   •  Ad  hoc  search  of  biomedical  literature  (PubMed   Central  Open  Access  Subset  –  1.25M  ar,cles)   •  Topics  are  pa,ent  descrip,ons  in  three   informa,on  need  categories   – Diagnosis   –  Test   –  Treatment   •  Currently  in  third  year  of  opera,on   •  Transi,oning  to  Precision  Medicine  Track   27   TREC  has  inspired  and  guided  other   challenge  evalua,ons  in  biomedicine   •  i2b2   –  hKps://www.i2b2.org/NLP   –  Various  NLP-­‐related  tasks,  including  extrac,on  and   de-­‐iden,fica,on   •  CLEF  eHealth   –  hKps://sites.google.com/site/clefehealth/home   –  Informa,on  extrac,on  and  pa,ent-­‐centered  IR   •  bioCADDIE   –  hKps://biocaddie.org/biocaddie-­‐2016-­‐dataset-­‐ retrieval-­‐challenge-­‐registra,on   – Data  set  retrieval   28   15 System-­‐oriented  retrieval  is  not   enough   •  My  ini,al  focused  on   concept-­‐based  searching   (Hersh,  1990)   –  Did  not  impart  value  over   word  indexing  and  searching   (Hersh,  JAMIA,  1994)   •  Experience  of  several   evalua,ons  led  to  concern   with  evalua,on  focus  on   recall/precision  (Hersh,  JASIS,   1994)   –  How  much  difference  is   meaningful?   –  How  valid  is  batch   evalua,on  for  understand   how  well  user  will  search?   29   Led  to  "task-­‐oriented"  evalua,on   approaches   •  Mo,vated  by  Egan  (1989)  and  MynaK  (1992)   •  Major  task  in  medicine:  answering  ques,ons   •  How  can  we  evaluate  systems  in  interac,ve  use   for  answering  ques,ons?   •  Undertook  parallel  approaches  in   – Medicine  –  using   •  Electronic  textbook  –  Scien,fic  American  Medicine  (Hersh,   1995)   •  Bibliographic  database  –  MEDLINE  (Hersh,  1996)   – General  news  –  TREC  Interac,ve  Track  (Hersh,  2001)   30   16 Factors  associated  with  successful   searching  (Hersh,  2002)   •  Medical  and  nurse  prac,,oner  (NP)  students  success  of  using  a  retrieval   system  to  answer  clinical  ques,ons   –  Had  to  provide  not  only  answer  but  level  of  evidence  suppor,ng  it   •  Yes  with  good  evidence   •  Indeterminate  evidence   •  No  with  good  evidence   •  Look  at  factors  associated  with  success   –  Based  on  model  of  factors  associated  with  successful  use  of  retrieval  systems   (Fidel,  1983)  adapted  to  this  serng   •  Including  recall  and  precision   –  Dependent  variable  was  correctness  of  answer   •  Major  results   –  Before  searching,  correct  rate  due  to  chance  (~32%)   –  Medical  students  (~50%)  but  not  NP  students  (~33%)  improved  with  searching   –  Spa,al  visualiza,on  associated  with  higher  rate  of  success   –  Recall  and  precision  had  no  associa,on  with  success   31   Conclusions   •  Importance  of  IR  in  biomedicine  will  not  diminish  as   volume,  variety,  and  velocity  of  science  con,nue  to  expand   •  Varying  benefits  for  different  use  cases,  but  in  general,   medical  vocabulary  resources  offer  most  value  via  query   expansion   •  While  ad  hoc  IR  for  general  informa,on  needs  rela,vely   solved,  s,ll  challenges  with   –  Novel  types  of  data,  e.g.,  EHRs  and  other  structured  data   –  High-­‐recall  tasks,  e.g.,  systema,c  reviews   •  Research  confounded  by  larger  issues,  e.g.,   –  Private  data   –  Proprietary  data   32   Information Retrieval in Biomedical Research: From Articles to Datasets
2017
Author & Article Info
Abstract
Information retrieval techniques have been applied to biomedical research for a variety of purposes, such as textual document retrieval and molecular data retrieval. As biomedical research evolves over time, information retrieval is also constantly facing new challenges, including the growing number of available data, the emerging new data types, the demand for interoperability between data resources, and the change of users' search behaviors. To help solve the challenges, I studied three solutions in my dissertation: (a) using information collected from online resources to enrich the representation models for biomedical datasets; (b) exploring rule-based and deep learning-based methods to help users formulate effective queries for both dataset retrieval and publication retrieval; and (c) developing a "retrieval plus re-ranking" strategy to identify relevant datasets, and rank them using customized ranking models.
In a biomedical dataset retrieval study, we developed a pipeline to automatically analyze users' free-text requests, and rank relevant datasets using a "retrieval plus re-ranking" strategy. To improve the representation model of biomedical datasets, we explored online resources and collected information to enrich the metadata of datasets. The rule-based query formulation module extracted keywords from users' free-text requests, expanded the keywords using NCBI resources, and finally formulated Boolean queries using pre-designed templates. The novel "retrieval plus re-ranking" strategy captured relevant datasets in the retrieval step, and ranked datasets using the customized relevance scoring functions that model unique properties of the metadata of biomedical datasets. The solutions proved to be successful for biomedical dataset retrieval, and the pipeline achieved the highest inferred Normalized Discounted Cumulative Gain (infNDCG) score in the 2016 bioCADDIE Biomedical Dataset Retrieval Challenge.
In a biomedical publication retrieval study, we developed the eXtended PubMed Related Citation (XPRC) algorithm to find similar articles in PubMed. Currently, similar articles in PubMed are determined by the PubMed Related Citation (PRC) algorithm. However, when the distributions of term counts are similar between articles, the PRC algorithm may conclude that the articles are similar, even though they may be about different topics. On the other hand, when two articles discuss the same topic but use different terms, the PRC algorithm may miss the similarity. For the above problem, we implemented a term expansion method to help capture the similarity. Unlike popular ontology-based expansion methods, we used a deep learning method to learn distributed representations of terms over one million articles from PubMed Central, and identified similar terms using the Euclidean distance between distributed representation vectors. We showed that, under certain conditions, using XPRC can improve precision, and helps find similar articles from PubMed.
In conclusion, information retrieval techniques in biomedical research have helped researchers find desired publications, datasets, and other information. Further research on developing robust representation models, intelligent query formulation systems, and effective ranking models will lead to smarter and more friendly information retrieval systems that will further promote the transformation from data to knowledge in biomedicine.
Main Content
Search
Services
FactsMission offers consulting and development services to create, process, store, publish and consume linked data. FactsMission helps you provide data in such a way that it can be easily used as the foundation of different applications as well as linked as an authoritative source. FactsMission helps you grow communities around your data to maximize its impact and increase your organization's influence.
For example, while the red list of endangered species has been available online for a long time, linked data now connects the species with related scientific treatments giving access to much more information and allowing more refined queries. One can now search for places where the plant has been spotted or search using older or newer scientific names than the ones used on the list.
Our Mission
Our mission is to aid people make better decisions by promoting technologies that facilitate verifying claims through linked data technologies.
Technology
Linked Data refers to a set of standards promoted by the W3C to create a web of data. Linked Data extends the Web by allowing data to be directly exposed and interlinked with other related data. Meaning, one no longer needs to search through a website or use proprietary APIs to access data, but even disparate sources can be queried using standardized means.
Now is the time to act
On one hand, it was never more necessary, on the other hand we never had better technologies to achieve it.
The times when truth-relativism was a harmless academic gimmick or even an emancipating story of liberation are long gone. The idea that there is no truth is not only used by authoritarian regimes to dismiss the quest for objectivity as propaganda but is more and more corroding the foundation even of established democracies.
The good news is, that we now have the technologies that turn the web into a huge decentralized database with cryptographically ascertained authorities and which allow to express statements about facts in a concise and verifiable manner.
Individuals
Are equipped with decision making tools to decide what to accept as facts.
Public agencies
Publish facts so that they can best be used. This increases the impact of the data and reduces costs to satisfy freedom of information and open data requirements.
Open Source
FactsMission is involved in various open source projects around Linked Data technology. We are more than happy to assist developers and development teams to choose the optimal tools and to make the best out of it.
Companies & organisations
Can make better decisions based on reliable knowledge.
FactsMission supports the W3C in making sure all people benefit from the Web. As a member of the W3C, FactsMission is involved in defining tomorrow's standards to make the Web better.
Twitter
© 2017 FactsMission All rights reserved.
Factsmission AG, Karl-Neuhaus-strasse 40, 2502 Biel/Bienne, Switzerland
+41 58 255 19 99 info@factsmission.com
Download article PDF
1 Introduction
Medical information search refers to methodologies and technologies that seek to improve access to medical information archives via a process of information retrieval (IR). Such information is now potentially accessible from many sources including the general web, social media, journal articles, and hospital records. Health-related content is one of the most searched-for topics on the internet, and as such this is an important domain for IR research. Medical information is of interest to a wide variety of users, including patients and their families, researchers, general practitioners and clinicians, and practitioners with specific expertise such as radiologists. There are several dedicated services that seek to make this information more easily accessible, such as the 'Health on the Net' system for the general public and medical practitioners ( http://www.hon.ch/ ). However, despite the popularity of the medical domain for users of search engines, and current interest in this topic within the IR research community, development of search and access technologies remains particularly challenging.
A central issue in medical IR is the diversity of the users of these services. In particular, they will have varying categories of information needs, varying levels of medical knowledge, and varying language skills. These challenges can be summarized as follows:
Varying information needs: While a patient with a recently diagnosed condition will generally benefit most from simple or introductory information on the disease and its treatment, a patient living with or managing a condition over a longer term will generally be looking for more advanced information, or perhaps support groups and forums. Similarly, a general practitioner might require basic information quickly while advising a patient, but more detailed information if deciding a course of treatment, and a specialist clinician might look for an exhaustive list of similar cases or research papers relating to the condition of a patient that they are currently seeking to advise. Understanding of various types of users and their information needs is one of the cornerstones of medical IR; development of effective, potentially personalized systems that address these needs is one of the greatest challenges.
Varying medical knowledge: The different categories of users of medical IR systems have different levels of medical knowledge, and indeed the medical knowledge of different individuals within a category can also vary greatly. This affects the way in which individuals pose search queries to systems and also the level of complexity of information which should be returned to them or the type of support in understanding of retrieved material which should be provided.
Varying language skills: Given that much of medical content is written in the English language, research to date in medical information search has predominantly focused on monolingual English retrieval. However, given the large number of non-English speakers on the Internet and the lack of content in their native language, effective support for them to search English sources is highly desirable.
The format, reliability, and quality of biomedical and medical information varies greatly. A single health record can contain clinical notes, technical pathology data, images, and patient-contributed histories, and may be linked by a physician to research papers. The importance of health and medical topics and their impact on people's everyday lives makes the need for retrieval of accurate and reliable information especially important. Determining the likely reliability of available information is challenging. Finally, as with IR in general, the evaluation of medical search tools is vital and challenging. For example, there are no established or standardized baselines or evaluation metrics, and limited availability of test collections. Further discussion and progression on this topic would be beneficial to the community.
2 The special issue
There are several regular workshops on medical natural language processing and text mining. For example, the BioNLP workshop is associated with ACL and has been running since 2002. It targets foundational research in language processing for the biological and medical domains. A shared task track is collocated with the workshop and organises text mining and fine-grained information tasks. The Louhi workshop offers another forum for researchers to explore health text mining and information analysis. While these two workshops are concerned with technologies and methodologies for medical information extraction and natural language processing, the LREC workshop on building and evaluating resources for biomedical text mining is concerned with work on resources and issues relating to their usability.
The IR community has tackled issues related to the medical domain via shared tasks over several years: the TREC genomics (2003–2007), medical records track (2011–2012), and clinical decision support (2014–2015) tracks, ImageCLEF (2003–2015), and CLEF eHealth (2013–2015). While these tracks mainly focus on medical IR, they are each concerned with a single specific task rather than on the field more broadly.
In 2013, the SIGIR workshop on health search and discovery (helping users and advancing medicine) sought to investigate ways to make medical and health information more accessible to laypeople, and ways to discover new medical facts and phenomena from information sought online (White et al. 2013 ). This exploratory workshop showed that the IR community has many links with the medical domains and highlighted many directions to explore. A key direction for further exploration was IR techniques for medical search. As a result, in 2014 we organised a workshop, entitled Medical Information Retrieval (Goeuriot et al. 2014 ), focusing on this direction, with emphasis on its multimodal and multilingual aspects. This workshop gave an overview of the current research directions taken in the medical IR domain. The lively discussions at the workshop showed us how active and rich the domain was, and led us to set up this special issue of the Information Retrieval journal.
3 Overview of papers
We received eighteen submissions for the special issue, of which eight papers were accepted. Authors of accepted papers come from Australia, Austria, France, Switzerland, and the USA. One was written by an international consortium. We now summarise these papers.
Koopman et al. ( 2016 ) present a graph inference retrieval model aiming at improving biomedical IR. Their technique aims at solving issues often encountered in the medical domain such as vocabulary mismatch, or granularity mismatch (Ely et al. 2000 ). To do so, their system integrates structured knowledge resources, statistical IR methods, and inference, in a system where corpora are represented by graphs and retrieval is driven by inference mechanism over the graphs. The evaluation of their system shows that inference can be effective, and identifies relevant documents that would not be retrieved by classical approaches. It also shows that inference should not be applied in all cases, but can help improve retrieval in the case of hard or complex queries.
Kovacs et al. ( 2016 ) examine the issue of large-scale data in hospitals, using the case of radiation dosage monitoring (requiring large quantity of data, such as scans). To deal with this issue, they propose a retrieval system to search for radiation dosage data and provide visualization and mining techniques for the results. They demonstrate in the paper the utility of such visual methods for medical professionals in practice.
Lossio-Ventura et al. ( 2016 ) propose a novel approach for biomedical terminology extraction. Terminologies and other knowledge sources are crucial for systems that mine biomedical data. Their approach extracts biomedical terms and ranks them using measures based on linguistic, statistical, and structural aspects. The evaluation of their approach shows that it outperforms state-of-the-art techniques. Furthermore, it proves to be efficient in several languages, namely English, French, and Spanish.
Markonis et al. ( 2016 ) investigate the effectiveness of relevance feedback techniques on biomedical image retrieval. Image retrieval can assist physicians as a support in their daily practice, for instance while making a diagnosis. While relevance feedback has been widely used in the text retrieval field, they propose a novel method using both text and visual information. Their results show that this combined relevance feedback approach improves retrieval results.
Roberts et al. ( 2016 ) present an overview of the TREC Clinical Decision Support (CDS) track. The goal of this evaluation track is to provide access to relevant biomedical literature in clinical settings, in order to help clinicians in their practice of evidence-based medicine. The paper describes the task, the participants' submissions, and provides an analysis of the results. This analysis shows that standard approaches such as word-based indexing and pseudo-relevance feedback gave the best results. However, as this track had only run once, the authors note that the lack of training data might have influenced results and caused classical approaches to be the most efficient. Tuning of systems using annotation, negation, and attribute extraction, or document type preference, should prove useful in the second iteration of the track, once participants get access to enough training data.
Soldaini et al. ( 2016 ) present a query clarification approach aiming at improving medical IR by lay people. This approach tackles the issue of vocabulary mismatch between lay queries and expert vocabulary, preventing patients from finding relevant or authoritative information. Query clarification is a form of expansion, where the most appropriate expression (that is, the most similar expert expression) is added to the query. Using three different synonym mappings and conducting two task-based retrieval studies, they show that users are more satisfied with the results using this approach.
Zheng and Yu ( 2016 ) aim at assisting patients in understanding their own electronic health records (EHR). To do so, they develop a system for retrieving consumer-oriented health educational material linked to complex sections of their EHR. They investigate techniques to extract EHR notes for specific queries, namely topic models (with Latent Dirichlet Allocation) and key concept identification (with Inverse Document Frequency or Conditional Random Fields). They show that identification of key concepts and pseudo relevance feedback gives the best results.
Palotti et al. ( 2016 ) investigates user behavior while searching for medical information online. Using multiple query logs, they categorize users by expertise and analyze their search behavior. They show that medical professionals are more persistent and interact more with the system. They also discovered that users seem to be more interested in diseases than symptoms. As an outcome of this analysis, they developed a classifier inferring user expertise, that could be used to adapt search results to the user.
4 Summary
Amongst the general public, health topics are one of the most common kinds of query, while medical practitioners also make extensive use of search in their work. The ability to find relevant, informative results can be critical in determining whether people seek treatment, or whether a treatment is found—factors that are helping to drive interest in medical search in the IR, NLP, and data mining communities. This interest has led to the creation of several workshops, including our own MedIR at SIGIR in 2014, and also to this special issue.
The papers in the special issue reflect the breadth of factors that might influence successful medical IR in practice. We believe that they are a valuable illustration of the scope of the field, as well as being significant contributions in their own right. We hope that you find this work as inspiring as we do.
Notes
Acknowledgments
We thank the co-editors of the journal Charles Clarke, Tetsuya Sakai and Arjen de Vries for their guidance during the editing of this special issue. We would also like to thank all the programme committee members: Sophia Ananiadou, Alan Aronson, Lucia Ballerini, Patrice Bellot, Catherine Berrut, Ben Carterette, Jean-Pierre Chevallet, Paul Clough, Dina Demner-Fushman, Thomas Deserno, Alba Garcia Seco de Herrera, Natalia Grabar, Allan Hanbury, Kyo Kageura, Charles Kahn, Theodore Kalamboukis, Jayashree Kalpathy-Cramer, Jung-Jae Kim, Gang Luo, Bjoern Menze, Andre Mourao, Philippe Mulhem, Aurélie Névéol, Iadh Ounis, George Paliouras, Gwenole Quellec, Frank Rudzicz, Laurianne Sitbon, Lynda Tamine Lechani, Chua Tat-Seng, Qi Tian, Theodora Tsikrika, Hugo Van Hamme, Sumithra Velupillai, Karin Verspoor, Ellen Voorhees, Elad Yom-Tov, Pierre Zweigenbaum, Guido Zuccon.
References
Ely, J., Osheroff, J., Gorman, P., Ebell, M., Chambliss, M., Pifer, E., et al. (2000). A taxonomy of generic clinical questions: Classification study. British Medical Journal, 321(7258), 429–432. CrossRef Google Scholar
Goeuriot, L., Jones, G. J. F., Kelly, L., Müller, H., & Zobel, J. (Eds.). (2014). Proceedings of the SIGIR workshop on medical. Google Scholar
Koopman, B., Zuccon, G., Bruza, P., Sitbon, L., & Lawley, M. (2016). Information retrieval as semantic inference: A graph inference model applied to medical search. Information Retrieval Journal. doi:  10.1007/s10791-015-9268-9 .
Kovacs, W., Weisenthal, S., Folio, L., Li, Q., Summers, R. M., & Yao, J. (2016). Retrieval, visualization, and mining of large radiation dosage data. Information Retrieval Journal. doi:  10.1007/s10791-015-9265-z .
Lossio-Ventura, J. A., Jonquet, C., Roche, M., & Teisseire, M. (2016). Biomedical term extraction: Overview and a new methodology. Information Retrieval Journal. doi:  10.1007/s10791-015-9262-2 .
Markonis, D., Schaer, R., & Müller, H. (2016). Evaluating multimodal relevance feedback techniques for medical image retrieval. Information Retrieval Journal. doi:  10.1007/s10791-015-9260-4 .
Palotti, J., Hanbury, A., Müller, H., & Kahn, C. E. (2016). How users search and what they search for in the medical domain. Information Retrieval Journal. doi:  10.1007/s10791-015-9269-8 .
Roberts, K., Simpson, M., Demner-Fushman, D., Voorhees, E., & Hersh, W. (2016). State-of-the-art in biomedical literature retrieval for clinical cases: A survey of the TREC 2014 CDS track. Information Retrieval Journal. doi:  10.1007/s10791-015-9259-x .
Soldaini, L., Yates, A., Yom-Tov, E., Frieder, O., & Goharian, N. (2016). Enhancing web search in the medical domain via query clarification. Information Retrieval Journal. doi:  10.1007/s10791-015-9258-y .
White, R., Yom-Tov, E., Horvitz, E., Agichtein, E., & Hersh, B. (Eds.). (2013). Proceedings of the SIGIR workshop on health search and discovery. Google Scholar
Zheng, J., & Yu, H. (2016). Methods for linking EHR notes to education materials. Information Retrieval Journal. doi:  10.1007/s10791-015-9263-1 .
Copyright information
© Springer Science+Business Media New York 2016
Authors and Affiliations
RESEARCH ARTICLE Open Access Performance of an electronic health record-based phenotype algorithm to identify community associated methicillin-resistant Staphylococcus aureus cases and controls for genetic association studies Kathryn L. Jackson1*, Michael Mbagwu1, Jennifer A. Pacheco1, Abigail S. Baldridge1, Daniel J. Viox1,2, James G. Linneman3, Sanjay K. Shukla4, Peggy L. Peissig3, Kenneth M. Borthwick5, David A. Carrell6, Suzette J. Bielinski7, Jacqueline C. Kirby8, Joshua C. Denny8, Frank D. Mentch9, Lyam M. Vazquez9, Laura J. Rasmussen-Torvik1 and Abel N. Kho1 Abstract Background: Community associated methicillin-resistant Staphylococcus aureus (CA-MRSA) is one of the most common causes of skin and soft tissue infections in the United States, and a variety of genetic host factors are suspected to be risk factors for recurrent infection. Based on the CDC definition, we have developed and validated an electronic health record (EHR) based CA-MRSA phenotype algorithm utilizing both structured and unstructured data. Methods: The algorithm was validated at three eMERGE consortium sites, and positive predictive value, negative predictive value and sensitivity, were calculated. The algorithm was then run and data collected across seven total sites. The resulting data was used in GWAS analysis. Results: Across seven sites, the CA-MRSA phenotype algorithm identified a total of 349 cases and 7761 controls among the genotyped European and African American biobank populations. PPV ranged from 68 to 100% for cases and 96 to 100% for controls; sensitivity ranged from 94 to 100% for cases and 75 to 100% for controls. Frequency of cases in the populations varied widely by site. There were no plausible GWAS-significant (p < 5 E −8) findings. Conclusions: Differences in EHR data representation and screening patterns across sites may have affected identification of cases and controls and accounted for varying frequencies across sites. Future work identifying these patterns is necessary. Keywords: ca_MRSA, Phenotyping, Electronic Health Record, ca-MRSA Phenotype, GWAS Background Methicillin-resistant Staphylococcus aureus is one of the most common causes of skin and soft tissue infections (SSTIs) in the United States [1]. Community-associated methicillin-resistant Staphylococcus aureus (CA-MRSA) has replaced traditional healthcare associated strains in many communities where it previously did not exist [2, 3]. Recent reports indicate that CA-MRSA strains contain more antibiotic resistance genes than previously encountered, and pose an enormous concern for patients, hospitals and public health entities [3]. Additionally, CA-MRSA strains express increased virulence factors leading to increased tissue destruction and more severe infections [4, 5]. A variety of genetic factors are suspected as a risk factor for recurrent CA-MRSA infection [4, 6, 7], with an increased prevalence in younger, healthier populations with no other identifiable risk factors [8]. * Correspondence: kathryn.jackson1@northwestern.edu 1Feinberg School of Medicine, Northwestern University, Chicago, IL, USA Full list of author information is available at the end of the article © The Author(s). 2016 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated. Jackson et al. BMC Infectious Diseases (2016) 16:684 DOI 10.1186/s12879-016-2020-2 http://crossmark.crossref.org/dialog/?doi=10.1186/s12879-016-2020-2&domain=pdf mailto:kathryn.jackson1@northwestern.edu http://creativecommons.org/licenses/by/4.0/ http://creativecommons.org/publicdomain/zero/1.0/ The Centers for Disease Control and Prevention (CDC) definition of CA-MRSA distinctly differs from Healthcare-Associated MRSA (HA-MRSA). A soft-tissue infection is considered CA-MRSA if (1) a subsequently positive wound culture was taken within 48 h of hospital admis-sion [9, 10], and (2) the patient did not have surgery, live in a long-term care facility, or undergo hemodialysis/peri-toneal dialysis during the past year, and (3) the patient did not undergo catheterization or insertion of indwelling per-cutaneous devices during present hospital admission [3]. This definition was intended to clearly delineate commu-nity and hospital acquired (HA) infections, with the pur-pose of adequately differentiating the two phenotypes and potentially guiding empiric therapy [11]. However, it is not clear whether the criteria for CA-MRSA can readily trans-late into a computable phenotype using electronic health record (EHR) data (an increasingly common source for clinical data) which can then be used for genetic analysis. Identifying disease phenotypes using EHR data has been a growing area of interest with the rapid increase in EHR adoption nationally [12, 13]. The Electronic Medical Records and Genomics (eMERGE) Network is a national consortium consisting of 9 funded sites (in phase II) formed to investigate the use of EHR systems for genetic research, in which phenotype identification algorithms can be proposed, disseminated and validated [14, 15]. The eMERGE Network has developed numer-ous phenotype algorithms using (EHR) data for use in genetic analyses [15–19]. In this paper, we describe the development and validation of a CA-MRSA case and control phenotype algorithm, implementation results and subsequent GWAS findings. Methods Algorithm development The CA-MRSA phenotype algorithm was based on the CDC definition and prior work in this space [3] and developed at Northwestern University (NU). Figures 1 and 2 show the case and control phenotype definitions, respectively. Case inclusion criteria included having a bacterial culture drawn from a skin and soft tissue (SSTI) infection site, in the outpatient or emergency de-partment setting or within 72 h of admission to an in-patient setting, which confirmed a MRSA infection. It should be noted that the CDC definition considers MRSA to be HA if the infection occurs >48 h after admission, whereas we use ≥72 h in our definition to minimize the possibility of incorrectly categorizing CA as HA. Potential cases were excluded if the patient had a hospitalization in the prior year before the MRSA infec-tion, a prior stay in a long term care facility or nursing home in the prior year, or had undergone catheterization Fig. 1 Algorithm for the identification of patients with CA-MRSA Jackson et al. BMC Infectious Diseases (2016) 16:684 Page 2 of 7 or insertion of indwelling percutaneous devices during the admission in which MRSA was detected. Given a recent clinical trend to empirically treat pre-sumed SSTIs as CA-MRSA without drawing cultures [20], we also included "silver standard" criteria which re-moved the requirement for microbiology confirmed MRSA, and instead relied on clinical documentation of International Classification of Diseases, Ninth Revision (ICD-9) codes related to CA-MRSA infections, as well as the presence of a SSTI within a week of initial diagno-sis, to determine case status (Fig. 1). All ICD-9 codes were based on the CDC definition for SSTIs associated with MRSA [21] (Additional file 1: Table S1). Due to the complexity of the phenotype, multiple data inputs from clinical notes, records of past hospitaliza-tions and laboratory culture results were needed to ac-curately define the phenotype. We leveraged prior work tracking MRSA within a health information exchange to create a list of the most common terms used in EHRs associated with SSTIs [22]. ICD-9 codes used to identify SSTIs were based on the CDC definition for SSTIs asso-ciated with MRSA [21]. Patients who had visited a primary care provider at least two times within a continuous 3-year period (i.e., received "routine primary care") and had never had a positive MRSA screen, no prior history of an SSTI or any MRSA infection were considered controls (Fig. 2). Algorithm validation The CA-MRSA phenotype algorithm performance was validated via manual chart review for a set number of randomly selected cases and controls, comparing the outcome of the phenotyping algorithm to the "gold standard" of individually abstracted information in the patient charts to verify the correctness of case or control status [18]. The algorithm was initially run, tested and validated on patients' biobank data at NU. The final al-gorithm was then distributed to two other validating sites (Geisinger Health System and Marshfield Clinic) for implementation and validation. Implementation and selection of cases and controls for validation of the algorithm were completed using all patients' data in each site's biobank. Charts for 50 cases and 50 controls were reviewed at NU; 25 cases and 25 controls were reviewed at Geisinger; 25 cases and 25 controls were reviewed at Marshfield. Positive predictive value (PPV), negative predictive value (NPV) and sensitivity were calculated at each site individually. Additionally, at NU, all cases included via the silver standard criteria alone were validated through chart review. All statistical analyses were performed using SAS 9.4 (SAS Institute Inc., Cary, NC, USA). Phenotypic data After outside site validation at Geisinger and Marshfield, the algorithm was distributed to the other eMERGE sites. In total, seven eMERGE institutions participated in this study (Northwestern University, Geisinger Health System, Marshfield Clinic, Children's Hospital of Philadelphia (CHOP), Group Health Cooperative, Mayo Clinic and Vanderbilt University). Each site has robust genomic biobank projects linked with EHR data, stored in their site-specific data warehouses, for use in determining genotype-phenotype associations. For purposes of this study, patients with relevant EHR records were included only if all genetic data necessary for GWAS was also available. In addition to case and control status, age (at time of infection for cases and at time of last visit for controls), gender and race/ethnicity were also collected from the EHR. Genetic data Details of the assembly of an imputed GWAS dataset for the eMERGE II Network have been published previously [23, 24]. In brief, SNPs were genotyped on a number of different platforms at different sites. Data were quality controlled at each site [25], then common SNPs were merged. SNPs were imputed to the 1000 Genomes Project phase 3 reference panel using IMPUTE [26]. Principal components fit to the pre-imputed SNP dataset were computed using EIGENSTRAT [27]. Genetic analysis GWAS analysis was run in SNPTest (version 2.4.1) [28] and included only those patients with phenotypes, eigenvectors and genetic information available. Models were stratified by race (African American (AA) and European American (EA) only, due to small sample sizes in all other groups) determined from principal components analyses. After com-pleting analysis, we filtered on imputation information >0.8, minor allele frequency (MAF) ≥0.05, and Hardy-Weinberg Equilibrium (HWE) >0.000001 to remove spurious associations. Fig. 2 Algorithm for the identification of CA-MRSA controls Jackson et al. BMC Infectious Diseases (2016) 16:684 Page 3 of 7 Results When implemented on the entire biobank population at each site, the CA-MRSA algorithm returned 124 cases and 1649 controls at NU, 76 cases and 2310 controls at Geisinger, and 61 cases and 7781 controls at Marshfield, without inclusion of the silver standard criteria. Table 1 summarizes the validation results from all three sites. PPV ranged from 68 to 100% for cases and 96 to 100% for controls; NPV ranged from 90 to 100% for cases and 80 to 100% for controls; sensitivity ranged from 94 to 100% for cases and 75 to 100% for controls. Application of the silver standard criteria did not yield significantly more patients at any of the validation sites. Only four additional cases at NU and four at Geisinger were identi-fied after applying the silver standard criteria; no new patients were identified at Marshfield. Chart review of the "silver" cases at NU showed all four to be valid cases of CA-MRSA. Across all seven sites, the CA-MRSA phenotype algo-rithm (including the silver standard criteria) identified a total of 349 genotyped cases and 7761 genotyped con-trols among the biobank populations. Table 2 shows the breakdown of cases and controls by site. Addition of silver standard criteria yielded a small number of add-itional cases at some sites. Four sites (CHOP, Marshfield, Mayo Clinic and NU) added no "silver" genotyped cases (although, as described previously, some additional cases were discovered in the entire biobank population). The frequency of cases in the genotyped biobank sample also varied widely by site, ranging from 0.1% (CHOP) to 13.1% (Geisinger). Table 3 shows patient demographic characteristics, both overall and by case/control status for all adult eMERGE sites. The demographic breakdown of cases and controls by site and race can be found in Additional file 2: Table S2. The CHOP site contributed only 2 cases, and the demographics of cases and controls were very different from all other eMERGE sites; as such, GWAS analysis excluded data from this site. Among the resulting sites, 5111 patients (269 cases; 4842 controls) were European American and 770 (71 cases; 699 controls) were African American. Only 0.9% of cases and 0.8% of controls were identified as Hispanic or Latino in the EHR. The majority of cases and controls were female (52.4 and 61.9%) respectively. The average age for cases was 42 (SD = 22); the average age for con-trols was 67 (SD = 14). There were no plausible GWAS-significant (p < 5E-8) hits among the 269 cases and 4842 controls in European Americans or the 71 cases and 699 controls in African Americans (Additional file 3: Figure S1A and B). One apparent signal in African Americans on chromosome 6 included only very low frequency SNPs in a gene desert, suggesting that this result was a false positive. Examin-ation of QQ plots and genomic inflation factors did not suggest any systematic inflation from the null distribution. Discussion Development of an algorithm to capture a complex phenotype like CA-MRSA poses several challenges. First of all, the epidemiological definition of CA-MRSA by the CDC is complex and has several requirements that typically are not captured in structured data and instead must be extracted from clinical and laboratory notes. Therefore, our algorithm relied on each site's ability to combine data from multiple EHR sources, including ICD-9 diagnosis codes, clinical notes, records of past hospitalizations and laboratory culture results in order to accurately extract phenotype cases and controls. The extraction of information from non-structured fields re-mains a significant obstacle to accurate phenotyping, suggesting the need for text-based strategies, such as na-tional language processing, for phenotyping. Second, differentiation between hospital and community-associated MRSA can be difficult. The issue is further com-plicated because of increasing presence of CA-MRSA strains in hospital settings and subcategories of HA-MRSA such as healthcare associated community onset (HACO) [29] and healthcare associated hospital onset (HAHO) [30] MRSAs. Our phenotype definition expands on previous epidemio-logic studies by Casey et al. [31, 32], in which HA-MRSA is differentiated from CA-MRSA primarily by having an in-patient visit at the time of positive MRSA culture/diagnosis, an indwelling catheter or subcutaneous device at the time of positive MRSA culture/diagnosis, or a hospitalization, dialysis, surgery or residence in a nursing home within the year prior to a positive MRSA culture/diagnosis at a single Table 1 Summary of chart review validation Cases Controls Northwestern Geisinger Marshfield Northwestern Geisinger Marshfield Total Na 124 76 61 1649 2310 7781 Total Reviewed 50 25 25 50 25 25 Sensitivity 0.94 1 1 0.75 0.96 1 PPV 0.68 0.96 1 0.96 1 1 NPV 0.90 1 1 0.80 0.96 1 aSample includes all patients in site's biobank Jackson et al. BMC Infectious Diseases (2016) 16:684 Page 4 of 7 healthcare system. Our study took place at seven institu-tions, spanning various EHR systems, each with unique complications to obtaining note-based requirements. There-fore, to increase the likelihood that the MRSA infection was community associated, we included a requirement that the site of infection must be an SSTI, in case of sparse clinical notes. Similarly, we required that any diagnosis of CA-MRSA be combined with an SSTI diagnosis within the week before or after the MRSA diagnosis. These differences may account for significant variation in performance of CA-MRSA case definitions, and as such, may also help explain the differences in frequency of cases across sites. While our final cohort of patients included cases and controls from multiple institutions spanning urban, sub-urban and rural geographies across the country, each in-dividual site's cohort only included information from one institution. Prior work has demonstrated significant fragmentation of key data on MRSA across institutions [22, 33]. Indeed, during the validation process, chart re-viewers noted that many patients not excluded from the case definition in the algorithm were found to have been hospitalized and/or underwent surgery at sites other than the institution where the CA-MRSA culture was drawn and, therefore, were incorrectly included as cases by the algorithm. Again, this supports the need for text-driven strategies in defining phenotypes, as well as effective health information exchange in regions in which overlap of patient populations across multiple in-stitutions is significant. The frequency of cases among the biobank popula-tions varied widely between each site. This algorithm was run at a total of seven sites which collectively repre-sent a diverse population of patients, clinicians, EHRs and conventions of documentation. While this variety enhances diversity of the population for analyses, it also provides a challenge to EHR extraction. Additionally, our study included only patients who also had genetic information captured as part of each institution's bio-bank. This may account for some of the cross-site differ-ence in case numbers used for analysis, particularly if biobanking efforts focused on specific populations. Our sites represent a sample of larger healthcare delivery in-stitutions and are located in areas with differing rates of CA-MRSA [34]. Institutional differences in screening practices (e.g., mandated active surveillance of specific populations) for MRSA may have also accounted for cross site differences in the frequency of cases among the genotyped populations from the biobank [35, 36]. Selection of a set number of cases and controls for val-idation purposes (rather than selecting numbers propor-tional to the prevalence of each group in the study population) may have led to inflation of sensitivity esti-mates due to validation bias. Given the low frequency of MRSA in this population, we felt enrichment of cases for validation was necessary. As all sites validated the same ratio of cases and controls, we anticipate this infla-tion to be similar across all three validation sites. Small sample size was a limitation of our study. Original application of the algorithm at the three validation sites (NU, Geisinger and Marshfield) returned only 112 valid cases to be used in the final GWAS. The ICD-9 based "sil-ver" criteria were added in order to obtain additional cases to increase GWAS power. However, despite anecdotal evidence that front-line clinicians empirically treat all sus-pected CA-MRSA patients and do not routinely draw wound cultures, we identified very few new cases of CA-MRSA using ICD-9 codes alone that were not otherwise Table 2 Summary of genotyped case and control subject countsa by institution Overall CHOPb Geisinger GHC Marshfield Mayo NU VU Total Cases 349 2 34 39 15 1 62 196 Silver Only 40 0 1 1 0 0 0 38 Total Controls 7761 1869 233 1131 1871 306 783 1568 Prevalence 4.3% 0.1% 12.7% 3.3% 0.8% 0.3% 7.3% 11.1% aSample includes only GWAS genotyped patients from the site's biobank bCHOP data was not included in GWAS analysis Table 3 Demographics of cases and controls in GWAS Overall Cases Controls N (%) N (%) N (%) Total 6239 347 (5.6) 5892 (94.4) Sex Male 2407 (38.6) 165 (47.6) 2242 (38.1) Female 3832 (61.4) 182 (52.4) 3650 (61.9) Ancestry European American 5111 (81.9) 269 (77.5) 4842 (82.2) African American 771 (12.4) 72 (20.8) 699 (11.9) Hispanic 14 (0.2) 0 (0) 14 (0.2) Other 343 (5.5) 6 (1.7) 337 (5.7) Ethnicity Hispanic or Latino 49 (0.8) 3 (0.9) 46 (0.8) Not Hispanic or Latino 6076 (97.4) 340 (98.0) 5736 (97.4) Unknown 114 (1.8) 4 (1.2) 110 (1.9) Age (years) (mean(SD)) 66 (16) 42 (22) 67 (14) Jackson et al. BMC Infectious Diseases (2016) 16:684 Page 5 of 7 picked up by the original definition (n = 8 in the entire bio-banked sample). Due to the small counts and 100% accuracy of patients obtained at NU, we did not require that other sites validate the "silver" algorithm. This criteria required an ICD-9 code for CA-MRSA, which clinicians may not often use to characterize SSTIs without the availability of con-firmatory cultures (i.e., preference for the use of ICD-9 codes for "Abscess" or "Cellulitis" when culture has not been performed), and is largely physician dependent. The lack of frequent clinician documentation using these ICD-9 codes may be responsible for low patient numbers falling into this category and currently limits the utility of this approach. Further research into the EHR-based documentation pat-terns of clinicians for SSTIs that represent suspected CA-MRSA cases will help address these concerns. The lack of GWAS-significant findings in either European or African Americans is disappointing, but not surprising given the limited sample size and anticipated modest effect size of any common genetic variants pre-disposing to CA-MRSA infection [6, 7, 37]. With CA-MRSA cases not de-fined in a uniform manner (as detailed above) and not screened for in a uniform manner (also detailed above), measurement error in the identification of cases would be expected to bias any association estimates between SNPs and CA-MRSA to the null. Despite these reports, differences in rates of host susceptibility to Staphylococcus aureus colonization and infection and differences in susceptibility to severity of related diseases point to a role for host genetic factors in susceptibility to CA-MRSA infections [6]. Conclusion The algorithmic extraction of CA-MRSA cases and controls from EHRs presents challenges and new possibilities for phenotypic-genotypic association studies. Our algorithm represents, to our knowledge, a first attempt at validating an otherwise complicated phenotype across multiple care sites. Variation in patient populations, screening practices, con-ventions of documentation and EHR data capture make standardization of an algorithm challenging and may ac-count for variation in algorithm performance. Future work should focus on identifying these specific differences, as ac-counting for institutional variations when defining the algo-rithm may assist in identifying additional valid cases and controls to provide additional power to detect genetic risk factors predisposing carriers to CA-MRSA. Additional files Additional file 1 Table S1: Staphylococcus aureus associated skin and soft tissue infections. (DOC 29 kb) Additional file 2 Table S2: Demographics of genotyped cases and controls by site. (DOC 47 kb) Additional file 3 Figure S1: A: CA-MRSA GWAS results in European Americans. B: CA-MRSA GWAS results in African Americans. (ZIP 294 kb) Acknowledgements None. Funding The eMERGE Network was initiated and funded by NHGRI through the following grants: U01HG006389 (Essentia Institute of Rural Health, Marshfield Clinic Research Foundation and Pennsylvania State University); U01HG006382 (Geisinger Clinic); U01HG006375 (Group Health Cooperative/University of Washington); U01HG006379 (Mayo Clinic); U01HG006380 (Icahn School of Medicine at Mount Sinai); U01HG006388 (Northwestern University); U01HG006378 (Vanderbilt University Medical Center); and U01HG006385 (Vanderbilt University Medical Center serving as the Coordinating Center). Availability of data and materials The datasets supporting the conclusions of this article will be made available in the database of Genotypes and Phenotypes (dbGaP) at http:// www.ncbi.nlm.nih.gov/gap, and the phenotype algorithm will be made available in the Phenotype KnowledgeBase (PheKB) at https://phekb.org. Authors' contributions KLJ completed data extraction at NU, phenotype validation analysis and authored the manuscript. MM assisted with phenotype development, completed chart review for NU, and was a significant contributor in writing the manuscript. JAP assisted with phenotype development and data extraction and validation at NU and edited the manuscript. ASB completed GWAS analysis and edited the manuscript. DJV completed chart review for NU and edited the manuscript. JGL assisted with data extraction and validation at Marshfield and edited the manuscript. SKS provided significant contribution to phenotype development and edited the manuscript. PLP assisted with data extraction and validation at Marshfield and edited the manuscript. KMB completed data extraction and validation at Geisinger and edited the manuscript. DAC completed data extraction at Group Health and edited the manuscript. SJB completed data extraction at Mayo Clinic and edited the manuscript. JCK completed data extraction at Vanderbilt and edited the manuscript. JCD completed data extraction at Vanderbilt and edited the manuscript. FDM completed data extraction at CHOP and edited the manuscript. LMV completed data extraction at CHOP and edited the manuscript. LJRT provided epidemiological and GWAS analysis support and was a significant contributor in writing the manuscript. ANK provided significant contribution to phenotype development and was a significant contributor in writing the manuscript. All authors critically reviewed and approved the final manuscript. Competing interests The authors declare that they have no competing interests. Consent for publication Not applicable. Ethics approval and consent to participate Approval of the study was obtained from each site's respective institutional review board (Children's Hospital of Philadelphia Institutional Review Board, Geisinger Health Systems Institutional Review Board, Group Health Research Institute Institutional Review Board, Marshfield Clinic Institutional Review Board, Mayo Clinic Institutional Review Board, Northwestern University Institutional Review Board, Vanderbilt University Institutional Review Board), and data sharing across sites was enabled through a common data use agreement. Author details 1Feinberg School of Medicine, Northwestern University, Chicago, IL, USA. 2Emory University School of Medicine, Atlanta, GA, USA. 3Biomedical Informatics Research Center, Marshfield Clinic Research Foundation, Marshfield, WI, USA. 4Marshfield Clinic Research Foundation, Marshfield, WI, USA. 5Geisinger Health System, Danville, PA, USA. 6Group Health Research Institute, Group Health Cooperative, Seattle, WA, USA. 7Mayo Clinic, Rochester, MN, USA. 8Department of Biomedical Informatics, Vanderbilt University, Nashville, TN, USA. 9The Center for Applied Genomics, Children's Hospital of Philadelphia, Philadelphia, PA, USA. Received: 9 July 2016 Accepted: 11 November 2016 Jackson et al. BMC Infectious Diseases (2016) 16:684 Page 6 of 7 dx.doi.org/10.1186/s12879-016-2020-2 dx.doi.org/10.1186/s12879-016-2020-2 dx.doi.org/10.1186/s12879-016-2020-2 http://www.ncbi.nlm.nih.gov/gap http://www.ncbi.nlm.nih.gov/gap https://phekb.org/ References 1. Chua K, et al. Antimicrobial resistance: Not community-associated methicillin-resistant Staphylococcus aureus (CA-MRSA)! A clinician's guide to community MRSA - its evolving antimicrobial resistance and implications for therapy. Clin Infect Dis. 2011;52(1):99–114. 2. Elston DM. How to handle a CA-MRSA outbreak. Dermatol Clin. 2009;27(1):43–8. 3. Maree CL, et al. Community-associated methicillin-resistant Staphylococcus aureus isolates causing healthcare-associated infections. Emerg Infect Dis. 2007;13(2):236–42. 4. Mediavilla JR, et al. Global epidemiology of community-associated methicillin resistant Staphylococcus aureus (CA-MRSA). Curr Opin Microbiol. 2012;15(5):588–95. 5. Shukla SK. Community-associated methicillin-resistant Staphylococcus aureus and its emerging virulence. Clin Med Res. 2005;3(2):57–60. 6. Shukla SK, Rose W, Schrodi SJ. Complex host genetic susceptibility to Staphylococcus aureus infections. Trends Microbiol. 2015;23(9):529–36. 7. Ye Z, et al. Genome wide association study of SNP-, gene-, and pathway-based approaches to identify genes influencing susceptibility to Staphylococcus aureus infections. Front Genet. 2014;5:125. 8. Sattler CA, Mason Jr EO, Kaplan SL. Prospective comparison of risk factors and demographic and clinical characteristics of community-acquired, methicillin-resistant versus methicillin-susceptible Staphylococcus aureus infection in children. Pediatr Infect Dis J. 2002;21(10):910–7. 9. Benoit SR, et al. Community strains of methicillib-resistant Staphlococcus aureus as potential cause of healthcare-associated infections, Uruguay, 2002–3004. Emerg Infect Dis. 2008. doi:10.3201/eid1408.071183. 10. Minnesota Department of Health. http://www.health.state.mn.us/divs/idepc/ diseases/mrsa/camrsa/hcp.html. Accessed 30 June 2016. 11. Millar BC, et al. Proposed definitions of community-associated meticillin-resistant Staphylococcus aureus (CA-MRSA). J Hosp Infect. 2007;67(2):109–13. 12. Wei WQ, et al. Impact of data fragmentation across healthcare centers on the accuracy of a high-throughput clinical phenotyping algorithm for specifying subjects with type 2 diabetes mellitus. J Am Med Inform Assoc. 2012;19(2):219–24. 13. Pathak J, Kho AN, Denny JC. Electronic health records-driven phenotyping: challenges, recent advances, and perspectives. J Am Med Inform Assoc. 2013;20(e2):e206–11. 14. McCarty CA, et al. The eMERGE Network: a consortium of biorepositories linked to electronic medical records data for conducting genomic studies. BMC Med Genomics. 2001;4:13. 15. Gottesman O, et al. The Electronic Medical Records and Gemonimcs (eMERGE) network: past, present, and future. Genet Med. 2013;15(10):761–71. 16. McCarty CA, et al. The eMERGE Network: a consortium of biorepositories linked to electronic medical records data for conducting genomic studies. BMC Med Genomics. 2011;4:13. 17. Muthalagu A, et al. A rigorous algorithm to detect and clean inaccurate adult height records within EHR systems. Appl Clin Inform. 2014;5(1):118–26. 18. Newton KM, et al. Validation of electronic medical record-based phenotyping algorithms: results and lessons learned from the eMERGE network. J Am Med Inform Assoc. 2013;20(e1):e147–54. 19. Kho AN, et al. Electronic medical records for genetic research: results of the eMERGE consortium. Sci Transl Med. 2011;3(79):79re1. 20. Mistry RD, et al. Clinical management of skin and soft tissue infections in the U.S. emergency departments. West J Emerg Med. 2014;15(4):491–8. 21. McCaig LF, et al. Staphylococcus aureus-associated skin and soft tissue infections in ambulatory care. Emerg Infect Dis. 2006;12(11):1715–23. 22. Kho AN, et al. A regional informatics platform for coordinated antibiotic-resistant infection tracking, alerting, and prevention. Clin Infect Dis. 2013; 57(2):254–62. 23. Crosslin DR, et al. Controlling for population structure and genotyping platform bias in the eMERGE multi-institutional biobnak linked to electronic health records. Front Genet. 2014;5:352. 24. Mosley JD, et al. A genome-wide association study identifieds variants in KCNIP4 associated with ACE inhibitor-induced cough. Pharmacogenomics J. 2015;16(3):231–7. 25. Zuvich RL, et al. Pitfalls of merging GWAS data: lessons learned in the eMERGE network and quality control procedures to maintain hight data quality. Genet Epidemiol. 2001;35(8):887–98. 26. Howie B, et al. Fast and accurate genotype inputation in genome-wide association studies through pre-phasing. Nat Genet. 2012;44(8):955–9. 27. Patterson N, et al. Population structure and eigenanalysis. PLoS Genet. 2006;2(12):e190. 28. Wellcome Trust Case Control Consortium. Genome-wide association study of 14,000 cases of seven common diseases and 3,000 shared controls. Nature. 2007;447(7145):661–78. 29. Lenz R, et al. The distinct category of healthcare associated bloodstreem infections. BMC Infect Dis. 2012;12:85. 30. Wang SH, et al. Molecular and clinical characteristics of hospital and community onset methicillin-resitant Staphylococcus aureur strains associated with bloodstream infections. J Clin Microbiol. 2015;53(5):1599–608. 31. Casey JA, et al. A population-based study of the epidemiology and clinical features of methicillin-resistant Staphylococcus aureus infection in Pennsylvania, 2001–2010. Epidemiol Infect. 2013;141(6):1166–79. 32. Casey JA, et al. High-density livestock operations, crop field application of manure, and risk of community-associated methicillin-resistant Staphylococcus aureus infection in Pennsylvania. JAMA Intern Med. 2013;173(21):1980–90. 33. Kho AN, et al. Use of a regional health information exchange to detect crossover of patients with MRSA between urban hospitals. J Am Med Inform Assoc. 2008;15(2):212–6. 34. Dukic VM, et al. Epidemics of community-associated methicillin-resistant Staphylococcus aureus in the United States: a meta-analysis. Otto M, ed. PLoS ONE. 2013;8(1):e52722. doi:10.1371/journal.pone.0052722. 35. Lyles RD, et al. Regional epidemiology of methicillin-resistant Staphylococcus aureus among critically ill children in a state with mandated active surveillance. J Pediatric Infect Dis Soc. 2015. 36. Kavanagh KT, et al. The use of surveillance and preventative measures for methicillin-resistant staphylococcus aureus infections in surgical patients. Antimicrob Resist Infect Control. 2014;3(1):1–7. 37. Brown EL, et al. Genome-wide association study of Staphylococcus aureus carriage in a community-based sample of Mexican-Americans in Starr County, Texas. PLoS One. 2015;10(11):e0142130. • We accept pre-submission inquiries • Our selector tool helps you to find the most relevant journal • We provide round the clock customer support • Convenient online submission • Thorough peer review • Inclusion in PubMed and all major indexing services • Maximum visibility for your research Submit your manuscript at www.biomedcentral.com/submit Submit your next manuscript to BioMed Central and we will help you at every step: Jackson et al. BMC Infectious Diseases (2016) 16:684 Page 7 of 7 http://dx.doi.org/10.3201/eid1408.071183 http://www.health.state.mn.us/divs/idepc/diseases/mrsa/camrsa/hcp.html http://www.health.state.mn.us/divs/idepc/diseases/mrsa/camrsa/hcp.html http://dx.doi.org/10.1371/journal.pone.0052722 Abstract Background Methods Results Conclusions Background Methods Algorithm development Algorithm validation Phenotypic data Genetic data Genetic analysis Results Discussion Conclusion Additional files Acknowledgements Funding Availability of data and materials Authors' contributions Competing interests Consent for publication Ethics approval and consent to participate Author details References Performance analysis and assessment of a tf-idf based Archetype-SNOMED-CT binding algorithm Sheng Yu Dublin Institute of Technology TeaPOT Research Group School Elect. Eng. Systems Kevin St., Dublin 8, Ireland Damon Berry Dublin Institute of Technology TeaPOT Research Group School Elect. Eng. Systems Kevin St., Dublin 8, Ireland Jesus Bisbal Universitat Pompeu Fabra Department of ICT Barcelona, Spain Abstract Term bindings in archetypes are at a boundary be-tween health information models and health terminol-ogy for dual model-based electronic health-care record (EHR) systems. The development of archetypes and the population of archetypes with bound terms is in its infancy. Terminological binding is currently performed "manually" by the teams who create archetypes. This process could be made more efficient, if it was supported by automatic tools. This paper presents a method for evaluating the performance of automatic code search approaches. In order to assess the quality of the automatic search, the authors extracted all the unique bound codes from 1133 archetypes from an archetype repos-itory. These "manually bound" SNOMED-CT codes were compared against the codes suggested by the au-thors' automatic search and used for assessing the algo-rithm's performance in terms of accuracy and category matching. The result of this study shows a sensitivity analysis of a set of parameters relevant to the matching process. 1. Introduction The harmonisation of clinical data models and ter-minology models is driven by advocates of both mod-elling methodologies. One major challenge associated with this work is the need to annotate the clinical infor-mation with an appropriate concept from a terminol-ogy to bring portability and interoperability. Recent research interest on this topic has focused on using an automatic means to annotate clinical information in an electronic health record with concepts from exter-nal terminology. The difference between annotating free text in clinical notes and tagging clinical concepts in modern EHRs is that data within modern EHRs are organised by a structurally constrained information model. One information modelling approach that has been gaining momentum employs so-called archetypes [4] which express the views of clinical experts and contain both structural and semantic constraints. SNOMED-CT is a large medical terminology system. Members of the openEHR organisation are working on provid-ing links to SNOMED-CT terms within archetypes and also to harmonise SNOMED and openEHR representa-tions in order to deliver enhanced semantic interoper-ability in e-health. A number of studies [8] [7] have al-ready been conducted under the assumption that auto-matic annotation with SNOMED-CT produces sound and reliable outcomes. However there are few publi-cations in the literature that report the effectiveness of automatic SNOMED-CT binding methods in terms of accuracy and reliability. Binding methods of this type should also adapt to the constantly developing SNOMED-CT to suit particular clinical scenarios. Hence it is necessary to quantitatively evaluate the performance of these annotation algorithms. This pa-per builds upon a previously published tf-idf based SNOMED-CT binding approach [11] and performs a sensitivity analysis to evaluate the effectiveness of the automatic binding process. The assessment can be ex-tended to facilitate and promote the continuous im-provement of binding algorithms in future work. 2. Background An Archetype model could be considered as a type of clinical meta-data model. Archetypes are clinician-created meta-information models that describe con-straints for data stored in an EHR. The archetype-based approach is a promising EHR paradigm be-cause with archetypes, medical knowledge can be sep-arated from technical implementation. Due to the nature of community-based development, archetypes are created according to agreed medical definitions by archetype modellers. Repositories with large numbers of archetypes are being created and the difficulty asso-ciated with managing archetypes is increasing. In the future, the large amounts of clinical content described by archetypes will need to be properly categorised and users should have easy access to them [5]. On the other hand, SNOMED-CT is a multi-purpose clinical vocabulary that consists of hundreds of thou-sands carefully modelled medical concepts. Its sub-stantial coverage of clinical content and phenomena makes it a highly-rated external terminology for dis-ambiguating and clarifying clinical statements. The hierarchical structure of SNOMED-CT means that it can be used as a classification system for various pur-poses. The association of archetype terms with exter-nal terminology improves the semantic interoperability during communication between different health organi-sations. While archetypes contain their own definitions of concepts, they can refer to formal terminology sys-tem such as SNOMED-CT thus to reduce the ambigu-ity in clinical data. Binding to commonly understood SNOMED-CT terms facilitates common understand-ing between diverse EHR systems and so promotes se-mantic interoperability. Other benefits of annotating archetypes with external terminology have been dis-cussed elsewhere [10] [2]. However, due to the large quantity of archetypes being created, purely manual annotation is not appropriate to obtain relevant and high quality bindings. Some form of automated sup-port is necessary. Sound algorithms need to be created to find appropriate SNOMED-CT concepts. 3. Related work Lezcano [7] et. al., used a UMLS thesaurus utility, comprising of a normalised string search to associate archetype nodes with SNOMED-CT concepts. The normalised search compares input strings with records of a large built-in string index. The process of build-ing indices of this type involves normalising terms in a UMLS thesaurus so that strings can be compared in a normalised way i.e ignore word sequence, tense etc. Similar to a database keyword look-up, the output of this type of binding process is largely dependent on its index. Our experience of this tool is that the input string has to be quite similar to the record for them to match. The system often fails to produce any match if the input query contains more words, given that some word might not appear in the index. Qamar [9] et al. adopted a multiple searching and filtering approach to associate archetype nodes with SNOMED-CT codes. Multiple natural language pro-cessing tools such as GATE wordsense disambiguation were used to aid the binding process. The filtering rules were chosen based on judgement and then assessed by a group of experts. However the assessment is not easily scalable and it would be difficult to apply this algo-rithm to other related research. By comparison, be-cause the assessment relies on a resource created by experts rather than the experts themselves, the contri-bution presented here provides more quantitative and re-applicable analysis of the binding method used to annotate archetypes. 4. Algorithm evaluation This paper aims to provide an initial demonstration of an evaluation technique as an example to support fu-ture studies which may utilise binding-assistance algo-rithms of this type. The rating approach is applicable for more complex algorithms. One reason for evaluat-ing the comparatively simple algorithm shown here is that the resources required for the algorithm are gener-ally available for ubiquitous demonstration. The "gold standard" that is used to assess the "effectiveness" of the automatic binding suggestion, the bound codes in archetypes, is publicly available for this method. An-other reason, is that the method is convenient (using a single tool Lucene) and transparent. The previous work utilised a free text indexing tool called Lucene[6] featuring the term frequency-inverse document frequency (tf-idf ) weighting scheme to en-able the ranking of search results. The tf factor is the frequency of a term inside a document and the idf fac-tor is the inverse of the frequency of a term among the documents in the collection[1]. It was adapted in the reported work to provide an automatic means to search by archetype terms and suggest their best matches in SNOMED-CT. The approach adopts the following techniques, 1.All SNOMED-CT terms are indexed to enable searching by free text. 2.Archetype terms are extracted and queried against the index. 3.Results are then generated to suggest a list of SNOMED-CT terms. This automated binding mechanism could be seen as an information retrieval (IR) system. In authors' view, in order to use the generated binding sugges-tions, an analysis of the underlying algorithm needs to be conducted to quantitatively demonstrate its ac-curacy. Precision and Recall [1] are used extensively to evaluate IR algorithms. Recall is the fraction of rel-evant items that have been retrieved over all relevant items, which can be attached to the question. This re-flects how many relevant items the algorithm managed to collect. Precision is an indication of the fraction of relevant items in a result set. It reflects the accuracy of the algorithm. 5. Method of analysis The evaluation of Information Retrieval methods typically feature a collection of documents and a set of queries, which are used to seek the "relevant" doc-uments. Also, a set of known answers associated with these queries indicate whether the document is relevant to the query or not. All testing documents, queries and answers are peer-reviewed by experts, in order to test new IR methods. Examples of such standard document collections include the Text Retrieval Con-ference (TREC) collection for general free text doc-ument retrieval and the Cystic Fibrosis collection for medical text retrieval [1]. In this paper, analogously to information retrieval, a "gold standard" is estab-lished by extracting existing manually-selected codes from the set of archetypes. These existing manually-bound SNOMED-CT codes are used as a reference to decide whether a search result has been successful or not. Because the returned answers will be ranked, this study applied thresholds to trim the number of an-swers, which were set at a range of different values to establish the impact on the results. Observation of performance related factors was made according to different thresholds. The details of these two different thresholds are described below. ∙ TopN a threshold to set the algorithm to gather the maximum number of collect-able an-swer(code)s ∙ MAJ a threshold to define at least how many in a result set which are of the same category can be considered the "majority" 6. Result By setting the threshold to an intended range at suitable data points, the following diagrams in Figure 1 were generated to show how the algorithm performed by measuring the average recall and precision of all the searches[11]. One may argue that under such conditions it is al-ways a 1:1 mapping (1 archetype node is mapped to 1 SNOMED-CT code, thus the number of relevant codes is 1) so the recall can be either 0 or 1. The precision would be 0 or 1 divide by the number of codes in the result set. The diagrams are not in the conventional "precision versus recall" style because both are chang-ing when different thresholds are applied. However the value of calculating these figures can be seen in the future when proper data sets are complete in which case an archetype node may be associated with many SNOMED-CT candidates 1:* (based on experts' judge-ment). Despite the small size of existing bound codes, this study can still reveal the capabilities of whether it retrieves the only relevant code or not. Figure 1(a) shows the average recall of the algorithm i.e the percentage of automated results that contain matches for manually bound codes. The trend shows that when more codes are retrieved, it is more likely that the result set will include the code suggested by the "gold standard". Although the average recall re-mained almost the same after the result set contained 20 and more codes. However more codes in a result set also indicate more "noise" i.e lower precision. Figure 1(b) shows the average precision of total searches using the algorithm with different thresholds at recall 1.0. The highest average precision value was achieved at threshold value 1 which indicates the result set consisted of only one code, i.e the top answer. The reason for its rapid decline in the range of 1 to 10 is be-cause although more codes were returned in the result set, the chance of retrieving the relevant code did not increase greatly. Only a few more searches retrieved the relevant code when the size of the result set is big-ger than 1. Therefore it is suggested that if this version of the algorithm is to be used to automate the binding process, 38.27% of its top answers can be considered as reliable references. Despite the low accuracy of this algorithm, better algorithms can be designed based on a more mature data set. Another type of comparison was conducted, which took the majority of a result set and checked whether (a) Average recall (b) Average precision Figure 1: recall and precision versus number of codes in result sets Figure 2: Comparing category of the majority and the bound code their SNOMED-CT category matched the category of the bound code. The vertical axis in Figure 2 refers to the number of any majority of returned codes in one result set having the same category. The horizontal axis represents the number of codes belonging to the same category in a result set. For instance, where the result set has 9 codes having the same category, the chance would be 77.7% for it to match the category of the bound code. The first 1,2,3 implies few codes shared a category. The category is taken randomly from any group of codes to match the bound code's. Where the majority codes dominated the result set, it is more likely to match the bound code's category in SNOMED-CT. The first two figures illustrate that gen-erally as more answers are included in a result set, it is more likely to include the manually selected code. The percentage of matches is an indicator that when results of the search are used, only a fraction of them are ac-curate. The third figure, however, shows that besides examining exact matches, other interesting features of the algorithm can be discovered. It reveals that there is a stable growth of category matching between the bound code and the majority of the result sets. It therefore shows the potential of using this algorithm to classify unbound archetype nodes. 7 Discussion Many interpretations could be used to explain the behaviour of the algorithm as shown in the diagrams. However to a great extent the underlying tf-idf scheme is playing an important role in the retrieval and ranking of codes. Although a review of the tf-idf formula and the vector-space model is not the focus of the paper, insight of how results are ranked can benefit future improvement. Equation 1 is the simplified formula for calculating the scores of each returned result. 𝑠𝑐𝑜𝑟𝑒 = ∑ 𝑡 (𝑡𝑓 ∗ 𝑖𝑑𝑓 ∗ 𝐹𝑖𝑒𝑙𝑑𝐹𝑎𝑐𝑡𝑜𝑟). (1) The field factor is a score to give higher weight to shorter terms. Equation 2 and equation 3 show how tf, idf factors are calculated respectively for each query. 𝑡𝑓 = 𝑡𝑒𝑟𝑚𝐹𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦 1 2 . (2) 𝑖𝑑𝑓 = 1 + log( 𝑛𝑢𝑚𝐷𝑜𝑐𝑠 𝑑𝑜𝑐𝐹𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦 + 1 ). (3) e.g A search for "blood" will retrieve "Blood" as the top answer in a result set containing the following score in total 1068278 SNOMED-CT terms: 5.5406284. By checking the factors from the result set, the fol-lowing scores were obtained : tf=1.0; idf=5.5406284; FieldFactor=1.0 (docFreq=11394, numDocs=1068278) In this scenario, the number of words in a term is quite small compared to document-like articles. Due to this fact, term frequency tf tends to be 1.0 i.e no repeated word in a single term. From the resulting score the fac-tor with the highest impact seems to be the inverse doc-ument frequency. This indicates that if a word appears to be "common" which frequently occurs in SNOMED-CT terms, then it will contribute less to the final score. However other factors such as the field factor, matter also. In the example given above, the top answer con-sists of only one word "blood" which makes the field factor much larger comparing to other answers with more words. As a result of this basic conclusion, it is suggested that since the idf factor is significant in ranking and retrieving terms, a study of total terms in SNOMED-CT and its distribution can help to improve the results. 8. Future work In the improved version of the binding algorithm, various natural language processing techniques will be added [3] such as synonym recognition and word disambiguation among archetype terms. The struc-ture of archetypes and reference model information could also be made to influence future algorithms, by incorporating information about the reference model class and archetype path associated with an archetype term to enhance the binding process. There are other SNOMED-CT related changes that could be made, for example, if an archetype is related to "Procedure", flags should be sent to indicate the results should give higher relevance to "Procedure" hierarchy and its re-lated concepts. An important feature of SNOMED-CT, post-coordination, needs to be considered when no result is suggested by the algorithm but also in some cases where results are returned. However this use of SNOMED-CT is rather advanced and requires more domain knowledge so future development of automatic post-coordinating SNOMED-CT codes should proceed with caution. Future work also involves investigating common words in SNOMED-CT world and how their synonyms are distributed. Because of the significance of idf, the subsetting of SNOMED-CT terms could contribute to specialised searching. This could also be combined with other modifications of the algorithm, such as boosting certain factors in the indexing and searching phase, in order to retrieve more desired SNOMED-CT codes. 9. Conclusion This work has demonstrated that the authors bind-ing algorithm exhibits an acceptable level of perfor-mance. The evaluation approach reported here can also be reapplied to analyse future potential automatic binding algorithms and provide a means to assess them. It is hoped and expected that the performance of dif-ferent algorithms can be compared in this way. The authors believe that this study can be the foundation of exploiting IR and other medical text processing tech-niques to benefit the archetype-SNOMED-CT binding process. References [1] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern In-formation Retrieval. Addison-Wesley Longman Pub-lishing Co., Inc., Boston, MA, USA, 1999. [2] J. Bisbal and D. Berry. Archtype alignment:a two-level driven semantic matching approach to interoperability in the clinical domain. In Proceedings of the Interna-tional Conference on Health Informatics, HEALTH-INF 2009, pages 216–221. INSTICC Press, 2009. [3] C. Friedman, L. Shagina, Y. Lussier, and G. Hripcsak. Automated encoding of clinical documents based on natural language processing. Journal of the American Medical Informatics Association, 11(5):392–402, 2004. [4] S. Garde, E. Hovenga, J. Buck, and P. Knaup. Ex-pressing clinical data sets with openEHR archetypes: A solid basis for ubiquitous computing. International journal of medical informatics, 76:S334–S341, 2007. [5] S. Garde, E. Hovenga, J. Granz, S. Foozonkhah, and S. Heard. Towards a repository for managing archetypes for electronic health records. In HIC 2006 and HINZ 2006: Proceedings, page 61. Health Infor-matics Society of Australia, 2006. [6] O. Gospodnetic and E. Hatcher. Lucene in Action (In Action series). Manning Publications, Dec. 2004. [7] L. Lezcano, S. Sánchez-Alonso, and M.-A. Sicilia. As-sociating clinical archetypes through umls metathe-saurus term clusters. Journal of Medical Systems, pages 1–10, 2010. 10.1007/s10916-010-9586-9. [8] R. Qamar, J. Kola, and A. Rector. Unambiguous data modeling to ensure higher accuracy term binding to clinical terminologies. In AMIA Annual Symposium Proceedings. American Medical Informatics Associa-tion, 2007. [9] R. Qamar and A. Rector. MoST: A System to Seman-tically Map Clinical Model Data to SNOMED-CT. In Semantic Mining Conference on SNOMED-CT, pages 38–43, 2006. [10] E. Sundvall, R. Qamar, M. Nystrom, M. Forss, H. Pe-tersson, D. Karlsson, H. Ahlfeldt, and A. Rector. In-tegration of tools for binding archetypes to snomed ct. BMC Medical Informatics and Decision Making, 8(Suppl 1):S7, 2008. [11] S. Yu, D. Berry, and J. Bisbal. An Investigation of Semantic Links to Archetypes in an External Clinical Terminology through the Construction of Terminolog-ical "Shadows". In Proceedings of International As-sociation for Development of the Information Society e-health 2010, pages 9–17, 2010. << /ASCII85EncodePages false /AllowTransparency false /AutoPositionEPSFiles true /AutoRotatePages /None /Binding /Left /CalGrayProfile (Gray Gamma 2.2) /CalRGBProfile (sRGB IEC61966-2.1) /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2) /sRGBProfile (sRGB IEC61966-2.1) /CannotEmbedFontPolicy /Warning /CompatibilityLevel 1.7 /CompressObjects /Off /CompressPages true /ConvertImagesToIndexed true /PassThroughJPEGImages true /CreateJobTicket false /DefaultRenderingIntent /Default /DetectBlends true /DetectCurves 0.0000 /ColorConversionStrategy /LeaveColorUnchanged /DoThumbnails true /EmbedAllFonts true /EmbedOpenType false /ParseICCProfilesInComments true /EmbedJobOptions true /DSCReportingLevel 0 /EmitDSCWarnings false /EndPage -1 /ImageMemory 1048576 /LockDistillerParams true /MaxSubsetPct 100 /Optimize true /OPM 0 /ParseDSCComments false /ParseDSCCommentsForDocInfo false /PreserveCopyPage true /PreserveDICMYKValues true /PreserveEPSInfo false /PreserveFlatness true /PreserveHalftoneInfo true /PreserveOPIComments false /PreserveOverprintSettings true /StartPage 1 /SubsetFonts true /TransferFunctionInfo /Remove /UCRandBGInfo /Preserve /UsePrologue false /ColorSettingsFile () /AlwaysEmbed [ true /AbadiMT-CondensedLight /ACaslon-Italic /ACaslon-Regular /ACaslon-Semibold /ACaslon-SemiboldItalic /AdobeArabic-Bold /AdobeArabic-BoldItalic /AdobeArabic-Italic /AdobeArabic-Regular /AdobeHebrew-Bold /AdobeHebrew-BoldItalic /AdobeHebrew-Italic /AdobeHebrew-Regular /AdobeHeitiStd-Regular /AdobeMingStd-Light /AdobeMyungjoStd-Medium /AdobePiStd /AdobeSansMM /AdobeSerifMM /AdobeSongStd-Light /AdobeThai-Bold /AdobeThai-BoldItalic /AdobeThai-Italic /AdobeThai-Regular /AGaramond-Bold /AGaramond-BoldItalic /AGaramond-Italic /AGaramond-Regular /AGaramond-Semibold /AGaramond-SemiboldItalic /AgencyFB-Bold /AgencyFB-Reg /AGOldFace-Outline /AharoniBold /Algerian /Americana /Americana-ExtraBold /AndaleMono /AndaleMonoIPA /AngsanaNew /AngsanaNew-Bold /AngsanaNew-BoldItalic /AngsanaNew-Italic /AngsanaUPC /AngsanaUPC-Bold /AngsanaUPC-BoldItalic /AngsanaUPC-Italic /Anna /ArialAlternative /ArialAlternativeSymbol /Arial-Black /Arial-BlackItalic /Arial-BoldItalicMT /Arial-BoldMT /Arial-ItalicMT /ArialMT /ArialMT-Black /ArialNarrow /ArialNarrow-Bold /ArialNarrow-BoldItalic /ArialNarrow-Italic /ArialRoundedMTBold /ArialUnicodeMS /ArrusBT-Bold /ArrusBT-BoldItalic /ArrusBT-Italic /ArrusBT-Roman /AvantGarde-Book /AvantGarde-BookOblique /AvantGarde-Demi /AvantGarde-DemiOblique /AvantGardeITCbyBT-Book /AvantGardeITCbyBT-BookOblique /BakerSignet /BankGothicBT-Medium /Barmeno-Bold /Barmeno-ExtraBold /Barmeno-Medium /Barmeno-Regular /Baskerville /BaskervilleBE-Italic /BaskervilleBE-Medium /BaskervilleBE-MediumItalic /BaskervilleBE-Regular /Baskerville-Bold /Baskerville-BoldItalic /Baskerville-Italic /BaskOldFace /Batang /BatangChe /Bauhaus93 /Bellevue /BellGothicStd-Black /BellGothicStd-Bold /BellGothicStd-Light /BellMT /BellMTBold /BellMTItalic /BerlingAntiqua-Bold /BerlingAntiqua-BoldItalic /BerlingAntiqua-Italic /BerlingAntiqua-Roman /BerlinSansFB-Bold /BerlinSansFBDemi-Bold /BerlinSansFB-Reg /BernardMT-Condensed /BernhardModernBT-Bold /BernhardModernBT-BoldItalic /BernhardModernBT-Italic /BernhardModernBT-Roman /BiffoMT /BinnerD /BinnerGothic /BlackadderITC-Regular /Blackoak /Bodoni /Bodoni-Bold /Bodoni-BoldItalic /Bodoni-Italic /BodoniMT /BodoniMTBlack /BodoniMTBlack-Italic /BodoniMT-Bold /BodoniMT-BoldItalic /BodoniMTCondensed /BodoniMTCondensed-Bold /BodoniMTCondensed-BoldItalic /BodoniMTCondensed-Italic /BodoniMT-Italic /BodoniMTPosterCompressed /Bodoni-Poster /Bodoni-PosterCompressed /BookAntiqua /BookAntiqua-Bold /BookAntiqua-BoldItalic /BookAntiqua-Italic /Bookman-Demi /Bookman-DemiItalic /Bookman-Light /Bookman-LightItalic /BookmanOldStyle /BookmanOldStyle-Bold /BookmanOldStyle-BoldItalic /BookmanOldStyle-Italic /BookshelfSymbolOne-Regular /BookshelfSymbolSeven /BookshelfSymbolThree-Regular /BookshelfSymbolTwo-Regular /Botanical /Boton-Italic /Boton-Medium /Boton-MediumItalic /Boton-Regular /Boulevard /BradleyHandITC /Braggadocio /BritannicBold /Broadway /BrowalliaNew /BrowalliaNew-Bold /BrowalliaNew-BoldItalic /BrowalliaNew-Italic /BrowalliaUPC /BrowalliaUPC-Bold /BrowalliaUPC-BoldItalic /BrowalliaUPC-Italic /BrushScript /BrushScriptMT /CaflischScript-Bold /CaflischScript-Regular /Calibri /Calibri-Bold /Calibri-BoldItalic /Calibri-Italic /CalifornianFB-Bold /CalifornianFB-Italic /CalifornianFB-Reg /CalisMTBol /CalistoMT /CalistoMT-BoldItalic /CalistoMT-Italic /Cambria /Cambria-Bold /Cambria-BoldItalic /Cambria-Italic /CambriaMath /Candara /Candara-Bold /Candara-BoldItalic /Candara-Italic /Carta /CaslonOpenfaceBT-Regular /Castellar /CastellarMT /Centaur /Centaur-Italic /Century /CenturyGothic /CenturyGothic-Bold /CenturyGothic-BoldItalic /CenturyGothic-Italic /CenturySchL-Bold /CenturySchL-BoldItal /CenturySchL-Ital /CenturySchL-Roma /CenturySchoolbook /CenturySchoolbook-Bold /CenturySchoolbook-BoldItalic /CenturySchoolbook-Italic /CGTimes-Bold /CGTimes-BoldItalic /CGTimes-Italic /CGTimes-Regular /CharterBT-Bold /CharterBT-BoldItalic /CharterBT-Italic /CharterBT-Roman /CheltenhamITCbyBT-Bold /CheltenhamITCbyBT-BoldItalic /CheltenhamITCbyBT-Book /CheltenhamITCbyBT-BookItalic /Chiller-Regular /CMB10 /CMBSY10 /CMBSY5 /CMBSY6 /CMBSY7 /CMBSY8 /CMBSY9 /CMBX10 /CMBX12 /CMBX5 /CMBX6 /CMBX7 /CMBX8 /CMBX9 /CMBXSL10 /CMBXTI10 /CMCSC10 /CMCSC8 /CMCSC9 /CMDUNH10 /CMEX10 /CMEX7 /CMEX8 /CMEX9 /CMFF10 /CMFI10 /CMFIB8 /CMINCH /CMITT10 /CMMI10 /CMMI12 /CMMI5 /CMMI6 /CMMI7 /CMMI8 /CMMI9 /CMMIB10 /CMMIB5 /CMMIB6 /CMMIB7 /CMMIB8 /CMMIB9 /CMR10 /CMR12 /CMR17 /CMR5 /CMR6 /CMR7 /CMR8 /CMR9 /CMSL10 /CMSL12 /CMSL8 /CMSL9 /CMSLTT10 /CMSS10 /CMSS12 /CMSS17 /CMSS8 /CMSS9 /CMSSBX10 /CMSSDC10 /CMSSI10 /CMSSI12 /CMSSI17 /CMSSI8 /CMSSI9 /CMSSQ8 /CMSSQI8 /CMSY10 /CMSY5 /CMSY6 /CMSY7 /CMSY8 /CMSY9 /CMTCSC10 /CMTEX10 /CMTEX8 /CMTEX9 /CMTI10 /CMTI12 /CMTI7 /CMTI8 /CMTI9 /CMTT10 /CMTT12 /CMTT8 /CMTT9 /CMU10 /CMVTT10 /ColonnaMT /Colossalis-Bold /ComicSansMS /ComicSansMS-Bold /Consolas /Consolas-Bold /Consolas-BoldItalic /Consolas-Italic /Constantia /Constantia-Bold /Constantia-BoldItalic /Constantia-Italic /CooperBlack /CopperplateGothic-Bold /CopperplateGothic-Light /Copperplate-ThirtyThreeBC /Corbel /Corbel-Bold /Corbel-BoldItalic /Corbel-Italic /CordiaNew /CordiaNew-Bold /CordiaNew-BoldItalic /CordiaNew-Italic /CordiaUPC /CordiaUPC-Bold /CordiaUPC-BoldItalic /CordiaUPC-Italic /Courier /Courier-Bold /Courier-BoldOblique /CourierNewPS-BoldItalicMT /CourierNewPS-BoldMT /CourierNewPS-ItalicMT /CourierNewPSMT /Courier-Oblique /CourierStd /CourierStd-Bold /CourierStd-BoldOblique /CourierStd-Oblique /CourierX-Bold /CourierX-BoldOblique /CourierX-Oblique /CourierX-Regular /CreepyRegular /CurlzMT /David-Bold /David-Reg /DavidTransparent /Desdemona /DilleniaUPC /DilleniaUPCBold /DilleniaUPCBoldItalic /DilleniaUPCItalic /Dingbats /DomCasual /Dotum /DotumChe /EdwardianScriptITC /Elephant-Italic /Elephant-Regular /EngraversGothicBT-Regular /EngraversMT /EraserDust /ErasITC-Bold /ErasITC-Demi /ErasITC-Light /ErasITC-Medium /ErieBlackPSMT /ErieLightPSMT /EriePSMT /EstrangeloEdessa /Euclid /Euclid-Bold /Euclid-BoldItalic /EuclidExtra /EuclidExtra-Bold /EuclidFraktur /EuclidFraktur-Bold /Euclid-Italic /EuclidMathOne /EuclidMathOne-Bold /EuclidMathTwo /EuclidMathTwo-Bold /EuclidSymbol /EuclidSymbol-Bold /EuclidSymbol-BoldItalic /EuclidSymbol-Italic /EucrosiaUPC /EucrosiaUPCBold /EucrosiaUPCBoldItalic /EucrosiaUPCItalic /EUEX10 /EUEX7 /EUEX8 /EUEX9 /EUFB10 /EUFB5 /EUFB7 /EUFM10 /EUFM5 /EUFM7 /EURB10 /EURB5 /EURB7 /EURM10 /EURM5 /EURM7 /EuroMono-Bold /EuroMono-BoldItalic /EuroMono-Italic /EuroMono-Regular /EuroSans-Bold /EuroSans-BoldItalic /EuroSans-Italic /EuroSans-Regular /EuroSerif-Bold /EuroSerif-BoldItalic /EuroSerif-Italic /EuroSerif-Regular /EuroSig /EUSB10 /EUSB5 /EUSB7 /EUSM10 /EUSM5 /EUSM7 /FelixTitlingMT /Fences /FencesPlain /FigaroMT /FixedMiriamTransparent /FootlightMTLight /Formata-Italic /Formata-Medium /Formata-MediumItalic /Formata-Regular /ForteMT /FranklinGothic-Book /FranklinGothic-BookItalic /FranklinGothic-Demi /FranklinGothic-DemiCond /FranklinGothic-DemiItalic /FranklinGothic-Heavy /FranklinGothic-HeavyItalic /FranklinGothicITCbyBT-Book /FranklinGothicITCbyBT-BookItal /FranklinGothicITCbyBT-Demi /FranklinGothicITCbyBT-DemiItal /FranklinGothic-Medium /FranklinGothic-MediumCond /FranklinGothic-MediumItalic /FrankRuehl /FreesiaUPC /FreesiaUPCBold /FreesiaUPCBoldItalic /FreesiaUPCItalic /FreestyleScript-Regular /FrenchScriptMT /Frutiger-Black /Frutiger-BlackCn /Frutiger-BlackItalic /Frutiger-Bold /Frutiger-BoldCn /Frutiger-BoldItalic /Frutiger-Cn /Frutiger-ExtraBlackCn /Frutiger-Italic /Frutiger-Light /Frutiger-LightCn /Frutiger-LightItalic /Frutiger-Roman /Frutiger-UltraBlack /Futura-Bold /Futura-BoldOblique /Futura-Book /Futura-BookOblique /FuturaBT-Bold /FuturaBT-BoldItalic /FuturaBT-Book /FuturaBT-BookItalic /FuturaBT-Medium /FuturaBT-MediumItalic /Futura-Light /Futura-LightOblique /GalliardITCbyBT-Bold /GalliardITCbyBT-BoldItalic /GalliardITCbyBT-Italic /GalliardITCbyBT-Roman /Garamond /Garamond-Bold /Garamond-BoldCondensed /Garamond-BoldCondensedItalic /Garamond-BoldItalic /Garamond-BookCondensed /Garamond-BookCondensedItalic /Garamond-Italic /Garamond-LightCondensed /Garamond-LightCondensedItalic /Gautami /GeometricSlab703BT-Light /GeometricSlab703BT-LightItalic /Georgia /Georgia-Bold /Georgia-BoldItalic /Georgia-Italic /GeorgiaRef /Giddyup /Giddyup-Thangs /Gigi-Regular /GillSans /GillSans-Bold /GillSans-BoldItalic /GillSans-Condensed /GillSans-CondensedBold /GillSans-Italic /GillSans-Light /GillSans-LightItalic /GillSansMT /GillSansMT-Bold /GillSansMT-BoldItalic /GillSansMT-Condensed /GillSansMT-ExtraCondensedBold /GillSansMT-Italic /GillSans-UltraBold /GillSans-UltraBoldCondensed /GloucesterMT-ExtraCondensed /Gothic-Thirteen /GoudyOldStyleBT-Bold /GoudyOldStyleBT-BoldItalic /GoudyOldStyleBT-Italic /GoudyOldStyleBT-Roman /GoudyOldStyleT-Bold /GoudyOldStyleT-Italic /GoudyOldStyleT-Regular /GoudyStout /GoudyTextMT-LombardicCapitals /GSIDefaultSymbols /Gulim /GulimChe /Gungsuh /GungsuhChe /Haettenschweiler /HarlowSolid /Harrington /Helvetica /Helvetica-Black /Helvetica-BlackOblique /Helvetica-Bold /Helvetica-BoldOblique /Helvetica-Condensed /Helvetica-Condensed-Black /Helvetica-Condensed-BlackObl /Helvetica-Condensed-Bold /Helvetica-Condensed-BoldObl /Helvetica-Condensed-Light /Helvetica-Condensed-LightObl /Helvetica-Condensed-Oblique /Helvetica-Fraction /Helvetica-Narrow /Helvetica-Narrow-Bold /Helvetica-Narrow-BoldOblique /Helvetica-Narrow-Oblique /Helvetica-Oblique /HighTowerText-Italic /HighTowerText-Reg /Humanist521BT-BoldCondensed /Humanist521BT-Light /Humanist521BT-LightItalic /Humanist521BT-RomanCondensed /Imago-ExtraBold /Impact /ImprintMT-Shadow /InformalRoman-Regular /IrisUPC /IrisUPCBold /IrisUPCBoldItalic /IrisUPCItalic /Ironwood /ItcEras-Medium /ItcKabel-Bold /ItcKabel-Book /ItcKabel-Demi /ItcKabel-Medium /ItcKabel-Ultra /JasmineUPC /JasmineUPC-Bold /JasmineUPC-BoldItalic /JasmineUPC-Italic /JoannaMT /JoannaMT-Italic /Jokerman-Regular /JuiceITC-Regular /Kartika /Kaufmann /KaufmannBT-Bold /KaufmannBT-Regular /KidTYPEPaint /KinoMT /KodchiangUPC /KodchiangUPC-Bold /KodchiangUPC-BoldItalic /KodchiangUPC-Italic /KorinnaITCbyBT-Regular /KozGoProVI-Medium /KozMinProVI-Regular /KristenITC-Regular /KunstlerScript /Latha /LatinWide /LetterGothic /LetterGothic-Bold /LetterGothic-BoldOblique /LetterGothic-BoldSlanted /LetterGothicMT /LetterGothicMT-Bold /LetterGothicMT-BoldOblique /LetterGothicMT-Oblique /LetterGothic-Slanted /LetterGothicStd /LetterGothicStd-Bold /LetterGothicStd-BoldSlanted /LetterGothicStd-Slanted /LevenimMT /LevenimMTBold /LilyUPC /LilyUPCBold /LilyUPCBoldItalic /LilyUPCItalic /Lithos-Black /Lithos-Regular /LotusWPBox-Roman /LotusWPIcon-Roman /LotusWPIntA-Roman /LotusWPIntB-Roman /LotusWPType-Roman /LucidaBright /LucidaBright-Demi /LucidaBright-DemiItalic /LucidaBright-Italic /LucidaCalligraphy-Italic /LucidaConsole /LucidaFax /LucidaFax-Demi /LucidaFax-DemiItalic /LucidaFax-Italic /LucidaHandwriting-Italic /LucidaSans /LucidaSans-Demi /LucidaSans-DemiItalic /LucidaSans-Italic /LucidaSans-Typewriter /LucidaSans-TypewriterBold /LucidaSans-TypewriterBoldOblique /LucidaSans-TypewriterOblique /LucidaSansUnicode /Lydian /Magneto-Bold /MaiandraGD-Regular /Mangal-Regular /Map-Symbols /MathA /MathB /MathC /Mathematica1 /Mathematica1-Bold /Mathematica1Mono /Mathematica1Mono-Bold /Mathematica2 /Mathematica2-Bold /Mathematica2Mono /Mathematica2Mono-Bold /Mathematica3 /Mathematica3-Bold /Mathematica3Mono /Mathematica3Mono-Bold /Mathematica4 /Mathematica4-Bold /Mathematica4Mono /Mathematica4Mono-Bold /Mathematica5 /Mathematica5-Bold /Mathematica5Mono /Mathematica5Mono-Bold /Mathematica6 /Mathematica6Bold /Mathematica6Mono /Mathematica6MonoBold /Mathematica7 /Mathematica7Bold /Mathematica7Mono /Mathematica7MonoBold /MatisseITC-Regular /MaturaMTScriptCapitals /Mesquite /Mezz-Black /Mezz-Regular /MICR /MicrosoftSansSerif /MingLiU /Minion-BoldCondensed /Minion-BoldCondensedItalic /Minion-Condensed /Minion-CondensedItalic /Minion-Ornaments /MinionPro-Bold /MinionPro-BoldIt /MinionPro-It /MinionPro-Regular /MinionPro-Semibold /MinionPro-SemiboldIt /Miriam /MiriamFixed /MiriamTransparent /Mistral /Modern-Regular /MonotypeCorsiva /MonotypeSorts /MSAM10 /MSAM5 /MSAM6 /MSAM7 /MSAM8 /MSAM9 /MSBM10 /MSBM5 /MSBM6 /MSBM7 /MSBM8 /MSBM9 /MS-Gothic /MSHei /MSLineDrawPSMT /MS-Mincho /MSOutlook /MS-PGothic /MS-PMincho /MSReference1 /MSReference2 /MSReferenceSansSerif /MSReferenceSansSerif-Bold /MSReferenceSansSerif-BoldItalic /MSReferenceSansSerif-Italic /MSReferenceSerif /MSReferenceSerif-Bold /MSReferenceSerif-BoldItalic /MSReferenceSerif-Italic /MSReferenceSpecialty /MSSong /MS-UIGothic /MT-Extra /MT-Symbol /MT-Symbol-Italic /MVBoli /Myriad-Bold /Myriad-BoldItalic /Myriad-Italic /MyriadPro-Black /MyriadPro-BlackIt /MyriadPro-Bold /MyriadPro-BoldIt /MyriadPro-It /MyriadPro-Light /MyriadPro-LightIt /MyriadPro-Regular /MyriadPro-Semibold /MyriadPro-SemiboldIt /Myriad-Roman /Narkisim /NewCenturySchlbk-Bold /NewCenturySchlbk-BoldItalic /NewCenturySchlbk-Italic /NewCenturySchlbk-Roman /NewMilleniumSchlbk-BoldItalicSH /NewsGothic /NewsGothic-Bold /NewsGothicBT-Bold /NewsGothicBT-BoldItalic /NewsGothicBT-Italic /NewsGothicBT-Roman /NewsGothic-Condensed /NewsGothic-Italic /NewsGothicMT /NewsGothicMT-Bold /NewsGothicMT-Italic /NiagaraEngraved-Reg /NiagaraSolid-Reg /NimbusMonL-Bold /NimbusMonL-BoldObli /NimbusMonL-Regu /NimbusMonL-ReguObli /NimbusRomNo9L-Medi /NimbusRomNo9L-MediItal /NimbusRomNo9L-Regu /NimbusRomNo9L-ReguItal /NimbusSanL-Bold /NimbusSanL-BoldCond /NimbusSanL-BoldCondItal /NimbusSanL-BoldItal /NimbusSanL-Regu /NimbusSanL-ReguCond /NimbusSanL-ReguCondItal /NimbusSanL-ReguItal /Nimrod /Nimrod-Bold /Nimrod-BoldItalic /Nimrod-Italic /NSimSun /Nueva-BoldExtended /Nueva-BoldExtendedItalic /Nueva-Italic /Nueva-Roman /NuptialScript /OCRA /OCRA-Alternate /OCRAExtended /OCRB /OCRB-Alternate /OfficinaSans-Bold /OfficinaSans-BoldItalic /OfficinaSans-Book /OfficinaSans-BookItalic /OfficinaSerif-Bold /OfficinaSerif-BoldItalic /OfficinaSerif-Book /OfficinaSerif-BookItalic /OldEnglishTextMT /Onyx /OnyxBT-Regular /OzHandicraftBT-Roman /PalaceScriptMT /Palatino-Bold /Palatino-BoldItalic /Palatino-Italic /PalatinoLinotype-Bold /PalatinoLinotype-BoldItalic /PalatinoLinotype-Italic /PalatinoLinotype-Roman /Palatino-Roman /PapyrusPlain /Papyrus-Regular /Parchment-Regular /Parisian /ParkAvenue /Penumbra-SemiboldFlare /Penumbra-SemiboldSans /Penumbra-SemiboldSerif /PepitaMT /Perpetua /Perpetua-Bold /Perpetua-BoldItalic /Perpetua-Italic /PerpetuaTitlingMT-Bold /PerpetuaTitlingMT-Light /PhotinaCasualBlack /Playbill /PMingLiU /Poetica-SuppOrnaments /PoorRichard-Regular /PopplLaudatio-Italic /PopplLaudatio-Medium /PopplLaudatio-MediumItalic /PopplLaudatio-Regular /PrestigeElite /Pristina-Regular /PTBarnumBT-Regular /Raavi /RageItalic /Ravie /RefSpecialty /Ribbon131BT-Bold /Rockwell /Rockwell-Bold /Rockwell-BoldItalic /Rockwell-Condensed /Rockwell-CondensedBold /Rockwell-ExtraBold /Rockwell-Italic /Rockwell-Light /Rockwell-LightItalic /Rod /RodTransparent /RunicMT-Condensed /Sanvito-Light /Sanvito-Roman /ScriptC /ScriptMTBold /SegoeUI /SegoeUI-Bold /SegoeUI-BoldItalic /SegoeUI-Italic /Serpentine-BoldOblique /ShelleyVolanteBT-Regular /ShowcardGothic-Reg /Shruti /SimHei /SimSun /SnapITC-Regular /StandardSymL /Stencil /StoneSans /StoneSans-Bold /StoneSans-BoldItalic /StoneSans-Italic /StoneSans-Semibold /StoneSans-SemiboldItalic /Stop /Swiss721BT-BlackExtended /Sylfaen /Symbol /SymbolMT /Tahoma /Tahoma-Bold /Tci1 /Tci1Bold /Tci1BoldItalic /Tci1Italic /Tci2 /Tci2Bold /Tci2BoldItalic /Tci2Italic /Tci3 /Tci3Bold /Tci3BoldItalic /Tci3Italic /Tci4 /Tci4Bold /Tci4BoldItalic /Tci4Italic /TechnicalItalic /TechnicalPlain /Tekton /Tekton-Bold /TektonMM /Tempo-HeavyCondensed /Tempo-HeavyCondensedItalic /TempusSansITC /Times-Bold /Times-BoldItalic /Times-BoldItalicOsF /Times-BoldSC /Times-ExtraBold /Times-Italic /Times-ItalicOsF /TimesNewRomanMT-ExtraBold /TimesNewRomanPS-BoldItalicMT /TimesNewRomanPS-BoldMT /TimesNewRomanPS-ItalicMT /TimesNewRomanPSMT /Times-Roman /Times-RomanSC /Trajan-Bold /Trebuchet-BoldItalic /TrebuchetMS /TrebuchetMS-Bold /TrebuchetMS-Italic /Tunga-Regular /TwCenMT-Bold /TwCenMT-BoldItalic /TwCenMT-Condensed /TwCenMT-CondensedBold /TwCenMT-CondensedExtraBold /TwCenMT-CondensedMedium /TwCenMT-Italic /TwCenMT-Regular /Univers-Bold /Univers-BoldItalic /UniversCondensed-Bold /UniversCondensed-BoldItalic /UniversCondensed-Medium /UniversCondensed-MediumItalic /Univers-Medium /Univers-MediumItalic /URWBookmanL-DemiBold /URWBookmanL-DemiBoldItal /URWBookmanL-Ligh /URWBookmanL-LighItal /URWChanceryL-MediItal /URWGothicL-Book /URWGothicL-BookObli /URWGothicL-Demi /URWGothicL-DemiObli /URWPalladioL-Bold /URWPalladioL-BoldItal /URWPalladioL-Ital /URWPalladioL-Roma /USPSBarCode /VAGRounded-Black /VAGRounded-Bold /VAGRounded-Light /VAGRounded-Thin /Verdana /Verdana-Bold /Verdana-BoldItalic /Verdana-Italic /VerdanaRef /VinerHandITC /Viva-BoldExtraExtended /Vivaldii /Viva-LightCondensed /Viva-Regular /VladimirScript /Vrinda /Webdings /Westminster /Willow /Wingdings2 /Wingdings3 /Wingdings-Regular /WNCYB10 /WNCYI10 /WNCYR10 /WNCYSC10 /WNCYSS10 /WoodtypeOrnaments-One /WoodtypeOrnaments-Two /WP-ArabicScriptSihafa /WP-ArabicSihafa /WP-BoxDrawing /WP-CyrillicA /WP-CyrillicB /WP-GreekCentury /WP-GreekCourier /WP-GreekHelve /WP-HebrewDavid /WP-IconicSymbolsA /WP-IconicSymbolsB /WP-Japanese /WP-MathA /WP-MathB /WP-MathExtendedA /WP-MathExtendedB /WP-MultinationalAHelve /WP-MultinationalARoman /WP-MultinationalBCourier /WP-MultinationalBHelve /WP-MultinationalBRoman /WP-MultinationalCourier /WP-Phonetic /WPTypographicSymbols /XYATIP10 /XYBSQL10 /XYBTIP10 /XYCIRC10 /XYCMAT10 /XYCMBT10 /XYDASH10 /XYEUAT10 /XYEUBT10 /ZapfChancery-MediumItalic /ZapfDingbats /ZapfHumanist601BT-Bold /ZapfHumanist601BT-BoldItalic /ZapfHumanist601BT-Demi /ZapfHumanist601BT-DemiItalic /ZapfHumanist601BT-Italic /ZapfHumanist601BT-Roman /ZWAdobeF ] /NeverEmbed [ true ] /AntiAliasColorImages false /CropColorImages true /ColorImageMinResolution 200 /ColorImageMinResolutionPolicy /OK /DownsampleColorImages true /ColorImageDownsampleType /Bicubic /ColorImageResolution 300 /ColorImageDepth -1 /ColorImageMinDownsampleDepth 1 /ColorImageDownsampleThreshold 2.00333 /EncodeColorImages true /ColorImageFilter /DCTEncode /AutoFilterColorImages true /ColorImageAutoFilterStrategy /JPEG /ColorACSImageDict << /QFactor 0.76 /HSamples [2 1 1 2] /VSamples [2 1 1 2] >> /ColorImageDict << /QFactor 1.30 /HSamples [2 1 1 2] /VSamples [2 1 1 2] >> /JPEG2000ColorACSImageDict << /TileWidth 256 /TileHeight 256 /Quality 10 >> /JPEG2000ColorImageDict << /TileWidth 256 /TileHeight 256 /Quality 10 >> /AntiAliasGrayImages false /CropGrayImages true /GrayImageMinResolution 200 /GrayImageMinResolutionPolicy /OK /DownsampleGrayImages true /GrayImageDownsampleType /Bicubic /GrayImageResolution 300 /GrayImageDepth -1 /GrayImageMinDownsampleDepth 2 /GrayImageDownsampleThreshold 2.00333 /EncodeGrayImages true /GrayImageFilter /DCTEncode /AutoFilterGrayImages true /GrayImageAutoFilterStrategy /JPEG /GrayACSImageDict << /QFactor 0.76 /HSamples [2 1 1 2] /VSamples [2 1 1 2] >> /GrayImageDict << /QFactor 1.30 /HSamples [2 1 1 2] /VSamples [2 1 1 2] >> /JPEG2000GrayACSImageDict << /TileWidth 256 /TileHeight 256 /Quality 10 >> /JPEG2000GrayImageDict << /TileWidth 256 /TileHeight 256 /Quality 10 >> /AntiAliasMonoImages false /CropMonoImages true /MonoImageMinResolution 400 /MonoImageMinResolutionPolicy /OK /DownsampleMonoImages true /MonoImageDownsampleType /Bicubic /MonoImageResolution 600 /MonoImageDepth -1 /MonoImageDownsampleThreshold 1.00167 /EncodeMonoImages true /MonoImageFilter /CCITTFaxEncode /MonoImageDict << /K -1 >> /AllowPSXObjects false /CheckCompliance [ /None ] /PDFX1aCheck false /PDFX3Check false /PDFXCompliantPDFOnly false /PDFXNoTrimBoxError true /PDFXTrimBoxToMediaBoxOffset [ 0.00000 0.00000 0.00000 0.00000 ] /PDFXSetBleedBoxToMediaBox true /PDFXBleedBoxToTrimBoxOffset [ 0.00000 0.00000 0.00000 0.00000 ] /PDFXOutputIntentProfile (None) /PDFXOutputConditionIdentifier () /PDFXOutputCondition () /PDFXRegistryName () /PDFXTrapped /False /CreateJDFFile false /Description << /ARA <FEFF06270633062A062E062F0645002006470630064700200627064406250639062F0627062F0627062A002006440625064606340627062100200648062B062706260642002000410064006F00620065002000500044004600200645062A064806270641064206290020064406440639063106360020063906440649002006270644063406270634062900200648064506460020062E06440627064400200631063306270626064400200627064406280631064A062F002006270644062506440643062A063106480646064A00200648064506460020062E064406270644002006350641062D0627062A0020062706440648064A0628061B0020064A06450643064600200641062A062D00200648062B0627062606420020005000440046002006270644064506460634062306290020062806270633062A062E062F062706450020004100630072006F0062006100740020064800410064006F006200650020005200650061006400650072002006250635062F0627063100200035002E0030002006480627064406250635062F062706310627062A0020062706440623062D062F062B002E> /BGR <FEFF04180437043f043e043b043704320430043904420435002004420435043704380020043d0430044104420440043e0439043a0438002c00200437043000200434043000200441044a0437043404300432043004420435002000410064006f00620065002000500044004600200434043e043a0443043c0435043d04420438002c0020043c0430043a04410438043c0430043b043d043e0020043f044004380433043e04340435043d04380020043704300020043f043e043a0430043704320430043d04350020043d043000200435043a04400430043d0430002c00200435043b0435043a04420440043e043d043d04300020043f043e044904300020043800200418043d044204350440043d04350442002e002000200421044a04370434043004340435043d043804420435002000500044004600200434043e043a0443043c0435043d044204380020043c043e0433043004420020043404300020044104350020043e0442043204300440044f0442002004410020004100630072006f00620061007400200438002000410064006f00620065002000520065006100640065007200200035002e00300020043800200441043b0435043404320430044904380020043204350440044104380438002e> /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e5c4f5e55663e793a3001901a8fc775355b5090ae4ef653d190014ee553ca901a8fc756e072797f5153d15e03300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002> /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc87a25e55986f793a3001901a904e96fb5b5090f54ef650b390014ee553ca57287db2969b7db28def4e0a767c5e03300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002> /CZE <FEFF005400610074006f0020006e006100730074006100760065006e00ed00200070006f0075017e0069006a007400650020006b0020007600790074007600e101590065006e00ed00200064006f006b0075006d0065006e0074016f002000410064006f006200650020005000440046002c0020006b00740065007200e90020007300650020006e0065006a006c00e90070006500200068006f006400ed002000700072006f0020007a006f006200720061007a006f007600e1006e00ed0020006e00610020006f006200720061007a006f007600630065002c00200070006f007300ed006c00e1006e00ed00200065002d006d00610069006c0065006d00200061002000700072006f00200069006e007400650072006e00650074002e002000200056007900740076006f01590065006e00e900200064006f006b0075006d0065006e007400790020005000440046002000620075006400650020006d006f017e006e00e90020006f007400650076015900ed007400200076002000700072006f006700720061006d0065006300680020004100630072006f00620061007400200061002000410064006f00620065002000520065006100640065007200200035002e0030002000610020006e006f0076011b006a016100ed00630068002e> /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000620065006400730074002000650067006e006500720020007300690067002000740069006c00200073006b00e60072006d007600690073006e0069006e0067002c00200065002d006d00610069006c0020006f006700200069006e007400650072006e00650074002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e> /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200064006900650020006600fc00720020006400690065002000420069006c006400730063006800690072006d0061006e007a0065006900670065002c00200045002d004d00610069006c0020006f006400650072002000640061007300200049006e007400650072006e00650074002000760065007200770065006e006400650074002000770065007200640065006e00200073006f006c006c0065006e002e002000450072007300740065006c006c007400650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000410064006f00620065002000520065006100640065007200200035002e00300020006f0064006500720020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e> /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f00730020005000440046002000640065002000410064006f0062006500200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e00200065006e002000700061006e00740061006c006c0061002c00200063006f007200720065006f00200065006c006500630074007200f3006e00690063006f0020006500200049006e007400650072006e00650074002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e> /ETI <FEFF004b00610073007500740061006700650020006e0065006900640020007300e400740074006500690064002000730065006c006c0069007300740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740069006400650020006c006f006f006d006900730065006b0073002c0020006d0069007300200073006f006200690076006100640020006b00f500690067006500200070006100720065006d0069006e006900200065006b007200610061006e0069006c0020006b007500760061006d006900730065006b0073002c00200065002d0070006f0073007400690067006100200073006100610074006d006900730065006b00730020006a006100200049006e007400650072006e00650074006900730020006100760061006c00640061006d006900730065006b0073002e00200020004c006f006f0064007500640020005000440046002d0064006f006b0075006d0065006e00740065002000730061006100740065002000610076006100640061002000700072006f006700720061006d006d006900640065006700610020004100630072006f0062006100740020006e0069006e0067002000410064006f00620065002000520065006100640065007200200035002e00300020006a00610020007500750065006d006100740065002000760065007200730069006f006f006e00690064006500670061002e> /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000640065007300740069006e00e90073002000e000200049006e007400650072006e00650074002c002000e0002000ea007400720065002000610066006600690063006800e90073002000e00020006c002700e9006300720061006e002000650074002000e0002000ea00740072006500200065006e0076006f007900e9007300200070006100720020006d006500730073006100670065007200690065002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e> /GRE <FEFF03a703c103b703c303b903bc03bf03c003bf03b903ae03c303c403b5002003b103c503c403ad03c2002003c403b903c2002003c103c503b803bc03af03c303b503b903c2002003b303b903b1002003bd03b1002003b403b703bc03b903bf03c503c103b303ae03c303b503c403b5002003ad03b303b303c103b103c603b1002000410064006f006200650020005000440046002003c003bf03c5002003b503af03bd03b103b9002003ba03b103c42019002003b503be03bf03c703ae03bd002003ba03b103c403ac03bb03bb03b703bb03b1002003b303b903b1002003c003b103c103bf03c503c303af03b103c303b7002003c303c403b703bd002003bf03b803cc03bd03b7002c002003b303b903b100200065002d006d00610069006c002c002003ba03b103b9002003b303b903b1002003c403bf0020039403b903b1002d03b403af03ba03c403c503bf002e0020002003a403b10020005000440046002003ad03b303b303c103b103c603b1002003c003bf03c5002003ad03c703b503c403b5002003b403b703bc03b903bf03c503c103b303ae03c303b503b9002003bc03c003bf03c103bf03cd03bd002003bd03b1002003b103bd03bf03b903c703c403bf03cd03bd002003bc03b5002003c403bf0020004100630072006f006200610074002c002003c403bf002000410064006f00620065002000520065006100640065007200200035002e0030002003ba03b103b9002003bc03b503c403b103b303b503bd03ad03c303c403b503c103b503c2002003b503ba03b403cc03c303b503b903c2002e> /HEB <FEFF05D405E905EA05DE05E905D5002005D105D405D205D305E805D505EA002005D005DC05D4002005DB05D305D9002005DC05D905E605D505E8002005DE05E105DE05DB05D9002000410064006F006200650020005000440046002005D405DE05D505EA05D005DE05D905DD002005DC05EA05E605D505D205EA002005DE05E105DA002C002005D305D505D005E8002005D005DC05E705D805E805D505E005D9002005D505D405D005D905E005D805E805E005D8002E002005DE05E105DE05DB05D90020005000440046002005E905E005D505E605E805D5002005E005D905EA05E005D905DD002005DC05E405EA05D905D705D4002005D105D005DE05E605E205D505EA0020004100630072006F006200610074002005D5002D00410064006F00620065002000520065006100640065007200200035002E0030002005D505D205E805E105D005D505EA002005DE05EA05E705D305DE05D505EA002005D905D505EA05E8002E002D0033002C002005E205D905D905E005D5002005D105DE05D305E805D905DA002005DC05DE05E905EA05DE05E9002005E905DC0020004100630072006F006200610074002E002005DE05E105DE05DB05D90020005000440046002005E905E005D505E605E805D5002005E005D905EA05E005D905DD002005DC05E405EA05D905D705D4002005D105D005DE05E605E205D505EA0020004100630072006F006200610074002005D5002D00410064006F00620065002000520065006100640065007200200035002E0030002005D505D205E805E105D005D505EA002005DE05EA05E705D305DE05D505EA002005D905D505EA05E8002E> /HRV <FEFF005a00610020007300740076006100720061006e006a0065002000500044004600200064006f006b0075006d0065006e0061007400610020006e0061006a0070006f0067006f0064006e0069006a006900680020007a00610020007000720069006b0061007a0020006e00610020007a00610073006c006f006e0075002c00200065002d0070006f0161007400690020006900200049006e007400650072006e0065007400750020006b006f00720069007300740069007400650020006f0076006500200070006f0073007400610076006b0065002e00200020005300740076006f00720065006e0069002000500044004600200064006f006b0075006d0065006e007400690020006d006f006700750020007300650020006f00740076006f00720069007400690020004100630072006f00620061007400200069002000410064006f00620065002000520065006100640065007200200035002e0030002000690020006b00610073006e0069006a0069006d0020007600650072007a0069006a0061006d0061002e> /HUN <FEFF00410020006b00e9007000650072006e00790151006e0020006d00650067006a0065006c0065006e00ed007400e9007300680065007a002c00200065002d006d00610069006c002000fc007a0065006e006500740065006b00620065006e002000e90073002000200049006e007400650072006e006500740065006e0020006800610073007a006e00e1006c00610074006e0061006b0020006c006500670069006e006b00e1006200620020006d0065006700660065006c0065006c0151002000410064006f00620065002000500044004600200064006f006b0075006d0065006e00740075006d006f006b0061007400200065007a0065006b006b0065006c0020006100200062006500e1006c006c00ed007400e10073006f006b006b0061006c0020006b00e90073007a00ed0074006800650074002e0020002000410020006c00e90074007200650068006f007a006f00740074002000500044004600200064006f006b0075006d0065006e00740075006d006f006b00200061007a0020004100630072006f006200610074002000e9007300200061007a002000410064006f00620065002000520065006100640065007200200035002e0030002c0020007600610067007900200061007a002000610074007400f3006c0020006b00e9007301510062006200690020007600650072007a006900f3006b006b0061006c0020006e00790069007400680061007400f3006b0020006d00650067002e> /ITA <FEFF005500740069006c0069007a007a006100720065002000710075006500730074006500200069006d0070006f007300740061007a0069006f006e00690020007000650072002000630072006500610072006500200064006f00630075006d0065006e00740069002000410064006f00620065002000500044004600200070006900f9002000610064006100740074006900200070006500720020006c0061002000760069007300750061006c0069007a007a0061007a0069006f006e0065002000730075002000730063006800650072006d006f002c0020006c006100200070006f00730074006100200065006c0065007400740072006f006e0069006300610020006500200049006e007400650072006e00650074002e0020004900200064006f00630075006d0065006e007400690020005000440046002000630072006500610074006900200070006f00730073006f006e006f0020006500730073006500720065002000610070006500720074006900200063006f006e0020004100630072006f00620061007400200065002000410064006f00620065002000520065006100640065007200200035002e003000200065002000760065007200730069006f006e006900200073007500630063006500730073006900760065002e> /JPN <FEFF753b97624e0a3067306e8868793a3001307e305f306f96fb5b5030e130fc30eb308430a430f330bf30fc30cd30c330c87d4c7531306790014fe13059308b305f3081306e002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b9069305730663044307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c306a308f305a300130d530a130a430eb30b530a430ba306f67005c0f9650306b306a308a307e30593002> /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020d654ba740020d45cc2dc002c0020c804c7900020ba54c77c002c0020c778d130b137c5d00020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e> /LTH <FEFF004e006100750064006f006b0069007400650020016100690075006f007300200070006100720061006d006500740072007500730020006e006f0072011700640061006d00690020006b0075007200740069002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b00750072006900650020006c0061006200690061007500730069006100690020007000720069007400610069006b00790074006900200072006f006400790074006900200065006b00720061006e0065002c00200065006c002e002000700061016100740075006900200061007200200069006e007400650072006e0065007400750069002e0020002000530075006b0075007200740069002000500044004600200064006f006b0075006d0065006e007400610069002000670061006c006900200062016b007400690020006100740069006400610072006f006d00690020004100630072006f006200610074002000690072002000410064006f00620065002000520065006100640065007200200035002e0030002000610072002000760117006c00650073006e0117006d00690073002000760065007200730069006a006f006d00690073002e> /LVI <FEFF0049007a006d0061006e0074006f006a00690065007400200161006f00730020006900650073007400610074012b006a0075006d00750073002c0020006c0061006900200076006500690064006f00740075002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006100730020006900720020012b00700061016100690020007000690065006d01130072006f007400690020007201010064012b01610061006e0061006900200065006b00720101006e0101002c00200065002d00700061007300740061006d00200075006e00200069006e007400650072006e006500740061006d002e00200049007a0076006500690064006f006a006900650074002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006f002000760061007200200061007400760113007200740020006100720020004100630072006f00620061007400200075006e002000410064006f00620065002000520065006100640065007200200035002e0030002c0020006b0101002000610072012b00200074006f0020006a00610075006e0101006b0101006d002000760065007200730069006a0101006d002e> /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken die zijn geoptimaliseerd voor weergave op een beeldscherm, e-mail en internet. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.) /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d00200065007200200062006500730074002000650067006e0065007400200066006f007200200073006b006a00650072006d007600690073006e0069006e0067002c00200065002d0070006f007300740020006f006700200049006e007400650072006e006500740074002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002000730065006e006500720065002e> /POL <FEFF0055007300740061007700690065006e0069006100200064006f002000740077006f0072007a0065006e0069006100200064006f006b0075006d0065006e007400f300770020005000440046002000700072007a0065007a006e00610063007a006f006e00790063006800200064006f002000770079015b0077006900650074006c0061006e006900610020006e006100200065006b00720061006e00690065002c0020007700790073007901420061006e0069006100200070006f0063007a0074010500200065006c0065006b00740072006f006e00690063007a006e01050020006f00720061007a00200064006c006100200069006e007400650072006e006500740075002e002000200044006f006b0075006d0065006e0074007900200050004400460020006d006f017c006e00610020006f007400770069006500720061010700200077002000700072006f006700720061006d006900650020004100630072006f00620061007400200069002000410064006f00620065002000520065006100640065007200200035002e0030002000690020006e006f00770073007a0079006d002e> /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f0062006500200050004400460020006d00610069007300200061006400650071007500610064006f00730020007000610072006100200065007800690062006900e700e3006f0020006e0061002000740065006c0061002c0020007000610072006100200065002d006d00610069006c007300200065002000700061007200610020006100200049006e007400650072006e00650074002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e> /RUM <FEFF005500740069006c0069007a00610163006900200061006300650073007400650020007300650074010300720069002000700065006e007400720075002000610020006300720065006100200064006f00630075006d0065006e00740065002000410064006f006200650020005000440046002000610064006500630076006100740065002000700065006e0074007200750020006100660069015f006100720065006100200070006500200065006300720061006e002c0020007400720069006d0069007400650072006500610020007000720069006e00200065002d006d00610069006c0020015f0069002000700065006e00740072007500200049006e007400650072006e00650074002e002000200044006f00630075006d0065006e00740065006c00650020005000440046002000630072006500610074006500200070006f00740020006600690020006400650073006300680069007300650020006300750020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e00300020015f00690020007600650072007300690075006e0069006c006500200075006c0074006500720069006f006100720065002e> /RUS <FEFF04180441043f043e043b044c04370443043904420435002004340430043d043d044b04350020043d0430044104420440043e0439043a043800200434043b044f00200441043e043704340430043d0438044f00200434043e043a0443043c0435043d0442043e0432002000410064006f006200650020005000440046002c0020043c0430043a04410438043c0430043b044c043d043e0020043f043e04340445043e0434044f04490438044500200434043b044f0020044d043a04400430043d043d043e0433043e0020043f0440043e0441043c043e044204400430002c0020043f0435044004350441044b043b043a04380020043f043e0020044d043b0435043a04420440043e043d043d043e04390020043f043e044704420435002004380020044004300437043c043504490435043d0438044f0020043200200418043d044204350440043d043504420435002e002000200421043e043704340430043d043d044b04350020005000440046002d0434043e043a0443043c0435043d0442044b0020043c043e0436043d043e0020043e0442043a0440044b043204300442044c002004410020043f043e043c043e0449044c044e0020004100630072006f00620061007400200438002000410064006f00620065002000520065006100640065007200200035002e00300020043800200431043e043b043504350020043f043e04370434043d043804450020043204350440044104380439002e> /SKY <FEFF0054006900650074006f0020006e006100730074006100760065006e0069006100200070006f0075017e0069007400650020006e00610020007600790074007600e100720061006e0069006500200064006f006b0075006d0065006e0074006f0076002000410064006f006200650020005000440046002c0020006b0074006f007200e90020007300610020006e0061006a006c0065007001610069006500200068006f0064006900610020006e00610020007a006f006200720061007a006f00760061006e006900650020006e00610020006f006200720061007a006f0076006b0065002c00200070006f007300690065006c0061006e0069006500200065002d006d00610069006c006f006d002000610020006e006100200049006e007400650072006e00650074002e00200056007900740076006f00720065006e00e900200064006f006b0075006d0065006e007400790020005000440046002000620075006400650020006d006f017e006e00e90020006f00740076006f00720069016500200076002000700072006f006700720061006d006f006300680020004100630072006f00620061007400200061002000410064006f00620065002000520065006100640065007200200035002e0030002000610020006e006f0076016100ed00630068002e> /SLV <FEFF005400650020006e006100730074006100760069007400760065002000750070006f0072006100620069007400650020007a00610020007500730074007600610072006a0061006e006a006500200064006f006b0075006d0065006e0074006f0076002000410064006f006200650020005000440046002c0020006b006900200073006f0020006e0061006a007000720069006d00650072006e0065006a016100690020007a00610020007000720069006b0061007a0020006e00610020007a00610073006c006f006e0075002c00200065002d0070006f01610074006f00200069006e00200069006e007400650072006e00650074002e00200020005500730074007600610072006a0065006e006500200064006f006b0075006d0065006e0074006500200050004400460020006a00650020006d006f0067006f010d00650020006f0064007000720065007400690020007a0020004100630072006f00620061007400200069006e002000410064006f00620065002000520065006100640065007200200035002e003000200069006e0020006e006f00760065006a01610069006d002e> /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f00740020006c00e400680069006e006e00e40020006e00e40079007400f60073007400e40020006c0075006b0065006d0069007300650065006e002c0020007300e40068006b00f60070006f0073007400690069006e0020006a006100200049006e007400650072006e0065007400690069006e0020007400610072006b006f006900740065007400740075006a0061002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e> /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d002000e400720020006c00e4006d0070006c0069006700610020006600f6007200200061007400740020007600690073006100730020007000e500200073006b00e40072006d002c0020006900200065002d0070006f007300740020006f006300680020007000e500200049006e007400650072006e00650074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e> /TUR <FEFF0045006b00720061006e002000fc0073007400fc0020006700f6007200fc006e00fc006d00fc002c00200065002d0070006f00730074006100200076006500200069006e007400650072006e006500740020006900e70069006e00200065006e00200075007900670075006e002000410064006f006200650020005000440046002000620065006c00670065006c0065007200690020006f006c0075015f007400750072006d0061006b0020006900e70069006e00200062007500200061007900610072006c0061007201310020006b0075006c006c0061006e0131006e002e00200020004f006c0075015f0074007500720075006c0061006e0020005000440046002000620065006c00670065006c0065007200690020004100630072006f0062006100740020007600650020004100630072006f006200610074002000520065006100640065007200200035002e003000200076006500200073006f006e0072006100730131006e00640061006b00690020007300fc007200fc006d006c00650072006c00650020006100e70131006c006100620069006c00690072002e> /UKR <FEFF04120438043a043e0440043804410442043e043204430439044204350020044604560020043f043004400430043c043504420440043800200434043b044f0020044104420432043e04400435043d043d044f00200434043e043a0443043c0435043d044204560432002000410064006f006200650020005000440046002c0020044f043a0456043d04300439043a04400430044904350020043f045604340445043e0434044f0442044c00200434043b044f0020043f0435044004350433043b044f043404430020043700200435043a04400430043d044300200442043000200406043d044204350440043d043504420443002e00200020042104420432043e04400435043d045600200434043e043a0443043c0435043d0442043800200050004400460020043c043e0436043d04300020043204560434043a0440043804420438002004430020004100630072006f006200610074002004420430002000410064006f00620065002000520065006100640065007200200035002e0030002004300431043e0020043f04560437043d04560448043e04570020043204350440044104560457002e> /ENU (Use these settings to create Adobe PDF documents best suited for on-screen display, e-mail, and the Internet. Created PDF documents can be opened with Acrobat and Adobe Reader 5.0 and later.) >> /Namespace [ (Adobe) (Common) (1.0) ] /OtherNamespaces [ << /AsReaderSpreads false /CropImagesToFrames true /ErrorControl /WarnAndContinue /FlattenerIgnoreSpreadOverrides false /IncludeGuidesGrids false /IncludeNonPrinting false /IncludeSlug false /Namespace [ (Adobe) (InDesign) (4.0) ] /OmitPlacedBitmaps false /OmitPlacedEPS false /OmitPlacedPDF false /SimulateOverprint /Legacy >> << /AddBleedMarks false /AddColorBars false /AddCropMarks false /AddPageInfo false /AddRegMarks false /ConvertColors /ConvertToRGB /DestinationProfileName (sRGB IEC61966-2.1) /DestinationProfileSelector /UseName /Downsample16BitImages true /FlattenerPreset << /PresetSelector /MediumResolution >> /FormElements false /GenerateStructure false /IncludeBookmarks false /IncludeHyperlinks false /IncludeInteractive false /IncludeLayers false /IncludeProfiles true /MultimediaHandling /UseObjectSettings /Namespace [ (Adobe) (CreativeSuite) (2.0) ] /PDFXOutputIntentProfileSelector /NA /PreserveEditing false /UntaggedCMYKHandling /UseDocumentProfile /UntaggedRGBHandling /UseDocumentProfile /UseDocumentBleed false >> ] >> setdistillerparams << /HWResolution [600 600] /PageSize [612.000 792.000] >> setpagedevice Study of query expansion techniques and their application in the biomedical information retrieval Andreia Rodŕıguez Rivas, Eva Lorenzo Iglesias, and Maŕıa Lourdes Borrajo Diz Computer Science Dept., Univ. of Vigo, Escola Superior de Enxeñeŕıa Informática, Ourense, Spain arrivas@correo.ei.uvigo.es,{eva,lborrajo}@uvigo.es Abstract. Information retrieval focuses on finding those documents whose content matches a user's query from a large collection of documents. Formulate queries which are well designed for retrieval purposes is difficult for most users, suggesting use query expansion to retrieve relevant information. Query expansion techniques are widely used in biomedical literature for improving the efficiency of textual information retrieval systems. This techniques help to overcome vocabulary mismatch issues, such as synonymy between the original query terms and a relevant document, by expanding the original query with additional relevant terms and reweighting the terms in the expanded query. In this paper, a variety query expansion approaches are presented for improving the documents initially retrieved by a query. A corpus belonging to MEDLINE, called Cystic Fibrosis, is used as a knowledge source. The results obtained are similar to the results obtained by other authors. Keywords: Query expansion, Biomedical information retrieval, Lemur, MEDLINE 1 Introduction Biomedical knowledge is growing at a high pace and large collections of publications offer an excellent opportunity for discovering hidden biomedical knowledge by applying information retrieval (IR) and related technologies. Information retrieval (IR) is related with representation, storage, organization and access to information terms. Terms must be represented and organizated in order to allow that user has an easy access to the information of interest (it is not a simple problem to recognize it). The user's requeriments must be presented in a good format so to be translated these information into a query which can be processed by the search engine (or IR systems). This translation is presented like a set of keywords (or index terms) which summarizes the information in which the user is interested [1]. The main goal of a IR system is to retrieve information useful or relevant to the user on a subject. Information Retrieval is not equal to data retrieval that satisfy a query which consists mainly in de-termining what documents in the collection contain the keywords entered in the query, not a subject. Information retrieval using only keywords (as in the case of retrieve data) is not usually very efficient. In general, information about a particular issue can be represented with different keywords, which could not coincide exactly with the terms entered in the query by the user. The user's query can include keywords that are not present in documents, but documents could be relevant because they have another words with the same meaning. Using query expansion(QE) a query is reformulated to improve retrieval perfor-mance obtaining additional relevant documents by expanding the original query with additional relevant terms and reweighting the terms in the expanded query. For this reason, query expansion techniques are widely used in biomedical literature for improving the efficiency of textual information retrieval systems, helping to overcome vocabulary mismatch issues including words in queries with same/related meaning. In this paper, different techniques of QE were analized in order to know whether techniques tested could provide a better or equal retrieval performance than the results shown by other authors. This research aims to improve the searchs based on the classic Boolean model (where documents are retrieved if to a small part of them is related with queries, as a word) performs Pubmed (a free database of references and abstracts on life sciences and biomedical topics) using expansion techniques. PubMed allow accessing primarily the MEDLINE1 database which contains documents with different fields such as MeSH field, where we can find MeSH Headings indexing documents, concepts that describe the main content of documents of great importance in research. 1 MEDLINE http://www.nlm.nih.gov/pubs/factsheets/medline.html http://www.nlm.nih.gov/pubs/factsheets/medline.html The rest of the article is organized as follows: section 2 presents an overview of the query expansion, section 3 describes methods of QE employed in this research for retrieving relevant documents and results obtained comparing them with results obtained by other authors. The article conclude with the conclusions and future work in section 4. 2 Query expansion Information retrieval searching is composed of two main processes, indexing and matching (see Fig. 1). The first step, in order to retrieve documents, is index documents and queries, belonging to a corpus composed of three main elements in information retrieval: documents, queries and relevance judgements given by the experts, for obtaining keywords to be used in the process (these keywords represent relevant words in documents and queries). At this point, is vital to analyze the use of stemming and stopwords lists (for obtaining the keywords that represent documents and queries) in order to reduce related words to their stem, base or root form. This is achieved launching affix removal for adapting different derivational or inflectional variants of the same word to a single indexing form and remove words without significant information to the document. Further, queries can be enhanced by expansion techniques modifying the words they contains such as the use of other keywords that represent them in a manner more consistent with the content of documents (for example MeSH Headings by means of MeSH Browser, a tool that can be selected from the sidebar menu in PubMed and provides users the option to search MeSH for matching concepts to the entered terms or phrases being used in the expansion process to locate MeSH Headings). Matching is the process of computing a measure of similarity between documents and queries by weighting algorithms of terms, being TF-IDF and BM25 the most important. Most retrieval systems return a ranked list of documents in response to a query which are ordered such that the documents the system believes more similar to the query are first on the list. Once obtained the first documents ranked for a query, can be applied expansion techniques. The analysis of previous documents retrieved is through taking into account new keywords representing them to add to initial query in order to reranking documents with the benefits of these new keywords. This process is known as feedback. For the process indexing, mapping or feedback, Lemur Language Modeling Toolkit2 (a software tool designed to facilite research in language modeling and information retrieval(IR) providing methods for parsing query, indexing documents and retrieve documents related to queries using several weiting algorithms) has been used [2, 3]. In order to evaluate results obtained (a set of documents for each query) in the retrieval process is used a program inside the TREC conference trec eval3. This program allows to get several measures like Total number of documents over all queries (Retrieved, Relevant and Rel ret(relevant and retrieved)) or MAP, R-prec and, Interpolated Recall-Precision Averages. These measures are used to compare our results with those of other authors. Details of the comments above have been presented in next sections. 2.1 Terms Definitions When working with a information retrieval system certain key issues in QE can improve the results. Understanding the process of query expansion some concepts related with Corpus of documents, Stem-ming, Stopwords, Use of Acronyms, Okapi BM25 weighting algorithm, TF-IDF weighting algorithm, Blind relevance feedback and Measures are introduced in the next subsections. Corpus of documents An information retrieval test corpus used to analyze the effectiveness of query expansion is composed by three main elements: the set of documents, the textual descriptions of the users queries called topics, and the right answers called relevance judgements given by the experts [4]. The documents represent a sampling of articles published, the format of the data uses a labeled bracketing, the topics are a description in natural language of the information that the user needs, typically one sentence, and finally, 2 Lemur Project http://www.lemurproject.org/ 3 trec eval http://trec.nist.gov/trec_eval/ 2 http://www.lemurproject.org/ http://trec.nist.gov/trec_eval/ Fig. 1. Information Retrieval System. the relevance judgements done by potential users (called experts or judges) in a care-fully controlled experiment, allow to calculate the effectiveness of IR systems. Most of the corpus used in biomedical information retrieval belong to MEDLINE (like those used in the conference TREC, Cystic fibrosis and OHSUMED4), a very large database mantained by the National Library of Medicine (NLM) with about 18 million of abstracts of research papers in medical domain. Each record is structured according to a specific set of fields5. From an IR perspective, the most important fields are the article Title(TI), the Abstract(AB) and the set of manually assigned MeSH Headings(MH) extracted from the MeSH6 Thesaurus. MeSH(Medical Subject Headings) is a extensive list of the controlled vocabulary thesaurus used to indexing journal articles for subject analysis of biomedical literature NLM. It imposes uniformity and consistency to the indexing of biomedical literature. MeSH has a hierarchical structure with sets of terms naming descriptors that permits searching at various levels of specificity. Expert annotators, based on indexed content of documents, assign MeSH Headings terms to the documents in order to allow to the user retrieve the information that explains the same concept with different terminology. On average, 5 to 15 subject headings are assigned by document, 3 to 4 of them being Major Headings and the others being Minor Headings. Major MeSH terms describing the primary/main topics of the document and Minor MeSH terms giving more details/secondary content about it [5–9]. Other databases have not MH field with these keywords. MeSH contains approximately 26 thousand terms and is updated annually to reflect changes in medicine and medical terminology. MeSH Headings often appear together with MeSH Subheadings that are used to help describe more completely a particular aspect of a subject. For example, the drug therapy of asthma is displayed as asthma/drug therapy in MeSH Terms[MH]. Furthermore, each MeSH Heading is related with several Entry terms. Entry terms, are synonyms, alternate forms, and other closely related terms in a given MeSH record that are generally used interchangeably with the MeSH Heading for the purposes of indexing and retrieval, thus increasing the access points to MeSH-indexed data. 4 OHSUMED http://ir.ohsu.edu/ohsumed/ohsumed.html 5 MEDLINE Fields http://www.ncbi.nlm.nih.gov/books/NBK3827/?rendertype=table&id=pubmedhelp.T44 6 MeSH Thesaurus http://www.ncbi.nlm.nih.gov/mesh/meshhome.html 3 http://ir.ohsu.edu/ohsumed/ohsumed.html http://www.ncbi.nlm.nih.gov/books/NBK3827/?rendertype=table&id=pubmedhelp.T44 http://www.ncbi.nlm.nih.gov/mesh/meshhome.html Stemming The aim of stemming (process for reducing related words to their stem, base or root form through affix removal) is to adapt different derivational or inflectional variants of the same word to a single indexing form. Affix removal is language-dependent and creating a new stemmer involves analyzing text resources to produce rules [10,11]. Stemmers have to be adaptables for new languages, incorporating the language-specific morphological rules in order to form words which can be an expensive and time-consuming task. The most stemmers are rule-based and are widely available only for English and other west European languages. There are two major stemmers in use for English IR: the Porter stemmer and the Krovetz stemmer. Porter Stemmer was developed by Martin Porter, at the University of Cambridge in 1980 and was first published in Porter, M.F. [12]. As described above, "The Porter stemming algorithm (or Porter stemmer)" is a process for removing the common morphological and inflexional endings from words in English. It has been very widely used and coded in various programming languages. It is based mainly on stemming operations that remove suffixes from words, such as gerunds, plurals, and replacing words ending. It is composed by rules where each of these deals with a specific suffix and having certain conditions to satisfy. The suffixes of words are checked against each rule in a sequential manner until it matches one, the conditions in the rule are tested and it may result in a suffix removal or modification [13,14]. Krovetz Stemmer was developed by Bob Krovetz, at the University of Massachusetts, in 1993. The Krovetz Stemmer removes inflectional suffixes in three steps, the conversion of a plural to its single form, the conversion of past to present tense, and the removal of -ing. The process firstly removes the suffix, and then through a process of checking in a dictionary returns the stem to a word [10]. Stopwords In information retrieval, a document is indexed by frequency of words in it. Statistical analysis of that process showed that some words have quite low frequency, while others have high frequency [15]. For example, and, of, and the appear frequently in the documents without significant information to the document. This set of words is referred as stop words. Elimination of stop words could significantly reduce the size of the indexing structure, can speed up the calculation and increase the accuracy. Up to now, a lot of stop word lists have been developed for English language for example The U. S. National Library of Medicines official list of stopwords7 and The stopword list built by Gerard Salton and Chris Buckley for the experimental SMART information retrieval system at Cornell University8. Acronyms Acronyms are widely used in biomedical literature. The names of many clinical diseases and procedures, and of common entities such as genes or proteins, have widely used acronyms so it is vital to recognize them in information retrieval. There are a lot of researchs related with acronyms, long forms and their relations [16, 17]. Identify acronyms and long forms is vital in query expansion because work with both forms in queries help to retrieve documents where appear one long form, one acronym or both [18–20]. Okapi BM25 weighting algorithm Okapi BM25 or BM25, is a ranking function used to rank documents according to their relevance to a given search query9. It is based on the probabilistic retrieval. Okapi BM25 function is used by many researchers and different corpus to retrieve relevant documents, such as TREC [21,22] or OSHUMED [23] where: score(d, q) = ∑ T∈Q idf(qi) · (k1 + 1) · dtf K + dtf · (k3 + 1) · qtf k3 + qtf (1) idf(qi) = log N − n(qi) + 0.5 n(qi) + 0.5 (2) – N is the total number of documents in the collection 7 NLM stop-word list http://www.netautopsy.org/umlsstop.htm 8 SMART stop-word list http://www.lextek.com/manuals/onix/stopwords2.html 9 INLS 490-154W: Information Retrieval Systems Design and Implementation http://www.inforetrieval.org/ 2009_fall/inls490_154w/syllabus.php 4 http://www.netautopsy.org/umlsstop.htm http://www.lextek.com/manuals/onix/stopwords2.html http://www.inforetrieval.org/2009_fall/inls490_154w/syllabus.php http://www.inforetrieval.org/2009_fall/inls490_154w/syllabus.php – n(qi) is the number of documents containing qi – Q is a query, containing terms T – K is k1 · ((1− b) + b · dlavdl ) – k1, b, and k3 are parameters which depend on the on the nature of the queries and possibly on the database; k1,and b default to 1.2 and 0.75 respectively, but smaller values of b are sometimes advantageous; in long queries k3 is often set to 7 or 1000 (effectively infinite) – dtf is the frequency of occurrence of the term within a specific document – qtf is the frequency of the term within the topic from which Q was derived – dl and avdl are respectively the document length and average document length measured in some suitable unit. When using feedback the Okapi BM25 formula was replaced by the following relevance weight: score(d, q) = ∑ T∈Q w(1) · (k1 + 1) · dtf K + dtf · (k3 + 1) · qtf k3 + qtf (3) – w(1) is the Robertson Sparck Jones weight [24,25] of T in Q log (R+ 0.5)/(R− r + 0.5) (n− r + 0.5)/(N − n−R+ r + 0.5) (4) • R is the number of documents known to be relevant to a specific topic • r is the number of relevant documents containing the term The values of the parameters k1, k3, and b [26–28] should be adjusted based on the collection and type of queries used, although the commonly default values: k1 and k3 between 1.2 and 2, usually 1.2 but k3 may increase to between 7 and 1000 in the case of long queries, and b = 0.75 although small values can sometimes report improvements. TF-IDF weighting algorithm The TF−IDF weighting algorithm (term frequency−inverse document frequency) is often used in infor-mation retrieval and text mining. This weight is a statistical measure used to evaluate how important a word to a document is in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Vari-ations of the tf−idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query [29,30]. (tf -idf)i,j = tfi,j · idfi (5) tfi,j = ni,j∑ k nk,j (6) Where the numerator is the number of occurrences of the term ti in document dj , and the denomina-tor is the sum of number of occurrences of all terms in document dj , that is, the size of the document |dj |. idfi = log |D| |{d : ti ∈ d}| (7) – |D|: cardinality of D, or the total number of documents in the corpus. – |{d : ti ∈ d}|: number of documents where the term ti appears (that is ni,j 6= 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to use 1 + |{d : ti ∈ d}| A high weight in tf -idf is obtained with an high term frequency in the document and a low document frequency in the whole collection. The weights hence tend to filter out common terms, if a term appears in all documents, his idf will be zero. TF-IDF algorithm as mentioned above have variations used in information retrieval to weight the documents and queries [30–32]. Each document and each query are represented by a term frequency vector −→ d = (x1, x2, · · · , xn) and −→q = (y1, y2, · · · , yn) respectively, where n is the total number of terms, or the size of the vocabulary and xi, yi are the frequency of term ti in d and q respectively. Given a 5 Table 1. tf formulas for TF-IDF weighting algorithm. tfd(xi) = k1·xi xi+k1·(1−b+b· ld lC ) Okapi BM25 TF formulas tfq(yi) = k1·yi yi+k1·(1−b+b· lq lC ) tfd(xi) = xi RawTF formulas tfq(yi) = yi tfd(xi) = log(rawTF + 1) LogTF formulas tfq(yi) = log(rawTF + 1) Table 2. Correspondence between parameters of algorithms. BM25 TF-IDF BM25 TF tfd k1 k1 tfd and tfq b b tfq k3 k1 collection C, the inverse-document-frequency idf = (|C|/nt) where nt is the number of documents with t, the weighted vectors for −→ d and −→q are: −→ d = (tfd(x1) · idf(t1), tfd(x2) · idf(t2), · · · , tfd(xn) · idf(tn)) (8) −→q = (tfq(y1) · idf(t1), tfq(y2) · idf(t2), · · · , tfq(yn) · idf(tn)) (9) In Table 1 appear tf formulas for TF-IDF weighting algorithm. The score of document −→ d against query −→q is s( −→ d ,−→q ) = n∑ i=1 tfd(xi)tfq(yi)idf(ti) 2 (10) Table 2 contains the correspondence between the parameters of BM25 weighting algorithm and TF-IDF weighting algorithm with BM25 TF formulas. User Relevance Feedback Relevance feedback is the most popular query reformulation strategy. Firstly the user selects the most relevant documents in the answer set obtained with the initial query - Only top 10 (or 20) ranked documents need to be examined. The representative terms (or expressions) of the selected documents are included in a new query formulation. The new query will be reformulated towards relevant documents and away from the non-relevant ones. Considers that the term-weight vectors of the documents identified as relevant (to a given query) have similarities among themselves (relevant documents seem each other and non-relevant documents have term-weight vectors which are different from the ones). The idea is reformulate the query such that it gets closer to relevant documents [10,33,34]. Advantages of the relevance feedback: 6 – It shields the user from the details of the query reformulation process because the user only provides a relevance judgement on documents. – It breaks down the whole searching task into a sequence of small steps which are easier to grasp. – It provides a controlled process designed to emphasize some terms (relevant ones) and minimize others (non-relevant ones). The Rocchio feedback approach (see Eq. (11)) is based on the assumption that most users have a general conception of which documents should be denoted as relevant or non-relevant. The motivation is that in practice the original query q may contain important information. Usually, the information contained in the relevant documents is more important than the information provided by non-relevant documents. This suggests making the constant γ which represents weight attached to the set of known non-relevant documents smaller than the constant β which represents weight attached to the set of known relevant documents. An alternative approach is to set γ to 0 which yields a positive feedback strategy. Weights are increased or decreased for a particular category of documents, the coordinates for the modified vector begin to move either closer, or farther away, from the centroid of the document collection. Thus if the weight is increased for relevant documents, then the modified vectors coordinate's will reflect being closer to the centroid of relevant documents. Standard Rocchio −→q m = α−→q + β |Dr| · ∑ ∀ −→ d j∈Dr −→ d j − γ |Dn| · ∑ ∀ −→ d j∈Dn −→ d j (11) where: Dr: set of relevant documents identified by the user among the retrieved documents Dn: set of non-relevant documents among the retrieved documents Cr: set of relevant documents among all documents in the collection Rd: set of retrieved documents by the user |Dr|, |Dn|, |Cr|: number of documents in the sets Dr, Dn, and Cr α,β,γ: tuning constants (The number of relevant and non-relevant documents allowed to enter a query) α: Original Query Weight β: Relevant Documents Weight γ: Non-Relevant Documents Weight The main advantages of the above relevance feedback techniques are simplicity (modified term weights are computed directly from the set of retrieved documents) and good results (are observed experimentally, the modified query vector does reflect a portion of the intended query semantics). The main disadvantage is that no optimality criterion is adopted. Measures In order to evaluate results exist one program which is called trec eval10 which allow to get several measures related with information retrieval [35]. The most commonly used are the following: Precision at 11 standard recall levels In IR systems the precision averages at 11 standard recall levels are used to compare the performance of different systems in a recall-precision graph. Each recall-precision average is computed by summing the interpolated precisions at the specified recall cutoff value (denoted by Pλ where Pλ is the interpolated precision at recall level λ) and then dividing by the number of topics.∑|Q| i=1 Pλ |Q| λ = {0.0, 0.1, 0.2, 0.3, · · · , 1.0} (12) 10 trec eval http://trec.nist.gov/trec_eval/ and http://trec.nist.gov/pubs/trec15/appendices/CE. MEASURES06.pdf 7 http://trec.nist.gov/trec_eval/ http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf Average Precision For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. Average precision emphasizes ranking relevant documents higher. It is the average of precisions computed at the point of each of the relevant documents in the ranked sequence: AveP = ∑N r=1(P (r)× rel(r)) Cr (13) r is the rank N the number retrieved rel(r) a binary function on the relevance of a given rank P (r) precision at a given cut-off rank P (r) = |{Dr|<=r}| r (14) Mean Average precision Is the mean of the average precision scores for each query. MAP = ∑Q q=1AveP (q) |Q| (15) R-precision R-Precision is the precision after R documents have been retrieved, where R is the number of relevant documents for the topic. 3 Methodology and Results In this section an outline is presented in accordance with the methodology of the process of expansion of this research and the results are shown (see Fig. 4). The corpus used for testing was Cystic Fibrosis Corpus11(CF) in our study. It consists of 1239 documents published from 1974 to 1979 discussing Cystic Fibrosis Aspects in the National Library of Medicine's Medline composed by Abstract, Title and MeSH fields between others (see Fig. 2), and a set of 100 queries with the respective relevant documents as answers belong to experts [1](see Fig. 3). Fig. 2. A sample of MEDLINE data Information Retrieval process has been divided in two: the expansion in Lemur and the expansion outside Lemur. Once arrived at a particular point of evidence, and given that Lemur is a closed tool, we decided, to perform weighting tasks with MeSH terms in documents giving more weight to the Major MeSH Headings that the Minor, to work outside of it. The tests performed within Lemur are based on an analysis of the benefits produced by the stemming and stopwords in the index of documents and queries and the benefits of using acronyms in the collection. To recognize acronyms is vital in order to 11 Cystic fibrosis Collection http://grupoweb.upf.es/WRG/mir2ed/ref.php 8 http://grupoweb.upf.es/WRG/mir2ed/ref.php Fig. 3. A sample of Query and its relevant documents by experts not be affected by stemming and stopwords processes and thus make a direct mapping between them in documents and queries. Expanding acronyms in queries could be obtained advantages to locate them in documents, in addition to own acronym, the long forms of the same. Parameterize weighting algorithms is an important point to get results commensurate with the collection used, for this, this was the next test in our process. Following previous studies by other authors, has been also tested the benefit of working with all fields of the documents compared to only work with one of them (so far our tests were focused on the Abstract). To improve the results of the above processes, we proceed to make relevance feedback. MeSH field in the documents is considered one of the most important fields in retrieving documents that are appropriate to the queries, so firstly, we have made tests against the MeSH field to see what results were obtained (with the queries and the expanded queries leaving only the MeSH Headings). We have explored also the possibility of feedback the Abstract results with the list of retrieved documents by searching in the MeSH field. To improve the results of the previous step, as mentioned above, process has been taken out of Lemur to analyze the possibility of giving more weight to the Major MeSH Headings that the Minor, within documents. The results have been combined with those obtained previously by the Abstract field to see how to improve them. Finally it has also been combined with the results obtained by the Title field to achieve the final result. Fig. 4. Steps into Methodology. 9 Query expansion in Lemur The expansion process is explained below performed within Lemur and then continue with subsequent steps out of the tool. Stemming and Stopwords Based on query expansion terms definitions discussed in section 2, we have analyzed the impact of stemming algorithms (Porter and Krovetz) and lists of stopwords (NLM and SMART) in the retrieving of documents for queries of corpus Cystic fibrosis. This test is made in order to obtain the improvements using stemming and stopwords lists. Okapi BM25 weighting algorithm was used with basic parameters (k1=1.2, b=0.75, k3=7) using Abstract field in order to see improvement [21,22,36]. Table 3. Improvements using different techniques combined of stemming and stopwords. ```````````Combinations Measures MAP R-prec Dr Baseline 0.1545 0.2098 683 Porter 0.1663 0.2154 747 Krovetz 0.1663 0.2231 740 Stopwords NLM 0.1681 0.2242 723 Stopwords SMART 0.1695 0.2243 728 Porter-Stopwords NLM 0.1790 0.2332 786 Porter-Stopwords SMART 0.1808 0.2342 790 Krovetz-Stopwords NLM 0.1799 0.2333 782 Krovetz-Stopwords SMART 0.1808 0.2324 786 Table 3 shows a comparison of stemming functions, firstly. We can see that stemming is an effective technique to improve MAP. Normally between weak (Krovetz) and strong (Porter) stemming methods, the performances are comparable although in our case by itself does not provide information because re-sults are the same. In terms of MAP, strong stemming is a bit better. In terms of R-prec, weak stemming is a bit better, as observed for R-prec in Porter and Krovetz in the table. If compared different stop word removal methods it shows that removing stop words improves the performance. From our experiments, using the stopword list more large (SMART) results are better than using list with less stopwords. We can see how the Porter stemmer with stopword list of SMART gives us the best results. In last four combinations (highlight with bold style), we don't see significative differences so we had conduct tests with these four combinations. Of this test can be deduced why most research groups use Porter stemmer combined with stopwords to index. We also note that the best result is to use a long list of stopwords (SMART). York University at TREC 2006 [36] analyzed how the use of stemming and stop word removal can improve the performance, but the degree of improvement depends on the stemming method and the stop word list used. According to our results obtained but using a different corpus, they concluded that in terms of MAP, strong stemming is a bit better (Porter in our case) but in terms of R-prec, weak stemming (Krovetz) is a bit better, with results in a range of 3% of improvement in MAP. Results obtained with our test had a range of 2% of improvement in our collection. They concluded that the degree of improvement depends on the stop-word list used achieving an improvement in a range of 1% in MAP. From our experiments, using the stop word list that contains few stop terms (NLM in our case) is better than using the larger list that contains a lot of stop words (SMART) achieved a range between 1-2% of improvement in our collection. Combining both options (stemming and stopwords) got a range of 3% of improvement. Acronyms Identify acronyms and long forms is vital in query expansion because work with both forms in queries helps to retrieve documents where appear one long form, one acronym or both [18–20]. Okapi BM25 10 weighting algorithm was used with basic parameters (k1=1.2, b=0.75, k3=7) using Abstract field in order to see improvement [19,21,22]. Table 4. Okapi BM25 with Acronym List. Combinations Without acronyms With acronyms Porter-Stopwords NLM 0.1790 0.1790 Porter-Stopwords SMART 0.1808 0.1808 Krovetz-Stopwords NLM 0.1799 0.1799 Krovetz-Stopwords SMART 0.1808 0.1808 In Table 4 we have worked with the recognition of acronyms in queries and documents so that neither the stemming nor stopwords lists act on them and keeping them intact in documents and queries for exact matching, through a recognition module of acronyms that Lemur12 offers. The table shows that acronyms in this case aren't beneficial. We can deduce that expanding the query using acronyms and their long forms will not be beneficial for this collection. After a study of the collection, we realized that the queries only have an acronym CF (apart of some isolated case) linked to long form cystic fibrosis, and in almost all documents, appears the same acronym. So, being the acronym CF in almost all queries and in almost all documents and being cystic fibrosis the central theme, we will not get benefit since it related a lof of documents to any question. Results are shown below related to this issue in the process out of Lemur. Parameterization of weighting algorithms In general Okapi BM25 with default parameters have been used by researchers for the study of the methods. One of the most important steps, when working with a information retrieval systems, is to choose the appropriate weighting algorithms for ranking documents. Once an investigation and a de-scription of weignting algorithms have been done in section 2 of the two most important, Okapi BM25 and TF-IDF, the parameters for Cystic fibrosis collection have been adjusted, since parameters can vary from collections to other. In Okapi BM25 the values of the parameters k1, k3, and b [26–28] should be adjusted based on the collection and type of queries used, although the commonly default values: k1 and k3 between 1.2 and 2, usually 1.2 but k3 may increase to between 7 and 1000 in the case of long queries, and b = 0.75 although small values can sometimes report improvements. MAP has been used as a measure in order to base our calculations. Table 5. Finding the best k1 parameter for BM25. k1 0 1 2 1.5 1.3 1.2 1.4 b 0.75 0.75 0.75 0.75 0.75 0.75 0.75 ```````````Combinations Parameters k3 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Porter-Stopwords NLM 0.1521 0.1803 0.1792 0.1792 0.1807 0.1798 0.1792 Porter-Stopwords SMART 0.1531 0.1820 0.1783 0.1788 0.1813 0.1815 0.1807 Krovetz-Stopwords NLM 0.1521 0.1797 0.1778 0.1802 0.1804 0.1806 0.1799 Krovetz-Stopwords SMART 0.1557 0.1828 0.1793 0.1801 0.1824 0.1815 0.1819 The best values that we obtained (see Tables 5, 6, 7) were: k1 = 1.3, b = 0.6, and k3 = 1.2. For TF-IDF weighting algorithm with tf formula given by Okapi we found that the parameters ob-tained above were indeed the best approach because it uses the BM25 approximation and its parameters. 12 Acronym list http://www.acronymslist.com/cat/human-genome-acronyms-p3.html 11 http://www.acronymslist.com/cat/human-genome-acronyms-p3.html Table 6. Finding the best b parameter for BM25. k1 1.3 1.3 1.3 1.3 1.3 1.3 1.3 1.3 b 0 1 0.75 0.65 0.6 0.55 0.7 0.6 ```````````Combinations Parameters k3 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Porter-Stopwords NLM 0.1652 0.1715 0.1807 0.1810 0.1824 0.1821 0.1799 0.1822 Porter-Stopwords SMART 0.1669 0.1695 0.1813 0.1816 0.1814 0.1808 0.1817 0.1819 Krovetz-Stopwords NLM 0.1667 0.1706 0.1804 0.1802 0.1819 0.1819 0.1795 0.1821 Krovetz-Stopwords SMART 0.1701 0.1707 0.1824 0.1823 0.1824 0.1821 0.1818 0.1817 Table 7. Finding the best k3 parameter for BM25. k1 1.3 1.3 1.3 1.3 1.3 1.3 1.3 1.3 b 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 ```````````Combinations Parameters k3 0 1 2 7 1000 1.5 1.2 1.3 Porter-Stopwords NLM 0.1824 0.1825 0.1824 0.1817 0.1809 0.1823 0.1824 0.1822 Porter-Stopwords SMART 0.1814 0.1813 0.1813 0.1810 0.1804 0.1814 0.1814 0.1814 Krovetz-Stopwords NLM 0.1811 0.1819 0.1825 0.1815 0.1810 0.1822 0.1819 0.1819 Krovetz-Stopwords SMART 0.1817 0.1822 0.1820 0.1815 0.1815 0.1824 0.1824 0.1824 Moreover, the values obtained with the other two formulas (LogTF and RawTF) for tf without param-eters were studied verifying that they are worse than the approximation BM25 (see Table 8: last two columns). Table 8. TF-IDF BM25 best parameters and results for RawTF and LogTF formulas. k1 1.2 1.3 1.2 b 0.75 0.6 0.7 RawTF Log-TF ```````````Permutations Parameters k3 1000 1.2 1.2 Porter-Stopwords NLM 0.1861 0.1868 0.1861 0.1422 0.1749 Porter-Stopwords SMART 0.1866 0.1898 0.1878 0.1445 0.1742 Krovetz-Stopwords NLM 0.1821 0.1843 0.1827 0.1420 0.1736 Krovetz-Stopwords SMART 0.1828 0.1866 0.1839 0.1436 0.1731 In Tables 7, 8 (highlight columns) we can see that TF-IDF Okapi tf formula was better than others, and so we continued our study with this approximation. Many researchers use the BM25 algorithm in articles conducting their studies to retrieve information in the several fields of the same not only in the Abstract field. By this assumption, we tested how the MAP increases if we look for documents related to the queries in the Abstract, Title and Mesh fields using the BM25, TF-IDF BM25, TF-IDF LogTF and TF-IDF RawTF formulas, and also checked how the values obtained were related to values obtained for Cystic Fibrosis Collection by other authors [36–38] (see Table 9). The results obtained with Okapi BM25 are consistent with those presented by Andrew Tortman in [37, 38] where shows a value of 0.2728 in MAP obtained with BM25 algorithm with Cystic Fibrosis Collection. Relevance Feedback The Lemur toolkit implements a simplified Rocchio feedback algorithm. Let R and R̄ be the relevant document set and non-relevant document set respectively, to be used for feedback. The rocchio algorithm 12 Table 9. Abstract, Title and MeSH fields. ```````````Combinations Algorithms BM25 TF-IDF BM25 LogTF RawTF Porter-Stopwords NLM 0.2717 0.2953 0.2683 0.2209 Porter-Stopwords SMART 0.2733 0.2930 0.2665 0.2221 Krovetz-Stopwords NLM 0.2719 0.2929 0.2684 0.2208 Krovetz-Stopwords SMART 0.2737 0.2904 0.2654 0.2228 simply "moves" the query vector closer to the centroid vector of R and away from the centroid vector of R̄: −→q new = −→q old + α −→ d R − β −→ d R̄ (16) −→ d R is the centroid vector of all weighted document vectors in R−→ d R̄ is the centroid vector of all weighted document vectors in R̄ In Lemur toolkit do not exist R̄ because it only use the centroid of relevant documents (β = 0). It uses a positive feedback algorithm. Parameters for Rocchio: M number of documents in feedback [10-100] K number of terms selected in feedback [10-100] α coefficient adjustment (0,4] Table 10. Finding the best M parameter for TF-IDF BM25 feedback. M 10 5 15 30 13 Baseline K 10 10 10 10 10 ```````````Combinations Parameters α 0.5 0.5 0.5 0.5 0.5 Porter-Stopwords NLM 0.1868 0.2079 0.1991 0.2070 0.2053 0.2035 Porter-Stopwords SMART 0.1898 0.2075 0.2026 0.2053 0.2052 0.2021 Krovetz-Stopwords NLM 0.1843 0.2094 0.1974 0.2007 0.2025 0.2007 Krovetz-Stopwords SMART 0.1866 0.2074 0.1998 0.2022 0.2056 0.2029 Table 11. Finding the best K parameter for TF-IDF BM25 feedback. M 10 10 10 10 10 10 K 10 20 30 40 25 28 ```````````Combinations Parameters α 0.5 0.5 0.5 0.5 0.5 0.5 Porter-Stopwords NLM 0.2079 0.2103 0.2117 0.2102 0.2113 0.2116 Porter-Stopwords SMART 0.2075 0.2083 0.2105 0.2087 0.2055 0.2100 Krovetz-Stopwords NLM 0.2094 0.2068 0.2075 0.2068 0.2077 0.2093 Krovetz-Stopwords SMART 0.2074 0.2061 0.2015 0.2032 0.2050 0.2033 The best values obtained for Abstract Relevance feedback (see Tables 10, 11, 12) were: M = 10, K = 28, and α = 0.5 obtaining a MAP value between 0.20 and 0.21. We tested how the MAP increases if we look for documents related to the queries in the Abstract, Title and Mesh fields using the TF-IDF BM25 13 Table 12. Finding the best α parameter for TF-IDF BM25 feedback. M 10 10 10 10 10 7 K 28 28 28 28 28 30 ```````````Combinations Parameters α 0.5 0.1 1 0.9 0.4 0.1 Porter-Stopwords NLM 0.2116 0.1957 0.2103 0.2099 0.2105 0.1972 Porter-Stopwords SMART 0.2100 0.1956 0.2035 0.2091 0.2093 0.1956 Krovetz-Stopwords NLM 0.2093 0.1904 0.2072 0.2097 0.2089 0.1907 Krovetz-Stopwords SMART 0.2033 0.1932 0.2024 0.2031 0.2061 0.1918 relevance feedback algorithm(see Table 13) obtaining a MAP value between 0.33 and 0.35. These results can be compared with those obtained by K. Shin and S.Y. Han in their expansion system presented in [39] where achieved a maximum value of 0.35 for R-prec, being our R-Prec of 0.37 or 0.38 in some case. Table 13. TF-IDF BM25 relevance feedback best parameters for Abstract, Title and MeSH fields. ```````````Combinations Measures MAP R-Prec Porter-Stopwords NLM 0.3468 0.3780 Porter-Stopwords SMART 0.3391 0.3731 Krovetz-Stopwords NLM 0.3475 0.3834 Krovetz-Stopwords SMART 0.3435 0.3790 MeSH Terms in Information Retrieval combined with Abstract Many authors have worked with MeSH and abstract fields to retrieve information, so we focused this part of our research to improve the expansion obtained by the abstract with the MeSH field [39]. As mentioned before, MeSH Terms index journal articles for subject analysis of biomedical literature NLM. It imposes uniformity and consistency to the indexing of biomedical literature. MeSH has a hierarchical structure with sets of terms naming descriptors that allow searching at various levels of specificity. Expert annotators, based on indexed content of documents, assign MeSH Headings terms to the documents in order to allow to the user retrieve the information that explains the same concept with different terminology. On average, 5 to 15 subject headings are assigned by document, 3 to 4 of them being Major Headings and the others being Minor Headings. We studied the advantage of using the MeSH field respect to the Abstract field (see Table 14: 2 column). Mesh search field did not benefit our retrieval, and the feedback obtained is worse than the original result (see Table 14: 3 column). Add MeSH terms to the query with feedback was not advantageous in this case. Table 14. TF-IDF BM25 different fields MAP XXXXXXXXXCombinations Runs AB MH MH Feedback MH QH MH QH Feedback AB FB with MH Porter-Stopwords NLM 0.1868 0.1841 0.1820 0.1948 0.1918 0.2061 Porter-Stopwords SMART 0.1898 0.1813 0.1773 0.1889 0.1823 0.2039 Krovetz-Stopwords NLM 0.1843 0.1853 0.1837 0.1926 0.1924 0.2006 Krovetz-Stopwords SMART 0.1866 0.1826 0.1782 0.1863 0.1819 0.2005 14 Then a new form of expansion was tested consisting in search the MeSH Headings of the Entry Terms in queries and launching queries against the field MeSH of documents. It is associated with the Entry Terms that are in the query and are replaced by the Preferred Terms would be the Descriptors (we used PubMed in order to obtain descriptors as commented above) contained in the MeSH field of documents, either Major or Minor. Our expansion method was based on work of Kwangcheol Shin and Sang-Young Han in their expansion system presented in [39] proving the advantage of using MeSH Headings (based on a previous study of MeSH and its structure) to expand the query instead of working with terms that are not related to the MeSH field on documents (see Table 14: 4 column Queries Headings). Feedback is non-relevant again (see Table 14: 5 column Queries Headings Feedback). Documents retrieved with the MeSH process were selected (see Table 14: 5 column Queries Headings) to make feedback and see if better results were obtained, and found that there is not much variation between the Abstract feedback with documents previously retrieved in the same field or feedback with the documents retrieved from the MeSH process (see Table 14: 6 column - Abstract Feedback with MeSH). We proceed to isolate the process of Lemur to see if we improve this list of documents retrieved in MeSH. Query expansion outside Lemur As Lemur is closed tool and we performed weighting operations on the terms of the documents, follow the rest of our tests out of it. To perform our tests out of Lemur, we first adapted the queries and MeSH field documents to conduct searches of MeSH Headings in documents. In the first run we worked only with frequencies of occurrence of the query term within documents without distinguishing whether they are part of the Major or Minor terms in them. The total weight of a document for a particular query is the sum of the frequencies of the different terms in query inside the document. As we commented before, proceeded to access at online tool PubMed for locate possible sentences of MeSH Heading terms within the queries, which matching with Entry Terms, and add the Descriptors (MeSH Headings) matched to the new queries. We realized that two of the queries have not related MeSH terms and we only retrieved results for 93 queries. MAP in this moment was of 0.1570. At this point, we performed a test to try to expand with the long form the acronym CF "Cystic fibrosis" queries, previously discussed in expansion into Lemur in paragraph related to Acronyms. We inserted it in all of queries where appears, previously we had not included it in order to reduce noise. By this way, we wanted to check whether the MAP increases with MeSH terms for all queries. We obtained a MAP of 0.1272, then we could see that MAP decreases and this was not beneficial. As previously discussed, Major MeSH terms describing the primary/main topics of the document and Minor MeSH terms giving more details/secondary content about it. For this reason in the next test, we gave values to the MeSH terms depending on whether they were on Major or Minor fields into documents. If the term was a Major term in the document, we gave it a weight of 5 and if the term was a Minor term of 1. The other proceedings did not vary in relation to previous test. We obtained a MAP of 0.1641 and processed 93 out of 100 queries - rest of them had not results. From the above results, we concluded that many MeSH terms may be contained in substrings within the Major and Minor Terms of documents, so we proceeded to find substrings instead of an identical mapping between them. We achieved a MAP of 0.1739. In this case, we retrieved results for 98 queries. To improve the system, we proceeded to do the mixing of the results obtained with the procedure in MeSH fields calculated with the results obtained in previous section for the Abstract field. We prioritized those documents on both lists and then did re-ranking of documents in order to place them in the resulting list according to their total weight(see Table 15). At this point we have improved our technique with the values obtained in Table 14, last column. We tried to give more weight to the list of MeSH terms to test what would be a new way of giving priority to expanding the list but results are worst, probably due to the fact that MeSH do not retrieve results for all queries (see Table 16). Finally, we added Title field in order to see how improved results (see Table 17) These results can be compared with those obtained by K. Shin and S.Y. Han in their expansion system presented in [39] where achieved a maximum value of 0.35 for R-prec, being our R-Prec of 0.31 or 0.32 in some case. Our results are located between the two highest values. Finally, a Recall-Precision Graph (see Fig. 5) have been included showing the improvement obtained with the last expansion method using the MeSH, Abstract and Title fields (see Table 16) with respect to the retrieve of documents using only the Abstract field with TF-IDF BM25 (see Table 8). Show results 15 Table 15. MeSH and Abstract fields. ```````````Combinations Measures MAP R-Prec Porter-Stopwords NLM Abstract 0.2344 0.2737 Porter-Stopwords SMART Abstract 0.2341 0.2778 Krovetz-Stopwords NLM Abstract 0.2344 0.2747 Krovetz-Stopwords SMART Abstract 0.2352 0.2723 Table 16. MeSH and Abstract fields plus weighting MeSH list. ```````````Combinations Measures MAP R-Prec Porter-Stopwords NLM Abstract 0.2300 0.2679 Porter-Stopwords SMART Abstract 0.2296 0.2708 Krovetz-Stopwords NLM Abstract 0.2314 0.2698 Krovetz-Stopwords SMART Abstract 0.2303 0.2707 for the combination Krovetz-SMART where curves closest to the upper right-hand corner of the graph (where recall and precision are maximized) indicate the best performance. 4 Conclusions and Future work We have developed and evaluated query expansion techniques for retrieving documents in several fields of biomedical articles belonging to the corpus Cystic Fibrosis, a corpus of MEDLINE documents. We have tested the benefit of using stemming and stopwords in the query expansion following the investigations of other authors. Besides were analyzed the benefits that could be obtained from the use of acronyms, not satisfactory in our case by the characteristics of the corpus. We compared the weighting algorithms Okapi BM25 and TF-IDF availables in the Lemur tool, con-cluding that TF-IDF with tf formula given by BM25 approximation is superior in its results. Document retrieval based on Abstract, MeSH and Title fields seems more effective than looking at each of these fields individually. Also, the use of feedback in retrieving of documents, a technique widely used by researchers in this field, results a great improvement in retrieving the relevant documents, based on this case in Rocchio, allowed us to obtain good results improving MAP and other measures. We performed a study to improve searching in the Abstract field with the information available in the MeSH field on documents. For this, we have enhanced queries locating Entry terms in them and obtaining MeSH Headings in PubMed in order to map with the documents, giving greater importance to those belonging to Major Terms in documents. The results have been good at making use of the Title and Abstract fields to improve the list of documents retrieved in the MeSH field compared to other research. Queries without Entry Terms or with few related documents produced a negative impact on our retrieval process based on MeSH. Table 17. MeSH, Title and Abstract fields. ```````````Combinations Measures MAP R-Prec Porter-Stopwords NLM Abstract 0.2815 0.3234 Porter-Stopwords SMART Abstract 0.2780 0.3182 Krovetz-Stopwords NLM Abstract 0.2822 0.3208 Krovetz-Stopwords SMART Abstract 0.2798 0.3191 16 Fig. 5. Recall-Precision Curve As future work, stemming could be used when working with expansion technique based on Entry Terms following the research of K. Shin and S.Y. Han [39]. Working with MeSH Headings and MeSH fields in documents, the mapping between terms should be sufficient. Also it would be interesting test whether o reduce the root of MeSH Headings in queries and documents could improve the results. Another method that we believe interesting would be using clustering techniques in order to expand queries [1]. It is based on the idea of co-occurrence of stems (or terms) inside documents (stems which co-occur frequently inside documents, those which distance between them is small inside documents or those with similar neighborhoods have some synonymity relatioship). The technique of cluster is a feedback technique in which, once retrieved relevant documents to a query, terms are localized inside the documents with higher frequency, or with a fixed distance or those that appear very often together, are used for improving initial query. On the other hand, we intend to study the possibility of using other tools for indexing and retrieval of information that may offer advantages over Lemur. Some have already been discussed as Zettair, Smart or Terrier but now we want to study the possibility of creating our own tool for this purpose. To continue our studies related to MeSH terms, we want to make QE using Entry terms. To do this, we must work on documents and queries by modifying the preprocessing and indexing of Lemur. In actual tests, we lose concepts composed of several words or symbols that are of vital importance when working with biomedical issues. Finally, we want to implement query expansion techniques applying different scientific dictionaries used in many investigations as: EntrezGene, HUGO, Eugenes and ARGH for gene/protein names; GO for molecular functions, biological processes and cellular components; UMLSKS for related names and symbols or WordNet for general words/phrases. 17 All tests so far and future will be focused on TREC13. TREC 2005 is an ad hoc retrieval task collection where we have queries derived from a generic topic template which includes a certain number of semantic types. TREC has different corpus depend on year. Queries in the 2005 ad hoc retrieval task were collected from real biologists. It has 50 queries derived from 5 generic topic templates (GTT), each of which has 10 instances. TREC 2005 has 10-year Medline subset, which includes 4,591,008 documents from year 1994 to 2003. The corpus has the same components that Cystic Fibrosis collection. TREC is a large corpus that will allow generalize results of different expansion techniques. References 1. R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. United States of America: Addison Wesley, 1999. 2. E. Eckard and J.C. Chappelier. Free Software for research in Information Retrieval and Textual Clustering. Technical report, 2007. 3. Z. Shi, B. Gu, F. Popowich, and A. Sarkar. Synonym-based query expansion and boosting-based re-ranking: A two-phase approach for genomic information retrieval. In The Fourteenth Text REtrieval Conference (TREC 2005, 2005. 4. D. Hiemstra and D. van Leeuwen. Creating a dutch information retrieval test corpus, 2002. 5. J.O. Ebbert, D.M. Dupras, and P.J. Erwin. Searching the medical literature using pubmed: a tutorial. volume 78, pages 87–91. Mayo Clinic Proceedings, January 2003. 6. A.A. Chang, K.M. Heskett, and T.M. Davidson. Searching the literature using medical subject headings versus text word with pubmed. The Laryngoscope, 116(2):336–340, 2006. 7. L. V. Gault, M. Shultz, and K. J. Davies. Variations in medical subject headings (mesh) mapping: from the natural language of patron terms to the controlled vocabulary of mapped lists. Journal of the Medical Library Association : JMLA, 90(2):173–180, April 2002. 8. K. Shin and S.Y. Han. Improving information retrieval in medline by modulating mesh term weights. In Farid Meziane and Elisabeth Mtais, editors, Natural Language Processing and Information Systems, volume 3136 of Lecture Notes in Computer Science, pages 137–166. Springer Berlin / Heidelberg, 2004. 10.1007/978-3-540-27779-8 36. 9. M. Coletti and H. Bleich. Medical subject headings used to search the biomedical literature. Journal of the American Medical Informatics Association, 8 (4), 2001. 10. J. Leveling and G.J.F. Jones. Sub-word indexing and blind relevance feedback for english, bengali, hindi, and marathi ir. 9:12:1–12:30, September 2010. 11. A.F. Zazo, C.G. Figuerola, J.L.A. Berrocal, and E. Rodŕıguez. Term expansion using stemming and thesauri in Spanish. In C. Peters, editor, Results of the CLEF 2002 Cross-Language System Evaluation Campaign. Working Notes for the CLEF 2002 Workshop 19–20 September, Rome, Italy, pages 177–183, 2002. [También en ĺınea: http://clef.iei.pi.cnr.it/workshop2002/WN/22.pdf - Consulta: 01/10/2002]. 12. M. Porter. An algorithm for suffix stripping. Program, 14(3):130–137, 1980. 13. F. Yamout, R. Demachkieh, G. Hamdan, and R. Sabra. Further enhancement to the porters stemming algorithm, 2004. 14. P. Willett. The porter stemming algorithm: Then and now. Program: Electronic Library and Information Systems, 40(3):219–223, 2006. 15. G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, Reading MA (USA), 1949. 16. H. Yu, W. Kim, V. Hatzivassiloglou, and W. J. Wilbur. Using medline as a knowledge source for disambiguat-ing abbreviations and acronyms in full-text biomedical journal articles. Journal of Biomedical Informatics, 40(2):150 – 159, 2007. 17. J. Pustejovsky, J. Castanho, R. Saur, A. Rumshinsky, J. Zhang, and W. Luo. Medstract: creating large-scale information servers for biomedical libraries. In Proceedings of the ACL-02 workshop on Natural language processing in the biomedical domain, pages 85–92, Morristown, NJ, USA, 2002. Association for Computational Linguistics. 18. L. Si, J. Lu, and J. Callan. Combining multiple resources, evidence and criteria for genomic information retrieval, 2006. 19. X. Huang, M. Zhong, and L. Si. York university at trec 2005: Genomics track. In TREC, 2005. 20. N. Stokes, Y. Li, L. Cavedon, and J. Zobel. Exploring criteria for successful query expansion in the genomic domain. Inf. Retr., 12:17–50, February 2009. 21. S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-Beaulieu, and M. Gatford. Okapi at trec-3. pages 109–126, 1996. 22. S. E. Robertson and S. Walker. Okapi/keenbow at trec-8, 1999. 13 TREC http://trec.nist.gov/ 18 http://trec.nist.gov/ 23. S. Yoo and J. Choi. On the query reformulation technique for effective medline document retrieval. Journal of Biomedical Informatics, 43(5):686–693, October 2010. 24. S. E. Robertson and Sparck K. Jones. Relevance weighting of search terms. Journal of the American Society for Information Science, 27(3):129–146, 1976. 25. J.C. Toucedo and D.E. Losada. Formulating good queries for prior art search. In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval ex-periments, CLEF'09, pages 418–425, Berlin, Heidelberg, 2009. Springer-Verlag. 26. S.E. Robertson and S. Walker. Microsoft cambridge at trec-9: Filtering track, 2001. 27. E. Greengrass. Information retrieval: A survey. technical report tr-r52-008-001, united states department of defense, 2001. 28. P. Raghavan C.D. Maning and H. Schtze. An Introduction to Information Retrieval. Cambridge University Press, Cambridge, England, 2008. 29. J. Ramos. Using tf-idf to determine word relevance in document queries, 2003. 30. S. Robertson. Understanding inverse document frequency: On theoretical arguments for idf. Journal of Documentation, 60, 2004. 31. C. Zhai. Notes on the Lemur TFIDF model. October 2001. 32. J. Urbain, N. Goharian, and O. Frieder. Iit trec 2005: Genomics track. In TREC'05, pages –1–1, 2005. 33. S. Abdou. Evaluation of stemming, query expansion and manual indexing approaches for the genomic task. In The Fourtheenth Text REtrieval Conference Proceedings (TREC 2005) http://trec.nist.gov/act part/t14 notebook/notebook.papers/uhospital-geneva.geo.pdf, pages 863–871, 2005. 34. Z. Yang, H. Lin, Y. Li, B. Liu, and Y. Lu. Trec 2005 genomics track experiments at dutai. In Text REtrieval Conference, 2005. 35. T. Brants and Google Inc. Natural language processing in information retrieval. In Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands, pages 1–13, 2004. 36. Y. Fan, X. Huang, and A. An. York university at trec 2006: Enterprise email discussion search. In TREC, 2006. 37. A. Trotman. Learning to rank. Information Retrieval, 8:381, 2005. 38. A. Trotman. An artificial intelligence approach to information retrieval, 2004. 39. K. Shin and S.Y. Han. Improving information retrieval in medline by modulating mesh term weights. In NLDB'04, 2004. 19 5 Resumen El conocimiento biomédico crece a pasos agigantados alimentando grandes colecciones de documentos cient́ıficos. Estas colecciones de documentos ofrecen una excelente oportunidad para extraer conocimiento biomédico aplicando técnicas de recuperación de información, que debe estar representada de un modo que vaya a permitir un buen acceso a ella por parte de los usuarios. Un sistema de recuperación de información textual debe ser capaz de procesar la consulta o petición de información de un usuario, de manera que extraiga las palabras clave necesarias para recuperar los documentos relevantes a esta consulta, es decir, aquellos que están relacionados con lo solicitado por el usuario. Para validar un sistema de recuperación de información utilizamos corpus formados por documentos, consultas y un fichero con los documentos que los expertos han considerado relevantes a cada consulta. De este modo verificamos los resultados obtenidos de las búsquedas sobre el corpus. Muchas veces los conceptos introducidos por los usuarios en las consultas no van a coincidir con los conceptos que aparecen en los documentos con los que están relacionados, por eso, las técnicas de expansión de consultas son de vital importancia ya que permiten obtener los términos clave que se necesitan para recuperar los documentos de una manera más precisa. A lo largo de nuestro art́ıculo hemos analizado varias técnicas de expansión de consultas para evaluar su eficiencia. La recuperación de la información está compuesta por dos procesos principales: el indexado y el mapeo. 1. El proceso de indexado o parseado de consultas trata de reducir los términos de una consulta a las palabras clave relevantes para asociar documentos con consultas. En este proceso, para la expansión de consultas son vitales las técnicas de stemming y stopwords para reducir las variaciones léxicas de las palabras a su ráız y eliminar aquellas que no van a ser relevantes, de manera que todas las pal-abras clave que partan de una misma ráız se unifiquen. Antes de realizar el parseado de las consultas, se realizan métodos de expansión, como el añadir a las consultas sinónimos o palabras relacionadas, para representar los mismos conceptos de diferentes modos ya que de antemano no se sabe con qué palabra exacta se va a representar un concepto en un documento determinado. 2. El proceso de mapeo consiste en utilizar algoritmos de pesado como pueden ser el BM25 o TF-IDF que nos van a pesar los documentos en función de los términos que contienen, relacionados con una consulta determinada, pesando más aquellos documentos que tienen más términos relacionados con la consulta y menos aquellos que sólo van a tener un emparejamiento parcial. Una vez que hemos obtenido los documentos relevantes a las consultas, podemos realizar otro proceso de expansión que va a consistir en añadir a la consulta inicial aquellos términos de más peso dentro de los documentos, de esta manera enriquecemos la consulta para que se vaya a acercar más a conceptos que tienen valor dentro de los documentos relevantes a la misma para aśı poder recuperar documentos más cercanos al tema concreto a tratar por la consulta. Existen diferentes herramientas para la recuperación de información. En nuestro art́ıculo nos cen-tramos en Lemur. Por otro lado, el corpus que centra nuestra investigación es el Cystic Fibrosis, formado por 1239 documentos y 100 consultas, además del fichero de relevancia dado por los expertos. Cada uno de los documentos va a tener varios campos, destacando el Abstract, T́ıtulo y términos MeSH. Los MeSH Headings son términos (asignados por expertos) que representan el contenido de un documento pudiendo distinguir entre Major terms y Minor terms. Los major terms son 4 o 5 MeSH Headings que van a representar de manera principal el contenido del art́ıculo, mientras que los Minor terms son conceptos que van a servir de base para completar la información acerca del contenido del mismo. Los Major terms definirán la temática concreta mientras los Minor nos van a reflejar temas secundarios tratados en ellos. Además, a la hora de tratar una consulta o documento, va a ser de vital importancia el reconocimiento de acrónimos, ya que tenemos que tener en cuenta un posible mapeo entre estos y sus formas largas, por lo que muchas veces son de vital importancia en expansión ya que no sabemos de qué forma pueden estar representados en documentos y consultas, y por ello se ha de trabajar buscando ambas formas. En nuestro art́ıculo mostramos que trabajar con reconocimiento de acrónimos no es beneficioso, ya que el corpus no posee un gran número de ellos, por lo que el reconocimiento o la expansión ensucia los resultados. 20 Nuestro proceso de recuperación de información utilizando expansión de consultas está dividido en dos partes: una de ellas realizada en Lemur y otra realizada fuera de Lemur. Se ha decidido realizarlo aśı debido a que una parte de la expansión de consultas está centrada en dar diferentes pesos a términos MeSH de las consultas presentes dentro de los documentos en función de si son Major o Minor terms y, dado que Lemur es una herramienta cerrada que no nos va a permitir hacer este tipo de proceso, por lo que llegados a este punto de expansión procedimos a sacar este proceso de la herramienta. La expansión dentro de Lemur comienza analizando las ventajas de utilizar stemming y stopwords aśı como del uso de los acrónimos en documentos y en consultas. A continuación se analiza la parametrización adecuada de los algoritmos de pesado utilizados que nos va a permitir obtener los mejores resultados para nuestra colección. Una vez que tenemos la parametrizacin adecuada (para lo que hemos usado el campo Abstract), obtenemos resultados para el campo MeSH con los mismo algoritmos, que nos van a servir de base para analizar los resultados de retroalimentar el Abstract con los términos de los documentos recuperados previamente para este campo o realimentarlo con los resutados obtenidos con el uso del campo MeSH. Fuera de Lemur vamos a analizar la posibilidad, anteriormente citada, de dar distintos pesos a los términos en función del campo en el que aparecen, para obtener el ranking de documentos. Esta lista se va a mezclar con la lista obtenida previamente en Lemur de documentos obtenidos buscando en el campo Abstract, priorizando aquellos que aparecen en ambas listas. Para mejorar el resultado final se amplia este proceso combinando lo anterior también con el campo Title, demostrando los resultados que la mejor combinación es usar el campo MeSH con mayor pesado para Major terms junto con el Abstract y el Title. 21 Lecture Notes in Computer Science Introduction Query expansion Terms Definitions Corpus of documents Stemming Stopwords Acronyms Okapi BM25 weighting algorithm TF-IDF weighting algorithm User Relevance Feedback Measures Methodology and Results Query expansion in Lemur Query expansion outside Lemur Conclusions and Future work Resumen Boolean and Ranked Information Retrieval for Biomedical Systematic Reviewing Stefan Pohl Department of Computer Science and Software Engineering, The University of Melbourne, Victoria 3010, Australia spohl@csse.unimelb.edu.au Submitted in total fulfilment of the requirements of the degree of Doctor of Philosophy February 2012 mailto:spohl@csse.unimelb.edu.au Abstract Evidence-based medicine seeks to base clinical decisions on the best currently available scientific evidence and is becoming accepted practice. A key role is played by systematic reviews, which synthesize the biomedical literature and rely on different information re-trieval methods to identify a comprehensive set of relevant studies. With Boolean retrieval, the primary retrieval method in this application domain, relevant documents are often ex-cluded from consideration. Ranked retrieval methods are able to mitigate this problem, but current approaches are either not applicable, or they do not perform as well as the Boolean method. In this thesis, a ranked retrieval model is identified that is applicable to systematic review search and also effective. The p-norm approach to extended Boolean retrieval, which gen-eralizes the Boolean model but, to some extent, also introduces ranking, is found to have a particularly promising prospect: identifying a greater fraction of relevant studies when typi-cal numbers of documents are reviewed, but also possessing properties important during the query formulation phase and for the overall retrieval process. Moreover, efficient methods available for ranked keyword retrieval models are adapted to extended Boolean models. The query processing methods presented in this thesis result in significant speed ups of a factor of 2 to 9, making this retrieval model an attractive choice in practice. Finally, in support of the retrieval process during the subsequent update of systematic reviews, a query optimiza-tion method is devised that makes use of the knowledge about the properties of relevant and irrelevant studies to boost the effectiveness of the search process. i ii Declaration This is to certify that: (i) the thesis comprises only my original work towards a Ph.D. except where indicated in the Preface, (ii) due acknowledgement has been made in the text to all other material used, and (iii) the thesis is less than 100,000 words in length, exclusive of tables, maps, bibliogra-phies and appendices. Stefan Pohl iii iv To Agata. v vi Acknowledgements Over the course of the last three years, many people had an influence on the work for this thesis. Although I can not mention all here, some of them should be given special mention. First, I would like to thank my supervisors, Alistair Moffat and Justin Zobel. They have been mentors in the fullest sense of the word, caring beyond what is required from the role of a supervisor. I am grateful for the constructive criticism, trust, encouragement, challenging discussions and opportunities that they provided. I would also like to mention Laurence Park and Andrew Turpin, who served on my advisory committee. The University of Melbourne and NICTA not only provided funding, but also an open, quality research environment with much guidance for research students and little adminis-trative overhead. I would also like to thank all members of the NICTA BioTALA project team for the interchange of ideas. Particular mention appertains to Sarvnaz Karimi; thank you for the enduring collaboration, encouragement and friendship. It would be remiss not to thank Tim Baldwin and Rebecca Dridan, who provided me with casual teaching opportunities, and to thank my fellow PhD students Sri Devi Ravana, Yuye Zhang and William Webber for the discussions and mutual support. Also all friends in Melbourne take their share having made the last three years an enjoyable time; in particular, thanks to Jens and Agnieszka Pfau, and Daniel Kless. Special thanks go to my wife Agata, without whom this thesis would not have been pos-sible. I extend my deepest appreciation for her trust, love and understanding. Finally, thanks to our friends and families on the other side of the world, for their patience and support. Stefan Pohl Melbourne, Australia, February 2012. vii viii Preface Publications arising from this thesis The measurement methodology for timings, proposed as part of Chapter 3, was presented at the 31st European Conference on Information Retrieval (ECIR 2009) [Pohl and Moffat, 2009]. Chapter 4 was presented in condensed form at the 33rd Australasian Computer Science Conference (ACSC 2010) [Pohl et al., 2010]. Chapter 5 is accepted for publication in IEEE Transactions on Knowledge and Data Engi-neering [Pohl et al., 2011]. The retrieval system developed for this thesis found further application as part of retrieval experiments not central to the topic of this thesis and were published in BMC Medical In-formatics and Decision Making [Karimi et al., 2010]. These results were also presented in preliminary form at the 3rd International Workshop on Data and Text Mining in Bioinfor-matics (DTMBIO 2009) [Karimi et al., 2009]. Document Preparation, Tools and Data This document was prepared using LATEX and BIBTEX; and employs the packages: book, times, setspace, fncychap, fancyhdr, booktabs, multirow, graphicx, subfigure, caption, ams-math, amssymb, amsthm, array, algorithm2e, url, hyperref and natbib. The figures have been prepared with the built-in graphic routines of R, the excellent ggplot2 package, and TikZ. ix The implementations are based on the indexing and inverted list access routines pro-vided by the Lucene IR system. Moreover, the Medline citation parser of the LingPipe project has been employed to parse the used document collection. The query parser has been generated using the ANTLR parser generator. The Medline collection of biomedical citations as of 2008 has been used throughout the thesis. The AHRQ query sets are due to Aaron M. Cohen and Sarvnaz Karimi. The TRC queries for both of these query sets are also due to Sarvnaz Karimi. Further query sets have been derived from logs of PubMed and from the information in the Cochrane Database of Systematic Reviews. x Table of Contents 1 Introduction 1 2 Background & General Concepts 9 2.1 Information Retrieval Models . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.1.1 Boolean Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.2 Ranked Keyword Retrieval . . . . . . . . . . . . . . . . . . . . . . 15 2.1.3 Ranking with Structured Queries . . . . . . . . . . . . . . . . . . . 20 2.2 Efficient Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.2.1 Inverted Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.2.2 Query Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.3 Information Retrieval Evaluation . . . . . . . . . . . . . . . . . . . . . . . 37 2.3.1 Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2.3.2 Effectiveness Measures . . . . . . . . . . . . . . . . . . . . . . . . 38 2.3.3 Efficiency Measures . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.4 Evidence-Based Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.4.1 Systematic Reviews . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.4.2 Current Retrieval Methods . . . . . . . . . . . . . . . . . . . . . . 54 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 3 Evaluation Methodology 65 3.1 Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3.1.1 Document Collections . . . . . . . . . . . . . . . . . . . . . . . . 66 3.1.2 Query Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 3.2 Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.2.1 Indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 xi 3.2.2 Query Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 3.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4 Ranking for Systematic Review Search 83 4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4.2 Ranking Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 4.2.1 Ranking Boolean Result Sets . . . . . . . . . . . . . . . . . . . . . 87 4.2.2 Ranked Keyword Retrieval . . . . . . . . . . . . . . . . . . . . . . 88 4.2.3 Ranking with Structured Queries . . . . . . . . . . . . . . . . . . . 90 4.2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.3 Ranking with the p-Norm EBR Model . . . . . . . . . . . . . . . . . . . . 92 4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4.4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4.4.2 Reproducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 4.4.3 Ranking Effectiveness . . . . . . . . . . . . . . . . . . . . . . . . 98 4.4.4 Relative Performance . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 5 Efficient Query Processing 107 5.1 Implementation Approaches . . . . . . . . . . . . . . . . . . . . . . . . . 107 5.2 A Query Processing Method for p-Norm Queries . . . . . . . . . . . . . . 112 5.2.1 Document Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . 112 5.2.2 Scoring Fewer Documents . . . . . . . . . . . . . . . . . . . . . . 116 5.2.3 Short-Circuiting Evaluation . . . . . . . . . . . . . . . . . . . . . 119 5.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 5.3.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . 125 5.3.2 Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 5.3.3 Timings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 xii 6 p-Norm Query Optimization 133 6.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6.2 A Weighting Method for p-Norm Queries . . . . . . . . . . . . . . . . . . 139 6.2.1 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 6.2.2 Weight Training . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 6.2.3 Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 6.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 6.3.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . 153 6.3.2 Generalization & Effectiveness . . . . . . . . . . . . . . . . . . . . 154 6.3.3 Impact on Rankings . . . . . . . . . . . . . . . . . . . . . . . . . 158 6.3.4 Term Importance . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 6.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 6.3.6 Other approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 7 Conclusions 167 Bibliography 173 xiii xiv List of Figures 2.1 Basic information retrieval model . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Disjunctions and conjunctions in the p-norm EBR model . . . . . . . . . . 23 2.3 Inverted index example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.4 Document-at-a-time processing with Max-Score . . . . . . . . . . . . . . . 36 2.5 Flow diagram describing the study selection progress . . . . . . . . . . . . 50 2.6 Hierarchy of evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 2.7 Medline query in Ovid syntax . . . . . . . . . . . . . . . . . . . . . . . . 59 3.1 Medline XML data file excerpt. . . . . . . . . . . . . . . . . . . . . . . . . 67 3.2 Schematic document set relations . . . . . . . . . . . . . . . . . . . . . . . 70 3.3 Distribution of included and resolved studies . . . . . . . . . . . . . . . . . 73 3.4 Query processing pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.5 Decomposition of overall execution time . . . . . . . . . . . . . . . . . . . 80 4.1 Case study of a faceted search query . . . . . . . . . . . . . . . . . . . . . 85 4.2 Score distributions for a selection of retrieval models . . . . . . . . . . . . 94 4.3 Retrieval model effectiveness comparison . . . . . . . . . . . . . . . . . . 102 5.1 Example p-norm EBR query tree . . . . . . . . . . . . . . . . . . . . . . . 115 5.2 TIB score bound calculation . . . . . . . . . . . . . . . . . . . . . . . . . 121 5.3 Average query execution times for retrieval of top-k documents . . . . . . . 131 6.1 Weighted example p-norm EBR query tree . . . . . . . . . . . . . . . . . . 141 6.2 Schematic representation of two nodes of a general query tree . . . . . . . 145 6.3 Training instances before and after unification of identical documents . . . 152 6.4 Dataset split to simulate the update of a systematic review . . . . . . . . . . 153 xv 6.5 Loss over the iterations for a typical query . . . . . . . . . . . . . . . . . . 155 6.6 Loss before and after training . . . . . . . . . . . . . . . . . . . . . . . . . 158 6.7 Per-topic improvements on the datasets as per RBP . . . . . . . . . . . . . 161 6.8 Correlation between RBP improvements on validation and test datasets . . . 162 6.9 The distribution of learned term weights . . . . . . . . . . . . . . . . . . . 163 xvi List of Tables 2.1 Contingency table of documents' relevance and retrieval status . . . . . . . 39 3.1 Medline collection and index statistics . . . . . . . . . . . . . . . . . . . . 69 3.2 Descriptive query set statistics . . . . . . . . . . . . . . . . . . . . . . . . 74 4.1 Boolean result set sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 4.2 Relative recall, measured at absolute ranks . . . . . . . . . . . . . . . . . . 100 4.3 Relative recall, measured at ranks relative to the Boolean result set size . . . 103 5.1 Maximally scoring term sets and their scores . . . . . . . . . . . . . . . . . 124 5.2 Needless document scorings and postings read from inverted lists . . . . . . 127 5.3 Average per-query execution times . . . . . . . . . . . . . . . . . . . . . . 129 6.1 Loss and standard deviation for selected p-values . . . . . . . . . . . . . . 156 6.2 Effectiveness of the optimization in terms of RBP and AUC . . . . . . . . . 159 6.3 Average RBP after reverting to baseline for inconsistent queries . . . . . . 162 xvii xviii Chapter 1 Introduction New research results are published at an astonishing pace. In the biomedical field alone, more than a thousand studies are added daily to major databases. Consequently, it becomes increasingly difficult, if not impossible, for clinical practitioners to keep up with the latest available evidence about the effects of health care interventions. As a result, decisions risk being based on common belief and experience of individual practitioners, which both can be biased and outdated. For instance, it is accepted practice to intubate critically injured patients suspected having suffered from traumatic brain injury to ensure an open airway and hence reduce the risk of secondary brain injuries. However, tracheal intubation can also lead to apnea, the opposite of what is aimed for, if it is performed by unskilled personnel or in unfortunate settings. It is hence controversial whether intubation should be performed in the pre-hospital environment [von Elm et al., 2009]. Evidence-based medicine aims to answer such questions on the basis of all available, high-quality scientific evidence and to guide decision making in clinical practice. To free practitioners of the laborious and time-consuming task of identifying and examining rele-vant research studies, dedicated organizations prepare systematic reviews of specific clinical questions of general interest. Today, these reviews are considered the primary tool to inform clinical practice guidelines and the health care decisions made by practitioners. To increase the credibility of a review, the retrieval process employed has to be transparent and scrutable, 1 INFORMATION RETRIEVAL and must follow an accepted systematic approach that can be repeated if necessary. More-over, it is essential that a comprehensive set of relevant publications is found, otherwise the review might be biased and, in the worst case, provide wrong advice. Hence, the prepa-ration of systematic reviews critically depends on the retrieval method used to locate the documents that determine its recommendation. Information Retrieval Humankind has long been collecting, organizing, preserving and disseminating knowledge in libraries. In the past, it has been possible to inspect and evaluate all documents for a par-ticular informational need, but for most collections that is now impractical. In 1945, Van-nevar Bush recognized the need for effective search mechanisms and he wrote [Bush, 1945]: Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, "memex" will do. A memex is a device in which an individual stores all his books, records, and com-munications, and which is mechanized so that it may be consulted with exceeding speed and flexibility . . . Later, when computers had been realized as a means to store and process large amounts of information, pioneers like Gerard Salton began to develop methods for retrieving infor-mation from digital collections [Salton and Lesk, 1965], shaping what is now considered a distinct research discipline. Information Retrieval (IR) deals with the search process for information entities of in-terest, such as documents, images, music, videos, web pages, or parts thereof. Despite the recent prevalence of digital media, retrieval from (textual) document collections is still of primary interest to the research field. Indeed, retrieval from non-text collections is often informed by text retrieval over meta-data, such as labels, titles, artist names, and so on. Typically, retrieval processes involve a query, by way of which the user formally expresses their information need, and processing of this query, according to the implementation of a retrieval model. Retrieval technologies are used for several important tasks within differ-ent application domains, each with its own set of properties and requirements, and, hence, preferred techniques. 2 CHAPTER 1. INTRODUCTION Today, web search engines are the most visible applications of search technology. The web contains billions of documents,1 being mainly HTML and XML pages, but also in-cluding complex full-text documents in various formats. They are identified by uniform re-source locators (URLs), interconnected via links and contain a rich structure that describes and formats their content. In this domain, users daily submit millions of queries worldwide, and they provide implicit feedback about document relevance by means of their subsequent actions on the result pages of major search engines. In contrast, enterprise search is char-acterized by a lack of link structure, the presence of multiple data sources that have to be integrated, and the requirement to restrict the access of users to particular documents. In the medical and legal application domains, of which the former is considered in this thesis, specialized vocabularies are used to facilitate the quality of retrieval results. Here, it is not enough to find any relevant document that answers a particular information need. Instead, since the results of the search can have significant monetary and societal consequences, users are willing to spend (much) more time on the search process in order to increase retrieval comprehensiveness. Two general aspects in information retrieval research are the effectiveness and efficiency of methods. For an IR system, effectiveness refers to its ability to produce useful, quality output for a given input; or, to provide equivalent outputs with less input effort on the part of the user. In contrast, efficiency deals with the minimization of resource usage to perform a task. The most obvious resource is time, being primarily a function of the implementation's CPU and disk usage patterns, and the load on the interconnects between system components, such as system buses or networks. A more efficient retrieval system reduces the time users wait for the results of queries, or reduces the cost associated with each search. The cost to process queries in a timely manner can easily become significant for a service provider that has to handle queries of many users at the same time. Successful retrieval models typically have acceptable performance in effectiveness and efficiency aspects. 1As of 2005, the number of documents in the World Wide Web has been estimated to be 11.5 billion pages [Gulli and Signorini, 2005]. As of May 2011, the search engine http://www.google.com/ estimates to have more than 25 billion documents with the term "a" in their index. Note, however, that these numbers will vary depending on the definition of duplicates. 3 http://www.google.com/ SYSTEMATIC REVIEW SEARCH Systematic Review Search A systematic review team, working on a specific biomedical question, typically includes a librarian who is familiar with the document collections that are to be searched. Today, search is performed on central citation databases, such as Medline or Embase, that keep track of publications appearing in thousands of biomedical journals and venues, and contain millions of entries. Complex Boolean queries are used to filter the set of documents in these large collections to obtain a more manageable subset, in an attempt not to exclude too many relevant studies. It typically takes multiple iterations of query refinement and laborious retrieval result assessment of large answer sets to attain a "final" query that is reasonably effective and has a manageable result set size. For every review, Boolean result sets are systematically triaged by multiple assessors to arrive at a, typically small, set of directly relevant publications, which are then analyzed in detail. After the appraisal and aggregation of the evidence appearing in these studies, a synthesis of the findings is published in form of a systematic review, along with a description of the search process performed and the query. This allows readers to assess the review for any bias. The work on a systematic review however does not finish with its publication; it has to be updated at regular intervals to incorporate new research findings. For this purpose, the same Boolean retrieval process is used. To further increase the comprehensiveness of the review, additional measures are taken, including following of citations in relevant documents, asking of experts, and hand-searching of particular journals that might not (yet) have been indexed. Unfortunately, these secondary measures either depend on the initial retrieval results or suffer from other forms of bias, which further underlines the importance of the primary retrieval mechanism. Note that any bias due to this primary retrieval mechanism is documented by publication of the Boolean query used. The Boolean approach allows retrieval results to be reproduced and hence, satisfies one of the primary systematic review search requirements. Further advantages include the effi-ciency of query processing implementations and the ability to specify complex information 4 CHAPTER 1. INTRODUCTION needs in the form of structured queries. On the other hand, the Boolean approach has the disadvantage of being too strict, so that relevant documents are often screened out due to marginal mismatches with the query, with the hope of being found again by one of the sec-ondary discovery measures. Moreover, the query reformulation process is difficult because it requires repeated assessment of query quality through sampling of large intermediate re-sult sets. The fact that, even for experts, queries are unpredictable with regard to the size of the result set that they produce, further complicates the process of converging to a query with a manageable result set size. For systematic reviews, it is paramount to find as many relevant documents as possible. Otherwise, evidence might be missed, the review biased, and its suggestions misleading. McLellan [2001] describe an instance in which missed evidence about complications of a drug led to the death of a 24 year old healthy participant of a clinical study. For systematic review search, a retrieval method can thus be considered more effective than another if it is able to identify a larger number of relevant documents when a comparable amount of effort is used. As a result the risk of missing evidence will be reduced. Despite effectiveness being the primary concern for this and many other medical information needs, efficiency is also important, so that response times and monetary costs associated with the usage of search services are not excessive. This Thesis Boolean retrieval has a long history and is widely used in structured data retrieval applica-tions, such as, in libraries, databases, and email archives. Much research has been devoted to explore its properties, including for the specific retrieval tasks involved in systematic review preparation. In particular, well performing Boolean filters that are reusable as part of the queries for many systematic reviews [Glanville et al., 2006, Eisinga et al., 2007, McKibbon et al., 2009] and techniques to further reduce the Boolean result sets without loss of (too many) relevant documents have been proposed [Cohen et al., 2006, Wallace 5 THIS THESIS et al., 2010b]. This line of research accepts the currently used Boolean retrieval approach and its limitations. In many application domains, the problems associated with Boolean querying led to the adoption of the ranked retrieval principle using simple, unstructured keyword queries. The output of these methods is a list of documents that might match only partially to the query, and which is ranked by decreasing, estimated relevance to the query. However, one reason why Boolean retrieval is still used is the transparency and repeatability of retrieval runs, so that documents, having been retrieved once, can reliably be found again. In contrast, ranked keyword retrieval models typically employ complex weighting schemes that are dependent on volatile collection properties and may hence not be repeatable. This thesis aims to examine ranked retrieval methods with regard to their applicability for the retrieval tasks involved in the preparation and update of systematic reviews. In these contexts, a retrieval model has to provide primarily advantages in terms of effectiveness to be competitive with the current Boolean approach. Nevertheless, the availability of efficient query processing methods is important for adoption in practice and thus also pursued in this thesis. Particular research questions for this thesis are: • Are there ranked retrieval models that are suitable for systematic review search; and do they provide advantages over the currently used Boolean model? • Can efficient query processing methods, proposed for ranked keyword retrieval mod-els, be adapted to be applicable to such a retrieval model? • After preparation of an initial review, additional information about properties of rel-evant and irrelevant documents is available. How can this information be used to improve the retrieval task during the update of a systematic review? 6 CHAPTER 1. INTRODUCTION Thesis Organization This thesis is organized as follows: Chapter 2 introduces background information about Information Retrieval, including IR models, their efficient implementation, and commonly used IR evaluation frameworks and measures. Systematic reviewing practice and the current retrieval method are also described. Chapter 3 presents the datasets used throughout the thesis, consisting of a comprehen-sive collection and multiple query sets. The preprocessing steps used for documents and queries are described in order to make the experimental results repeatable. Also, the evalua-tion methodology is detailed, including a technique to accurately measure query processing timings. In Chapter 4, different ranked retrieval approaches are examined with respect to their ap-plicability to systematic review search. The p-norm approach to extended Boolean retrieval, one eligible and promising retrieval model, and its application to the systematic review re-trieval task is described and evaluated against the current Boolean baseline. The superiority of the Boolean approach – and the extended version thereof – over simpler keyword queries is validated. This chapter concentrates on the effectiveness of the method, because of its paramount importance for the task. Consequently, Chapter 5 is devoted to make this and similar retrieval models more com-petitive by provision of efficient implementations. For this, ideas from methods proposed for ranked keyword query retrieval are adapted to make them applicable to the p-norm, and similar, structured retrieval models. The attained algorithm is then complemented with new methods to further improve efficiency. Chapter 6 proposes the use of weighted p-norm queries for the retrieval process involved in updating systematic reviews and presents an approach to train these queries on the basis of the knowledge about the relevance of past documents. Finally, Chapter 7 summarizes the contributions of this thesis and gives suggestions on future work. 7 Chapter 2 Background & General Concepts Information Retrieval (IR) aims to best possibly satisfy information needs on the basis of unstructured information entities. Although the information retrieval research field gener-ally deals with a variety of media and finds application in several domains, the text form is still the principal method for communicating knowledge; and other media often have a textual representation by which they can be described. As a consequence, this thesis focuses on retrieval techniques for text documents, in particular, biomedical research publications. This second chapter introduces basic concepts and gives the background information necessary to understand the design decisions made throughout this thesis. First, informa-tion retrieval models are formally described. Section 2.2 then discusses efficient implemen-tations for these models, that reduce their execution costs and make their usage practical. Section 2.3 introduces IR evaluation frameworks and measures. Finally, Section 2.4 de-scribes systematic reviewing practice, its requirements, and the problems that have been noted with the currently used retrieval approach. 2.1 Information Retrieval Models An often paraphrased quote of Einstein says [Einstein, 1934]: It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the ade-quate representation a single datum of experience. 9 2.1. INFORMATION RETRIEVAL MODELS F Retrieval Function R(q, d) Output Document Model D Query Model Q Document Information Need d q Figure 2.1: Basic information retrieval model. Models, being simplifications of reality, are inherently false. Nevertheless, they are useful if they are able to explain all or a large fraction of the phenomena of interest, or are amenable to efficient methods that are dependent on the model assumptions. Information Retrieval is a particular area, where, in practice, additional gains in effectiveness of a more appropriate model are often traded off against efficiency and operational costs. Over the years, many information retrieval methods have been proposed. There is, however, no accepted, standard way to describe these models. According to Baeza-Yates and Ribeiro-Neto [1999], an information retrieval model can be formally described with the quadruple {D,Q,F , R(q, d)}, where D and Q refer to document and query models, respectively; F is a common framework to model D and Q, and the relationship between them; andR(q, d) is a retrieval function that assigns a (possibly real) value to each document d, presuming that a particular query q has been supplied. In the Boolean retrieval model, for example, set theory can be seen as the common framework F which motivates the use of document term sets as a document model D, and Boolean algebra as the query model Q. The implementation of the retrieval function directly results from the semantics of the Boolean operators used in any given query. Figure 2.1 contextualizes these distinct components graphically. The query model Q describes a means by which users can express their information needs to the IR system. Queries typically consist of terms, keywords that best describe the concepts of the information need. Some models allow terms to be interspersed with formal operators to qualify and relate them. In these cases, a query language exists that defines 10 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS the syntax and semantics of operators, and allows users to better express their information needs by means of structured query representations. The document model D describes how documents are conceptually represented. For instance, bag-of-words models consider a document to be an unordered set of terms. Al-though this model is in general arguable, it seems to make appropriate assumptions in face of traditional libraries. Here, documents are typically annotated with meta-data such as au-thors, titles and publication years and a set of distinct index terms drawn from a taxonomy. Of course, not all information needs can be foreseen and suitable index terms provided, and it is fortunate that dramatic increases in computational power, progresses in digitization and the availability of efficient, automatic indexing methods make it possible to index every word in the full text of document collections. Bag-of-words models, effectively ignoring document structure, word order and proximity of terms, might not be the most effective choice in such a setting [Metzler, 2007]. Obviously, queries are to be executed on the basis of the information present in the document model, constraining the complexity of the query model Q. Further, to understand the semantics of a particular query model, D and Q have to find their place in a common framework F . For the traditional Boolean retrieval model, this framework would consist of set theory and the three basic Boolean operators. The framework F usually also motivates the construction of a suitable retrieval function. Conceptually, the retrieval function R assigns a (possibly real) value to each document of the collection as an estimate of its likelihood to be relevant to the information need, as expressed by q. The output of the system can then be generated based on these document retrieval scores, and usually consists of either a set or a ranked list of documents. If queries and documents have the same model, such an estimate can be based on a similarity measure between the two. Similarity scores are hence often used synonymously to refer to these estimates. A natural classification of IR models would be on the basis of their underlying theory, used to derive each model, or the similarity in their modeling frameworkF . Some textbooks follow this presentation style (for instance, Baeza-Yates and Ribeiro-Neto [1999]), and are 11 2.1. INFORMATION RETRIEVAL MODELS best consulted for a comprehensive description of the theories behind IR models. For the purpose of this thesis, IR models are better characterized on the basis of the properties that are either directly perceived by users, or otherwise important to them. 2.1.1 Boolean Retrieval The Boolean retrieval model has a long history and broad adoption in data and information retrieval applications such as in databases [Witten et al., 1999], email [Dumais et al., 2003] and file search [Büttcher, 2007]. The legal and biomedical domains are particular examples, in which Boolean retrieval is still the prevalent approach to search, and the outcomes of the search can have significant monetary and societal consequences. The model is based on set theory and Boolean algebra, has straightforward semantics, and can be efficiently implemented. In the Boolean model, every term in a document collection implicitly defines the set of documents in which the term occurs. Queries consist of Boolean expressions, containing terms and Boolean operators as connectives. The three basic operators in Boolean algebra are conjunction (AND), disjunction (OR), and negation (NOT). In terms of set theory, a conjunction of terms results in the intersection of the corresponding document sets, and a disjunction results in their union. Negations give rise to the complement set with respect to the document collection, that is, the set of documents that either do not contain the given term or do not satisfy the subexpression. Consequently, the output of the model is also a document set. Conceptually, the output document set can be generated on the basis of individual per-document inclusion decisions, made for each document independently. In order to define the employed retrieval function algebraically, it is common to use the following: R(q, d) =  1, if q is a term and present in document d, 0, otherwise. (2.1) 12 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS On this basis, the retrieval functions for the Boolean operators can then recursively be de-fined as: ROR(q1, . . . , qm, d) = max i R(qi, d) , (2.2) RAND(q1, . . . , qm, d) = min i R(qi, d) , (2.3) RNOT(q, d) = 1−R(q, d) , (2.4) where i ranges from 1 to m, the number of arguments for each operator. The recursive application of these retrieval functions results in a final document score of either 1, leading to inclusion of the document d in the output set, or 0, meaning that the document is excluded from the output set and should not be shown to the user. In practice, the output set is typically presented as a list of documents and its order either determined by the documents' processing order, or by a document-based sort criterion such as reverse chronological order. Driven by commercial search engines, several model extensions have been developed over the years. Proximity operators are one of the most notable extensions that take the position of terms in a document into account [Mitchell, 1973]. These operators require terms to occur close to each other, either in sequence or unordered, and within a window of a particular number of terms. The search for phrases then becomes a special case, using an ordered window with a required position offset of one between each consecutive term in the phrase. Instead of assuming one "bag of words" for a whole document, fields are another ex-tension by way of which documents can be modeled as multiple bags of words. That is, different regions of a document can be distinguished by assigning the contained terms to different fields, effectively modeling some degree of document structure. Then either sin-gle or multiple fields can be queried. As well, possibly overlapping fields can be defined that aggregate distinct fields and provide several alternative document representations for the user to choose amongst. A field might also have a particular type, so that more specific operators can be supported for these types, for instance, range queries on numbers or dates. 13 2.1. INFORMATION RETRIEVAL MODELS All of these extensions seek to give the searcher more fine-grained control. A few inherent problems remain, however. Consider the information need to find documents containing information about lasting pain other than due to neuralgia. A possible query would be: (persistent OR chronic) AND pain AND NOT neuropathic This query appears to be an almost direct translation of the information need, exemplifying the power of a structured query language. This expressiveness does, however, not imply that qualitatively high retrieval results can be achieved with this query. The query uses conjunctions to combine the three concepts mentioned in the informa-tion need description, effectively requiring every potential output document to satisfy all of the concepts, as described by the Boolean subexpressions. The representation of one concept is restricted to the term "pain" only, which might exclude documents that happen to have only used the plural form of the word if query and index terms are not stemmed to their base form and which might also exclude documents with the similar word "ache". The first concept in the query has a better description, enumerating two related synonyms. The information need dictates that we might not be interested in finding documents describing neuropathic pains, but translating the information need this way can lead to unintended ex-clusion of relevant documents, namely, those that describe a pain of interest but, for some reason, also refer to neuropathic pain. Although the example is perhaps contrived, it intu-itively illustrates the problem of generating an effective query in general, and an effective Boolean query in particular. Despite these problems, Boolean retrieval is still employed in several problem and appli-cation domains. An obvious reason for this is the expressivity of the query model, allowing the description of complex information needs on the basis of operators with straightforward semantics. Moreover, these operators can efficiently be implemented, their semantics can easily be understood by others and retrieval results readily reproduced. Further rationale for the use of the Boolean retrieval model as the principal discovery method for systematic review search will be presented in Section 2.4.2. 14 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS 2.1.2 Ranked Keyword Retrieval Simple Boolean queries often do not perform well, but generating more complex queries takes time and can be a hard task for novices and lay users [Frants et al., 1999]. If queries contain mistakes, or do not contain the right keywords, an empty result set might be re-turned, which is a very unsatisfying situation for the user. Similarly unsatisfying is the situation in which a long, unsorted list of documents is returned. The user is neither likely to inspect the whole result set, nor to identify documents within it that are most likely to be relevant. A particular problem in this regard is that result sets of Boolean query for-mulations increase in size when the collection grows [Turtle, 1994]. Boolean queries must thus be refined to not to produce excessively large result sets, and this tuning depends on collection size. As a consequence, research has also focused on ranked keyword retrieval models which are known to have a number of advantages over the Boolean approach. First, if a set of loose keywords can be used as queries, users can freely pose natural language queries and do not have to be trained in a query language. A particular example are web search engines that produce results even when queries contain mistakes. They also support lay users in the query generation process, for instance via query suggestions [Kelly et al., 2009]. Second, the output of these systems is a ranked list of documents, ordered by decreasing estimated relevance to the query. Moreover, the documents in these rankings do not have to contain all of the query terms to be allowed as suggested answers. This partial match property, typical for ranked systems, has the advantage that an output is always generated and that the success rate of a query can quickly be assessed based on the relevance of the top-ranked documents. Studies have shown that users are able to achieve results with a ranked system that are comparable to Boolean retrieval systems for a range of tasks [Robertson and Thompson, 1990, Paris and Tibbo, 1998], and the prevalence of ranked keyword search in domains such as the web provides further evidence for its usefulness. If ranked results can even be produced at every keystroke when a query is typed [Bast and Weber, 2006], as is also the case in current web search engines, users can easily decide if it is necessary to enter additional letters or keywords to make their information need more specific. 15 2.1. INFORMATION RETRIEVAL MODELS The simplest ranking method is coordinate matching [Baeza-Yates and Ribeiro-Neto, 1999]. Here, the retrieval function is correlated with the number of terms a document has in common with the query. This method, however, ignores the importance of individual terms for the query as well as for the document, and also privileges longer documents, which are more likely to contain query terms. More sophisticated models have been proposed, of which the most significant are described in this section. Vector Space Model The vector space model, first described by Salton, is an intuitive, statistical ranking model [Salton and Lesk, 1968, Salton et al., 1975]. It considers queries and documents as bags-of-words, maps both of them into the same (very) high-dimensional space and defines a retrieval function based on vector algebra. Furthermore, it introduces the notion of TF·IDF term weighting, which stipulates that a retrieval function that measures the similarity be-tween a query and a document d should be positively correlated with the term frequency fd,t of each query term t within document d, and negatively correlated with each query term's document frequency ft, which is the number of documents that contain the query term t. These two parts are commonly referred to as term frequency (TF) and the inverse document frequency (IDF), and are typically normalized and multiplied with each other [Salton and Buckley, 1988, Witten et al., 1999]. Each of the n distinct collection terms are assumed to be independent, and are mapped to the axes of an n-dimensional vector space. Hence, each document in the collection can be represented as a document vector: ~d = (wd,t1 , wd,t2 , . . . , wd,tn) , where wd,t are weights that reflect the "importance" of the term t with respect to docu-ment d. To calculate these weights, typically a TF·IDF-based weighting scheme is em-ployed. Similarly, a query is represented as a vector in the same space: ~q = (wq,t1 , wq,t2 , . . . , wq,tn) . 16 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS The query weights wq,ti allow users to manually specify the relative importance of each query term, or can simply be set to 1. Having queries and documents represented as vectors in the same space, vector alge-bra can be used to calculate measures that resemble the similarity between these vectors. A smaller absolute angle between the query and a document vector is an intuitive choice for such a similarity measure, reflecting greater similarity in term distributions and being independent of vector length and hence the size of documents. Instead of the angle, more generally, the cosine similarity of the vectors can be employed: R(q, d) = cos^(~q, ~d) = ~q·~d ‖~q‖·‖~d‖ . (2.5) Obviously, documents only contain a subset of collection terms and queries are typically even shorter. Hence, most of the weights are 0. Moreover, the normalization uses constants with respect to the query and each document so that normalization factors can be precom-puted. This means that the calculation can be done efficiently, with only terms present in q and d needing to be considered. Note that if binary weights w ∈ {0, 1} are used, the only difference to rankings pro-duced by coordinate matching is the use of document length normalization. Zobel and Moffat [1998] review the effectiveness of several other weighting schemes that have been proposed. Probabilistic Models The vector space model is a simple, effective and efficient ranking method. However, the statistics used to weight terms are derived from empirical analysis of frequency distributions of relevant and irrelevant documents. Probabilistic retrieval models seek to provide a more theoretically founded approach [Robertson et al., 1981]. Robertson's probability ranking principle [Robertson, 1977] states that an optimal document ranking (under the assumptions of the model) would be achieved if documents would be ranked in order of decreasing probability of relevance (to a query, or to the underlying information need). In practice, the 17 2.1. INFORMATION RETRIEVAL MODELS difficulty is in reliably estimating these probabilities, and this is where the various proposed probabilistic models differ [Baeza-Yates and Ribeiro-Neto, 1999]. Today, the dominant probabilistic retrieval function is one of the Okapi BM25 family, first published in the TREC-3 experiments [Robertson et al., 1994]. The retrieval function described here stems from TREC-7 and includes refinements of earlier versions [Robertson et al., 1998]: R(q, d) = ∑ t∈q∩d log N − ft + 0.5 ft + 0.5 · (k1 + 1)·fd,t k1·((1− b) + b·ld/lavg) + fd,t · (k3 + 1)·fq,t k3 + fq,t , (2.6) where N refers to the number of documents in the collection; ft, as before, to the number of documents containing term t; fd,t and fq,t to the frequency of term t in the document and query, respectively; and ld and lavg are the length of document d and the average length over all documents, respectively, and can be measured in any arbitrary units. Finally, b, k1 and k3 are constants that can be tuned to the properties of a particular document collection. The formulation sums over the terms t present in both query and document, and consists of multiple factors, which play roles similar to the components of the TF·IDF formulation in the vector space model. That is, the first factor is similar to other IDF formulations and the other factors reflect term frequency (TF) contributions with respect to the document and the query, respectively. New concepts in this model are term frequencies within the query and how the document length is incorporated in the formulation. The constants can be used to control the relative importance of the different model components. Language Models A language model M is a probability distribution over lexical entities of a given language, such as characters, terms, or phrases of particular length (n-grams). However, because of difficulties in reliably estimating conditional probabilities, unigram language models are a common approach, in which term independence is equally assumed. A unigram language model stores a (prior) probability for every collection term and can be used to either gener-ate terms by sampling from the distribution, or to calculate the probability associated with 18 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS a sample of terms being generated by the model. In the context of information retrieval, language models can be used to model documents, queries, or both. Thus, we can compute the likelihood that either queries are generated from document models, documents are gen-erated from query models, or that query and document models are actually the same and have similar distributions [Lafferty and Zhai, 2001]. A popular instantiation is the query likelihood model, first proposed by Ponte and Croft [1998]. Here, for every document d, an unigram language model Md is generated and used to calculate a document score which can be interpreted as the likelihood for the query being generated by the language model Md. The maximum likelihood estimate for the unigram probability of a term t in a document d is: P (t |Md) = fd,t ld , (2.7) where ld denotes the number of terms in document d. Because it is obviously incorrect to assume that particular terms can never be used by the author of a document just because fd,t is 0 for some collection terms, these estimates can often be improved by smoothing (interpolation) with, or backoff to, a background probability [Zhai and Lafferty, 2001]. One particular method is Bayesian smoothing with Dirichlet priors, in which the prob-ability estimate for a term t is defined as: P (t |Md) = fd,t + µ·P (t | C) ld + µ , (2.8) where C is the collection background model, P (t|C) is a probability estimate for the ap-pearance of term t over the whole collection, and µ is a smoothing parameter with a typical value of 2,000. The maximum likelihood estimate for P (t|C) is ft/ ∑ t ft. The retrieval function is simply the query likelihood and becomes: R(q, d) = ∏ t∈q P (t |Md) = ∏ t∈q fd,t + µ·ft/ ∑ t ft ld + µ . (2.9) 19 2.1. INFORMATION RETRIEVAL MODELS Note that this formulation also contains document length and TF·IDF components; and that µ controls their relative importance. 2.1.3 Ranking with Structured Queries When users know the content of documents they intend to retrieve or some part thereof, it is often sufficient to simply type in a few keywords that are present only in those documents. In a ranked keyword retrieval model, neither a complex query language nor the document model, say, for instance, the field name for the part of the document in which each keyword occurs, has to be remembered in order to retrieve a particular document. This facilitates the generation of meaningful and syntactically correct queries by novices [Brown and French, 1997], because any query will be accepted by the system and, as long as the query contains any collection terms, will always produce result documents. Finally, the ranked output allows user to quickly inspect the top ranked documents for relevance and to reformulate the query, if necessary. For more complex information needs, and when the result of the search entails monetary or societal consequences, search experts are often employed; but even typical lay users may be willing to spend more time to formulate an effective query for these purposes. In such cases, IR models using bag-of-words keyword queries can become a limitation. High quality, structured queries often outperform unstructured keyword queries [Metzler and Croft, 2004, Zhu et al., 2007]. This could be one of the reasons why Boolean retrieval, with its structured query model, is still the preferred method of choice in the legal and biomedical domains [Oard et al., 2008, Hersh, 2008]. Although each retrieval model has been described with one instantiation for each of the concepts in the general IR model (Figure 2.1), these components can be understood orthogonally so that new models could be defined as combination of others. For instance, the documents in the result set of a Boolean retrieval run might be ranked with a proba-bilistic retrieval model on the basis of a bag-of-word representation of the same Boolean query. However, it is not clear how to best transform structured queries into flat represen-tations, and, if structured queries indeed contain more information about the information 20 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS need, retrieval models that directly produce rankings on the basis of structured queries have the opportunity to perform better. This section introduces two retrieval model families that provide structured query mod-els to describe information needs, but also produce document rankings. Extended Boolean Retrieval Given its maturity, it is not surprising that various extensions have been proposed to the Boolean retrieval model, including to the range of supported operators. More signifi-cantly, extended Boolean retrieval (EBR) models seek to overcome the main problem of the Boolean model, by providing ranked outputs on the basis of structured, Boolean query expressions [Fox et al., 1992]. Several approaches have been suggested that basically differ only in terms of the re-trieval function and term weighting employed, even though often being derived from dif-ferent theoretical frameworks. The soft Boolean retrieval model [Radecki, 1979] is based on fuzzy set theory and considers query terms as fuzzy document sets in which every doc-ument has a partial membership, even if it does not contain the term. For instance, a fuzzy membership function can be used that is correlated with the frequency of the term in the documents, but still assign a retrieval score R(q, d) > 0 if the document is missing the term. To implement the EBR operators the same retrieval functions are employed as in the Boolean model (Equation 2.2–2.4). As a result, a range of different retrieval scores is assigned to the documents by way of which they can be ranked. Other approaches to EBR include those of Waller-Kraft [Waller and Kraft, 1979], Paice [Paice, 1984], p-norm [Salton et al., 1983c] and Infinite-One [Smith, 1990]. Lee [1995] gives a good overview of the properties and retrieval functions used by these models. He also points out that only the p-norm model has key properties that promote good rankings [Lee, 1994]. This is in agreement with experimental results for the systems on several collections [Lee and Fox, 1988]. For these various reasons, the p-norm approach to ex-tended Boolean retrieval is a particularly interesting method to generate ranked outputs on the basis of complex, structured queries. Moreover, Baeza-Yates and Ribeiro-Neto [1999] 21 2.1. INFORMATION RETRIEVAL MODELS conclude that the model provides a neat framework, generalizing Boolean and vector space approaches, and that while it has not yet found much attention, it might reveal itself useful in the future. The p-norm model, first described by Salton et al. [1983c] and Fox [1983a], defines retrieval scores to be in the interval [0, 1]. Hence, any term weighting scheme, including binary term weights, can be used. Instead of interpreting the two basic Boolean operators logically, the model measures distances in the vector space over the query terms using a particular p-norm. The retrieval score for disjunctions is defined as the distance from (0, 0), the point that is to be avoided, whereas conjunctions seek to get close to (1, 1), the point that is desired. Figure 2.2 shows term weight combinations for two terms that result in an output score of 0.5 when one of the three selected p-values is used. For p = 2, the attained retrieval function is equivalent to the Euclidean distance. The recursive retrieval functions for the three basic operators of the model are: ROR,p(q1, . . . , qm, d) = [ 1 m m∑ i=1 R(qi, d) p ]1/p , (2.10) RAND,p(q1, . . . , qm, d) = 1− [ 1 m m∑ i=1 ( 1−R(qi, d) )p]1/p , (2.11) RNOT(q, d) = 1−R(q, d) . (2.12) For each operator, the user can freely choose a value for p between 1 and ∞.1 When p = ∞ and terms have binary weights, the model reduces to standard Boolean retrieval and produces the same output; and when p = 1 the formulas reduce to a vector space formulation in which there is no difference between conjunctions and disjunctions. Salton and Voorhees [1985] and Fox and Sharan [1986] discuss values for p. The overall scoring function for an EBR query is the overall recursive application of the retrieval functions, governed by the query tree. The final similarity score can hence be com-puted bottom-up, starting from term weights in the query leaves based on information from 1The triangle inequality is a defining property of distance measures and only holds for p ≥ 1. 22 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS w1 w 2 0 1 0 1 p=∞ p=2 p=1 (a) Disjunction (OR) w1 w 2 0 1 0 1 p=∞ p=2 p=1 (b) Conjunction (AND) Figure 2.2: Equi-distance lines for an output score of 0.5 and different p-values for (a) disjunctions and (b) conjunctions of two terms with weights w1 and w2, respectively. The p-value determines what the weight for termw2 needs to be for each value of w1 in order to retain an output score of 0.5. That is, it shows the extent to which one term can compensate for the absence of another term, if at all possible [Salton et al., 1983c]. inverted lists. Every query node outputs an intermediate score that refers to the similarity of a document with the corresponding query sub-tree. Any Boolean query can be transformed into a p-norm extended Boolean query by as-signment of p-values to the operators: [ (persistent ORp=2 chronic) ANDp=1 pains ] ANDp=∞ NOT neuropathic In the example, a p-value of∞ enforces strict Boolean evaluation so that documents con-taining the term "neuropathic" will have a retrieval score of 0, independent of other docu-ment properties with regard to the query. Conjunctions and disjunctions become equivalent when a p-value of 1 is assigned and simply return intermediate scores that are proportional to the score of each clause. In contrast, a p-value of 2 in the disjunction results in interme-diate scores that are dominated by the term with the largest term weight. Depending on the query structure, the choice of p-values can influence the document order in the produced rankings. 23 2.1. INFORMATION RETRIEVAL MODELS Inference Network Model The family of inference network models is an alternative to EBR models and also pro-duces ranked output and allows structured queries. These probabilistic models are based on the formalism of Bayesian networks, a special form of graphical model [Bishop, 2007, Chapter 8], and have the ability to perform ranking on the basis of multiple representations of an information need, whether a Boolean or a bag-of-word keyword query [Croft et al., 2009]. An inference network consists of nodes and directed edges, forming a directed acyclic graph (DAG) [Turtle and Croft, 1990, Turtle, 1991]. Nodes represent binary random vari-ables, are of different types, and can be categorized into document, index term, and query nodes. Edges between documents and terms indicate that occurrence of these documents yields improved belief on their term nodes. A query then aggregates the beliefs of some of the term nodes by way of query operators such as "weighted-sum", "AND" and "OR", and edges between term and operator nodes. Finally, the (possibly) multiple query representa-tions in the query nodes are combined in an information need node which represents the overall belief that a document is relevant. How to combine beliefs is determined by the query operator, and can be expressed with link matrices [Greiff et al., 1995, 1999], containing an entry for each of the possible states of each of the predecessor nodes. Because these tables can become very large, it becomes impractical to estimate probabilities for each of the possible state combinations. Hence, simplifying assumptions are made, similar to those of other retrieval models. Several extensions to this basic inference network approach have been proposed, includ-ing the use of language models to estimate probabilities instead of the empirical intuition behind TF·IDF weightings [Metzler and Croft, 2004]. Ribeiro and Muntz [1996] further generalized the belief network model. Inference network models have been found to show slightly increased effectiveness over the p-norm EBR approach [Turtle and Croft, 1991]; however, in practice, the gains have to be weighed against added complexity and efficiency considerations [Greiff et al., 1995]. 24 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS 2.2 Efficient Implementation So far the description of the retrieval models focused on their functionality. An impor-tant orthogonal aspect is the question of how to implement these models efficiently so that resources, such as user time and money, are not squandered. A naı̈ve implementation, not requiring any special data structure, is to simply iterate over all documents in the collection, and to match each of them with the query. This is the approach, employed by text search utilities such as the Unix grep command, and it facilitates document retrieval based on subtle document properties that can be formally defined in complex query languages, such as regular expressions. It is the simplicity of this method that justifies its consideration as a baseline, and as a competitive method in cases where queries are rarely executed, the collections are small, or when the query semantics require the inspection of most of the document content [Manber and Wu, 1994]. However, if vector space retrieval models should be implemented this way, additional passes will be required to determine document collection statistics, required by some of the models. The use of additional data structures can greatly improve query processing times, at the cost of additional space and time overheads for generating and storing these structures. 2.2.1 Inverted Indexes All modern search engine implementations employ a data structure that is equivalent to the index at the back of a book, commonly referred to as inverted index, or inverted file [Zobel and Moffat, 2006, Croft et al., 2009]. Other index data structures have been proposed, most notably signature files and suffix arrays. Signature files are related to digital fingerprinting and Bloom filters because they represent documents as bit sequences, generated from term hashes. The data structure is, however, less flexible than inverted indexes [Zobel et al., 1998]. The operations supported by some suffix array variants are more versatile [Navarro and Mäkinen, 2007], most notably sub-string search. Moreover, documents can be recovered from the data structure so that the original documents do not have to be stored separately. 25 2.2. EFFICIENT IMPLEMENTATION However, most of the proposed variants assume all data to fit into memory and even in this context, performance comparisons with inverted indexes have been inconsistent [Puglisi et al., 2006]. Hence, inverted indexes remain the data structure of choice. Data Structures Whereas documents can be seen as lists of terms, an index for a collection can be used to directly access lists of documents that contain particular terms, exemplifying the inverted nature of the data structure. A primary component of every index is the dictionary, which contains an entry for every term in the collection that can be efficiently be retrieved. Each of the term entries point to an inverted list of postings, each of which representing an oc-currence of the term in the collection. Postings are conventionally used to store (internal) document identifiers and, depending on the information required by retrieval models to be supported, further details that qualify the occurrence of the term. For example, many ranked retrieval models require term frequencies fd,t for every document d containing a query term t. And if proximity operators are to be supported, information about term positions inside of the documents needs to be stored too. Moreover, document frequencies ft count the num-ber of postings and give an indication of the commonness of the term, but are historically stored next to the dictionary entries. Finally, a document map translates the internal docu-ment identifiers back into their original form to locate the retrieved documents. Figure 2.3 depicts a typical index layout not including the document map. With the help of an index, queries can be processed much faster than with the naı̈ve implementation on basis of the information in the inverted lists. For example, a disjunctive query can be evaluated by examining the inverted lists for the query terms and computing the union of their lists of documents. This set of documents is commonly referred to as the OR-set of the query terms. In this context, a beneficial property of natural language text is that most of the inverted lists are short. And because queries are short in contrast to the number of terms in a reasonably sized collection, much less data is involved in query processing than with the naı̈ve method. 26 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS information 10 retrieval 2 reviewing 4 systematic 4 Term t ft . . . . . . . . . . . . 2, 3 4, 1 5, 3 7, 5 9, 1 11, 2 12, 6 18, 3 26, 1 27, 8 18, 1 29, 5 4, 2 16, 1 19, 3 27, 5 9, 1 16, 3 18, 9 29, 2 〈d, fd,t〉∗ Vocabulary Inverted Lists Figure 2.3: Inverted index example with hypothetical document-ordered inverted lists for selected dictionary terms. The postings consist of document identifier and term frequency pairs 〈d, fd,t〉. The document identifiers d can be used to locate the original document. Popular implementations order inverted list postings by document identifier [Croft et al., 2009]. One obvious reason for this is the simplicity of operator implementations. For instance, the conjunction of query terms can be computed by merging the presorted, in-verted lists in a process somewhat similar to that used in merge sort. Alternatively, the documents from a very short list can be looked up in other lists if additional data structures are available to support these operations. Finger-search is an efficient method which uses a search tree to navigate between inverted list entries in sub-logarithmic time [Culpepper and Moffat, 2010]. In the context of ranked keyword retrieval, other approaches include frequency-sorted [Persin et al., 1996] and impact-sorted indexes [Anh and Moffat, 2006a]. These index organizations support methods for producing approximate rankings without ex-haustive examination of all postings in the query OR-set. However, in the case of Boolean filters, or when exact results are required, these methods revert to exhaustive evaluation, and it is not obvious how they can be combined with more efficient implementations such as those described in Section 2.2.2. In the rest of this thesis, we assume document-ordered indexes. If a search engine is to support proximity operators, it has to store term positional in-formation in its index. In practice, different layouts for such positional indexes occur. It is straightforward to interleave the term positions into the inverted lists and make them part 27 2.2. EFFICIENT IMPLEMENTATION of each individual posting [Witten et al., 1999]. This has the advantage that the positions are directly accessible at examination of each posting so that the lists can still be read se-quentially, in effect, minimizing cache faults and, if applicable, disk seeks. However, not all queries use proximity operators and, even if they do, some query operators within a query might still ignore them. Then, the positions are unnecessarily read and accessed, just to be bypassed. As an alternative, the positions can be stored separately as part of each inverted list, or in a different area of the inverted index; and then only examined when they are needed [Anh and Moffat, 2006b]. This layout decreases overhead when positions are not required, but otherwise leads to more costly, co-sequential access patterns. Finally, special auxiliary index structures can be an option, for instance if only phrase queries are of interest [Williams et al., 2004]. Compression Further efficiency gains can be achieved through compression. Before the information in inverted lists can be processed, it has to traverse the memory hierarchy: starting from large, but slow block-oriented media, such as magnetic disk drives, up to fast, but relatively tiny processor caches. Compression means that more data can be stored and transferred through-out this hierarchy, making more efficient use of resources. Hence, if decompression over-head can be kept small so that processor load stays below capacity, compression not only reduces space requirements but also results in time savings during query processing. Key to gains through compression is how the information in the inverted lists is represented. Effective compression requires that the integers in the inverted lists are not randomly distributed, and that the frequent numbers be represented with smaller codes. Fortunately, highly skewed term frequency distributions are prevalent in natural language texts. But even for positions and document identifiers, this property can easily be attained with delta-encoding if they are stored in document number order. In this encoding scheme, the first 28 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS integer is carried over, but then, only the differences of consecutive integers, the d-gaps, are stored. For example, the sequence of numbers: 2, 4, 5, 7, 9, 11, 12, 18, 26, 27 can be encoded as: 2, 2, 1, 2, 2, 2, 1, 6, 8, 1 If inverted lists are processed in order and from the beginning, the original sequence can be recovered. Several bit-aligned codes have been proposed, including unary, gamma, delta and Go-lomb [Witten et al., 1999], each of which is most effective on a particular frequency distri-bution. Although these codes are space efficient, they incur non-trivial bit manipulation for which current processors are not optimized. Byte-aligned codes, such as the variable-byte (vbyte) code, can provide for a better trade-off point; still providing reasonable compression performance, but faster decoding [Scholer et al., 2002]. In this compression method, one bit in every byte is reserved to indicate if the bits in the next byte are also part of the current integer. Recent work shows, however, that at the same time better compression and even faster decoding speeds can be achieved with block-oriented coding schemes that decode multiple integers at once and reduce the number of conditional branches in the code. The Simple-9 [Anh and Moffat, 2004] and PFOR-Delta codes [Héman, 2005] are examples for this class of algorithms. Augmentations Compression increases efficiency, however, the possibility of random access into inverted lists is lost, if variable-length codes are used. A consequence is that operations such as conjunctions require merging of the inverted lists, that is, examination of all postings in the query term OR-set. Moreover, while the result set size is potentially decreasing with every additional query term, the OR-set is increasing. In the past, Moffat and Zobel [1996] realized this effect and proposed the introduction of skip information into the compressed 29 2.2. EFFICIENT IMPLEMENTATION inverted lists so that posting blocks can selectively be bypassed. Moreover, the approach can be applied recursively, so that skipping can be performed on multiple levels with dif-ferent skip lengths, and when inverted list entries are compressed [Boldi and Vigna, 2005, Campinas et al., 2011]. This guarantees logarithmic worst case access time to any inverted list entry. As a result, the number of examined postings and query processing times are decreased despite some minor loss in compression efficiency. Another application for skipping is in implementations that store term position infor-mation as part of the postings. Not all query processing modes require access to this in-formation. The term frequencies fd,t indicate the number of positions, which could be represented in a varying number of bytes due to compression. Then all positions have to be examined in order to advance to the next posting. The addition of skip pointers to all of these fine-grained information entities allows to skip the position information, but can be-come a significant overhead. If skip information is introduced selectively, or when the skip information is used instead of term frequencies, significant efficiency gains can be attained without sacrificing effectiveness [Pohl and Moffat, 2008]. Due to changing trade-offs in current hardware, efficient representations for inverted lists are still an active area of research [Culpepper and Moffat, 2010]. 2.2.2 Query Processing How to use the information in the inverted lists to process queries is an important design decision. Although the choice for an evaluation strategy is generally orthogonal to the particular index layout, not all possible combinations are practical and competitive. In fact, the data layout has to support the particular algorithm if the full potential of the method is to be realized. Two principal query processing strategies for inverted indexes are term-at-a-time and document-at-a-time [Croft et al., 2009]. Term-at-a-time Evaluation In data-driven applications disk related costs are seen to dominate overall running times. As a consequence, many designs optimize for these costs, even if this means that additional 30 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS processor resources are deployed. An evaluation strategy along these lines is to iterate through the inverted lists of the query terms, one term at a time. Moreover, the order in which they are processed can be based on their physical location so that disk seek times are reduced [Bast et al., 2006]. The advantage of this method is that data access is sequential and random accesses are minimized and are proportional to query size [Kaszkiel et al., 1999]. If inverted lists have to be read from disk and overall execution times are dominated by disk seeks, term-at-a-time is an appropriate choice. The disadvantage is that non-trivial amounts of memory can be required during query processing to keep track of the partial evaluation of documents. For evaluation of ranked keyword queries, each document is typically assigned an accumulator which stores its current, partial document score. While it is possible to iterate the postings of all terms this way, Moffat and Zobel [1996] suggest to either "quit" the evaluation after a pre-specified number of accumulators have been created, or to "continue" to fully score the documents already in the accumulators, skipping to them in the remaining inverted lists. If the iteration starts with the term of highest score contribution, that is, with the smallest document frequency, large efficiency gains can be achieved in terms of query execution time while effectiveness levels keep high [Moffat and Zobel, 1996]. Similarly, conjunctive queries can benefit from the availability of skip information and be efficiently implemented under the term-at-a-time strategy. Pseudo-code for conjunctive evaluation is provided in Algorithm 1. The algorithm starts with the set of document identifiers in the shortest inverted list and then tries to navigate to each of them in the next shortest inverted list. With every term, the set of candidate documents is likely to be reduced, so that less documents have to be looked up in the longer lists. As a result, fewer postings need to be examined. Although the number of accumulators can be reduced with pruning techniques [Lester et al., 2005], their necessity is a main criticism of the term-at-a-time evaluation method. Moreover, the accumulators and operator implementations have to be much more complex for queries that contain proximity operators or are otherwise structured. 31 2.2. EFFICIENT IMPLEMENTATION Algorithm 1: Conjunctive term-at-a-time processing with skipping Input : An array of inverted lists L of length n Output: The set of documents that satisfy the conjunction Data : A document-ordered accumulator A, containing candidate documents 1 Sort L by increasing ft 2 A←L1 . initialize with shortest inverted list 3 for t← 2 to n do . iterate over remaining query terms 4 for d ∈ A do . iterate over candidate documents in order 5 Lt.skipTo(d) . skip list Lt to d, or the next posting thereafter 6 if Lt.docid 6= d then A← A− {d} . remove d from candidate documents 7 end 8 end 9 return A . all candidate documents in A contain all query terms Document-at-a-time Evaluation Full evaluation of one document at a time is conceptually more straightforward because all computation can be done at once and no intermediate storage is required, except for what is involved in scoring the current document. Document-at-a-time evaluation also greatly sim-plifies operator implementations for structured queries [Strohman et al., 2005]. However, document-ordered posting lists are required so that all postings of a document for the query terms can easily be accessed when the lists are concurrently processed and thereby merged. A problem with this method is the increased number of random accesses; data has to be read from multiple locations for each document in the query OR-set. Although considered a less favourable approach in the past, several factors mitigate the severity of this problem: 1. If the data has to be read from disk, read-ahead caches in disk drives and operating systems are implicitly filled with more data from each inverted list before the disk seeks to a new position in another inverted list. 2. Furthermore, inverted list data for frequent terms is likely to be cached anyway and readily available from memory. 3. Explicit buffering of list data gives more control over memory use and can further decrease random disk accesses. 32 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS Algorithm 2: Conjunctive document-at-a-time processing with skipping Input : An array of inverted lists L with length n Output: The set of documents that satisfy the conjunction Data : Answer set A 1 Sort L by increasing ft 2 A← ∅; i← 1; d← L1.next() . initialization 3 while Li did not reach list end do . no more results, once any list end is reached 4 if Li.docid 6= d then 5 d← Li.docid; i← 1 . restart with new document d 6 else 7 if i = n then . all lists are positioned on d 8 A← A+ {d} 9 i← 1; d← L1.next(); continue . start over with next d 10 else 11 i← i+ 1 . advance to next list 12 end 13 end 14 Li.skipTo(d) . skip list Li to d, or the next posting thereafter 15 end 16 return A 4. The additional memory requirement for list buffers might be similar as needed for accumulators, but here it can be chosen as a space-time trade-off. 5. Finally, if memory capacity allows, all index data can be held permanently in memory, entirely eliminating disk seek and transfer times during query processing. Also in this processing mode, skip information can be beneficial for the implementation of operators. An implementation of a document-at-a-time Boolean conjunctive operator is described in Algorithm 2. This implementation also starts with the shortest list and repeat-edly advances all other lists to the same (or a greater) document identifier. If this can be done, another document is found for the result set, satisfying the conjunction. Otherwise, the shortest list is advanced to the next document found in the current list. 33 2.2. EFFICIENT IMPLEMENTATION Ranked Retrieval Because ranked retrieval systems generally allow partial match between queries and docu-ments, all documents that contain at least one of the query terms become part of the ranking. This is equivalent to the execution of a disjunctive Boolean query, with an additional over-head for scoring. Instead, queries could be executed as conjunctions so that, in effect, the ranking con-sists only of documents that contain all of the query terms. This might be fine if enough documents are returned, as often is the case for short queries and in large collections. Web search is a particular IR application domain with these properties. But otherwise, empty result sets are likely, and the advantages of partial match lost. Fortunately, another type of optimization is possible when ranked answer lists are required. The typical behaviour of ranked retrieval system users is to inspect the ranking top-down until the information need is satisfied, or until the search is abandoned due to little further expected gains [Granka et al., 2004]. Users typically use a variety of stopping rules to determine how far to go down in result rankings before the search is stopped [Kraft and Waller, 1981, Dostert and Kelly, 2009]. But if the number of documents that a user is likely to examine, k, can be upper-bounded, the system only has to generate the top k answers. For instance, web applications commonly present 10 result documents at once. Then, either k is set to 10 and, if the user requests more documents, the query is re-executed with a larger value for k; or, k is directly set to a larger number to decrease the likelihood of re-executions. Several such lazy query evaluation and pruning algorithms have been proposed for different index arrangements, of which we review one that can be coupled with document-ordered indexes [Turtle and Flood, 1995, Theobald et al., 2004, Anh and Moffat, 2006a]. Max-Score Originally presented in the context of keyword queries and simple sum retrieval functions, the Max-Score optimization is an effective and safe method to retrieve k top-scoring doc-uments [Turtle and Flood, 1995]. The method reduces two cost factors: the number of 34 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS documents that have to be examined; and the cost associated with the scoring of each of the remaining documents. Assuming document-at-a-time evaluation, the algorithm starts by fully scoring the first k documents in the OR-set of all query terms. Thereafter, the lowest score of any of these currently k highest scoring documents is tracked and serves as an entry threshold τ , which the remaining documents have to surpass to make it into the top-k. Before the evaluation continues, all query terms are sorted by decreasing document frequency ft. Moreover, for each of the terms ti, with i = 1, . . . , n, the maximum term score contribution s∗i made by this term to any document in the collection is determined and stored during indexing. Then, for each of the terms ti in the ordering, a cumulative maximum score c∗i is computed for a hypothetical document containing all of the terms t1, · · · , ti. The score c∗i+1 can efficiently be computed from c∗i by adding the maximum term contribution s∗i+1. During the processing of a query, the entry threshold τ monotonically increases. When τ > c∗1, it can be concluded that documents containing only the commonest query term t1 and none of the other terms, can not be ranked in the first k results. At this point, the set of candidate documents can be reduced to the OR-set of t2, · · · , tn. Because the commonest term is removed first from the OR-set, many documents can be skipped in these lists. Processing then continues until τ > c∗2. At that point, the second term can also be removed from the OR-set. Moreover, if the retrieval model down-weights the influence of common terms via some IDF-component in the retrieval function, the c∗i Max-Scores for these terms will be small, accelerating their removal from the OR-set. Independently, the Max-Score bounds can also be used to short-circuit the scoring of candidate documents. After examination of the first i inverted lists and an intermediate score sum of si, the final score of the document can not reach the threshold if si + (c∗n− c∗i ) < τ . Figure 2.4 depicts the processing scheme for a hypothetical ranked query and document collection, including examples for both of the cases, in which postings can be bypassed. For the first two documents in the query OR-set, all postings are inspected and com-plete document scores computed. Note that the actual retrieval scores R(q, d) are naturally 35 2.2. EFFICIENT IMPLEMENTATION i s∗i c ∗ i 2 4 5 7 9 11 † 12† 16 18 19† 26† 27† 29 1 information 1 1 2 reviewing 2 3 3 systematic 2 5 4 retrieval 3 8 R(q, d) 0.5 1.8 0.2 1.0 2.5 3.5 5.0 ≤3.2 top-k 2 4 2 4 2 4 7 9 4 16 9 18 16 18 16 τ – 0.5 0.5 1.0 1.8 2.5 3.5 3.5 documents ft k = 2 Figure 2.4: Document-at-a-time processing of the query "information retrieval systematic review-ing" with Max-Score to retrieve the top k = 2 documents. The postings have been aligned for illustration purposes and the maximum score contributions s∗i are assumed to be integer values for simplicity. The grey boxes refer to postings that have been skipped, either because the documents only contained terms that have been excluded from the OR-set (denoted with †); or, as in case of doc-ument 29, because it can be concluded after examination of the first three terms that the document can not reach τ . The dashed frames contain the postings of the current OR-set. smaller than the sum of the maximum score contribution s∗i for the terms present in each document. At this point, the threshold τ is initialized to the score of the lowest ranked doc-ument in the current top-k (that is, document 2 with a score of 0.5). In the example, three more documents have to be scored until τ exceeds s∗1 = 1, at which point the OR-set is reduced to include only documents that contain at least one of the terms 2, 3, or 4. The doc-uments 12 and 13 can hence be safely ignored and are skipped, but term 1 is still inspected for postings of each document in the now reduced OR-set. Due to another OR-set reduction after scoring the documents 16 and 18, further documents can be ignored. After inspection of the first three terms and an intermediate score sum si of 0.2 for document 29, it can be concluded that the document can achieve a score of at most 0.2 + (8 − 5) = 3.2 < τ and thus can not make it into the top-k. As a result, large efficiency improvements are gained when the Max-Score optimization is employed, close to these of conjunctive processing; but with the added benefit that the required number of documents is retrieved each with their correct score so long as the initial all-terms OR-set contains at least k documents. 36 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS This section reviewed basic implementation approaches of Information Retrieval sys-tems. By means of the described algorithms and an appropriate choice of the underlying data structure, query processing costs can significantly be reduced. In the next section, frameworks and measures are introduced that can be used to evaluate Information Retrieval systems. 2.3 Information Retrieval Evaluation In order to objectively measure the effect of system changes, such as the use of a differ-ent retrieval function or implementation, evaluation is required. Interestingly, approaches that have better theoretical foundations, or are otherwise considered superior, do not always translate into significantly better system performance. To guard against subjective pref-erence of any model, evaluation frameworks define performance measures and collection data, on which the metrics underlying the measures can be directly gauged. This section first reviews IR evaluation frameworks. A discussion of widely used effec-tiveness and efficiency measures then follows. 2.3.1 Framework The task of every IR system is to retrieve documents from a document collection that are relevant to information needs expressed as queries. Hence, to measure a system's ability to produce useful output, what is required is a document collection, queries, and information about the relevance of the returned documents. Moreover, these test collection resources have to be fixed and shared between researchers to allow the comparison of different IR methods. In this regard, two historically important evaluation frameworks are described in the following. The Cranfield evaluation framework, designed in the early 1960s, was primarily used to evaluate the effectiveness of Boolean retrieval approaches on the basis of manually in-dexed terms of library documents [Voorhees, 2002]. In order to find all relevant documents, entire document collections had previously been evaluated for every test information need. 37 2.3. INFORMATION RETRIEVAL EVALUATION To make comprehensive evaluations feasible and to leverage experimentation on the limited computational resources at that time, collections had been limited to relatively small sizes. Moreover, complete relevance judgements have been found more important to evaluate sys-tems than large collection sizes [Cleverdon, 1991]. The test collections, built to evaluate the SMART system, followed the same framework [Salton, 1971]. Later, when typical collections increased in size, experience showed that collection size is indeed an important factor and should not be ignored. In 1992, the Text REtrieval Con-ference (TREC) was initiated by the National Institute of Standards and Technology (NIST) and the U.S. Department of Defense to support the IR research field to perform large-scale evaluation of text retrieval methods.2 Yearly, TREC challenges researchers to test their sys-tems in tracks consisting of representative retrieval tasks. For this, document collections, performance measures and information-need descriptions, so-called topics, are provided for which participants submit retrieval runs. These runs are then evaluated and the results pub-lished at the TREC conference. Because collections are too large to provide comprehensive relevance judgements upfront, instead a pooling process is employed [Voorhees and Har-man, 2005]. Here, only a subset of the documents contained in the submitted retrieval runs are retrospectively inspected and judged for relevance. All other documents are assumed to be not relevant. Over the years of the program, many TREC tracks have been run, including the ad hoc, filtering, web, legal and genomic tracks. Similar methodological frameworks have been adopted by other evaluation forums such as CLEF [Braschler and Peters, 2004], NTCIR [Kando et al., 1999] or INEX [Fuhr et al., 2002], which differ in the targeted re-search field, tasks or language. 2.3.2 Effectiveness Measures Retrieval effectiveness measures gauge different aspects of system performance on the basis of the system output for one or a set of queries. A proper choice is hence dependent on a particular user task and on the system output; and as a result, several effectiveness measures are in common use. Lack of particular features in the existing measures, for instance, the 2See http://trec.nist.gov/overview.html for a comprehensive description of the program. 38 http://trec.nist.gov/overview.html CHAPTER 2. BACKGROUND & GENERAL CONCEPTS Relevant Retrieved Yes No Total Yes TP FP |P| No FN TN |N | Total |R| |S| |C| Table 2.1: Contingency table of documents' relevance and retrieval status for one query. support of graded, or incomplete relevance judgements are also another reason for new evaluation measure proposals. To introduce a selection of pertinent measures, the following definitions are helpful. Assuming binary relevance with regard to an information need, every document in the collection C belongs either to the set of relevant documentsR, or to the set of irrelevant doc-uments S. Systems that generate a document result set for a submitted query also categorize the documents in the collection into two classes. They can hence also be seen to perform a classification task, assigning the positive class P to the documents that are returned, and the negative class N to all other documents. In general, these two categorizations can be considered orthogonal, splitting the documents in the collection, C, into four distinct sets, as shown in Table 2.1. Obviously, it is desirable that a system's output P is equal to the relevant document set R, that is, the number of true positives (TP) is equal to |P| and |R|; and, the number of true negatives (TN) is equal to |N | and |S|. This necessitates that the number of false positives (FP) and false negatives (FN) is 0. Several performance measures can be defined on the basis of the table entries, describing the degree of achieving this ideal, or aspects thereof. Precision and Recall The classic effectiveness metrics in IR are precision (P) and recall (R), which are defined as: P = TP TP + FP ; R = TP TP + FN . (2.13) 39 2.3. INFORMATION RETRIEVAL EVALUATION Because each formulation only incorporates one type of error, the two aspects of system per-formance are separately measured. Precision P is a measure of exactness and is maximized when every document in a system's result set is relevant. In contrast, recall normalizes TP with a different denominator, the number of all relevant documents, and is hence a measure of the completeness of a system's result set. In the biomedical field, a more common name for recall is sensitivity. Precision can be determined by assessment of the relevance of each document in the result set, which is typically small for practical Boolean queries. Measurement of recall, however, requires an estimate of the number of relevant documents in the collection. For small collections, such as those used in the Cranfield and SMART experiments, all docu-ments in the collection could be judged. In larger collections it appears plausible to use statistical sampling strategies in order to make estimates for the whole collections. But typ-ically, immense uncertainties are associated with such estimates because usually only a tiny proportion of a collection is relevant. If comparative performance is of interest, it is often enough to employ relative recall, which normalizes the number of retrieved relevant documents (TP) by the number of known relevant documents [Sampson et al., 2006b]. Then care has to be taken when conclusions are drawn based on the absolute value of the measure. Experience shows that some researchers conflate the meaning of these recall-related concepts [Zobel et al., 2009]. Although precision and recall are defined on the basis of sets, they are also widely used in the context of ranked retrieval systems, assuming some prefix of the ranked output list to be the result set. For instance, precision-at-10 measures the fraction of relevant documents within the 10 first entries in the ranked list, and is popular in application domains such as the web where users are most likely to only look at the 10 results, shown on the first result page. More generally, rankings can be characterized by precision-recall curves that manifest when precision and recall are calculated at every rank cut-off and reported over these two axes. 40 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS F-Measure The F-measure is a single value summary of a system's precision and recall. Because it is based on the harmonic mean between the two values, good performance in both basic measures is emphasized. Composite measures raise the question of what influence each of the basic measures should have. In situations where one of the basic measures is more important than the other, it is desirable to emphasize either measure in the calculation of the overall value. A more general formulation of the F-measure is: Fβ = (1 + β 2) P ·R β2P +R , (2.14) where β is a weight that controls the importance of recall over precision. The common case where precision and recall are equally weighted can be recovered by setting β = 1. Values of β > 1 increase the influence of recall; values of β less than 1 emphasize precision. While the F-measure is still widely used in evaluation, it appears to be of limited use in IR contexts in which it is easier to achieve high recall than high precision [Kamps et al., 2008]. Then, the F-score for systems that simply return the whole collection (R = 1 and P → 0) might be higher than for any other system. Adjustment of β might only be of limited help in such cases. Average Precision Another composite measure is average precision (AP). It averages over the precision values at the rank positions of relevant documents, and is formally denoted as: AP = ∑ d∈R P (rd) |R| , (2.15) where P (rd) refers to the precision at the rank at which document d appears in the output. In order to reliably evaluate the performance of systems, many representative queries are required. AP, however, only generates a single-value summary for each query. Mean 41 2.3. INFORMATION RETRIEVAL EVALUATION average precision (MAP) constitutes one approach to aggregate individual AP-values by simply taking their mean. MAP is commonly used in application domains such as the World Wide Web. Note, however, that the definition of AP in Equation 2.15 assumes a ranking that con-tains all relevant documents, which is only guaranteed when the whole collection is ranked and returned to the user. In practice, AP is often calculated over the relevant documents that are actually contained in the ranking (the subset of size TP of that ranking), or over the rele-vant documents returned in the top 1,000, the typical cut-off used in the TREC evaluations. Rank-Biased Precision An alternative to AP, which does not require knowledge about all relevant documents, is rank-biased precision (RBP) [Moffat and Zobel, 2008]. It assigns contributions to each rank position that follow a geometric distribution, and are then summed over the relevant documents. The RBP measure is hence monotonic in the number of relevant documents and is defined as: RBPp = (1− p) ∑ d∈P∩R prd−1 , (2.16) where rd is the rank of document d, 1 < rd < |S|; and p is in the interval [0, 1), reflecting the persistence of the user. The formulation allows for a probabilistic interpretation of user behavior. A user is assumed to always inspect the top ranked document for relevance; and then to either continue to inspect the next document with a probability p, or, conversely, to abandon the examination (with a probability of 1 − p). The persistence parameter p can be chosen to fit the distribution of ranks at which users typically abandon their search, for instance, derived from click-through logs [Zhang et al., 2010]. Finally, the factor (1 − p) ensures that an optimal ranking, containing only relevant documents and having infinite length, receives a maximum RBP-value of 1, effectively normalizing the measure to a score interval of [0, 1] and hence, making it comparable between rankings. A nice property of RBP is that the impact of any documents' relevance status onto the final RBP-value can be quantified solely on the basis of its rank. This allows uncertainty 42 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS about document relevance to be handled by calculation of lower and upper score bounds. The true RBP score will then lie somewhere in between these bounds. A common source of uncertainty is regarding the relevance status of the tail of the ranking, which is either not inspected by the user, or might otherwise not be available. In this case, a base score can be calculated with Equation 2.16 with the set of retrieved documents P containing the examined ranking prefix only, and the residual error for a ranking examined until depth d can be bound by pd. Area under the ROC curve Many retrieval measures reward systems that retrieve relevant documents at the top of the ranking, reflecting the system's usefulness for users that inspect the ranked list top-down and are likely to stop early in the retrieved list. This behavior is typical in the web domain. On the other hand, highly persistent users might be almost as likely to review the documents retrieved first or, say for instance, at rank 100. A receiver operating characteristic (ROC) curve is a two-dimensional representation of the performance of a system that outputs a ranking over documents in the collection and assigns the positive class to the documents with a ranking score above some threshold (respectively rank) and the negative class otherwise [Croft et al., 2009]. For each rank cutoff, the true positive rate (TPR) and false positive rate (FPR) can be calculated as: TPR = TP TP + FN , FPR = FP FP + TN . (2.17) If the resulting coordinates are drawn over these axes for a ranking of the whole collection, a curve results that connects the points (0, 0) and (1, 1) and by way of which the performance of systems can be compared for different rank cut-off choices. Note that TPR is just another name for Recall, and that FPR is inversely correlated with Precision. A ROC curve is hence very related to a Precision-Recall curve. However, for ROC curves a widely used measure has been introduced that allows to subsume the whole curve into one figure, the area under the ROC curve (AUC). While summary statistics also have been suggested for precision and 43 2.3. INFORMATION RETRIEVAL EVALUATION recall, such as the F -measure, they typically trade precision against recall at a single ranking depth, but do not summarize the characteristics of multiple precision recall value pairs. By means of the normalizations of the axes, the optimal AUC value is 1. The average performance of a baseline that assigns random class labels according to the prior probabili-ties of the classes would achieve a value of 0.5. AUC can trivially be calculated on the basis of trapezoidal approximations. However, Bamber [1975] shows that AUC is equal to the number of correct pair-wise orderings normalized by all possible orderings: AUC = ∑|R| i=1 ∑|S| j=1 Iri<rj |R|·|S| , (2.18) where |R| and |S| are the number of relevant and irrelevant documents, respectively; and Ix refers to the indicator function that returns 1 if its condition x is satisfied, and 0 otherwise. The condition ri < rj is satisfied when the relevant document i has a lower rank ri than the irrelevant document j. This makes AUC also equivalent to the Mann-Whitney U test statistic [Mann and Whitney, 1947], typically used in the Wilcoxon significance test. Equation 2.18 can easily be rearranged to: 1 |R| ∑|R| i=1 # irrelevant docs ranked below ri |S| , (2.19) which reveals another interpretation of AUC as being the average number of irrelevant doc-uments ranked below each relevant document, normalized by the number of irrelevant doc-uments. An increase in AUC is thus equivalent to an average increase in the position of relevant documents in the ranking produced for a query. Hence, AUC is a measure of choice to compare rankings when the positions of all relevant documents are of interest. Other measures Several other effectiveness measures have been suggested that balance precision and recall in different ways, and vary in their other properties, such as support of graded relevance, and incomplete judgements. 44 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS In question-answering tasks, it is often enough to find any relevant document that con-tains the answer to the question and is considered trustworthy. User satisfaction with a system will then be inversely correlated with the number of documents they have to exam-ine before they find the first relevant document. The reciprocal rank (RR) is an effectiveness metric that reflects this user behavior [Voorhees, 1999]. Discounted cumulative gain (DCG) is similar to RBP in that it assigns decreasing score contributions to document ranks and monotonically increases with each relevant document that is encountered in the ranking. In its original form, this measure suffers however from the problem of being unbounded. Normalized versions, such as NDCG, again require the estimation of the number of relevant documents [Järvelin and Kekäläinen, 2002]. Buckley and Voorhees [2004] proposed BPref as a measure, based on pairwise binary preference between relevant and irrelevant documents. Despite being designed to be more robust than measures like AP in the face of incomplete relevance judgements, the number of relevant documents is part of its formulation. Magdy and Jones [2010] describe the PRES metric for recall-oriented tasks as a func-tion of recall, the ranks of the retrieved documents and the maximum number of result documents a user will be examining. While the measures described above can all be calculated automatically on the basis of the relevance of retrieval units, there is a whole class of other measures which are based instead on user behaviour, such as Expected Search Length [Cooper, 1968], Tolerance-to-Irrelevance [de Vries et al., 2004], PRUM [Piwowarski et al., 2007] and ePRUM [Pi-wowarski and Dupret, 2006]. This thesis focuses, however, on automatic evaluation which does not involve user studies and hence, makes the results easier reproducible. 2.3.3 Efficiency Measures The main function of an IR system is query execution, but to speed up the process of match-ing queries against documents in a collection, additional resources are employed; in particu-lar, an inverted index has to be generated to aid subsequent query processing. Hence, several efficiency measures are typically used to characterize a particular system. 45 2.3. INFORMATION RETRIEVAL EVALUATION The aspect of efficiency that is most visible to IR system users is query latency, that is, the time they have to wait between issuing a query and results being returned. A particular latency threshold of interest is around 150 milliseconds, below which delays are considered instantaneous by humans [Dabrowski and Munson, 2001]. To make efficient use of hard-ware resources, IR systems typically execute multiple queries at the same time. Hence, the number of queries per time unit can be increased above the inverse of average query latency, which makes the case for separate measurement of query throughput. Moreover, throughput and latency are generally conflicting measures so that an increase in desirable throughput typically leads to larger latencies, and vice versa. A general problem with these measures is their dependency on specifics of the underlying hardware. Hence, different methods have to be compared on the same hardware and under fair conditions, or necessitate the use of similar hardware. Otherwise, publication of information about the used hardware allows researchers to make at least rough comparisons. The efficiency task in the TREC Terabyte Track is a particular example, in which this methodology was followed [Büttcher et al., 2006]. Alternatively, surrogate measures can be gauged via simulation experiments that are correlated with query execution time but are more easily repeatable. The size of the index is another figure of interest because enough disk space has to be made available to accommodate the final index and intermediate files which are typically proportional in size. Due to advances in hardware technology, it became practical to cache significant parts of the index in main memory; or, if index size permits, even the whole index. Although less crucial than for query processing, the time it takes to index a number of documents or a document collection can be an important consideration if the system has to be able to keep up with a stream of new documents. Detailed measurements regarding resource usage of CPU, disk, memory and bandwidth can be taken during indexing and query processing to further inform system design deci-sions and choice of desirable hardware configurations. 46 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS 2.4 Evidence-Based Medicine Evidence-based medicine (EBM) is an increasingly popular paradigm in which it is argued that medical decision-making should be based on reliable, contemporary research findings instead of individual expert opinion or possibly outdated beliefs. Clinicians face the chal-lenge of turning clinical problems into specific research questions, systematically searching for evidence related to these questions, critically appraising the findings, and then acting on their outcomes [Rosenberg and Donald, 1995]. However, the number of questions, their complexity, and the immense amount of published research literature, make it too laborious for individual practitioners to perform this task for every clinical question. Staying up to date with newly published evidence presents immense challenges [Helmer et al., 2001]. In order to combine the individual efforts, journals and organizations have taken respon-sibility for coordinating the production and dissemination of systematic reviews that give advice for repeatedly occurring questions of clinical importance [Davidoff et al., 1995]. Moreover, Clinical Practice Guidelines (CPG) collate the conclusions of relevant system-atic reviews, to give practitioners succinct access to the latest evidence, and to determine daily decision-making in health-care. This section describes the systematic reviewing process and the IR methodology that is currently followed. Moreover, rationales for the choice of IR model are given, and problems with the approach are raised. 2.4.1 Systematic Reviews Literature reviews are an important tool for summarizing primary research outcomes on a particular topic or research field, and draw together material which might be scattered over time and different publication venues. In contrast to such narrative reviews, a systematic review (SR) seeks to answer a highly focused question, and employs formal methods to find, analyse and synthesize published research literature. Its principal goal is to minimize bias in the review conclusions, by applying standardized, transparent, rigorous and explicit methods to find a comprehensive set of studies, pertinent to the research question. 47 2.4. EVIDENCE-BASED MEDICINE Organizations In the biomedical domain, the effect of a health-care intervention is a repeating topic of in-terest for care-givers and patients. The Cochrane Collaboration,3 established in 1993, is the principal, most significant international effort to prepare, maintain and publish systematic reviews on these topics. It is named after Archibald Leman Cochrane, an epidemiologist, who stated that [Cochrane, 1979]: It is surely a great criticism of our profession that we have not organised a critical summary, by specialty or subspecialty, adapted periodically, of all relevant randomised controlled trials. The collaboration publishes the Cochrane Library4 on a quarterly basis and consists of an international network of groups and centres, each preparing reviews in a particular area of health-care. One of the databases in this collection is the Cochrane Database of Systematic Reviews (CDSR)5 that, as of 2011, contains more than 4,500 systematic reviews and almost 2,000 protocols (review proposals). Every Cochrane review has to follow the methods that are set out in the Cochrane Handbook for Systematic Reviews of Interventions [Higgins and Green, 2009] and developed by the Cochrane Methods Group. A particular member of this group is the Cochrane Information Retrieval Methods Group that can be consulted for advice on search tasks. Example review titles, taken from the Cochrane Database of Sys-tematic Reviews,6 are "Vitamin C for Preventing and Treating the Common Cold", "Laser Trabeculoplasty for Open Angle Glaucoma", and "Allergen Immunotherapy for Asthma". Several nationally funded organizations, similar in nature to the Cochrane Collabora-tion, include the Agency for Healthcare Research and Quality (AHRQ)7 [Atkins et al., 2005], the American College of Physicians (ACP) Journal Club,8 the Clinical Evidence ef-fort of the British Medical Journal (BMJ) Group,9 and the National Health Services Centre 3http://www.cochrane.org/ (accessed on 12 May 2011) 4http://www.thecochranelibrary.com (accessed on 12 May 2011) 5http://www.cochrane.org/reviews/ (accessed on 12 May 2011) 6http://www.cochrane.org/reviews/ (accessed on 13 May 2011) 7http://www.ahrq.gov/ (accessed on 13 May 2011) 8http://www.acpjc.org/ (accessed on 13 May 2011) 9http://clinicalevidence.bmj.com/ (accessed on 13 May 2011) 48 http://www.cochrane.org/ http://www.thecochranelibrary.com http://www.cochrane.org/reviews/ http://www.cochrane.org/reviews/ http://www.ahrq.gov/ http://www.acpjc.org/ http://clinicalevidence.bmj.com/ CHAPTER 2. BACKGROUND & GENERAL CONCEPTS for Reviews and Dissemination (CRD) at the University of York in England.10 The latter maintains the Database of Abstracts of Reviews of Effects (DARE), which is also published as part of the Cochrane library. The Reviewing Process According to the Cochrane Handbook, the production of a systematic review involves a series of steps [Higgins and Green, 2009]: 1. Objective formulation and development of a review protocol; 2. Locating and selecting studies; 3. Study quality assessment; 4. Appraisal and synthesis, conclusion; 5. Publication; and 6. Regular update, or withdrawal. Because there is no single accepted standard process to follow, we choose to describe the steps followed by the Cochrane Collaboration, one of the most influential organizations in this regard. To initiate a systematic review, first a review protocol has to be produced which, in ad-vance, documents the methods that are to be used. In particular, the objective of the review has to be determined, along with its scope, expressed by explicit inclusion and exclusion criteria. These protocols are then peer-reviewed to minimize bias; for instance, the defini-tion of the review question might be influenced by prior knowledge of the review authors about relevant studies. Once the protocol is accepted, exhaustive search strategies are used to retrieve a set of potentially eligible citations, which are then independently and exhaustively triaged by mul-tiple assessors. For citations that require further examination, the full text is retrieved and examined in detail against the review criteria. The remaining studies are further assessed to arrive at the set of included studies which are then appraised and synthesized, and govern the review's conclusion. Figure 2.5 depicts this process for a typical Cochrane systematic 10http://www.crd.york.ac.uk/ (accessed on 13 May 2011) 49 http://www.crd.york.ac.uk/ 2.4. EVIDENCE-BASED MEDICINE Citations identified (primarily by Boolean search over multiple collections): n = 1,167 Citations retrieved for full-text evaluation: n = 203 Selected studies for assessment & appraisal: n = 39 Studies included in review: n = 37 Citations excluded by screening titles and abstracts (clearly not eligible): n = 964 Ineligible studies excluded: Reason n Studies of pre-emptive therapy 10 Studies of antiviral medications alone 32 Other ineligible interventions 14 Non-randomized patients 73 Review articles 23 Multiple publications 12 164 Studies excluded during detailed analysis (reasons explained in review): n = 2 Figure 2.5: Flow diagram describing the study selection progress for the Cochrane systematic re-view "Immunoglobulins, vaccines or interferon for preventing cytomegalovirus disease in solid or-gan transplant recipients" [Hodson et al., 2007], following the presentation style suggested by the Quality of Reporting of Meta-Analyses (QUOROM) group [Moher et al., 1999]. review. Of the large number of citations that initially have to be screened, typically in the range from hundreds to thousands, only a small fraction are finally included in the review. The quality of these studies then has to be assessed to balance the (possibly contradic-tory) evidence they provide. Once this is done, the extracted data can be appraised and synthesized to form a conclusion for the question of the review. For publication in the Cochrane Library, review authors have to adhere to a specified document structure [Higgins and Green, 2009] containing the sections: Background; Ob-jectives; Methods (including explicit description of the used search strategies); Results; Discussion; Conclusion; and References to included (and possibly related, but excluded) 50 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS studies. Of particular significance is that the search strategies used to arrive at a final, com-prehensively screened document set have to published as part of the review, along with the date of the search; both are required in order to make the retrieval process transparent and reproducible. A sample of reviews can hence be used to generate a retrieval test collection, consisting of the queries used, and the studies that are referenced in the produced review. The work on a systematic review does not finish with its publication. New evidence can become available at any time, and, by the time a review is completed, it could possibly be already outdated. Historically, systematic reviews have been published in journals. This practice has been criticized because limited space does not allow for the required detail in description and also hinders the dissemination of updates [Chalmers and Haynes, 1994]; it might even mislead practitioners relying on recent, but outdated information. The Cochrane Library hence appears quarterly on CD-ROM and online, and requires review authors to update their reviews at intervals of two years or less, otherwise they get withdrawn. Significant effort goes into the creation of every systematic review. Beahler et al. [2000] report that 41,871 documents had to be screened for the creation of 12 systematic reviews. Zhang et al. [2006] estimate that 800 to 1,000 hours are required to finish a typical review. Because reviews are performed by specialists, this can amount to monetary costs of up to a quarter of a million dollars [McGowan and Sampson, 2005]. Locating a comprehensive subset of relevant studies is crucial to the reviewing process. For instance, Avenell et al. [2001] refer to a case in which missed evidence led to the death of a trial participant. This is why a large fraction of the efforts that go into a review is at the time of the search process, and why improvements in the information retrieval methods are of particular significance. This thesis hence focuses on the information retrieval tasks that are part of the lifecycle of systematic reviews, that is, steps 2 and 6 of the reviewing process. 51 2.4. EVIDENCE-BASED MEDICINE Meta-Analyses Randomized Controlled Trials Cohort Studies Case Series, Case Reports Expert Opinions, Clinical Observations Systematic Reviews BiasEvidence Figure 2.6: Hierarchy of evidence. The pyramidal shape symbolizes that there are typically more studies available for categories that provide weaker evidence. Systematic Reviews are currently seen to provide the strongest evidence and smallest bias. (Based on the hierarchies suggested by Hoppe et al. [2009] and Evans [2003].) Study Types and Quality An important part of the synthesis process is to assess the quality of the studies that are included. Of course, studies that do not use sound methods should entirely be excluded. But even sound studies differ in the reliability of their conclusions. For example, a trial that has the sole purpose to demonstrate the safety of a new drug, and has a hundred thousand participants, can be more relied upon than a report of a case in which a patient developed a disease after administering the drug. The latter could be coincidence and also have happened without dispensing the treatment; and is more likely to be biased. The case is, however, very informative to guide decision-making, because the efficacy of the drug can be balanced with possible side effects. Moreover, there are typically more case reports and other forms of weak evidence than there are expensive trials; indeed, there has to be some form of initial evidence to justify an expensive, rigorous trial. The experimental setup, primarily determining the type of the study, plays an important role for quality assessment. Figure 2.6 depicts these trade-offs for a selection of study types. Randomized Controlled Trials (RCTs) are one of the most rigorous study types [Hig-gins and Green, 2009], characterized by random assignment of participants to one or more treatment groups or a control group and anonymizing identifiers while data is being ana-lyzed. Bias can further be reduced by "blinding", that is, not revealing the assigned group 52 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS to participants, caregivers or investigators, for example, by the use of placebos in the control group. Historically, these study designs have been referred to as single, double, and triple blind RCTs, respectively, but recent guidelines postulate to precisely mention which of the stakeholders have been blinded [Moher et al., 2010]. The results of different RCTs can be integrated into a Meta-Analysis by means of statis-tical techniques [Glass, 1976]. Because means and standard deviations of different studies (assuming normally distributed effects) are aggregated, bias is further reduced. The size of effects can more reliably be estimated on the basis of multiple studies with small sam-ple sizes than one study with a large sample. Unfortunately, meta-analysis is not always possible due to insufficient reporting of results, or because of incomparable study designs. Several meta-analyses are independently published, but, if possible, are typically also in-cluded in systematic reviews. When ethical reasons do not allow a controlled experiment, observational studies can be used instead. As one particular example, cohort studies follow up on a group of people with particular characteristics and compare them with either the population, or another cohort with similar properties but without the particular characteristic. Case studies and reports provide mere anecdotal evidence because only one patient is examined, but they can be very detailed in their description. Expert opinions and clini-cal observations build the foundation of the hierarchy and can be informative if stronger evidence is unavailable. The discovery of relevant randomized controlled trials is crucial for the search pro-cess [Dickersin et al., 1994], and the search strategies focus on finding these and similarly reliable study types, if they exist for the review question. Note, however, that it is the inclusion of evidence from all of the study types that sets systematic reviews apart from meta-analyses, and places them on the top of the hierarchy [Rychetnik et al., 2004]. Proliferation Systematic reviewing is a general methodology that seeks to minimize bias and is thus not specific to the biomedical domain, or questions around interventions. Hence, although this 53 2.4. EVIDENCE-BASED MEDICINE thesis focuses on biomedical systematic reviews, its research outcomes potentially have broader applicability. Moher et al. [2007a] estimate that around 2,500 systematic reviews are published per year, of which only 20% are Cochrane reviews. The Campbell Collaboration11 is a sibling organization to the Cochrane Collaboration and aims to prepare, maintain and promote systematic reviews on topics of general policy-making. Similarly, the Evidence for Policy and Practice Information and Co-ordinating Centre (EPPI)12 has a broad interest in reviews of topics in education, health promotion, employment, social care, and justice. An example of an institution with a more focused scope is the Center for Evidence-Based Dentistry of the American Dental Association.13 Individual researchers also have an interest in undertaking systematic reviews, as a way of supporting the validity of their research questions. Staples and Niazi [2007] perform a systematic review of the motivations for the adoption of a software engineering model. Moreover, Kitchenham [2004] provide software engineering researchers with procedures, adopted from the ones used in the biomedical domain. Finally, e-discovery refers to a retrieval task in the legal domain that takes place in litigation and regulatory settings, and has similar requirements as the search for systematic reviews [Oard et al., 2008]. 2.4.2 Current Retrieval Methods New research results are published at an astonishing rate. In 2010, Medline, one of the largest citation databases of biomedical research publications, added nearly 700,000 new entries14 and continues to grow at a phenomenal rate [Hunter and Cohen, 2006]. Helmer et al. [2001] estimate that, even if only randomized controlled trials are of interest, an indi-vidual would need to spend 57 hours per day reading to keep up with the 15,000 annually published RCTs. It is hence out of the question to review all documents for a particular 11http://www.campbellcollaboration.org/ (accessed on 13 May 2011) 12http://eppi.ioe.ac.uk (accessed on 13 May 2011) 13http://ebd.ada.org/ (accessed on 13 May 2011) 14http://www.nlm.nih.gov/bsd/bsd_key.html (accessed on 13 May 2011) 54 http://www.campbellcollaboration.org/ http://eppi.ioe.ac.uk http://ebd.ada.org/ http://www.nlm.nih.gov/bsd/bsd_key.html CHAPTER 2. BACKGROUND & GENERAL CONCEPTS question. In addition, because the number of included studies is very small, sampling can not be expected to retrieve enough, if any, relevant documents and is thus not an option. Hence, some form of information retrieval model has to be applied to the task. Currently, Boolean retrieval is the principal search strategy and accepted standard for systematic review search. A search specialist is part of every review team, typically a li-brarian who knows each of the databases that are to be searched [McGowan and Sampson, 2005]. In interaction with the review team, a complex, structured query is constructed that best represents the review question, and produces manageable answer sets for the collec-tions it is run against, typically of the size of a few hundred to a few thousand answer set documents. Because exhaustiveness is crucial, search is accompanied by other discovery methods of varying sensitivity and precision [Savoie et al., 2003]. For instance, citation data can be used to establish relationships between studies, even ones that are not indexed in the searched collections or found by the search query. Hand searching, the manual page-by-page inspection of pertinent journals and editorial letters, helps to discover articles that might not have been indexed. Further, "grey" literature – for example, non-refereed jour-nals, non-indexed conference proceedings, theses, dissertations, or unpublished company reports – should also be considered. The grey literature can influence the conclusions of a review because negative results often take longer to be published due to publication bias [Hopewell et al., 2007]. As a last resort, experts in the area of the review question are asked to nominate relevant studies. Because these other discovery mechanisms are biased by the initial seed of documents identified via Boolean retrieval or the choice of hand searched journals and experts, better search methods can reduce bias, if they find these additional studies. Rationale for Boolean Retrieval Ranked retrieval, as suggested by the retrieval models in Section 2.1, has superseded the Boolean approach as primary retrieval method in many application domains. However, in the biomedical and similar domains, Boolean retrieval is still the prevalent approach. 55 2.4. EVIDENCE-BASED MEDICINE In the literature, several arguments can be found that support the Boolean approach: • Efficiency: Given an appropriate implementation, Boolean queries can be efficiently processed. This was a critical argument when computing power was more limited and costly [Witten et al., 1999]. However, the availability of efficient implementations are still essential for a competing retrieval model to be adopted in practice. • Confidence: Boolean retrieval has been used for decades, and researchers and review creators have built up experience with it, manifested in written guidelines for devel-oping search strategies [Higgins and Green, 2009]. Moreover, experienced searchers still prefer the method over natural language searching [Hersh et al., 1998]. • Complex information need descriptions: Complex concepts can be expressed with Boolean queries. Several papers confirm the intuition that complex, structured queries can be more effective than bag-of-words queries, when generated by experts [Metzler and Croft, 2004, Zhu et al., 2007]. • Composability & re-use: Individual concepts can be described and evaluated indepen-dently and recombined into more complex query structures. Glanville et al. [2006] and McKibbon et al. [2009] give overviews of the many search strategies for iden-tification of controlled trials that have been suggested by researchers over the years. A particular example is the Cochrane Highly Sensitive Search Strategy (HSSS), con-sisting of multiple "phases", that is queries, with different recall-precision trade-offs [Eisinga et al., 2007]. Boolean filters can also be re-used to update systematic re-views. • Reproducibility: Because the retrieval function of the Boolean model only depends on each document itself (compare Equation 2.1), and not on the properties of an evolving document collection, inclusion or exclusion of any given document can eas-ily be explained with knowledge of the query; this fact is stipulated as an important requirement for the integrity of a review [Sampson et al., 2008b]. 56 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS • Scrutability: Simply by inspection of the query, review readers can understand prop-erties of retrieved documents and assess the reliability, or any bias of the review [Sampson and McGowan, 2006]. For instance, language restrictions are sometimes employed; • Strictness: Strict inclusion and exclusion criteria are inherently supported, for instance, based on meta-data; • Pragmatism: The review team is dependent on the retrieval methods provided by the principal databases. The primary method supported by these systems is still Boolean retrieval. Boolean retrieval fulfills many requirements of systematic retrieval tasks and is hence cur-rently the accepted method in this domain. Collections Historically, primary research results are disseminated via publication in journals. For re-searchers to keep up to date, it would be very inefficient to subscribe to all journals in which pertinent literature could appear. The Index Medicus, published from 1879 until 2004, im-proved this situation for biomedical researchers by indexing the articles of journals, satis-fying some minimal scientific quality level [Kunz, 1979]. This famous index has however been superseded with the emergence of searchable online databases. Currently, one of the largest bibliographic databases is Medline, which is compiled by the U.S. National Library of Medicine (NLM). As of 2010, it contains more than 18 million citations from more than 5,000 biomedical journals. Note that due to copyright restrictions, bibliographic databases generally can not index the full-text of articles. However, next to location and author information, they typically index the study title and its abstract, and a range of meta-data tags from biomedical ontologies as part of the citation. Several service providers licence Medline and offer search interfaces; most notable are PubMed,15 which 15http://www.ncbi.nlm.nih.gov/pubmed/ (accessed on 13 May 2011) 57 http://www.ncbi.nlm.nih.gov/pubmed/ 2.4. EVIDENCE-BASED MEDICINE provides free access and is maintained by NLM, and Ovid,16 a subscription-based interface to several collections that is popular for systematic review search. In contrast to Medline, which mainly contains English articles, the Excerpta Medica Database (Embase)17 covers over 7,000 biomedical journals from 70 countries and is pro-duced by Elsevier. Although there is considerable overlap between the two collections, searching both databases can yield additional findings and is hence advised [Sampson et al., 2003]. Several special registers aim to leverage retrieval by preselecting studies of particular type or content. The Cochrane Central Register of Controlled Trials (Central), for instance, contains a comprehensive list of all published controlled clinical trials in databases such as Medline and Embase, and taken from other published and unpublished sources. The Cumu-lative Index to Nursing and Allied Health Literature (CINAHL), and PsycINFO, covering psychological literature, are further examples. Systematic Review Queries In the web domain, research focuses on short queries, mostly containing only two or three keywords [Spink et al., 2001]. In this context, "long" queries are considered to have a length of 5–12 keywords [Bendersky and Croft, 2009]. In contrast, the queries that are used for biomedical systematic reviewing typically contain dozens, sometimes hundreds, of terms, which are connected in complex ways by means of query operators. Intuitively, more descriptive and detailed queries are better representations of information needs. The information need for biomedical systematic reviews are specific clinical questions derived from clinical problems. A systematic way of formulating a question initially is to think of it in terms of the PICO framework, requiring the question to have each of the as-pects: patient (or problem), intervention, comparison and outcome [Huang et al., 2006]. In practice, research methods, such as a requirement for randomized controlled trials, are often added to the query as an inclusion criteria. Because studies are by definition only relevant 16http://www.ovid.com/ (accessed on 13 May 2011) 17http://www.embase.com/ (accessed on 13 May 2011) 58 http://www.ovid.com/ http://www.embase.com/ CHAPTER 2. BACKGROUND & GENERAL CONCEPTS 1. randomized controlled trial.PT. 2. controlled clinical trial.PT. 3. randomized controlled trials/ 4. random allocation/ 5. double blind method/ 6. single blind method/ 7. OR/1-6 . RCTs 8. animal/ NOT (animal/ AND human/) 9. 7 NOT 8 . RCTs, except those that only consider animals 10. clinical trial.PT. 11. EXP clinical trials/ 12. (clin$ ADJ25 trial$).TI,AB. 13. cross-over studies/ 14. (crossover OR cross-over OR cross over).TW. 15. ((singl$ OR doubl$ OR trebl$ OR tripl$) ADJ25 (blind$ OR mask$)).TI,AB. 16. placebos/ 17. placebo$.TI,AB. 18. random$.TI,AB. 19. research design/ 20. OR/10-19 . other quality study types 21. 20 NOT 8 . . . . except those only about animals 22. 21 OR 9 . RCTs or other quality study types about humans 23. EXP cytomegalovirus/ OR EXP cytomegalovirus infection/ 24. EXP organ transplantation/ 25. 24 NOT EXP bone transplantation/ . solid organ, not cell transplantation 26. 23 AND 25 . patient symptoms 27. EXP acyclovir/ OR EXP ganciclovir/ OR EXP antiviral agents/ 28. ac?clovir.TW. OR ganc?clovir.TW. OR valac?clovir.TW. OR valganc?clovir.TW. OR cidofovir.TW. 29. EXP immunoglobulins/ OR EXP gamma-globulins/ OR EXP immunoglobulins, intravenous/ OR EXP immunotherapy/ 30. 27 OR 28 OR 29 . interventions 31. 26 AND 30 . patient symptoms AND interventions 32. 22 AND 31 . quality study type AND review specific PICO concepts Figure 2.7: Medline query in Ovid syntax extracted from the same Cochrane systematic review as described in Figure 2.5 on page 50 [Hodson et al., 2007]. Representations of query concepts are noted on separate lines and then combined via reference to their line numbers. The last query line describes the overall result set. For the purpose of readability, operators are uppercased, terms lowercased, and concept descriptions added. when they contain each of the PICO aspects, it is natural to define Boolean queries as con-junctions, whereas each conjunct refers to one these concepts. Then, several representations of each concept can be enumerated in disjunctions of terms or more complex sub-queries. In general IR, this type of querying is known as faceted search [Tunkelang, 2009, Hersh, 2008]. Figure 2.7 gives an example of a complex query, as published in a typical system-atic biomedical review and submitted to Ovid. The query is split up into numbered query lines, each describing a particular query term or concept. Higher level concepts then refer 59 2.4. EVIDENCE-BASED MEDICINE back to these line numbers and combine them using Boolean operators. Furthermore, the query model is more complex than the traditional Boolean retrieval model, as introduced in Section 2.1.1. Statistics on the size of typical queries can be found in Chapter 3 on page 74. There are a number of points to note: • Proximity operators are used to define concepts that are best described with lexical affinities. The adjacency operator ADJ25, for instance, matches on documents in which the two operator arguments appear up to 25 words from each other, in any order. Phrases, a special case of ordered proximity operator, requires the phrase terms to appear consecutively next to each other. • An n-ary disjunction over the lines with the numbers 1 to 6 can economically be described by OR/1-6. These notations are however used interchangeably, because there is no difference in the results when Boolean semantics are used to process the query. Note further that the operator NOT has the semantics of AND NOT. • Fields allow the qualification of specific document parts and of meta-data types. A large number of fields are supported, including title (TI), abstract (AB), publication type (PT), publication year (YR), and, as aggregation of title and abstract terms, any text word (TW). • Documents are indexed with name descriptors from medical ontologies such as the Medical Subject Headings (MeSH),18 a controlled vocabulary taxonomy developed and maintained by the National Library of Medicine. In a query, these descriptors can then be referenced using the slash ("/") operator. Because MeSH is a taxonomy, with more specific subject headings as child nodes of general descriptors, an option of this operator is the "explosion" (EXP) of the MeSH term, that is, the replacement of the term by a disjunction of itself and all its more specific child descriptors. Moreover, MeSH terms can be qualified with the star ("*") operator to retrieve documents that have a topical focus related to the term. 18http://www.nlm.nih.gov/mesh/ (accessed on 13 May 2011) 60 http://www.nlm.nih.gov/mesh/ CHAPTER 2. BACKGROUND & GENERAL CONCEPTS • Truncation ($) and wildcard (?) operators likewise expand to disjunctions of terms with similar notation, for instance, due to inflection, or differences due to transcrip-tion into other languages. The query "wom?n" would at least match on the terms "woman" and "women"; and the query "mask$" would expand to any term with that prefix, such as "masked", "masking", and "mask" itself. An alternative would be to apply stemming, an IR technique that unifies related index and query terms into their base form to simplify querying for lay users. However, the potential loss of information that goes along with this method contradicts the aim of most biomedi-cal users. Query-specific manual expansion is more flexible, and hence the preferred method for these users. It can be seen that the query enforces result documents to be quality studies; however, only patient and intervention-related aspects of the PICO framework are present. This is common practice, because the title and abstract often do not contain mentions of the other aspects [Higgins and Green, 2009]. The example illustrates the complexity of biomedical queries in terms of size, structure and operator richness. However, it was chosen in part for its brevity, and other queries are much longer, some of them containing more than a hundred query lines, deeper nesting of concepts, and a broader choice of query operators and fields. For instance, the example query only employs two of the three phases of the highly sensitive search strategy to identify quasi-RCTs, suggested by the Cochrane Collaboration for inclusion into review queries; and these three phases alone contain 34 query lines of similar length and complexity [Higgins and Green, 2009, Dickersin et al., 1994]. Note that due to query term expansion and MeSH heading "explosion", queries become even larger, sometimes containing more than a thousand terms before they are processed. For search service providers to be able to execute these queries, efficient query processing methods are required. The development of these queries requires search experts that are able to express biomed-ical information needs with the available index terms of a given document collection [Bates, 1998]. Initial queries are then refined and reformulated in an iterative process that aims to 61 2.4. EVIDENCE-BASED MEDICINE minimize loss of relevant studies while the Boolean answer set size is brought down to a cardinality that is feasible to be reviewed comprehensively [Higgins and Green, 2009]. This process requires repeated assessment of result set quality, which is a laborious undertaking due to the fact that Boolean result sets are unordered, and typically contain in the order of hundreds to thousands of studies. Problems Despite the rationale for using the Boolean retrieval model for systematic review search, the current approach is not without problems. The most critical problem is that the result sets of the final queries often have low (relative) recall. That is, no single query retrieves all of the relevant documents that are finally included in the review, and that have been found by the union of all of the applied discovery measures. In an early study, it was found that 30%–90% of relevant studies available in Medline are not discovered with Boolean search strategies [Dickersin et al., 1994]. Another study confirmed later that nearly half of the eligible studies are missed if only Medline is searched [Avenell et al., 2001]. This is partly because the databases differ in the way they parse and index the citations, and partly because not all relevant documents are contained in every collection. However, there is considerable overlap between the major databases Embase and Medline, and Avenell et al. also find that studies can not be retrieved even though they are in the collection. Searching other databases mitigates such failures, but can not be relied on either. While a more recent study shows that indexing for retrieval of Randomized Controlled Trials has improved in Medline since 1994 [Glanville et al., 2006], the problem remains for unforeseen retrieval tasks. A second problem is the strictness of the Boolean retrieval model. This property can lead to exclusion of documents that cover most of the concepts, but do not satisfy one of the conjuncts, possibly because of a missed spelling variant for one concept that happened to be used in these documents. Similarly, a document that matches only on one disjunct is seen to be as relevant as documents containing all of them. Further, the careless use of negations is widely discouraged, because it can lead to unintended exclusion of documents 62 CHAPTER 2. BACKGROUND & GENERAL CONCEPTS [Sampson et al., 2008a]. For instance, consider the query in Figure 2.7: if it contained "NOT animal/" in order to exclude documents about animals, documents would be ex-cluded, even if they only peripherally mention animals. A third problem concerns how to arrive at a query that can be considered "final" so that its answer set can be comprehensively triaged. A typical course of action is to start with keywords and then to reformulate and refine the query; Lu et al. [2009] found this behaviour typical of PubMed users. Because the output of a Boolean system is unordered and large answer sets are usual, it is hard to decide whether a change improved the query. Moreover, the low density of eligible studies impedes probing of the result set. Nevertheless, reformulation has to take place until a manageable result set size is achieved, even if this involves loss of relevant studies. A related problem is the general difficulty of controlling the result set size of queries in order to match them to the reviewing capacity available. Boolean retrieval is known to be susceptible to either return too many, too few, or even no results [Witten et al., 1999]. Ob-viously, whereas additional conjuncts can reduce answer set sizes, more disjuncts possibly lead to enlarged result sets. The magnitude of changes is however unknown, and has to be tested. Despite the care taken to formulate queries and undertake the review, a number of issues have been noted in regard to reporting practice of the queries, rendering the replication of review results a difficult task. Yoshii et al. [2009] summarizes papers on this topic, each of which have taken a sample of systematic reviews and measured the fraction of correctly denoted search strategies. The results range from 3.5% up to 95.3% of reviews contain errors, so that their search strategy could not be replicated. Sampson and McGowan [2006] analyse common error types, including: spelling errors; missed spelling variants; incorrect MeSH terms; and wrong line references. Some of these errors might have been introduced during report writing and can be corrected, but others might potentially have led to the exclusion of eligible studies. 63 2.5. SUMMARY 2.5 Summary This chapter gave an overview of general information retrieval models, and the specifics of the use of one of them, Boolean retrieval, in the application domain of interest for this thesis, biomedical systematic reviewing. Furthermore, query processing algorithms have been introduced that make the implementation of most of the models practical. Finally, standard IR evaluation frameworks and measures have been described that will be adopted throughout this thesis. The next chapter describes the evaluation methodology that is used for the experiments in the coming chapters. 64 Chapter 3 Evaluation Methodology Throughout this thesis, experiments are described that validate the proposed methods. The design of these experiments follows the widely accepted setup used in the TREC challenges, which consists of a document collection, queries and corresponding non-exhaustive rele-vance judgements to measure retrieval method effectiveness. Because TREC currently does not provide a test collection appropriate to the task of interest, systematic review search, the work for this thesis necessitated establishment of a tailored test environment. This chapter is dedicated to the description of that environment. Section 3.1 introduces the data sets that have been used in experiments. The configuration of used and undertaken implementa-tions is outlined in Section 3.2. Finally, general evaluation considerations are described in Section 3.3, in particular, a measurement technique for efficiency evaluations. 3.1 Data Sets The test environment of this thesis consists of a large, comprehensive document collection of biomedical studies and a series of query sets that have been developed for this collection. This section first substantiates the choice of document collection. Then, the query sets used throughout this thesis are described and, if applicable, the associated relevance judgements. 65 3.1. DATA SETS 3.1.1 Document Collections One of the most comprehensive indexes of biomedical studies is Medline, containing over 18 million references to journal articles as of 2011.1 A majority of biomedical literature searches go through one of the several interfaces and service providers that commercially li-cence this collection, most notably PubMed, a free service provided by the National Library of Medicine (NLM). The National Library of Medicine provides daily collection updates that are fed into PubMed and similar systems. More significantly, the Medline Baseline Repository2 provides researchers with free access to yearly snapshots of the whole collec-tion, facilitating research and, in retrospect, the reproducibility of research results. A second comprehensive document collection, frequently used to search for biomedical studies, is Embase,3 which contains even more citation entries than Medline (over 24 mil-lion as of 20114). But many of its studies are written in languages other than English, and hence are often excluded from systematic reviews due to language requirements in the in-clusion criteria. Moreover, access to Embase is limited and hence, use of it is not an option for general experimentation. Other important biomedical databases include the Cumulative Index to Nursing and Allied Health Literature (CINAHL)5 and PsycINFO,6 each covering studies relevant to their specific research field. Throughout this thesis, all of Medline, as of the end of 2008, serves as a universal doc-ument collection for a number of query sets and can be obtained from the Medline Baseline Repository mentioned above. The collection is formatted as a number of XML files that con-tain complete information for each citation, including meta-data such as publication date, when the Medline citation entry for the publication has been created and possibly revised, journal name, and author information; taxonomy descriptors; and abstracts (if available). Furthermore, each file lists citations that are to be deleted from indexes. Figure 3.1 gives an example of a Medline XML data file. 1http://www.nlm.nih.gov/pubs/factsheets/medline.html (accessed on 12 May 2011) 2http://mbr.nlm.nih.gov/ (accessed 12 May 2011) 3http://www.embase.com/ (accessed on 12 May 2011) 4http://www.embase.com/info/what-is-embase/coverage (accessed on 12 May 2011) 5http://www.cinahl.com/ (accessed on 12 May 2011) 6http://www.apa.org/psycinfo/ (accessed on 12 May 2011) 66 http://www.nlm.nih.gov/pubs/factsheets/medline.html http://mbr.nlm.nih.gov/ http://www.embase.com/ http://www.embase.com/info/what-is-embase/coverage http://www.cinahl.com/ http://www.apa.org/psycinfo/ CHAPTER 3. EVALUATION METHODOLOGY <MedlineCitationSet> <MedlineCitation Owner="NLM" Status="MEDLINE"> <PMID>10567533</PMID> <DateCreated> <Year>2000</Year><Month>01</Month><Day>06</Day></DateCreated> <DateCompleted><Year>2000</Year><Month>01</Month><Day>06</Day></DateCompleted> <DateRevised> <Year>2008</Year><Month>10</Month><Day>02</Day></DateRevised> <Article PubModel="Print"> <Journal> <ISSN IssnType="Print">0270-7306</ISSN> <JournalIssue CitedMedium="Print"> <Volume>19</Volume> <Issue>12</Issue> <PubDate><Year>1999</Year><Month>Dec</Month></PubDate> </JournalIssue> <Title>Molecular and cellular biology</Title> <ISOAbbreviation>Mol. Cell. Biol.</ISOAbbreviation> </Journal> <ArticleTitle>Axl-gas6 interaction counteracts E1A-mediated cell growth suppression and proapoptotic activity.</ArticleTitle> <Pagination><MedlinePgn>8075-82</MedlinePgn></Pagination> <Abstract> <AbstractText>The adenovirus type 5 early region 1A gene (E1A) has previously been known as an immortalization oncogene because E1A is required for transforming oncogenes, such as ras and E1B, to transform cells in primary cultures. ...</AbstractText> </Abstract> ... <AuthorList CompleteYN="Y"> <Author ValidYN="Y"><LastName>Lee</LastName><ForeName>W P</ForeName> <Initials>WP</Initials></Author> <Author ValidYN="Y"><LastName>Liao</LastName><ForeName>Y</ForeName> <Initials>Y</Initials></Author> ... </AuthorList> <Language>eng</Language> ... </Article> ... <MeshHeadingList> <MeshHeading> <DescriptorName MajorTopicYN="Y">Apoptosis</DescriptorName> </MeshHeading> ... <MeshHeading> <DescriptorName MajorTopicYN="N">Humans</DescriptorName> </MeshHeading> ... </MeshHeadingList> </MedlineCitation> ... <DeleteCitation> <PMID>18398618</PMID> <PMID>18405851</PMID> <PMID>18462419</PMID> ... </DeleteCitation> </MedlineCitationSet> Figure 3.1: Medline XML data file excerpt. 67 3.1. DATA SETS The citations in Medline include manually assigned Medical Subject Headings (MeSH),7 descriptors from a biomedical, hierarchical taxonomy. While the MeSH descriptors can normally be treated as terms (that occasionally contain whitespace), the use of explosion operators in queries requires knowledge about the descriptor relationships within MeSH. To resolve queries with these operators, a copy of the entire 2008 MeSH taxonomy is used, containing more than 24,000 descriptors.8 Each of the descriptors contains a series of syn-onyms that are used to expand queries, potentially resulting in considerable increases in the number of query terms. For instance, the descriptor of the MeSH heading "Apopto-sis", referred to in the citation entry in Figure 3.1, enlists the synonyms "Programmed Cell Death, Type I", "Apoptosis, Intrinsic Pathway", "Extrinsic Pathway Apoptoses" and six other spelling variants thereof. The more specific MeSH heading "Anoikis" can be found below "Apoptosis" in MeSH. Because the assignment of MeSH descriptors is done manually, it can take months until new citation records for existing publications are available in Medline. Moreover, historical papers have been added in the past in batches years after the initial publication of these papers. Out of these reasons, Medline specific dates are ignored throughout this thesis and the recorded, actual publication date is used to sort result lists and to divide the collection in training, validation and test sets. Furthermore, papers published at the end of 2008 are never used in evaluations in order to avoid the possibility of any effects due to missing citations. Medline collection and index statistics are summarized in Table 3.1. 3.1.2 Query Sets Given a document collection, representative queries are required to demonstrate IR system performance for a particular task. Moreover, query sets need to be compiled with relevance judgements in order to evaluate the effectiveness of a system. This section introduces the query sets used throughout this thesis. 7http://www.nlm.nih.gov/mesh/ (accessed 12 May 2011) 8http://www.nlm.nih.gov/mesh/filelist.html (accessed on 12 May 2011) 68 http://www.nlm.nih.gov/mesh/ http://www.nlm.nih.gov/mesh/filelist.html CHAPTER 3. EVALUATION METHODOLOGY MB '000 Collection size, 68,598 Citations 17,105 compressed (gzip -9) 9,130 Citations with abstract 9,367 Index size 11,954 Distinct index terms, all text fields 10,882 Dictionary file size 228 title 1,167 Inverted file size 6,013 abstract 4,100 Position file size 5,713 abstract+title 4,429 Table 3.1: Medline collection and index statistics, when indexed with Lucene as described in Section 3.2. AHRQ A data set was made available by Cohen et al. [2006] and stems from three Evidence-based Practice Centers (EPCs) of the Agency for Healthcare Research and Quality (AHRQ). It is referred to as AHRQ COHEN and consists of the queries for 15 systematic reviews, called Evidence Reports by AHRQ, that had been completed by 2005 in these EPCs. For each of these reviews, references to studies are provided that have been returned by the query, screened on the abstract-level, and included in the review. One artifact of this dataset is that the references have been restricted to the citations of the 10 year Medline corpus used in the TREC Genomics Track [Hersh et al., 2004], containing studies indexed in Medline between 1994 and 2003 (inclusive). Consequently, most of the provided citations can be resolved in our 2008 Medline corpus. Cohen et al. [2006] give descriptive statistics for Boolean result set sizes and note that only 0.5% to 27% (median of 3.8%) of the retrieved documents are finally included in the review. Another query set has been compiled by Karimi et al. [2010] on the basis of more than 100 AHRQ reviews published so far by all EPCs. It consists of 17 systematic reviews, each of which reports a search query and references to the studies that have been included at the abstract and full-text level. A second query set used in the experiments for this thesis, AHRQ KARIMI, is made up of a subset of 13 of these queries, for each of which a reasonably sized result set (neither zero, nor greater than 5,000, rendering it impractically large) could be replicated on the basis of each published query and the previously introduced Medline document collection. 69 3.1. DATA SETS Document Collection Retrieved Studies Included Studies Screened Studies Other Discovery Methods Figure 3.2: Schematic set relations between the document collection, retrieved documents and stud-ies, included in the systematic review. The shaded area refers to the set of included studies that could be resolved to entries in the document collection and are hence used as ground truth to evaluate re-trieval methods throughout this thesis. For both of these query sets, AHRQ COHEN and AHRQ KARIMI, Karimi et al. [2009] provide alternative keyword query representations of the review information needs. They are based on the review title (T), research question (R), inclusion and exlusion criteria (C), or all three (TRC). For the purpose of this thesis, studies are considered to be relevant if they are included in the final review and can be resolved to entries in our document collection. While the screened studies are a subset of the initially retrieved documents, the set of included studies also contains those that have been found by other means than querying. This set is hence less biased by the choice of Boolean queries and more representative of the set of documents relevant to the information need in each systematic review. Figure 3.2 depicts the relation between the document sets. PubMed The queries in the previous query sets are complex; one example is given in Section 2.4.2 on page 59. Two query sets with less complex queries serve as another comparison point in the evaluation of the efficiency of processing methods, described in Chapter 5. Because relevance judgements are not required, when efficiency is being assessed, any query log submitted to the Medline collection can serve as a source for query samples. 70 CHAPTER 3. EVALUATION METHODOLOGY The PUBMED SIMPLE query set consists of 50 short PubMed queries with only a single (n-ary) conjunction in each, generated by taking a random sample of all such queries in the query log reported in Herskovic et al. [2007]. A PUBMED SIMPLE query example is: vierstra AND review AND ubiquitin This query set can be used to measure pure conjunctive processing of queries of varying length. In contrast, the 50 queries in the PUBMED STRUCTURED query set are randomly sampled from the same query log, but contain both conjunctive and disjunctive operators at least once in every query. Consequently, they possess more structure, as can be seen from the following example: (perineale OR perineum) AND (prostataektomie OR prostate OR ectoderm) Note that most PubMed queries are ad hoc, short and do not have a systematic review background. In particular, PubMed only provides a one-line query box, which does not facilitate the entry of complex, multi-stage queries. However, the queries that are finally processed by PubMed can be much more complex due to query expansions performed as part of PubMed's query processing. Cochrane Beside document dates, the experiments undertaken in Chapter 6 require knowledge of search dates. The fact that not all reviews of the AHRQ query set contain this information motivates the compilation of another query set for these experiments, the preparation of which is described in the following. For evaluation purposes, it is important that review queries have a reasonable number of identified relevant documents associated with them, that is, included studies. Fortunately, the Cochrane Database of Systematic Reviews (CDSR)9 contains thousands of systematic reviews, so that even in face of our requirements and other strict inclusion criteria, large query sets should be possible. 9http://www.cochrane.org/reviews/ (accessed on 12 May 2011) 71 http://www.cochrane.org/reviews/ 3.1. DATA SETS The CDSR, published as part of Issue 1 of the Cochrane Library in 2008, contains 3,384 reviews and 1,787 protocols; and serves as the root set for the COCHRANE query sample, used in Chapter 6. Because a majority of reviews contain only a few included studies and not all of the included studies can be resolved to a citation in our Medline collection, it would be very laborious to examine randomly selected reviews for their appropriateness. Moreover, such a process would lead to a query set with very few relevance judgements per query, complicating evaluation. Fortunately, access to the CD-ROM version of the library and their structured reviews allowed automatic methods to be employed to identify appropriate reviews. In order to curate the COCHRANE query set, the titles of the included studies from each review were extracted and matched to the paper titles in the Medline collection. To minimize the risk of false positives, studies were only considered to be resolved if they uniquely matched the full title of one citation in Medline. Using this simple process, on average more than half of the included studies were able to be resolved, an overall total of 20,114 out of 37,751. While a fully random sample is generally the best choice to draw conclusions about the population, evaluation necessitates that a reasonable number of relevance judgements is available for each query to reliably detect differences in the effectiveness of retrieval methods. For instance, topics for TREC evaluations are chosen on the basis of estimates about their number of relevant documents and excluded if they have too few or too many relevant documents (for a comprehensive review of test collection based evaluation, refer to Sanderson [2010]). In contrast, all reviews were able to be selected for the generation of the COCHRANE query set, but the number of resolved relevant references has been used as the basis for the computation of selection probabilities. In effect, reviews without resolved included studies were not selected, and those with many resolved references were more likely to be selected. However, due to the large number of reviews that have very few references, a fair mix over all reviews is represented in the sample. Following this approach, the CDSR was sampled until 50 systematic reviews had been found that report a search strategy and the date when the search was executed. Figure 3.3 72 CHAPTER 3. EVALUATION METHODOLOGY Systematic Reviews In c lu d e d S tu d ie s 1 2 5 10 20 50 100 200 1 500 1,000 1,500 2,000 2,500 3,000 3,500 Figure 3.3: Distribution of included and resolved studies in CDSR of the Cochrane Library, Issue 1, 2008. The reviews are sorted by the number of resolved studies. Each dot denotes the number of included studies for a review and the points are jittered in the y-axis to avoid over-plotting. The sampled reviews are indicated by the vertical line segments at the top of the graph. depicts the reviews in the CDSR, their number of included and resolved studies, and the sampled reviews. For almost 2,500 systematic reviews, at least one included study could be resolved to entries in our Medline collection. Moreover, the CDSR contains more than 600 reviews with at least 10 included and resolved studies. Averaged over all reviews, more than 50% of included studies could be resolved (median of 57%, mean of 54%), which is indicated by the density of included studies relative to the number of resolved studies. As a result of the described sampling procedure, the COCHRANE query set contains 20 queries with more than 20 relevance judgements, but it also includes 7 queries with less than 6 relevance judgements. To facilitate further research on systematic review search and to make the experiments in Chapter 6 reproducible, the COCHRANE query set has been made publicly available.10 Summary Table 3.2 summarizes properties of all five query sets that have been described. Because the descriptive statistics for the AHRQ and COCHRANE query sets indicate very similar properties, they will be used as alternatives in the experiments. 10http://www.csse.unimelb.edu.au/˜spohl/CochraneQuerySet/ (last modified on 8 Mar 2011) 73 http://www.csse.unimelb.edu.au/~spohl/CochraneQuerySet/ 3.2. IMPLEMENTATIONS AHRQ PUBMED COCHRANE COHEN KARIMI SIMPLE STRUCTURED CDSR Queries 15 13 50 50 50 Query lines (avg.) 27.7 44.0 1 1 34.2 Query terms (avg.) 75.1 99.7 2.3 7.1 72.3 Query nodes (avg.) 91.2 145.9 3.3 12.8 97.7 Query levels (avg.) 5.5 6.5 2.0 4.0 5.4 Studies, included in review / resolved in the Medline collection min. 9 / 9 12 / 10 n / a n / a 3 / 2 median 41 / 40 83 / 71 n / a n / a 24 / 16 max. 146 / 136 440 / 408 n / a n / a 112 / 88 Boolean result set size min. 587 23 0 0 0 median 2,070 1,272 4 190 617 max. 17,330 8,574 61,104 65,090 23,687 Queries with no results 0 0 19 6 2 TRC queries available X X n / a n / a – Used in Chapters 4, 5 4 5 5 6 Table 3.2: Descriptive statistics for the query sets, their relevance judgements and the size of their Boolean result sets. 3.2 Implementations Because the focus of this thesis is on query evaluation rather than index construction, an off-the-shelf retrieval system has been used for the latter. The implementations undertaken for this thesis are based on version 2.4 of Lucene, a widely used and extensible open-source IR system,11 implemented in Java. Lucene supports field search and complex operators; both Boolean and ranked retrieval models are provided; and it has reasonable indexing efficiency. Because Lucene implements its own ranking scheme and does not provide out-of-the-box implementations for state-of-the-art ranking functions, the experiments in Chapter 4 use version 0.9.3 of zettair,12 another IR system that has proved to be effective in TREC evaluations [Garcia et al., 2006, Büttcher et al., 2006]. 11http://lucene.apache.org/ (accessed on 12 May 2011) 12http://www.seg.rmit.edu.au/zettair/ (accessed on 12 May 2011) 74 http://lucene.apache.org/ http://www.seg.rmit.edu.au/zettair/ CHAPTER 3. EVALUATION METHODOLOGY 3.2.1 Indexing Version 3.5 of the Medline citation parser from the LingPipe library13 was used to read the XML files of the Medline collection. In order to prevent loss of information, biomed-ical search engines currently use a minimum of preprocessing. To replicate these condi-tions, only light stemming was used. In particular, Lucene's StandardTokenizer was used, which generates tokens by splitting the text on whitespace and also removes punctuation from word endings. These tokens are then passed to the StandardFilter, which removes dots from acronyms and trailing apostrophes and possessive forms, and then further, to the LowerCaseFilter. Most citations are in English, for which the meaning of terms typically does not depend on its case. Obviously, query terms will also first be lowercased before they are looked up in the index. Meta-data fields, such as MeSH headings, keywords and publication types, were indexed as they appeared, and were not tokenized. In terms of index layout, Lucene stores postings in document order, uses d-gap and v-byte encoding and possesses efficient operator implementations that make use of skip information stored next to the inverted lists. See Croft et al. [2009] for an overview of these techniques. In Lucene version 2.4, a skip interval of 16 and a maximum of 10 skip levels guarantee logarithmic access times to any posting entry. Positions are stored in parallel to each inverted list in a separate file so that postings and positions can co-sequentially be iterated if both should be required, but positions do not interfere when they are not required. 3.2.2 Query Processing To make the complex and structured systematic review queries amenable to automatic meth-ods, an experimental query parser has been implemented using ANother Tool for Language Recognition (ANTLR),14 a query parser generator. Its query language has been purpose-fully defined to be similar to the one of Ovid to simplify the transcription of queries into this syntax. 13http://alias-i.com/lingpipe-3.9.3/demos/tutorial/medline/read-me.html (accessed on 25 Feb 2011) 14http://www.antlr.org/ (accessed on 12 May 2011) 75 http://alias-i.com/lingpipe-3.9.3/demos/tutorial/medline/read-me.html http://www.antlr.org/ 3.2. IMPLEMENTATIONS The output of the parser is an abstract syntax tree (AST), which is then fed through a pipeline of tree manipulations: 1. Query line references are resolved and one hierarchical tree representation produced for each query. 2. Once a convenient query representation is available, the query can be checked for errors. 3. Field specifications are propagated from query nodes down to tree leaves in order to qualify query terms with field names. 4. Deep cascades of the same binary operator are flattened into n-ary versions of the same query operator. The necessity for this step is motivated later in Section 4.3. 5. MeSH operators are replaced with disjunctions of the union of MeSH descriptor syn-onyms, or, for explosion operators, all synonyms of any more specific descriptor in MeSH. 6. Truncation and wildcard operators have to be expanded. 7. The query tree is mapped to actual operator implementations of a particular informa-tion retrieval model and executed. Figure 3.4 depicts the whole query processing pipeline. Boolean operators are typically implemented such that they return a stream of (ordered) document identifiers that satisfy the query sub-expression and use the document identifier streams of their clauses as input. However, next to document identifiers, additional in-formation might be required by operators further up the query tree. In particular, proximity operators require position information about where the clauses match with the current docu-ment. To avoid having to access and process position information throughout the query tree against the eventuality that a proximity operator will be reached, each operator is informed by its parent operator whether position information is required in an initial requirements propagation step. The execution of complex Boolean queries is significantly sped up by means of this optimization, resulting in a competitive, state-of-the-art baseline implemen-tation for the efficiency experiments in this thesis. Finally, note that for conjunctions there 76 CHAPTER 3. EVALUATION METHODOLOGY Q u er y T ex t L e x e r P a rs e r L in e R e fe re n c e R e so lu ti o n S e m a n ti c E rr o r C h e ck in g F ie ld P ro p a g a ti o n O p e ra to r C a sc a d e F la tt e n in g M e S H R e so lu ti o n W il d c a rd E x p a n si o n O p e ra to r M a p p in g Q u e ry P ro c e ss in g R et ri ev a l O u tp u t T o ke n S tr ea m A S T Q u er y T re e V a li d Q u er y Q u a li fi ed T er m s B a la n ce d T re e E xp a n d ed Q u er y Query Preprocessing Figure 3.4: The query processing pipeline, used in the experiments for this thesis. are no clear semantics of how to combine the positions of operator clauses. In an opera-tional system, additional steps might be inserted into this general query processing pipeline to handle such semantically ill-defined, or otherwise invalid queries. Because the queries in the query sets are either in the syntax of the Ovid or PubMed query language, they are transcribed into the query language of the experimental parser for execution. Obvious errors in the queries have been fixed not to disadvantage the Boolean approach due to bad reporting practice, and all query terms have been examined to match to an index term. The latter was effective to detect mismatches due to changes in the Medical Subject Headings (MeSH), which were corrected to reflect the changes. 3.3 Evaluation In the experiments for this thesis, statistical significance tests are employed so that reliable conclusions can be drawn from the samples used in experiments. To be cautious with the assumption of normality, a paired Wilcoxon test is used instead of the more common t-test, with typical alpha levels of 0.05 and, if more informative, of 0.01, respectively [Sanderson, 2010]. 77 3.3. EVALUATION Where parameter settings are estimated on the basis of training data, separate hold-out data has been used to test the settings. For Chapter 4, this translates to the use of two query sets; and, for Chapter 6, to collection splits based on time. The rest of this section describes a measurement methodology that was established in preliminary work for this thesis, and then applied during the efficiency experiments in Chapter 5. Runtime Measurement Even if queries and collection size are fixed, the measurement of timings is highly dependent on the hardware employed, which makes it hard to reproduce experiments. More stable measurements can be taken in simulation experiments, in which, for instance, the resource usage of methods is gauged or the number of basic computations counted. Simulation results can be very informative and give indications about the effect of the method, however, decreases in these surrogate measures do not always translate into wall-clock time savings, meaning that additional runtime measurements are indispensable. To validate the superiority of a method, competing approaches are typically executed on the same hardware so that conclusions can be drawn for this and similar hardware configu-rations. Unfortunately, hardware also changes. For IR systems, disk access has historically been considered the bottleneck for overall query execution times [Witten et al., 1999]; to-day, hardware developments made it possible to economically employ much more, fast, random-access memory. This allows for the possibility to use it not only for operational data structures, but also to cache [Baeza-Yates et al., 2007], or even store (all) index data [Strohman and Croft, 2007]. Nevertheless, due to collection sizes and memory to disk cost ratio consideration, overall query execution times will, in general, still consist of time due to disk access, and time due to the actual remaining computation being performed. From a system designer's perspective, it is of interest to save resources that constitute current system bottlenecks, even at the expense of other resources. For instance, for a disk-bound system, a change is likely to be beneficial if it reduces the time due to disk access, even if at the cost of additional computation. But, at the same time, the method could be 78 CHAPTER 3. EVALUATION METHODOLOGY counterproductive in an in-memory system if the computational overhead is already sig-nificant. It is hence more informative to evaluate the influence of a method on all possibly affected resources, for IR systems most notably, the time spent on disk access and the actual computation. To increase the informative value of efficiency measurements for today's environments, separate timings should be taken for, at least, the two extreme cases: 1) all data has to be accessed and read from disk; and 2) caches work at maximum performance and no disk access is required during query processing. A real stream of queries will lead to some degree of caching; real long-term, average caching performance will hence lead to query processing times between the bounds given by the two measurements. Moreover, the effect of the method on these resources becomes apparent in the relative change in both of these bounds. Finally, such a measurement enables system designers to better assess the applicability of the method, given their current trade-off point. In order to decompose overall query execution times, two separate measurements are required: worst-case performance, or cold-start; and best-case hot start performance, or cached. The former can be attained by flushing or polluting system caches, in particular, the large disk buffers of the operating system. And because it is an optimal situation for a cache to receive the same request twice consecutively, or even more often, separate measurement of one of these repeated executions will reflect cached performance. The difference between these two timings will then be an estimate for the time that is due to disk activity. Figure 3.5 illustrates the measurement methodology for a modification to an IR system, by way of which posting-interleaved position information can be skipped [Pohl and Moffat, 2008]. An upper bound for the performance of a skipping method is naturally given when there is no information that has to be skipped. Timings for a document-level index, that does not store any positions, are hence presented to serve as an additional point of comparison. For different index sizes, a sequence of queries are executed and average query execution times determined. Each issuing of each query is consecutively submitted multiple times and separate measurements taken. Before each query, the buffers are flushed so that the first query submission is a cold-start, estimating worst-case behavior of the system. Preliminary 79 3.4. SUMMARY Index size (billion postings) T im e ( s e c o n d s ) 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 Baseline − Positions Modification − Skipped positions Lower bound − No positions Repetition Number C P U D is k C P U D is k 1 2 3 4 5 Figure 3.5: Repeated query submission and measurement allows for decomposition of overall exe-cution time into disk access (Disk) and computation (CPU), exemplified for the timings when the largest index was used. experiments have shown that, on the hardware employed for this thesis, it was sufficient to flush the operating system disk buffers. Executing additional, unrelated queries between the queries of the query sets in order to pollute all other remaining system caches, such as processor and disk drive caches, did not significantly change the timings. Already the second query submission gives a reliable estimate of query processing time when most, or all required inverted list data is cached in memory. As a result of this measurement method-ology, it becomes apparent that the method is able to reduce time due to computation, but did not have any noticeable effect on time due to disk access [Pohl and Moffat, 2009]. In this thesis, measurements for efficiency experiments follow the described method-ology. Simulations are also undertaken to measure the effect of the proposed methods on system parameters. All experiments have been carried out on a Quad-Core Xeon 2.33 GHz with 64-bit and 16 GB memory, running Ubuntu Linux. 3.4 Summary This chapter has provided an overview of the datasets, implementations and evaluation methodology that are used in the experiments for this thesis. First, the choice of docu-ment collection and query sets was motivated and descriptive statistics presented. Then, 80 CHAPTER 3. EVALUATION METHODOLOGY implementations were described by way of which documents and queries are (pre-)processed. Finally, general evaluation considerations and a measurement methodology for timings were introduced. In Chapter 4, we next investigate the use of ranked retrieval methods for the purpose of systematic review search. 81 Chapter 4 Ranking for Systematic Review Search Boolean retrieval is widely used for structured data retrieval tasks such as those arising in databases, and for email and file search. However, when the retrieval items are not known beforehand, or the search is based on unreliable properties such as ambiguous natural lan-guage terms, the well known IR precision-recall trade-offs arise, and Boolean result sets become less meaningful. Due to the difficulty of authoring effective queries, Boolean re-trieval has been superseded by ranked methods in many application domains. Nevertheless, the preferred method in the biomedical and legal domains is still Boolean search. Several reasons were noted in Chapter 2, which include the fact that strict requirements of the search task are fulfilled by the Boolean model, but are not easily reconciled with many ranked re-trieval approaches. That is, a retrieval method not only has to fulfill the requirements of the search task, but also needs to demonstrate advantages over the current approach in order to become a viable alternative. This chapter examines the applicability of existing ranked retrieval methods to system-atic review search. First, the need for ranked methods is motivated. Then, different ranking approaches are discussed and one promising ranked retrieval model chosen for further ex-amination. After a description of how this model could be applied to systematic review search, experiments are undertaken to validate the effectiveness of the proposed approach. 83 4.1. MOTIVATION 4.1 Motivation The current Boolean approach for systematic review search poses several difficulties. Its foremost problem is the strict categorization of documents into one of two groups: those that satisfy the query, are retrieved, and can be examined; and those that do not satisfy the Boolean expression, are not retrieved, and are never shown to the user. This implies that documents that do not satisfy one query conjunct receive the same score as those that do not satisfy any of the conjuncts, that is, documents that contain many query terms are treated as equivalent to those that do not contain any and would not be part of the query's OR-set. Moreover, in citation databases, typographic and indexing errors, or unanticipated synonyms and abbreviations are more likely to lead to exclusion of documents than in full-text collections. Given the laborious systematic reviewing efforts that typically involve the examination of hundreds to thousands of studies, it appears wasteful to not consider studies that are missing a few conjuncts. Similarly, there is no differentiation within these two groups on the basis of the number and importance of matching query disjuncts. Boolean result sets are, as such, unordered and are either returned in document identifier order, or sorted by some other secondary criterion. Result set size determines the labour involved in examining each of the retrieved studies; moreover, it is inversely correlated with precision. Because the well-known precision-recall trade-off continues to apply, in general, smaller result sets will necessarily lead to decreases in recall. Figure 4.1 gives an excerpt of a typical systematic review query, demonstrating this trade-off. Overall, recall is obviously not larger than for the least successful conjunct, but without that conjunct result sets might be too large to review. Hence, to maximize recall all that can be done is to match the number of retrieved documents with the available reviewing resources. The queries are thus partly constructed so as to have a manageable result set size, even if that requires sacrifices in recall. Cohen et al. [2009] state that, on average, the Boolean result sets of AHRQ COHEN contain 2,122 documents; similarly, Sampson et al. [2006a] assess the typical number of reviewed studies to be in the range of 1,000–6,000 studies. 84 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH Query Tree Result Set Size Relative Recall AND 2,935 0.56 OR 258,560 0.67 headache 37,758 0.22 "muscle cramp" 1,617 0.11 . . . OR 62,337 0.78 . . . human/ 10,885,697 1.00 . . . Figure 4.1: Case study of a faceted search query with reasonable result set size, but overall relatively low effectiveness. Each line corresponds to the result of that sub-expression. Unfortunately, it is very difficult to control the size of Boolean result sets. All that a user can do is to add and remove query terms, or to recombine query expressions with different operators, until a manageable result set size is attained. In consequence, different query candidates can vary largely in result set size. That is, if the candidate query with the largest result set below some predefined reviewing resource limit is chosen to be final, reviewing resources are not fully utilized. Because all retrieved documents are reviewed for the task of systematic review search, their presentation order seems unimportant. This is however only true for the final and re-ported query. Beforehand, an iterative query refinement process takes place that requires re-peated query quality assessment. The large, unordered result sets, typical for these queries, do not allow for exhaustive evaluation; and precision estimates based on sampling have broad confidence intervals due to the small number of relevant documents. Even more dif-ficult is the quality assessment of independent query parts, each of which might have much larger result sets (see Figure 4.1). This illustrates the importance of research on the perfor-mance of Boolean search filters and the advantage of their re-use. For instance, the highly sensitive search strategy (HSSS) is known to have almost perfect recall in identifying ran-domized controlled trials [Sampson et al., 2006b], and their use is thus suggested in search guidelines [Higgins and Green, 2009]. Ranked retrieval provides a solution to these problems. While the Boolean model has been criticized for its inherent single operand dependency problem due to the use of 85 4.2. RANKING APPROACHES min-max functions [Lee, 1994], the retrieval functions of ranked IR models usually depend on every operator clause and query term, respectively; and output multiple levels of rele-vance likelihood. In the absence of duplicates and with sufficient arithmetic precision, every document in a collection might attain a different similarity score with IR models that incor-porate a wealth of relevance factors. These retrieval function output scores can then be used to rank the documents by decreasing relevance. Users can consciously choose the effort they are willing to invest into the search by examination of an arbitrarily sized prefix of the ranking. The size of this chosen result set can be based on the number of documents that the reviewing team is capable of examining, and adapted under consideration of all docu-ments with the retrieval score at that position in the ranking. Moreover, a retrieval score can directly be used as a cut-off criterion. Finally, the availability of a relevance ranking could possibly reduce costs during the query reformulation process if query quality can be assessed by probing of the top results. Also, the distribution of retrieval scores is instruc-tive. For example, document subsets with similar properties with regard to the query can be identified and then divided by informed changes to the query. 4.2 Ranking Approaches A trivial approach to ranking, commonly employed in current systems, is to sort the Boolean result set by some document property that is assumed to be correlated with relevance, or usefulness. Inverse publication date order is one typical criterion used for this purpose, being an appropriate choice if recent documents are assumed to be most likely to be of greatest interest. For systematic reviews, the use of such an ordering is generally not useful and often unacceptable because of the temporal bias that would be introduced. This section focuses on alternative ranking approaches, based on relevance with regard to a query. First, the use of each class of ranked IR models, as presented in Chapter 2, for the purpose of systematic review search is considered; then one applicable model is chosen for further exploration. 86 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH 4.2.1 Ranking Boolean Result Sets A similar approach is to order the Boolean result set based on a model of document rele-vance, for instance, using the similarity to the query by means of any of the ranked retrieval models. In its simplest case, the number of Boolean query terms present in a document, can serve as an ordering criterion – the coordinate matching approach. Sampson et al. [2006a] empirically studied the use of a ranked keyword search engine to order Boolean systematic review search result sets, and found an increased density of relevant documents in the first 500 results in the ranked list. If only this prefix would have been examined, on average, 70% of the relevant studies in the result sets would have been found; enough to speed up the formation of an evidence base, but too few to avoid examina-tion of the remaining documents. The work of Cohen et al. can also be seen in this category, seeking to reduce the triage efforts by work prioritization, that is, ranking the Boolean result sets on the basis of the output scores of document classifiers and showing the user only the documents above some trained threshold score [Cohen et al., 2006, 2009, Cohen, 2008]. Training can be undertaken on the basis of samples of relevant and irrelevant documents. Wallace et al. [2010b] take this approach further and actively re-train the classifier every time a few more relevance judgements are available. Not to overly bias this active learning process towards initially identified studies – a problem that is known as hasty generalization – Wallace et al. [2010a] suggest global sampling techniques. While these approaches might be valuable for query quality assessment and the review of the finally retrieved studies, their effectiveness is constrained by the recall levels attained by the Boolean queries used in the first place. Karimi et al. [2009] seek to address this problem by relaxing some of the most restrictive query elements.1 The consequently larger result sets are then ranked on the basis of the BM25 similarity measure using keyword query representations of the review information needs. The combination of these two retrieval models turned out to have the best retrieval results, but, depending on the result set size of these relaxed queries, the approach becomes 1The author of this thesis participated in the work for this publication, but it is not reported as contribution for this thesis. 87 4.2. RANKING APPROACHES very similar to directly using ranked keyword querying, which would have the added benefit of reduced processing complexity. Moreover, the incorporation of BM25 leads to other problems, as detailed next. 4.2.2 Ranked Keyword Retrieval For ranked keyword retrieval to serve as a replacement for the Boolean approach, it has to be possible and practical to generate keyword queries that are at least equally effective. Additionally, the requirements of systematic reviews have to be satisfied. Both of these prerequisites are examined in the following. In the work of Karimi et al. [2009, 2010],2 the Boolean approach is compared with ranked keyword alternatives that use the terms of the Boolean query, systematic review title (T), research question (R), inclusion and exclusion criteria (C), or all of them (TRC). When ranked with BM25, one of the currently best performing retrieval models [Armstrong et al., 2009], more information included as part of the query leads to better retrieval performance. The combination of terms from review sections (TRC) resulted in the best retrieval results, better than when the terms in the Boolean query were used. This exemplifies the non-trivial problem of constructing effective keyword queries. Furthermore, while the performance of Boolean queries varied dramatically (partly due to reporting errors), ranked retrieval always found a reasonable number of relevant studies. Like Cohen et al. [2006], Martinez et al. [2008] train a classifier on an examined study subset from the first 10,000 results of the TRC queries. Assuming a typical bag-of-word feature representation, this relevance feedback method effectively re-ranks the first set of results based on the learned relevance of individual terms. As such, it can be seen as another form of ranked keyword retrieval, with a trained retrieval function. The results of these approaches exemplify the advantages of ranking: a higher density of relevant studies are found when prefixes of the ranked retrieval list are examined and, when the number of examined studies is consciously chosen to be above any particular Boolean 2The author of this thesis participated in the work for these publications, but they are not reported as a contribution of this thesis. 88 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH result set size, a larger number of relevant studies can be found. As long as all relevant studies are in the collection and match at least one query term, ranked methods can achieve total recall when the end of the ranked list is reached; but the OR-set of the query terms can be very large, making exhaustive evaluation impractical. Nevertheless, if additional gains justify examination of more documents than can be retrieved with a Boolean query, the ranked approach will be of advantage for the task at hand. That is, with ranked retrieval, resource allocation can be done adequately. Unfortunately, it remains to be seen whether ranked keyword queries can outperform retrieval models that support structured queries when search experts are employed. Instead, there is good evidence that structured, faceted queries outperform keyword queries [Lin and Demner-Fushman, 2006, Zhu et al., 2007, Boudin et al., 2010]. Moreover, despite the advantages of ranked retrieval in general, the adoption of ranked keyword methods is also unlikely for biomedical systematic reviewing because of contradictory requirements, most significantly: • Reproducibility: All of the keyword retrieval models described in Section 2.1.2 make use of document collection statistics, in particular, the inverse document frequen-cies of the query terms. The fact that collections grow over time inevitably leads to changes in these collection statistics. To make the retrieval results for systematic reviews reproducible, all of the involved collection statistics would have to be pub-lished. Alternatively, every change to a collection would have to be recorded, made publicly available, and a timestamp published in the review, so that every collection state could be traced back to any point in time. The wealth of factors that go into the ranking procedure render exact reproduction difficult, even more so for trained and combined models. • Scrutability: Because retrieval of a document also depends on properties of other documents, it is not obvious why a particular document has or has not been considered during the reviewing process. 89 4.2. RANKING APPROACHES • Composability and re-use: The semantics of recombined ranked queries, constructed from individual query primitives, are not clearly understood. That is, it is difficult to reason about the performance of a new query on the basis of knowledge about effectiveness of query parts. • Complex information need descriptions: The flat structure of keyword queries im-poses boundaries on the expressiveness of the model. For instance, distinct concepts cannot separately be described. Sampson et al. [2006a] found that keyword queries were not able to make the fine discriminations that are typical for the relevance to systematic reviews and often defined by strict inclusion and exclusion criteria. To satisfy these requirements, a retrieval model needs to have two properties: 1) a struc-tured query model has to be supported, and 2) document scores, being the result of the em-ployed retrieval functions, must only depend on properties of each considered document. The latter is satisfied for binary term weighting schemes, but more generally also for any other weighting scheme that only uses TF-components; but not IDF, or any other collection statistic. The first property motivates investigation of the applicability of ranked retrieval models that use structured queries. 4.2.3 Ranking with Structured Queries Two classes of retrieval models with structured query languages were reviewed in Chapter 2. Extended Boolean retrieval (EBR) models rank on the basis of Boolean query spec-ifications. Approaches to EBR differ in the retrieval functions employed, the weighting scheme used, and support of query term weights. Binary term weights only depend on the (query) terms in the document and are supported by most of the models; hence the whole class of models becomes applicable to systematic review search. With use of binary term weights, the soft Boolean retrieval model [Radecki, 1979] would become equivalent to the Boolean model due to the use of the same retrieval functions. However, more sophisticated document-dependent term weighting schemes are also possible. 90 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH Of particular interest is the p-norm approach to EBR of Salton et al. [1983c], because it has shown to have desirable properties [Lee, 1994] and it was also found in early research to be empirically more effective than the other approaches [Lee and Fox, 1988]. More importantly it generalizes the Boolean retrieval model and can be parametrized such that the Boolean model can be recovered as a special case (p → ∞). Because of this property, the EBR model should perform at least as well as the Boolean approach, while adding the advantages of ranking. Although the p-norm model is able to support any term weighting scheme that produces scores within the range [0, 1], binary term weights are the only weights suggested so far in the literature that meet that requirement. It is also not obvious how to derive better document-centric weights from short citation entries for which most term frequencies are 1. Moreover, the strict Boolean baseline can also be seen to use binary term weights; and in a comparison of competing models it is desirable to minimize the number of changes because it is more informative to attribute performance disparities to a smaller set of differences between models. Note also that, despite the use of binary term weights, the p-norm model does not become equivalent to coordinate matching; query sub-expressions on a higher tree level, which may represent higher-level concepts, are given more influence on the final score. The inference network model is another candidate model for structured retrieval, but has been proposed in the context of TF·IDF or term weighting schemes based on language models, both dependent on collection statistics. To suit the requirements of biomedical systematic review search, the model would have to be adapted. 4.2.4 Summary Ranked keyword retrieval is appealing for many tasks; for systematic review search, how-ever, these models are not directly applicable and would have to be adapted. Previous work suggests [Karimi et al., 2009] that even with a retrieval model that uses a state-of-the-art term weighting scheme, retrieval effectiveness is inferior to the current Boolean method. Note that, for these results, obvious keyword queries have been used that might have dis-advantaged the ranked method. For a fair comparison of Boolean and ranked retrieval, the 91 4.3. RANKING WITH THE P -NORM EBR MODEL input of the ranked method should hence be the same information need description as for the Boolean method with the same amount of effort spent on query development. Of the two structured retrieval models discussed, only the p-norm model is directly applicable to the task of interest. Moreover, previous effectiveness results and the theoretical prospect of being at least as good as the Boolean retrieval model for appropriate parameter settings, motivate further exploration of this model. Finally, the similarity to the Boolean model has the appealing prospect of quick acceptance and adoption in practice. The only difference to current practice would be to report p-values as part of the published query, plus a score-cutoff, corresponding to the retrieval set size. Depending on the choice of these values, the same retrieval results can be attained as with the Boolean model. We now pursue in detail the p-norm approach to EBR with binary term weights. 4.3 Ranking with the p-Norm EBR Model Because the current Boolean retrieval method and the p-norm EBR model both use struc-tured queries as information need descriptions, it is desirable to reuse the complex system-atic review queries. However, in order to do so, two challenges have to be overcome. While many specific conjunctive operators have been added to the basic Boolean re-trieval model, for instance, based on the position of terms in the document, the p-norm model lacks corresponding extensions. One way of making the complex Boolean system-atic review queries compatible with the EBR model is to relax specific operators to simple conjunctions. For instance, a phrase can be considered as a conjunction with additional con-straints. If these constraints are dropped, there might be false positives that would impact precision, but not recall. More importantly, the extended Boolean interpretation of these restrictive operators might allow documents to be ranked highly that otherwise would have been strictly excluded. The relaxation of operators to the three basic Boolean operators AND, OR and NOT is thus a practicable method. Finally note that, in the past, phrases have commonly been transcribed to conjunctions when storage and processing of position information was still considered too expensive. 92 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH A second complication is that the queries have been prepared with the Boolean model in mind, not the p-norm EBR model. In particular, the associativity property in Boolean logic: a OR b OR c = (a OR b) OR c = a OR (b OR c) , and a AND b AND c = (a AND b) AND c = a AND (b AND c) , does not hold for the p-norm model. This property allows systems to implement binary Boolean operators only, and then to resolve n-ary operator versions by repeated application of the binary operators. For p-norm systems, query languages and parsers instead have to also support n-ary operator versions. Despite the fact that the query language of Ovid contains both versions: OR/1–7 ≡ 1 OR 2 OR · · · OR 7 , the queries in systematic reviews use them interchangeably due to their equivalence under the Boolean retrieval model. That means that concepts, as described by query subtrees, that just happened to be on a lower query tree level due to the associativity of binary operators, might have a lower influence on the overall score than peer concepts. One possibility to counteract this obstacle is to flatten nested cascades of the same op-erator into one n-ary operator. For instance, a query: AND(OR(a,AND(b, c)),AND(d,OR(e,OR(f, g)))) . would be normalized to AND(OR(a,AND(b, c)), d,OR(e, f, g)) . With these two modifications, systematic review queries can be executed in an ex-tended Boolean fashion, and a retrieval score calculated for each document by way of which they can be ranked in descending order. The resulting score distributions for different re-trieval models, depicted in Figure 4.2, reveal interesting properties of the resulting rankings. 93 4.3. RANKING WITH THE P -NORM EBR MODEL 0 500 1,000 1,500 2,000 2,500 3,000 0 1 Rank R et ri ev al S co re (N or m al iz ed ) Boolean p-norm, binary p-norm, TF·IDF term weights Ranked, BM25 Figure 4.2: Score distributions for a selection of retrieval models for one systematic review query from the AHRQ COHEN query set. The scores of the ranked retrieval models have been normalized to the range [0, 1] over the first 3,000 ranks. Note that the discrete score levels between ranks are connected for presentational purpose only. The Boolean retrieval model follows a sharp step function because documents can be seen to be assigned a score of 1 if they are returned, and 0 otherwise. At the other extreme, BM25 assigns a different retrieval score to almost every document as long as there are no duplicates. EBR models stand between these two extremes. When binary term weights are used, the number of output levels is limited by the number of term combinations found in documents and the query structure. Hence, the ranking consists of a sequence of unordered sets in which all documents have the same score. As with the Boolean model, such ties can be broken by a secondary sort criterion. While it might be more advantageous to have a continuous ranking, a study of the distinct score levels and the query structure can be valuable to determine document sets and their cardinality that miss specific conjuncts. Note that, the higher levels in the query tree dominate the retrieval score. In the example of Figure 4.2, the sharp drop in the retrieval score designates documents that do not contain all conjuncts in the top level of the query. However, note that the drop does not happen at the same rank as for the Boolean model, due to a few documents that do not satisfy conjuncts of minor importance, but satisfy all main concepts required for the systematic review. It would not only be wasteful to dismiss these additional documents for examination of relevance, but these particular documents can also be very informative in order to refine the query. 94 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH 4.4 Experiments To validate the effectiveness of the suggested ranking approaches empirically, this section describes retrieval experiments. First, the experimental setup is explained. Then, the ex-perimental Boolean prototype implementation is validated to retrieve the same, or similar, results as a real, currently used system. Section 4.4.3 compares the effectiveness of the sys-tems from a ranking perspective; and Section 4.4.4 complements this with results relative to Boolean result sets. Finally, the findings are summarized. 4.4.1 Experimental Setup For the experiments in this chapter, we focus on the AHRQ COHEN and AHRQ KARIMI query sets, using the TRC queries developed by Karimi et al. [2009]. The available rel-evance judgements for the AHRQ COHEN query set have been reduced to contain only the documents in the 10 year Medline corpus used in the TREC Genomics Track [Cohen et al., 2006], that is, to documents for which indexing for Medline has been completed be-tween the years 1994 and 2003 (inclusive) [Hersh et al., 2004]. Only documents that have been included in the particular systematic review are considered relevant; everything else is assumed irrelevant for evaluation purposes. While this assumption is the root of much discussion [Sanderson, 2010], systematic reviews, if successful, are by definition compre-hensive in their relevance judgements. To match the collection with the relevance judgements of the query sets, a Medline collection subset has been generated to include only documents that have been published between the years 1993 and 2003 (inclusive) so that we can have reasonably complete knowledge about the relevance of documents in the collection. This step is necessary to circumvent the possibility of disadvantaging retrieval systems that rank relevant documents outside of this year range highly. In effect, 4,515,866 citations remain in the document collection. To compare the different retrieval approaches, an experimental system is used. Lucene serves as basic indexing engine, on top of which the three basic EBR operators of the p-norm 95 4.4. EXPERIMENTS retrieval model, and all complex operators (including proximity constraints) of the Boolean retrieval model are implemented. Term implementations directly access the inverted lists. At the end of the query processing chain (refer to Figure 3.4 on page 77), the query nodes are mapped to the operator implementations of each retrieval model. However, more specific operators are mapped to one of the three basic Boolean operators in the case of the p-norm retrieval model. As already discussed, phrases are considered as conjunctions without enforcing the additional proximity constraints. The output of the system is either a Boolean result set, or a ranked list, ordered by de-creasing retrieval score. In both cases, reverse chronological publication order, the default criterion in most biomedical Boolean search systems, is used as a tie-breaker to sort docu-ments with the same retrieval score; and, in case of the Boolean retrieval model, to sort the whole result set. Instead of implementing BM25 as another baseline, the freely available search engine implementation zettair is used. It is already proven in TREC evaluations to have an effective BM25 implementation [Garcia et al., 2006, Büttcher et al., 2006]. Assuming random document order, relevant documents will evenly be distributed over the result set of a Boolean system. In contrast, an effective relevance ranking method should have a higher density of relevant documents at the top of its ranked list. To measure this effect, several evaluation measures could be used. The simplest is to count the absolute number of relevant documents encountered at particular ranks. However, normalization of these cardinality counts by the number of relevant documents, that is recall, leads to an evaluation measure that is more evenly distributed over the different queries, with aggregate performance less dominated by any one query. Because even for systematic reviews, the total number of relevant documents can only be assumed to be approximated by the number of known relevant documents, relative recall is used throughout the results presented in this chapter. Assume an information need for which 6 relevant documents are found after evaluation of a ranking prefix or Boolean result set, and a total of 12 relevant documents are present in the collection, out of which only 10 are known. This results in a relative recall of 610 = 0.6, an overestimate of the actual recall 6 12 = 0.5. 96 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH 4.4.2 Reproducibility Before the systems can be compared, it has to be verified that the experimental system is representative, that is, produces the same, or very similar, retrieval results as systems that are currently used for the purpose of systematic review search. Moreover, reproducibility is a key requirement for systematic reviews. Unfortunately, typographic errors in the reporting of the queries, and differences in document collection sets, document indexing, query parsing, and their transformation pro-cesses, lead to variations in the retrieval results of different systems [Sampson and Mc-Gowan, 2006]. The stipulated requirement to search multiple databases alleviates this issue to some degree, even if the document collections largely overlap. Furthermore, current sys-tems do not provide historical views of their document collections and the implementations used at that time, so that some of the studies that could be retrieved before might have been consequently removed from the collection as stipulated by Medline. The continuous refinement of the MeSH taxonomy might further add to the possibility of changes. To investigate the severity of these reproducibility issues, and to validate the represen-tativeness of the experimental system, the queries were also submitted to Ovid, the system they were originally developed for. PubMed identifiers are extracted from the Boolean re-sult sets and then reduced to the documents in the Medline collection subset used for these experiments. Because the inherent retrieval difficulty is of primary interest in this thesis, obvious errors in the reported queries have been fixed. Query terms have been examined to match index terms and to reflect the originally intended MeSH headings. The number of results for each query line, provided by some review authors, served as a guide in query debugging. Table 4.1 shows the cardinalities of the result set for each query of AHRQ COHEN and AHRQ KARIMI when submitted to Ovid and the experimental system. The size of these individual set intersections reflects how many studies are retrieved by both systems. Due to the use of a subset of Medline, result set sizes for both systems are generally smaller than stated in Section 3.1.2. Ovid returns often slightly larger result sets; and, in the ma-jority of cases, almost all of the documents returned by the experimental system were also 97 4.4. EXPERIMENTS AHRQ COHEN AHRQ KARIMI Query Ovid Repro. Overlap Ovid Repro. Overlap 1 1,597 1,457 1,452 1,530 4,136 1,400 2 979 884 884 796 602 579 3 353 342 342 322 338 314 4 735 629 629 3,223 3,213 3,207 5 3,856 3,805 3,727 145 8 8 6 1,377 1,340 1,337 1,718 1,710 1,709 7 306 248 248 292 345 204 8 286 271 271 2,336 1,626 1,625 9 742 823 629 1,992 1,375 1,367 10 282 258 258 884 986 716 11 1,108 1,106 1,106 186 135 135 12 718 682 682 198 140 139 13 2,460 2,331 2,104 302 229 228 14 413 372 372 – – – 15 823 801 801 – – – Mean 1,069 1,023 989 1,071 1,141 894 Table 4.1: The Boolean result set size for Ovid and the experimental system (column "Repro.") on the Medline subset for each query of AHRQ COHEN and AHRQ KARIMI. The "Overlap" columns show the cardinality of the intersection between the two result sets. retrieved by Ovid. Note that the differing result set sizes are typical for the issues with Boolean queries; their strictness can lead to exclusion of documents due to marginal query and system differences. In summary, the experimental system reasonably reproduces Ovid, one of the systems used as a reference point for systematic review search. Despite not being directly compara-ble, the Ovid runs are used as another baseline in the remaining experiments of this chapter. Due to slightly larger result sets, they possibly contain a greater fraction of the relevant documents and hence have higher recall; however, at the same time, relevant documents are also spread out over a larger result set due to the used relevance-independent list ordering, possibly leading to worse precision at any rank cut-off. 4.4.3 Ranking Effectiveness Having established the validity of the Boolean baseline for the experimental system, we can investigate the effectiveness of ranking, and compare the p-norm EBR approach to 98 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH the Boolean baseline of the experimental system. Although it does not satisfy systematic review search requirements, a weighted version of the p-norm EBR model on the basis of the normalized TF·IDF formulation proposed by Salton et al. [1983c] has been measured. In particular, inverse document frequencies idft for a term t are determined by logN/ft, where N refers to the number of documents in the collection and ft to the number of documents containing the term; and they are then used to compute the normalized weight wd,t of a term t with respect to a document d as: wd,t = fd,t maxi∈d fd,i · idft maxi∈d idfi , (4.1) where fd,t refers to the term frequency of t in d. Finally, the TRC query baseline of Karimi et al. [2010], based on BM25, has been reproduced using the same default parameter settings of zettair that were originally employed (k = 1.2 and b = 0.75). The p-norm EBR model requires the specification of a value for p for each individual conjunction and disjunction query operator. Out of lack of a better assignment, we assume a single p-value for all query operators and chose a relatively large value of p = 9, the largest value suggested in previous work [Salton et al., 1983c], in order to attain retrieval results close to those of the Boolean method. All systems were executed on the two sets of test queries and their output considered as ranked list, including the Boolean systems. Then relative recall is computed at absolute ranks (100, 300, 1,000, 3,000 and 10,000) and averaged over each query set. Using this measurement methodology, the Boolean runs are disadvantaged at ranks larger than the re-sult set size of each Boolean query, because their result sets are limited and they cannot produce additional (relevant) documents. However, such a comparison reflects the funda-mental problems with the Boolean model. Table 4.2 presents results for discrete rank cut-offs and with statistical tests, and Fig-ure 4.3 depicts them graphically. Several observations can be made: • The experimental system has similar results as Ovid, further supporting the validity of this baseline. 99 4.4. EXPERIMENTS AHRQ COHEN AHRQ KARIMI p-norm, p=9 Boolean BM25 p-norm, p=9 Boolean BM25 Rank Bin.a TF·IDFb Repro.c Ovidd TRCe Bin.a TF·IDFb Repro.c Ovidd TRCe 100 0.18cd 0.16c 0.09 0.10 0.15 0.10 0.08 0.06 0.05 0.11 300 0.40c ′d′e 0.34 0.29 0.27 0.27 0.22 0.17 0.15 0.16 0.19 1,000 0.62c ′d′e 0.57 0.48 0.49 0.48 0.36c ′ 0.31 0.27 0.29ac 0.30 3,000 0.72c ′d′ 0.70c 0.58 0.60 0.66 0.45c ′d 0.41 0.36 0.37 0.40 10,000 0.79c ′d′ 0.77c ′d′ 0.59 0.61 0.81c ′d′ 0.54b ′c′d′ 0.47c ′d 0.36 0.37 0.48c Table 4.2: Relative recall for different systems and a range of absolute ranks, averaged over the queries of each AHRQ query set. The superscripts refer to the table columns defined in the table header and statistical significant improvements. For every system pair, a one-sided, paired Wilcoxon signed-rank test has been used to test for statistically significant improvements at the 0.05 (super-scripts a· · ·e) and 0.01 (superscripts a′· · ·e′) alpha levels. • The p-norm EBR system with binary term weights finds significantly more relevant documents than both of the Boolean baselines in the AHRQ COHEN query set; this is less significant in the smaller AHRQ KARIMI query set, but the same trend can also be observed. • Binary term weights always have better performance than TF·IDF-based term weights. This is surprising, because real-valued term weights should be at least as good as bi-nary ones, if the weights are appropriately chosen. It remains to be seen if other weighting schemes, for instance based on term frequency, results in better rankings. • The broader TRC bag-of-words queries were only able to match with the best p-norm EBR model at retrieval depths of at least 5,000 documents (compare Figure 4.3), confirming the findings of Martinez et al. [2008]. • In the range considered, the relative recall levels for the ranked EBR approach (61.7% for AHRQ COHEN and 36.4% for AHRQ KARIMI, at a retrieval depth of 1,000) are higher than for the Boolean system (48.5% and 27.1%, respectively, at the same measurement point). But even at a depth of 10,000, perfect (relative) recall cannot always be achieved with a single query. 100 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH • While the ranked systems have strictly increasing recall values, the Boolean systems are limited by their result set size, and their average performance hence flattens off much earlier (compare Figure 4.3). The introduction of some elements of ranking allows users to look behind the strict inclusion cut-off of Boolean queries and to consciously choose the investment they are willing to make in the search. Moreover, the increased density at the beginning of the ranking has the potential to facilitate query reformulation. While the results illustrate the ability of ranking to always return a large number of doc-uments, with many of the relevant ones at the beginning of the ranked list, the aggregation of recall at ranks greater than the Boolean result set size means that the ranked retrieval models are also likely to return a larger number of relevant documents than the Boolean systems. Because we measure at the same fixed ranks for all queries, some queries will inevitably be measured at ranks larger than their result set size. Hence performance relative to individual Boolean result set sizes is evaluated next. 4.4.4 Relative Performance Given the large exponent, p = 9, the retrieval scores are close to those of the Boolean baseline. It is hence counter-intuitive that EBR queries are able to retrieve a larger number of relevant documents, even if the same number of documents is examined. To investigate this issue, in this section we probe at ranks relative to each individual query's Boolean result set size, denoted Bq. Table 4.3 has the same layout as before, but (relative) recall is measured and averaged at ranks relative to individual Boolean result set sizes of the experimental system, in particular, when one fourth, half, equal, or double that many documents are inspected. The Boolean system can not extend its result set and retains the same recall level. Overall, similar trends emerge. Again, the differences on AHRQ KARIMI are less significant than those noted for AHRQ COHEN, but follow the same trend. However, addi-tional conclusions can be drawn. 101 4.4. EXPERIMENTS 0 1,000 2,000 3,000 4,000 5,000 0 0.2 0.4 0.6 0.8 Rank R el at iv e R ec al l p-normBIN, p=9 p-normTF·IDF, p=9 Boolean, Repro. Boolean, Ovid TRC queries, BM25 (a) AHRQ COHEN 0 1,000 2,000 3,000 4,000 5,000 0 0.2 0.4 0.6 Rank R el at iv e R ec al l p-normBIN, p=9 p-normTF·IDF, p=9 Boolean, Repro. Boolean, Ovid TRC queries, BM25 (b) AHRQ KARIMI Figure 4.3: Average retrieval model effectiveness, measured as the fraction of found relevant doc-uments to the number of known relevant documents (Relative Recall) as a function of the number of documents examined for the two AHRQ query sets. For typical systematic reviewing efforts, the p-norm EBR approach with binary term weights retrieves on average a greater number of relevant documents than the Boolean baseline, Ovid, or the unstructured TRC bag-of-word queries. 102 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH AHRQ COHEN AHRQ KARIMI p-norm, p=9 Boolean BM25 p-norm, p=9 Boolean BM25 Rank Bin.a TF·IDFb Repro.c Ovidd TRCe Bin.a TF·IDFb Repro.c Ovidd TRCe 0.25Bq 0.28 cd′ 0.22 0.16 0.14 0.20 0.19 0.12 0.13 0.13 0.16 0.5Bq 0.44 de 0.43 0.34 0.31 0.29 0.26 0.24 0.22 0.21 0.21 Bq 0.61 be′ 0.59e 0.59e 0.58e 0.41 0.34 0.34 0.36de 0.30 0.27 2Bq 0.69 b′c′d′e 0.63 ⊥ 0.61† 0.58 0.40c′ 0.37 ⊥ 0.37† 0.33 Table 4.3: Relative recall for different systems, averaged over the queries of each AHRQ query set, and measured at ranks based on multiples of Bq , the Boolean result set size attained with the experimental Boolean system (column "Repro.") for each individual query. The symbol ⊥ indicates that the Boolean result set could not be extended so that the same relative recall as at rank Bq applies. The relative recall values of the Ovid system marked with the superscript † have been achieved at ranks larger than Bq , but much smaller than 2Bq . For every system pair, a one-sided Wilcoxon signed-rank test has been used to test for statistically significant improvements at the 0.05 (superscripts a· · ·e) and 0.01 (superscripts a′· · ·e′) alpha levels. When a quarter of the documents are examined, the number of found relevant documents is increased by 50%–75%; and when twice as many documents are inspected a significantly larger number of relevant documents is found. However, the EBR systems cannot find significantly more relevant documents in the top Bq documents, confirming the conjecture that a high p-value should lead to the same documents in the first Bq documents in the ranking as in the Boolean result set. Note, however, that the performance of a system at these ranks is not of particular importance because Bq cannot consciously be chosen, and there might be capacity to review more documents, or a wish to review fewer. Of particular significance is that the EBR queries can keep up with the Boolean counterparts even though they have a reduced set of operators and thus often less specificity. Chronological order appears to lead to a very similar distribution as random order, in-dicating that publication time is not of primary importance during the initial retrieval for a systematic review to be eligible for inclusion. Ovid returns on average slightly more documents, and some of the additional ones are indeed relevant. The TRC queries identify fewer relevant studies than Boolean or EBR queries, when only Bq documents are examined. 103 4.5. SUMMARY The use of a high p-value implicitly leads to similar documents in the first Bq ranks, en-suring that p-norm retrieval is at least as effective as Boolean retrieval. In fact, while other p-value assignments could theoretically lead to even better rankings, anecdotal evidence from experiments with smaller p-values showed mixed results in regard to retrieval effec-tiveness when the same number of documents are retrieved as with the Boolean baseline and the same p-value is applied to all query operators. Due to the combinatorially large number of possible p-value assignments and because it is more worthwhile to propose a method that is at least as good as the current approach than to propose a method with mixed results, more thorough investigation of policies to assign different p-values to queries is deferred to future work. But even if p-values are chosen that perform worse when the same number of documents are retrieved, the ranked approach can be combined with Boolean retrieval in at least two ways. First, the query specification can be used to rank the Boolean result set; and second, result sets can be extended to documents that do not satisfy particular conjuncts, or a given number of conjuncts. Such documents might be very informative during the query reformulation process because they can guide the search for alternative formulations of specific conjuncts. 4.5 Summary In this chapter, ranked retrieval approaches have been evaluated and the p-norm approach to extended Boolean retrieval with binary term weights shown to satisfy the specific require-ments of systematic review search. This particular model generalizes Boolean retrieval and has been demonstrated to retrieve a greater fraction of relevant documents when typical numbers of documents are examined. While for our choice of p-value similar recall levels are achieved with the p-norm model as with the Boolean model when the same number of documents are retrieved, the size of these Boolean result sets is largely out of control for the user, and they are hence likely not to match the available reviewing resources. When Boolean result sets are extended, a greater number of relevant documents are found which reduces bias in favour of the initially retrieved document set and the risk to miss eligible 104 CHAPTER 4. RANKING FOR SYSTEMATIC REVIEW SEARCH studies. Finally, ranked approaches possibly provide advantages in the query reformulation process because of higher relevance densities in the top of the rankings that can be used to assess query quality. Because the p-norm EBR model with binary term weights achieved best effectiveness results for the task of interest, we focus on this particular model in the rest of this thesis. A series of efficiency optimizations have been proposed for processing of ranked keyword queries which reduce the number of documents and postings that have to be processed. In the next chapter, efficient implementation methods for the p-norm EBR model are discussed. 105 Chapter 5 Efficient Query Processing For users, the effectiveness of a search service and the price they have to pay for the service are of primary concern. Consequently, search service providers are interested in providing competitive effectiveness levels at the smallest possible cost, hence, added benefits in effec-tiveness of a method have to outweigh any extra costs. Efficient query implementations aid this trade-off, and make information retrieval models for which efficient processing methods are known more likely to be adopted in practice. The previous chapter established some advantages of ranking when performed in the context of complex Boolean queries and the p-norm approach to extended Boolean retrieval. Because retrieval models have to be effective and efficient to be a viable choice in practice, this chapter investigates how query processing for the p-norm EBR model can be optimized. 5.1 Implementation Approaches Search service providers in the biomedical domain receive orders of magnitude fewer queries than are submitted in general web search; and collections are much smaller. However, most web queries are very short, and also likely to be repeated, so that they are cheap to evaluate and caching techniques are effective. In the extreme, the results for all single term queries can be precomputed and stored as such in inverted lists [Strohman and Croft, 2007]. As indicated in Chapter 3, biomedical queries not only contain more terms, but are also much 107 5.1. IMPLEMENTATION APPROACHES more complex – during the query preprocessing steps, queries might expand to thousands of terms. Moreover, the retrieval process involves a great deal of reformulation and query quality assessment. Because the latter can only be gauged after query processing, addi-tional (complex) queries have to be executed. In combination, these various factors mean that efficient query processing methods for effective retrieval models that are applicable to systematic review search are an important direction of research. Several approaches to the implementation of the p-norm EBR model have been sug-gested in the literature that differ in terms of time and space complexities. This section re-views these approaches and examines the applicability of more recent processing methods. Document-at-a-time Baseline A straightforward implementation for any retrieval model is to identify and iterate over all documents in the query OR-set and calculate a score for each using the retrieval function of the model, after which the OR-set documents can be sorted by decreasing score. To calculate document scores in EBR retrieval models, query trees can be recursively processed from the leaves to the root and, at each operator node, the scores of clauses aggregated by the corresponding retrieval function. The initial term scores that start the process can be calculated on the basis of the information drawn from the inverted lists of the terms. The use of the document-at-a-time evaluation strategy in conjunction with document-ordered indexes simplifies this implementation. This is the method that was originally implemented for the p-norm retrieval model in the SMART system [Salton et al., 1983c, Fox, 1983b]. The algorithmic time complexity of this approach is O(l·n), where l refers to the size of the query measured as the number of query nodes, and n is the number of documents in the query OR-set. Unfortunately, both of these factors are likely to be large for biomedical queries. First, they have a large number of terms, and their structure means that further nodes are added. Increasing the number of terms also increases the likelihood of a large OR-set – and the use of any one of a few common meta-data terms, such as Humans/, is sufficient for the OR-set size to quickly approach the size of the collection, |C|. In practice, hidden constants also have a significant impact on query execution times. To process a query, all 108 CHAPTER 5. EFFICIENT QUERY PROCESSING inverted lists have to be read, decoded and merged to generate the query OR-set. Then, to score each individual document, the complexity of the retrieval function adds further to processing times. The efficiency of an implementation approach will hence depend on its ability to reduce these factors. In the document-at-a-time strategy the space complexity is simply O(l). To collect scores for all documents, further space is required in the order of O(n). However, if only the k � n documents with the highest scores need to be retained, a min-heap of k items can be used. Term-at-a-time List Aggregation In the baseline document-at-a-time approach, every query node is processed for each doc-ument in the OR-set, even if the document only contains a small subset of the query terms. Smith [1990] proposes a more efficient algorithm that complements the baseline by revers-ing the roles of documents and the query. Instead of repeatedly processing the query for different documents, the query is processed only once, but, for each operator, all documents involved in that query sub-expression are processed. More specifically, the inverted lists of the clauses of an operator are merged to obtain the OR-set of that query sub-expression and an intermediate score for each of the documents is computed. This, however, requires inter-mediate storage of the OR-sets for all clauses of an operator and the partial document scores. In this, the method is similar to the term-at-a-time evaluation strategy (see Section 2.2.2 on page 30), but requires multiple accumulators. As a result, not every query node is examined for each document in the overall OR-set, and the time complexity reduces to the sum over the OR-sets of the query sub-expressions. However, the method comes at a price – added space requirements. Multiple accumulators with intermediate results have to be stored during processing of every query. In the final aggregation for the query root node, together these accumulators will have at least the size of the overall OR-set. For biomedical queries, this will often be a multiple of the collection size due to the frequent use of common (meta-data) terms, and hence a significant cost. 109 5.1. IMPLEMENTATION APPROACHES Efficient Retrieval Functions The analysis sketched above ignores the cost of the elementary operations because they are assumed to be constant between the methods that are being compared. The exponentiation-based retrieval functions of the p-norm model are, however, more complex than the sums employed by other retrieval methods [Lee et al., 1993]. Hence, reductions in the computa-tional footprint of the retrieval functions are worth investigating. Smith's Infinity-One retrieval model [Smith, 1990] seeks to provide an approximation to the p-norm model that can be computed more efficiently. In the model, the range of p-values is realized by linear interpolation between the use of p = 1 and p = ∞, both of which can be implemented without exponentiation. The experimental results showed much improved retrieval times, but also slightly reduced effectiveness. Also the SMART system sought to replace floating-point operations with approximations that use integer arithmetics only [Fox and Koll, 1988]. The impact of more complex retrieval functions might be smaller in context of the hardware available today. If input values to the retrieval function are repeating (for example, if they are discretiza-tions of the continuous values), an alternative approach is to employ caching of computa-tions or the pre-computation of a set of inputs. Recent Approaches So far, the suggested implementation approaches for the p-norm EBR model inspect all of the postings in the inverted lists of all of the query terms. In ranked retrieval systems, however, it is often sufficient to retrieve the k documents with the highest retrieval score, or above some score threshold τ , because k can be chosen to be larger than the typical number of documents a user would examine; and, even if more documents are requested, the query can be re-executed with a larger k. For ranked keyword queries, Max-Score is a particularly effective optimization that skips postings of documents that cannot make it into the top-k and can hence further reduce processing costs [Turtle and Flood, 1995, Strohman et al., 2005]. The Max-Score method 110 CHAPTER 5. EFFICIENT QUERY PROCESSING orders query terms by decreasing document frequency and seeks to bypass postings for the most common terms if they are not matched by corresponding postings for other less common terms. The detailed workings of this algorithm are described in Section 16 on page 34. In contrast, Broder et al. [2003] propose an implementation for their Weak AND (WAND) operator that continuously keeps the terms sorted by the next document identifier in their inverted lists and tries to skip postings in the first terms. This possibly further reduces the number of postings that can be skipped through exploitation of the document identifier distributions in the inverted lists, but entails additional overhead because of the repeated partial sorting step. Other implementation approaches try to only approximate the retrieval results attained by these exact methods via pruning of query evaluation based on heuristics [Fox et al., 1993]. While such methods can lead to even more efficient query processing, they are typically at the expense of retrieval effectiveness. Moreover, the used heuristics are often also dependent on characteristics of the whole document collection, making them inapplicable in context of systematic review search. If term contributions differ between documents, higher initial entry thresholds can be attained when top-scoring documents for each term are stored in additional inverted lists at indexing time, and then merged for each query before the evaluation starts. Strohman et al. [2005] demonstrate that this method indeed reduces retrieval times and reduces the number of documents that have to be scored. In the context of systematic review search, such a threshold could even be provided by the user, for instance, using the retrieval score cut-off published with a final review query that the user wants to replicate. Such an approach makes the retrieval reproducible because only documents above the threshold are retrieved, but the result sets also increase in size as the collection grows. Unfortunately, these optimizations are not directly applicable for structured queries be-cause overall retrieval functions do not decompose into simple sums; that is, to score a document an order has to be followed, which is dictated by the query structure. 111 5.2. A QUERY PROCESSING METHOD FOR P -NORM QUERIES 5.2 A Query Processing Method for p-Norm Queries To date, efficient methods available for keyword ranked queries have not been applied to structured queries. This chapter investigates methods by way of which similar efficiency gains can potentially be achieved for retrieval models with structured query languages such as the p-norm EBR model and addresses that gap. Our investigation focuses on binary term weights, because the best effectiveness results (see Figure 4.3 on page 102) have been achieved with this weighting scheme. Generally, query processing costs increase as the product of query complexity and the number of documents that have to be considered from the collection. This section describes processing methods to reduce both of those costs. First, a document scoring method is described that does not have to process the whole query tree for most documents. Then, elements of the Max-Score and WAND algorithms are generalized to be applicable in the context of p-norm query processing. Finally, term-independent bounds are proposed that complement the bounds obtained from Max-Score, but which can also be used by them-selves under the evaluation scheme in the WAND algorithm. 5.2.1 Document Scoring Most of the retrieval functions for keyword queries attain a final similarity score by summa-tion of term contributions, which does not impose any order in which these terms have to be examined. However, the recursive nature of EBR retrieval functions make it necessary for scores on lower levels of the query tree to be calculated first. A consequent implemen-tation strategy would hence be to add processing logic to each query operator, each acting independently on the outputs of their clauses. Both Salton et al. [1983c] and Smith [1990] follow this approach, but their implementations differ in the inverted list evaluation strategy employed. Max-Score could directly be applied to such an implementation, but its effec-tiveness would be restricted to skip document identifiers returned by the clauses of the query root node because a score threshold is only available for the final similarity score. The im-plementations of intermediate operators cannot know if other query parts – not under their 112 CHAPTER 5. EFFICIENT QUERY PROCESSING control – are able to compensate for low scores in this operator. Each of these clauses would hence still have to compute scores for the complete OR-set of its subexpression. This moti-vates consideration of alternative implementation approaches to score documents, in which global knowledge about the influence of individual terms can be used to bypass pointers in the inverted lists of low-importance terms. Obviously, it is beneficial to not process the whole query tree for every document. Smith achieves this implicitly by use of a term-at-a-time evaluation strategy that might have more efficient disk access patterns, but adds significant space requirements for storage of inter-mediate results. However, a similar effect can be achieved within the document-at-a-time evaluation strategy, as shown in Algorithm 3. The key idea is to visit only the query nodes that are active for the current document, that is, nodes in which at least one clause has a non-zero score. No computation has to be performed for inactive nodes; they can be as-sumed to have an output score of 0, which is the result of the retrieval functions when all inputs are 0. Note that this does not hold for negations, which, for simplicity of description, are assumed to be absent. However, they can be dealt with, for instance, by considering all of their child nodes to be always active. The algorithm assumes that each query node Ni is assigned an index i that is smaller than for any of its child nodes, so that Ni.P < i for all query nodes, where Ni.P refers to the index of the parent node of Ni. It necessarily follows that N1 is the query root node, assuming indexing to start from 1. The indexes of the example in Figure 5.1 have been attained through a breadth-first search over the query tree and assignment of consecutive numbers. Furthermore, query nodes are differentiated into term nodes T , the leaves of the query tree, and Boolean operator nodes B, the inner nodes. Each node Ni has a retrieval score Ni.s assigned to it, that, for term nodes, is set to 0 or 1 depending on the information in its inverted list; and, for Boolean operator nodes, is computed on the basis of its retrieval function and the scores of its child nodes. Additionally, each operator node Bi has a set of clauses Bi.C; an operator type Bi.type ∈ {AND,OR}; and a p-value Bi.p. 113 5.2. A QUERY PROCESSING METHOD FOR P -NORM QUERIES Algorithm 3: CalcScore() – Query tree scoring via reverse breadth-first search of tree nodes with non-zero score. Input : T , a set of query tree terminals; and B, a set of internal tree nodes; collectively they form N , a set of tree nodes, numbered so that child nodes have larger indexes. The query root node is thus referred to as N1. Output: Retrieval score 1 S ← {Ti ∈ T | Ti.s > 0} . the set of non-zero score terms S is given, or determined 2 while S 6= {N1} do 3 j = arg maxj{Si ∈ S | Bj = Si.P} . determine largest parent node index j 4 A = {Si ∈ S | Si.P = Bj} . determine active clauses of Bj in S 5 Split A into the two sets As=1 and A0<s<1 6 if |A0<s<1| = 0 then . when operands are all binary 7 Bj .s← TableLookup(Bj , |As=1|) . lookup pre-computed score 8 else if Bj .type = OR then 9 Bj .s← { 1 |Bj .C| [ |As=1|+∑i (A0<s<1i .s)Bj .p]} 1Bj.p . compute score for Bj 10 else if Bj .type = AND then 11 ks=0 ← |Bj .C| − |A0<s<1| − |As=1| . determine number of clauses with s = 0 12 Bj .s← 1− { 1 |Bj .C| [ ks=0 + ∑ i (1−A0<s<1i .s)Bj .p ]} 1Bj.p . compute score for Bj 13 end 14 S ← S −A+ {Bj} . remove processed nodes from S, and add their parent 15 end 16 return N1.s In step 1, the algorithm first determines the set of terms S ⊆ T with a score contribution greater than 0. Note that in document-at-a-time evaluation, this set S is readily available during processing and does not require inspection of all query terms. For example, assume the query given in Figure 5.1 and a document that contains only the query terms S = {t4, t6} = {Humans/,Valium}. The algorithm would first compute the intermediate score for node N2, considering node N5 to have a score of 0. Then N1 would be reached and the overall score calculated, again assuming a score of 0 for inactive clauses not in S. Compared to a naı̈ve document-at-a-time implementation that might initialize all eight term nodes and calculate scores in each of the five operator nodes, this algorithm only initializes two term nodes and only two operator nodes. This example demonstrates that although CalcScore() involves additional (partial) sorting overhead, similar reductions in 114 CHAPTER 5. EFFICIENT QUERY PROCESSING 1 AND, p = 2 2 OR, p = 1 5 AND, p = 10 10 Muscle 11 Relaxant 6 Valium 3 OR, p = 2 7 Headache 8 AND, p = 100 12 Brain 13 Injury 9 TI:Trauma 4 Humans/ Id. 4 6 7 9 10 11 12 13 Term Humans/ Valium Headache TI:Trauma Muscle Relaxant Brain Injury ft 9,912,283 603 41,671 36,313 461,254 7,244 733,025 274,994 Figure 5.1: Example p-norm EBR query tree, containing operators with p-values and terms. Docu-ment frequencies ft are denoted for every term. the number of visited tree nodes can be attained as in the term-at-a-time method of Smith [1990], but without any intermediate storage. A number of optimizations can be employed to further reduce costs associated with each scoring. When all clauses have binary scores, the p-norm retrieval functions can be directly calculated on the basis of the number of "1" values amongst the constituent clauses. For every operator, all of these possibilities can be precomputed and stored in a ta-ble [i, 0 ≤ ks=1 ≤ |Bi.C|] 7→ s, which maps the operator index i and its number of clauses with a score of 1 (ks=1) to its retrieval score. The table is then consulted during scoring. Even if not all clauses are binary, the number of clauses with a score of 1 (|As=1|) or 0 (ks=0) can be factored out of the exponentiation in the formulas for disjunctions and conjunctions, respectively. The proposed scoring method is more efficient than the naı̈ve baseline approach and encapsulates the complexities involved in scoring a document with a complex query tree. Knowledge of the set of terms S that are present in a document is then all that is required as document-specific input to the method. This allows the free optimization of the inverted list processing strategy, because the query structure does not impose any order constraints. 115 5.2. A QUERY PROCESSING METHOD FOR P -NORM QUERIES 5.2.2 Scoring Fewer Documents A critical observation is that Max-Score can be applied to a broader class of retrieval func-tions. That is, sum-scoring functions are not the only ones that can benefit from this op-timization. The required property is monotonicity – given a set of terms S appearing in a document, no superset S′ ⊃ S of those terms can give rise to a lower retrieval score. The p-norm EBR model possesses this property, provided there are no negations in the query. In this section, first the monotonicity property is proven, and methods discussed that mitigate the restriction with regard to negations present in the query. Then several process-ing methods are suggested that are motivated by Max-Score and WAND. Proof of Monotonicity The following theorem considers term scores si ∈ [0, 1] and p-values in [1,∞). The special case of p =∞ can be ignored since it is best evaluated with a pure Boolean implementation. Theorem 5.1. The document similarity score of an arbitrary query tree in the p-norm ex-tended Boolean retrieval model is monotonically increasing with term scores, and with the presence of terms, respectively. The proof is by induction on query (sub-)trees: Basis The simplest query (sub-)tree consists of exactly one term. Then, the scoring func-tion becomes f(s) = s. Because its first derivative with respect to the term score is f ′(s) = 1 > 0, it follows trivially that single-term clauses produce strictly increasing scores. Induction Now, this result is generalized to arbitrary query trees. Every query node con-tains a set of terms T distributed over its set of clauses C and aggregates the clauses' indi-vidual scores fc∈C . Each clause c is a sub-tree that contains a subset of terms Tc ⊂ T with 116 CHAPTER 5. EFFICIENT QUERY PROCESSING a particular assignment of scores sj , where j ∈ Tc. All of the partial derivatives of the OR retrieval function are described by: ∂fOR(fc∈C(sj)) ∂si = ∂ [ 1 |C| ∑ c∈C fc(sj) p ]1/p ∂si = 1 p︸︷︷︸ >0 · [ 1 |C| ∑ c∈C fc(sj) p ]1/p−1 ︸ ︷︷ ︸ ≥0 · p|C|︸︷︷︸ >0 · fc(si)p−1︸ ︷︷ ︸ ≥0 · ∂fc(si) ∂si ≥ 0 ⇐⇒ ∂fc(si) ∂si ≥ 0 . A similar argument holds for the AND scoring function: ∂fAND(fc∈C(sj)) ∂si = ∂ { 1− [ 1 |C| ∑ c∈C(1− fc(sj))p ]1/p} ∂si = 1 p︸︷︷︸ >0 · [ 1 |C| ∑ c∈C (1− fc(sj))p ]1/p−1 ︸ ︷︷ ︸ ≥0 · p|C|︸︷︷︸ >0 · (1− fc(si))p−1︸ ︷︷ ︸ ≥0 · ∂fc(si) ∂si ≥ 0 ⇐⇒ ∂fc(si) ∂si ≥ 0 . Then, because fc(si) is in all cases one of the two scoring functions that inherit monotonic-ity from its clauses, and because a leaf in a query tree is always a term, the overall query tree score is monotonically increasing with term scores. Note that this proof does not hold if negations are allowed in the query tree. Fortunately, De Morgan's laws extend to the p-norm EBR model [Salton et al., 1983c]: ¬AND(a, b) ≡ OR(¬a,¬b) , ¬OR(a, b) ≡ AND(¬a,¬b) , and can be applied recursively to propagate negations towards the term nodes, at which point term scores can be inverted and the negations removed. While this approach mitigates the restriction, the use of negations is generally discouraged for systematic review search 117 5.2. A QUERY PROCESSING METHOD FOR P -NORM QUERIES because it can lead to reduced recall and bias due to unintended exclusion [Sampson et al., 2008a, 2009]. If general queries are to be executed, then the optimization is either applied only when no negations are present in the query, or a predictive model about the gains of the optimization is used to decide for or against employing the optimized implementation. Max-Score To take advantage of the Max-Score optimization for structured p-norm EBR queries, the query terms are again sorted by decreasing document frequency ft and indexes i = 1, . . . , |T | assigned. To attain the Max-Scores c∗i , term contributions cannot simply be cumulated as for ranked keyword queries. Instead, overall EBR scores Li have to be calculated for hypo-thetical documents, containing every incremental term subset t1 · · · ti, for i = 1, . . . , |T |. This can be done using the CalcScore() function shown in Algorithm 3 on page 114. Obvi-ously, this score is always 0 for the empty term set, and, for binary term weights, will be 1 for i = |T | because all terms are present, and because, by definition, each has a term weight of 1. The proof ensures that these scores are monotonically increasing and that there is no strict term subset S ⊂ {t1· · ·ti} that would lead to a retrieval score larger than Li. The rest of the algorithm remains unchanged. In document-at-a-time evaluation, the first k documents are scored to establish an entry threshold τ , being the kth largest score encountered so far. Once τ exceeds Li, documents that only contain a subset of the first i, most common terms cannot achieve an overall score above τ to make it into the top-k. It is hence enough to consider documents for scoring that appear in one or more of the inverted list of the terms ti+1 · · · t|T |, that is, documents in a reducing OR-set. Some of the postings in the excluded lists will still be examined, in order to fully score documents in the current OR-set, but the – hopefully numerous – documents that only occur in these lists will be completely skipped. In the example in Figure 5.1, the most common term (Humans/) is present in almost 10 million documents. Once enough documents are found with a score above documents that only contain Humans/ (overall retrieval score of 0.184), the number of candidate docu-ments that have to be scored decreases from more than 9,912,282 (the OR-set of all query 118 CHAPTER 5. EFFICIENT QUERY PROCESSING terms will be at least as large as the document frequency of the most common query term) to between 733,025 (the document frequency of the second most common term) and 1,555,104 (the sum of the document frequencies of the remaining seven query terms). Computing the Max-Scores Li is more expensive than computing the c∗i for ranked keyword queries because an entire EBR query has to be computed for every Li. However, the score bounds can be generated lazily in an on-demand manner because Li+1 is required only once the entry threshold τ exceeds Li. Moreover, only a few nodes are evaluated for small i when CalcScore() is used. 5.2.3 Short-Circuiting Evaluation As described so far, the adaption of Max-Score potentially reduces the number of documents that have to be considered. However, the term bounds Li cannot be used to short-circuit the evaluation of any of the remaining candidate documents, because the interaction between the term contributions is non-linear. As a result, all candidate documents have to be fully scored to find out if they exceed the threshold τ , no matter how many (and which) terms they contain. To add this beneficial capability, term-independent score bounds (TIB) are now introduced, to capture the maximum attainable score for any given number of terms. A pre-computed lookup table of score bounds Mr is consulted to determine if the presence of r query terms in the candidate document can lead to a score higher than the current threshold. Only when it can is it necessary to use the scoring method to determine the actual retrieval score. Computing Score Bounds Each entry Mr, with r = 0, . . . , |T |, of a TIB lookup table is defined by: Mr = max{CalcScore(S) | S ⊆ T and |S| = r} . 119 5.2. A QUERY PROCESSING METHOD FOR P -NORM QUERIES An obvious problem is that the number of term combinations that could occur in docu-ments is O( (|T | r ) ) for each value of r, and O(2|T |) in total, which is intractable if every combination needs to be examined. However, because the retrieval functions in the p-norm EBR model only depend on the scores associated with clauses, the computation can be broken down into smaller subprob-lems, solved for each query sub-tree individually, and then aggregated. The simplest query sub-tree consists of just one term, for which the solution is trivial: Mr = 0 for r = 0; and Mr = 1 for r = 1, assuming binary term weights. For a particular operator node with the clauses C, let nc denote the total number of terms that appear anywhere in the query sub-tree of clause c ∈ C. The table for this operator node would then consist of maximum score bounds Mr for r = 0, . . . , ∑ c∈C nc, of which each entry can be computed on the basis of the retrieval function for the operator and the maxi-mum score bounds in the tables of the clauses. In particular, all possibilities to decompose r into a sum over the clauses r = ∑ c∈C rc have to be considered. Figure 5.2 describes the overall computation for an example query and Algorithm 4 gives a recursive description on how all possibilities to decompose a given r can be iterated in an operator node to find the corresponding TIB score. The cost to generate the whole table for a particular query node is generallyO(∏c∈C nc). However, in practice, the average cost is much smaller when some additional optimizations are applied. Optimizations As was also the case with the adaptation of Max-Score, the score bound Mr+1 only has to be computed once the threshold τ surpasses Mr. However, Mr grows quickly so that only the score bounds for small r will ever have to be calculated to screen out the (many) documents that have only a few unimportant terms in common with the query. To generate Mr for a node, the tables of all of its clauses only have to be filled up to r, hence, no nodes in the query tree require more than r table entries to be computed. 120 CHAPTER 5. EFFICIENT QUERY PROCESSING r=1 r=2 r=3 a r 0 1 Mr 0 1 b r 0 1 Mr 0 1 ORp=1 r 0 1 2 Mr 0 0.5 1 c r 0 1 Mr 0 1 ANDp=2 r : 0 1 2 r Mr Mr: 0 0.5 1 0 0 0 0.21 0.29 1 1 0.29 0.65 1 ORp=1 c Figure 5.2: TIB score bound calculation for the query "(a OR b) AND c", with binary term weights and two different p-values. For the query root node table, all computations performed are unfolded and the final TIB score bounds marked in bold. Second, for nodes containing only terms with binary term weights, the table can be directly computed from the number of terms present. Third, TIB computations can be stopped once a predefined time limit is reached; at this point, the gains of additional score bounds can simply be deemed to not outweigh the effort required to generate them. Fourth, it can be shown (proof analogous to Theorem 5.1) that the OR scoring function is convex, and the AND scoring function concave, respectively. For query sub-trees that only contain one kind of these operators, these properties can be used to further reduce the computational complexity. Finally, instead of using the p-norm retrieval functions directly for the evaluation of a term combination, the computation can be transformed into a different space by applying the inner part of the functions (exponentiation) to the Mr values in the clause tables. Due to monotonicity of the exponential function in the outer part of the retrieval functions, the search then translates into a maximization of score sums for disjunctions, and minimization of score sums for conjunctions, respectively. Once a solution is found, actual score bounds can be regenerated by application of the remaining part of the retrieval functions. In effect, this optimization pre-computes the exponentiations that are part of the p-norm retrieval functions, and hence reduces the computational cost for each combination. For example, to compute a score bound Mr for the conjunction in the query root node in Figure 5.2, 121 5.2. A QUERY PROCESSING METHOD FOR P -NORM QUERIES Algorithm 4: ComputeTIB(r, i) – Computes the TIB score table entry corresponding to the term count r for one operator node on the basis of the pre-computed table entries of its clauses C. Input : r, the number of terms present in a document; i, a clause index with 1 ≤ i ≤ |C|, which will be 1 at the initial call to the function; ni, the overall number of terms in the query subtree of clause i; n̂i, the cumulative number of terms in the subtrees of clauses i, . . . , |C|; M , the TIB scores of all clauses, indexed by a clause index i and the number of terms ri present in that query subtree; and R, the retrieval function for this operator node. Data : Variables with global scope: s∗, the maximum score found so far, initialized to 0; and ri, the number of terms currently assigned to be present in clause i. Output: TIB score, corresponding to r 1 if i < |C| then . recursion step: as long as last clause is not reached 2 for ri ← max(r − n̂i+1, 0) to min(r, ni) do . assign all possible ri to current clause i 3 ComputeTIB(r − ri, i+ 1) . decompose remaining terms over remaining clauses 4 end 5 else . base case: term counts ri are assigned to all clauses, except the last one 6 ri ← r . r|C| is set to the remaining number of terms r 7 s← R(M [1, r1], . . . ,M [|C|, r|C|]) . compute score for current decomposition 8 s∗ ← max(s∗, s) . update maximum score 9 end 10 return s∗ it is sufficient to find the r∗c that add up to r = ∑ c∈C rc and minimize ∑ c∈CM ′ rc , with M ′rc = (1−Mrc)p being once precomputed. To recover the score boundMr, the remaining part of the AND retrieval function Mr = 1− ( 1 |C| ∑ c∈CM ′ r∗c )1/p is then applied. Applications There are at least two distinct applications for the proposed TIB score bounds. The initial motivation for developing them was that they would be used to complement the Max-Score bounds to short-circuit the evaluation of candidate documents. The adaptation of Max-Score is only able to reduce the set of documents that have to be considered, but for each of the remaining candidates every inverted list is inspected to determine the set of terms present in that document. The use of TIB score bounds might be able to eliminate some of these documents, even before all inverted lists are consulted, simply based on the number of 122 CHAPTER 5. EFFICIENT QUERY PROCESSING terms it could contain. For instance, if the inverted lists are examined in order of increasing document frequency, it might be known that a candidate document contains one term and possibly the most common query term which is yet to be determined by consulting the corresponding inverted list. If the current entry threshold τ translates into a minimum of three terms that have to be present in a document to make it into the top-k, the candidate document can be discarded. In effect, the document does not have to be scored, meaning that fewer postings are inspected and larger skips are made on the longer inverted lists. Second, TIB score bounds can be used by themselves. While Max-Score imposes an order on the query terms, term-independent bounds make it possible for the excluded terms to be selected in any desired manner. In particular, if the query terms are (partially) sorted by the next document identifier in their inverted lists and the current threshold surpasses Mr, then the first r lists can be skipped to the next document identifier in the r + 1'th list. Only if the first r + 1 inverted lists contain the same document identifier has a can-didate document been found, and a call to CalcScore() required. This approach is similar to the processing in the WAND algorithm [Broder et al., 2003], but here score bounds are not dynamically calculated due to the demanding score computation. Because the first r terms might continuously change with every skip operation, this (partial) sorting approach might be able to exploit the document identifier distribution over the inverted lists. On the other hand, additional (partial) sorting overhead is incurred, and TIB bounds are less tight than Max-Scores and the bounds that WAND uses in conjunction with sum-score retrieval functions. Extensions The TIB scores are at least as large as those attained with Max-Score and likely to grow much faster, because they assume a worst-case assignment of terms to be present in doc-uments. Particular terms might be responsible for large score bound increases, but not be present in many candidate documents. In such cases, it might be advantageous to compute term-dependent bounds (TDB) that apply only in the absence of a particular term. To choose a term to condition on, a trade-off arises between choosing a term with a small document 123 5.3. EXPERIMENTS Max-Score Term-Independent Bounds (TIB) Term-Dependent Bounds (TDB) r Term Set Lr Term Set Mr Term Set M6r 0 ∅ 0 ∅ 0 ∅ 0 1 {4} 0.184 {4} 0.184 {4} 0.184 2 {4,12} 0.186 {4,7∨9} 0.374 {4,7∨9} 0.374 3 {4,12,10} 0.199 {4,6,7∨9} 0.623 {4,7,9} 0.414 4 {4,12,10,13} 0.391 {4,6,7,9} 0.693 {4,7∨9,10,11} 0.623 5 {4,12,10,13,7} 0.433 {4,6,7∨9,10,11} 0.756 {4,7,9,10,11} 0.693 6 {4,12,10,13,7,9} 0.442 {4,6,7,9,10,11} 0.895 {4,7,9,10,11,12∨13} 0.693 7 {4,12,10,13,7,9,11} 0.712 {4,6,7,9,10,11,12∨13} 0.895 T − {6} 0.712 8 T 1 T 1 – – Table 5.1: Maximally scoring term sets and their scores Lr and Mr for different numbers of terms r present in a document for Max-Score, term-independent bounds (TIB), and term-dependent bounds (TDB) conditioned on the exclusion of Term 6 (Valium), applied to the example query given in Figure 5.1. frequency, which is desired to increase the applicability of the bounds, and the influence of that term on retrieval scores. The latter is desired to be large, so that tighter bounds can be attained in absence of the term, and correlated with the level in the query tree, in which it appears, but the influence of a term is also dependent on properties of the parent operators (p-values, types and numbers of clauses) and appearance of other terms. Table 5.1 continues the previous example and shows the score bounds Lr; Mr; and the conditional Mr, corresponding to the term Valium. 5.3 Experiments Two experiments are undertaken to validate the proposed methods. Of particular interest is whether resource costs and query processing times can be significantly reduced. As a consequence of the effectiveness results in Chapter 4 and the repeatability requirement for systematic review search, we focus on binary term weights throughout the experiments. In this section, the experimental setup is described, then the effects of the methods are compared in simulation experiments. Finally, timings are taken to confirm that the effects translate into real time savings. 124 CHAPTER 5. EFFICIENT QUERY PROCESSING 5.3.1 Experimental Setup For the experiments of this thesis, a number of prototypes have been implemented in Java on top of Lucene version 2.4, that directly access the inverted lists. This Lucene version, by default, has a minimal skip interval of 16 and at most 10 skip levels. The baseline system is a naı̈ve implementation that recursively processes every query tree node for every document in the query's OR-set and collects the top-k documents in a heap data structure. To examine the effect of CalcScore() on query processing times in isolation, another baseline identified in the timing experiments only replaces the document scoring method. Note that both baselines examine every posting in the full OR-set of all query terms. A third system implements the adaptation of Max-Score. The fourth sys-tem dynamically orders the terms by their next document identifier and uses TIB scores to skip the first inverted lists. The final implementation extends the adaptation of Max-Score with TIB scores in order to short-circuit the evaluation of candidate documents. Finally, a Boolean implementation serves as another point of comparison. All score bounds have been calculated in an on-demand manner. Moreover, an overall time-limit of 50 ms was applied for TIB computation. In the test data, this limit was ex-ceeded on only four queries. Without a time-limit even the computation for those queries would always have taken less than 300 ms, validating the feasibility of the approach. Where possible, all implementations in this work employ the same data structures and program-ming style to reduce the possibility of confounding influences arising from overheads for different constructs and particularities of the programming language. Except for the Boolean system, k = 100 documents are retrieved, as an estimate of the number of results that might be retrieved in each iteration during the preparation of a systematic review query to assess query quality. Throughout the experiments, Medline is used as the document collection, against which three query sets are executed. AHRQ COHEN is used as a sample of complex systematic review search queries. To confirm the benefit of the methods on a broader range of queries, differing in complexity, two further samples have been prepared: PUBMED STRUCTURED and PUBMED SIMPLE. Each of these query sets and preprocessing steps are described 125 5.3. EXPERIMENTS in detail in Chapter 3. To account for the fact that they have not been written to be exe-cuted under an EBR model, all queries were normalized as described and found effective in Chapter 4. Where necessary, De Morgan's laws have been applied to propagate negations to the terms. In the absence of a better deterministic strategy, all operators were assigned the same p-value. However, two different p-values are examined that represent interesting measurement points: p = 10 is a relatively large exponent and likely to entail computational costs due to exponentiation; and for p = 1, no exponentiation has to be performed. Moreover, the implementation for the latter returns the same rankings for simple queries that contain one operator only, such as those of PUBMED SIMPLE. This is because, for binary term weights, the p-norm scoring functions reduce to monotone transformations applied to the number of present query terms, resulting in scores simply scaled by p. The relative order of scored documents is however unaffected. 5.3.2 Simulations For a first simulation experiment, all implementations were instrumented to measure two key indicators, both affected by the methods used and believed to be correlated with retrieval times [Croft et al., 2009]: the number of candidate documents that have to be (fully) scored, but are below the entry threshold (needless scorings); and the number of postings read from the inverted lists. The former will be 0 for a perfect heuristic. The advantage of simulation is that the measurements are not dependent on the hardware employed. These runs were also used to confirm that, apart from localized rank-swaps due to rounding errors, all systems returned the same retrieval output. Several observations can be made from the results in Table 5.2. The adaptation of Max-Score always leads to significant reductions in both measurements. By itself, the TIB system can attain even lower counts when p = 1, but is not consistent over all queries. Moreover, the method is not competitive with Max-Score for higher p-values because the TIB scores rise too quickly. Conditional score bounds can be expected to improve on this, given an appropriate choice of terms to condition on. Throughout, best performance is 126 CHAPTER 5. EFFICIENT QUERY PROCESSING p = 1 p = 10 Query set and system Needless Postings Needless Postings scorings processed scorings processed P U B M E D S IM P L E Boolean – 25,090† – – EBR, Baseline 1,954,153† 2,107,220† – – EBR, Max-Score only 319,919 462,621 – – EBR, TIB only 109,096 321,761 – – EBR, Max-Score + TIB 11,598† 444,109† – – P U B M E D S T R U C T U R E D Boolean – 89,899† – – EBR, Baseline 5,544,283† 15,391,506† 5,545,521† 15,391,506† EBR, Max-Score only 1,078,782 4,660,586 1,102,909 4,707,938 EBR, TIB only 803,041 4,147,582† 1,315,693 7,009,658† EBR, Max-Score + TIB 387,820† 3,708,108† 762,653† 4,455,036† A H R Q C O H E N Boolean – 913,963† – – EBR, Baseline 14,316,891† 105,541,788† 14,335,493† 105,541,788† EBR, Max-Score only 2,958,833 65,316,498 2,589,252 61,355,097 EBR, TIB only 2,253,448† 64,204,793 5,109,325† 89,370,092† EBR, Max-Score + TIB 2,057,016† 61,388,434† 2,487,645† 61,178,140† Table 5.2: Average per-query counts of the number of needless document scorings and of the number of postings read from the inverted lists, for each of the query sets and two choices of p. Baseline refers to either Tree Iteration, or CalcScore(). Dagger superscripts indicate significance at the 0.01 alpha level using a paired, two-sided Wilcoxon signed-rank test compared to the "Max-Score only" results. achieved with the combination of Max-Score, to screen documents from consideration, and TIB, to short-circuit the evaluation of remaining candidate documents. In these measures, it is not surprising that the combination of both methods is as good as Max-Score alone, but the magnitude of the reductions gives hope that the overhead to compute the bounds is outweighed by the gains. Reflecting the same issue as with TIB alone, its effectiveness is reduced for larger p. Except for systems using the TIB scores, higher p-values did not generally have a large influence on the performance of the methods. Overall, the number of unnecessarily scored documents is correlated with the number of postings processed, but, through the relative changes for these two measurements, it becomes evident that many of the screened candidate documents contain only a small subset of the query terms. 127 5.3. EXPERIMENTS A general trend is the increasing complexity of the query sets, apparent in the increas-ing number of unnecessary scorings and the differing magnitudes of postings that have to be processed. For AHRQ COHEN, almost all documents in the collection have to be con-sidered in the baseline method. The number of postings that are processed by the pure Boolean systems are magni-tudes lower, but the size of their result sets are of indeterminate size: many queries in the PUBMED query sets even return no results at all (see Table 3.2). While the actual number of queries with no results might be smaller when executed on PubMed due to the employed query expansion mechanisms, users will be likely to submit more Boolean queries, so that individual query costs can not be assessed alone. 5.3.3 Timings A second experiment was used to test whether the efficiency improvements seen in the simulation experiments indeed translate into faster query processing. Moreover, the effect of varying the number of retrieved documents k is of interest. All instrumentation was removed from the implementations and the time measured to process each query until the result set is attained (excluding query pre-processing and docu-ment lookups). The methodology described in Section 3.3 is followed, resulting in separate worst-case (Cold-Start) and best-case (Cached) timings. The timings of multiple runs for each query set were repeatable with small standard deviation; and their mean is presented in Table 5.3 for each of the systems and the two p-value choices. Although the CalcScore() scoring method requires partial sorting of tree node identi-fiers, it is consistently faster than the obvious baseline of recursively processing the whole query tree. For the systems, similar trends become apparent, supporting the validity of the surrogate measures in the simulations. In particular, processing times are always significantly reduced in the Max-Score system. The pure TIB system shows similar reductions, but they are always smaller than for the simpler adapted Max-Score method. This is because the (partial) sorting overhead incurred by the method, because it is even the case when the method has to 128 CHAPTER 5. EFFICIENT QUERY PROCESSING Time [s] Query set and system p = 1 p = 10 Cold-Start Cached Cold-Start Cached P U B M E D S IM P L E Boolean 0.07† 0.01† – – EBR, Tree Iteration baseline 0.52† 0.46† – – EBR, CalcScore() baseline 0.27† 0.20† – – EBR, Max-Score only 0.17 0.05 – – EBR, TIB only 0.18† 0.06 – – EBR, Max-Score + TIB 0.26† 0.11† – – P U B M E D S T R U C T U R E D Boolean 0.47† 0.03† – – EBR, Tree Iteration baseline 5.30† 4.50† 8.80† 7.78† EBR, CalcScore() baseline 2.77† 2.15† 5.50† 4.60† EBR, Max-Score only 1.40 0.62 2.12 1.27 EBR, TIB only 1.44† 0.68† 2.67† 1.85† EBR, Max-Score + TIB 1.23 0.48 1.97 1.14 A H R Q C O H E N Boolean 10.38† 0.38† – – EBR, Tree Iteration baseline 40.96† 30.03† 65.87† 53.00† EBR, CalcScore() baseline 33.61† 23.86† 48.98† 37.81† EBR, Max-Score only 22.25 10.20 25.12 12.18 EBR, TIB only 25.59† 12.78† 31.77† 17.67† EBR, Max-Score + TIB 20.19† 8.84† 22.10† 10.20† Table 5.3: Average per-query execution times in seconds for each of the query sets and two choices of p. Dagger superscripts indicate significance at the 0.01 alpha level using a paired, two-sided Wilcoxon signed-rank test compared to the "Max-Score only" results. score less documents. The method might however be more competitive for queries with non-binary term weights, as presented by Broder et al. [2003] for ranked keyword queries. In contrast, if the TIB scores are used in addition to Max-Score, the shortest query processing times are attained for the queries of AHRQ COHEN. The advantage is less significant on the less complex PUBMED STRUCTURED queries; and for the simple PUBMED SIMPLE, the bound computations appear to take more time than is saved by the method. Non-unit p-values lead to non-trivial processing time increases due to exponentiation. While this is particularly evident in the baselines of the complex AHRQ COHEN queries, it is less so for the proposed improved methods due to the reductions in the number of documents that have to be scored. 129 5.4. SUMMARY The gap between the disk-dominated and cached timings is more or less constant; and neatly correlates with the number of query terms typical for each query set, reflecting the number of necessary random disk accesses due to term lookups. Computation for the Boolean retrieval model is magnitudes faster, but does not incur any scoring and involves substantially fewer postings. Moreover, its retrieval output is less useful and does not give any guarantee about the size of the result set. The differences in the timings become much smaller when disk accesses are involved. Despite that the absolute values might appear large for a user to wait for a retrieval result, query processing is a read-only task and can hence easily be parallelized under use of more hardware. For instance, the collection can be split and queries processed on multiple machines at the same time; or, multiple processing cores are employed to process complex queries. Finally, Figure 5.3 examines the influence of the k parameter. The more documents are to be retrieved, the less effective the top-k methods are. But even when 10,000 docu-ments are required, a number well above the capacity of a typical systematic review team, execution times are significantly below the baselines. 5.4 Summary In this chapter, efficient query processing methods have been proposed for the p-norm ap-proach to extended Boolean retrieval. The suggested document scoring method reduces the number of query tree nodes that have to be examined, and the resulting time savings out-weigh additional costs due to partial sorting overhead. Also, the number of documents and postings that have to be examined can be reduced if retrieval of a ranking prefix is sufficient. While the adaption of the Max-Score algorithm is most effective, further gains are possible using additional score bounds. With the best method, speed ups of a factor of 2 to 9 have been achieved. Although the methods have been described in the context of this model, they are appli-cable to a broader range of retrieval models with structured queries and monotonic retrieval 130 CHAPTER 5. EFFICIENT QUERY PROCESSING 1 10 100 1,000 10,000 0 2 0 4 0 6 0 8 0 Top−k T im e [ s ] Baseline, CalcScore() EBR, Max−Score Boolean Cold−Start Cached (a) AHRQ COHEN 1 10 100 1,000 10,000 0 2 4 6 8 Top−k T im e [ s ] Baseline, CalcScore() EBR, Max−Score Boolean Cold−Start Cached (b) PUBMED STRUCTURED Figure 5.3: Average query execution times for retrieval of top-k documents on the two structured query sets for p = 10 and the adapted Max-Score variant, compared with two baselines. functions. For search service providers, these results mean that the p-norm EBR model becomes a more competitive and viable alternative; or, that users have to pay less for the provided service. In the next chapter, methods are explored to improve the effectiveness of existing queries to aid the retrieval process during the update of systematic reviews. 131 Chapter 6 p-Norm Query Optimization Clinical practice guidelines are developed as aides to the decision-making of clinicians and medical practitioners. Typically, they are informed by systematic reviews, which are in-tended to aggregate all available evidence in regard to one particular medical question of interest. Because of new published research results, systematic reviews have to be updated to accommodate newly discovered effects, insights or treatments. Otherwise, the informa-tion provided in reviews will become out of date and hence misleading. Each systematic review is a significant investment, incurring the expenditure of many months of expert labor [McGowan and Sampson, 2005]. As of 2010, the Cochrane library of systematic reviews alone contains more than 4,130 reviews that need regular updates. If these are not undertaken, they run the risk of being withdrawn, rendering the effort that went into them futile. Other systematic reviews are published in specialized journals, amounting to an overall arrival rate of around 2,500 new systematic reviews per year [Moher et al., 2007a]. By definition, a systematic review becomes outdated once a publication appears that is not included in the review, but would have been if the review was redone. The Cochrane Collaboration requires that each review is updated every two years, or the authors have to defend a lack of update [Higgins and Green, 2009]. If the reason for not updating is lack of time or resources, reviews are withdrawn. Recent research results indicate that 23% of a 133 6.1. RELATED WORK sample of Cochrane reviews became out of date within two years after publication, and that 7% were already out of date by the time the review was published [Shojania et al., 2007]. Another study shows that an update policy based on time intervals might be too simplistic, because updates of reviews in slow-moving fields might be wasteful whereas there are other reviews that need an even higher update frequency than bi-yearly [French et al., 2005]. The process that is currently followed for an update is similar to that used for generating the initial review. The same search process is applied, however, little has to be done if no further evidence is identified. Otherwise, the costs for an update can become as high as for the initial review [Shekelle et al., 2001, Eccles et al., 2008]. Better retrieval techniques have the potential to find additional relevant documents, to provide higher coverage on which to base decision-making; or to reduce the labor and costs, which could lead to more up-to-date reviews (better decisions) or reductions in the number of withdrawn reviews. In this chapter, optimization of weighted EBR queries is proposed as a means for im-proved information retrieval to facilitate the task of updating systematic reviews. First, related work is discussed and the merits of different approaches assessed. Then, the use of weighted EBR queries is motivated and a methodology described by way of which weights can be learnt that improve retrieval on current, but possibly also future document sets. The experiments show that for a significant fraction of reviews, the rankings for optimized queries are better than for unweighted queries on typical IR evaluation measures. 6.1 Related Work A number of approaches are applicable in order to improve current information retrieval practice for the update of systematic reviews. This section reviews their merits and refers to relevant literature. Boolean Retrieval Baseline In the domain of interest, Boolean queries are currently not only used for the final docu-ment retrieval in review creation, but also as a filter when reviews are to be updated. This 134 CHAPTER 6. P -NORM QUERY OPTIMIZATION preserves scrutability and repeatability, and also takes advantage of the effort that went into query generation. However, although designed to be highly sensitive, these queries often screen out relevant documents or return a large number of documents that would have to be manually inspected – exactly as was the case at the time of the initial review. These problems have been described in detail in Chapter 4. While a reasonable approach to improve the effectiveness of Boolean queries is their reformulation on the basis of feedback about the relevance of initially retrieved documents, the inherent problems of strict Boolean query evaluation remain. Salton et al. [1984] com-pare two automatic methods to generate new query terms and conjunctions on the basis of statistics derived from judged documents. In contrast, previous work on systematic review updates focuses on questions related to the mere application of the Boolean retrieval model, such as when and how to perform an update. Moher et al. [2007b] give a comprehensive overview of relevant literature. Filtering & Recommendation In a general IR setting, the TREC Filtering Track [Robertson and Soboroff, 2002] studied tasks similar to the one at hand. Here, a stream of incoming documents has to be matched to a set of long term (standing) information needs, which is in contrast to the once-off task of ad hoc retrieval. Different modes of operation have been investigated in three subtasks: adaptive filtering, batch filtering and routing. Adaptive filtering refers to an interactive process in which a user judges an output document of the system for relevance, allowing the system to adapt its profile of the information need and present further documents to the user. In contrast, batch filtering and routing periodically evaluate sets of new documents and either produce a subset of potentially relevant documents, or a ranking, respectively. While the approaches differ, the result for all of them is a set or ranked list of the new documents that are recommended to the user. Most of the TREC participants chose to employ for the task machine learning algorithms [Robertson and Soboroff, 2002], such as Support Vector Machines [Vapnik, 1995]. These classifiers are usually trained on features extracted from relevant and irrelevant documents, 135 6.1. RELATED WORK such as meta-data and the terms used in the text. Classifiers with continuous, relevance-correlated output scores can directly produce rankings or make binary relevance decisions based on a threshold. Such a threshold can also be estimated based on training data. Note that, because information needs in the TREC environment are described by means of natural language synopses and summary titles instead of queries, many participants did not exploit information contained in queries or information need descriptions. If queries are generated by experts, they are likely to contain expert knowledge about the properties and features of relevant documents, which classifiers otherwise have to estimate based on statistical differ-ences in the distribution of features over relevant and irrelevant documents. Some studies have investigated document classification using term and meta-data fea-tures, for example Cohen et al. [2006], Martinez et al. [2008] and Wallace et al. [2010b]. However, most of these studies considered the result set of the Boolean queries only, trying to screen out further irrelevant documents without losing too many relevant ones. These approaches do not address the low recall problem of the Boolean queries used in the first place. Further, for many systematic reviews only a few documents are considered relevant. This makes it particularly hard for a classifier to identify features that give comparable per-formance on training and test documents. Finally, the classification functions are complex to describe, rendering them inscrutable and inapplicable for systematic reviewing practice. Relevance Feedback Relevance feedback is a general IR method that seeks to use the knowledge about the rel-evance of result documents to improve retrieval results [Baeza-Yates and Ribeiro-Neto, 1999]. Feedback can be provided by users either explicitly or implicitly, or it can blindly be assumed that the top ranked documents of an initial query are relevant. In contrast to classification approaches, the query is adapted, not the similarity function used to generate retrieval results. Changes to the query can be either in the form of adapting the impor-tance (weight) of query terms, or adding and removing query terms. In the vector-space model, where bag-of-words queries are represented as vectors in the same high-dimensional space as documents, both can be done simultaneously. The well-known Rocchio algorithm 136 CHAPTER 6. P -NORM QUERY OPTIMIZATION [Rocchio, 1971] moves the query vector in the direction of relevant documents and away from irrelevant documents so that query term weights are improved and additional terms possibly receive non-zero weights. Several techniques have been proposed to generate, or to improve Boolean and extended Boolean queries. For instance, Brown and French [1997] generate Boolean queries based on relevance judgements using a decision tree induction algorithm. Choi et al. [2006] map extended Boolean queries to neural networks and train them using relevance judgements, which motivates the chosen approach in this chapter, but differs in its execution: the EBR scoring functions are discarded and replaced by typical neural net activation functions to ensure convergence when derivatives are directly used to update weights. By doing so, scrutability and negation is lost in the described mapping process. Moreover, their evalua-tion is not conclusive, because improvements are measured on documents that are also used for training. Using feedback in order to add terms to a query leads to another form of query expan-sion, which is discussed next. Query Expansion A common theme in IR is the word-mismatch problem, which occurs when a query contains different terms to the ones used in relevant documents. To counteract this problem, queries can be expanded with terms that are related to the terms in the original query [Fox, 1988]. Synonyms, morphological alternatives, and spelling variations are examples for expansion terms [Ruthven, 2003]. Query expansion is primarily used to increase recall, but can be also beneficial for precision. However, care has to be taken not to apply query expansion blindly if consistent improvements are required [Billerbeck and Zobel, 2004]. To overcome the word-mismatch problem, expert searchers that know the document collections and the used nomenclatures are typically consulted in the process of query gen-eration for systematic reviewing. A key reason for the size of the queries is the manual expansion of query terms to related concepts. Assuming that these expansions are effective, adjusting the importance of query terms remains an opportunity to improve queries. 137 6.1. RELATED WORK Learning to Rank The aim of "Learning to Rank" approaches is to generate better document rankings for queries and a given document collection on the basis of knowledge about (other) queries and their relevant documents, and possibly the optimal relative ordering between them. Differ-ent input features and similarity metric primitives are aggregated into an overall similarity scoring function that is intended to perform better than the primitives alone. Aggregation is typically done using a class of parameterized functions, such as a weighted sum, for which a best assignment of parameters is then searched. Challenges are the complexity of the resulting similarity function, leading to a lack of interpretability and requiring effective countermeasures to ensure that its performance generalizes to new queries. The goal of learning to rank is however different to the task at hand in at least three regards: • Retrieval performance is to be increased for queries, for which no information about the relevance of documents is available. This is not the case for reviews that are to be updated; • Because current widely used evaluation measures such as MAP average over the per-formance of individual queries, a performance loss on some training queries is ac-ceptable if it leads to an increase in overall performance; and • Scrutability and repeatability is usually not considered a requirement. Consequently, similarity functions generated by Learning to Rank approaches might be applicable if features are carefully chosen and their complexity restricted so that they can be practically communicated, but are likely to be better applied in the retrieval runs for new, possibly related reviews, as the work of Cohen et al. [2009] indicates. Extended Boolean Retrieval In the setting of a filtering task, extended Boolean retrieval has the same advantages over pure Boolean retrieval as in a ranked retrieval setting, discussed previously in Chapter 4. 138 CHAPTER 6. P -NORM QUERY OPTIMIZATION Whereas pure Boolean retrieval has an unpredictable result set size and possibly screens out relevant documents, extended Boolean retrieval allows the user to extend the result sets of a pure Boolean run to retrieve more documents; or to choose a more manageable subset of documents that still includes most of the high-scoring documents. In addition, the availabil-ity of non-binary similarity scores allows generation of a ranking and also derivation of a threshold based on the scores of relevant documents seen so far. By means of this thresh-old, the arrival rate of relevant documents (Precision) can be traded off with the fraction of relevant documents returned (Recall). The use of weights for terms has been the basis for successful ranked retrieval systems for a long time [Witten et al., 1999]. The weight formulations, however, are the same for all queries and either general-purpose similarity formulations based on TF·IDF, such as BM25 [Robertson et al., 1995], or trained on a particular collection and anticipated queries by means of learning to rank approaches. Independent of this kind of term weighting, several retrieval models, such as the p-norm EBR model examined in this thesis, support queries with weights associated with the query terms, allowing the user to specify the rel-ative importance of terms for the particular information need at hand. Although Salton et al. [1983a,b] already suggested automatic relevance feedback methods for the extended Boolean retrieval model, their methods assign term weights irrespective of the used query and hence appear suboptimal. 6.2 A Weighting Method for p-Norm Queries Rather than simply re-executing the queries that have been used to seed the set of documents evaluated for the initial review, updating a review gives rise to the possibility of learning from the past, and improving the query's performance based on the properties of relevant and irrelevant documents discovered during the initial review process. Adapting the impor-tance of terms and concepts in a complex query is one way in which query performance might be influenced. 139 6.2. A WEIGHTING METHOD FOR P -NORM QUERIES The methodology proposed in this chapter is distinct to previous work in the following aspects: • key properties of Boolean retrieval are retained, but refined rankings produced; • by refinement of the query, expert knowledge contained in the complex, structured queries is exploited; and • an efficient training and evaluation method is proposed that scales to current collec-tion sizes. The p-norm retrieval model – such as many other retrieval models – has been extended to support query weights [Salton et al., 1983c]. Rather than simply assigning weights to all query terms, the structured nature of extended Boolean queries allows assignment of weights to high-level concepts, as represented by the subtrees in the query. In this section, the weighted-query extension proposed by Salton et al. [1983c] is con-sidered. For each operator node of a query, a weight wi is associated with the score si of each of its clauses i ∈ {1, . . . ,m}. The output o of each particular node depends on its assigned operator, but can be compactly defined as o =  fOR(w, s, p) = [∑m j=1w p j s p j∑m j=1w p j ]1/p fAND(w, s, p) = 1− [∑m j=1w p j (1− sj)p∑m j=1w p j ]1/p = 1− fOR(w, 1− s, p) fNOT(s) = 1− s . (6.1) This formulation is a generalization of the standard EBR scoring functions, which can be recovered by setting all wi to the same value, say for instance 1.0. To ensure that the range of the output values stays consistent with the input scores, weights are normalized to sum up to 1. This normalization also makes the weights independent of their absolute value. Figure 6.1 gives an example of a weighted EBR query. Assuming binary term scores, the conjunction node 5 returns the same output score for documents that contain either Muscle or Relaxant. By way of increasing the weight for one term relative to another 140 CHAPTER 6. P -NORM QUERY OPTIMIZATION 1 AND, p = 2 0 .92 OR, p = 1 1 .15 AND, p = 10 1 .010 Muscle 1 .011 Relaxant 0 .96 Valium 0 .93 OR, p = 2 0 .97 Headache 1 .18 AND, p = 100 1 .0512 Brain 0 .9513 Injury 0 .79 TI:Trauma 1 .54 Humans/ Figure 6.1: Weighted example p-norm EBR query tree. Note that the weights associated with the clauses of a node do not have to add to one due to the weight normalization performed within the p-norm retrieval functions, presented in Equation 6.1. (as, for instance, for node 12), documents containing only the higher weighted term will receive a higher score than documents containing only the other term, and as a result these documents will be ranked higher. Further, when more than two clauses are present (node 3) and the weights are chosen to be close to each other, the presence of multiple terms (Headache and TI:Trauma) are possibly able to compensate for a low score in node 8, al-though scoring high in this clause is more important than for any other individual clause. Note that higher p-values amplify the weights. Hence, although the weight sum for nodes 2 and 3 is larger than the weight of node 4 (1.8 > 1.5), a document that contains the meta-data tag Humans/ (having a score of at least 0.35) will always be ranked higher than any document not containing Humans/, even if such a document contains all other terms (having a score of at most 0.24). Such a weight assignment is advantageous if relevant documents can be assumed to contain this meta-data tag. Salton et al. [1983c] suggested that query weights can be set by the user. In practice, however, users have difficulty coming up with appropriate weights. In this chapter we investigate methods to automatically learn weights on the basis of the properties of relevant and irrelevant documents. Note that the weighting of binary term weights is equivalent to the assignment of non-binary term weights, but does not allow for the weighting of concepts. If all relevant documents for a query are known, which is unlikely but nevertheless is the goal of systematic reviews, all other documents in a collection can be classified as irrelevant. 141 6.2. A WEIGHTING METHOD FOR P -NORM QUERIES Then, the document collection can be seen as a training dataset with each document being a training instance with either the class relevant (1) or irrelevant (0). Finally, despite the generalization to use non-unit clause weights, the efficient imple-mentations, presented in Chapter 5, can easily be adapted to be applicable to weighted p-norm and similar EBR models. 6.2.1 Challenges Several challenges have to be addressed in order to obtain query weights that are better than the obvious baseline of executing queries unweighted, that is, with equal weight for each query clause. First, data scarcity can be a problem if too little training data is available relative to the number of parameters that are to be learnt. For instance, with only a few judged-relevant documents available, it is hard to determine statistically significant differences for the pro-portions of term occurrences in the two classes. Hence, discriminative terms might not be identified, or too much weight assigned to uninformative, noisy terms. Cohen et al. [2009] demonstrate that improved classification performance can be achieved when addi-tional, even loosely related training documents can be added to small training data sets. Using a fixed set of terms already present in a query somewhat mitigates this problem be-cause it is known which terms are supposedly useful for discrimination between relevant and irrelevant documents. Second, class imbalance can lead to dominance of the majority class if the classifier treats errors on both classes as equal. In information retrieval scenarios, the number of relevant documents is usually much smaller than the collection size, leading to an extreme imbalance if trained on the whole collection. This challenge can easily be accounted for by introduction of different costs for errors on the two classes. Another approach is subsam-pling of the majority class, but this method might overly reduce training set sizes, possibly leading to the scarcity problem already described. Third, the task of updating the set of relevant documents for a review means that the op-timized query will be executed on future, unseen documents. Hence, improved performance 142 CHAPTER 6. P -NORM QUERY OPTIMIZATION on documents that the query is trained on is not enough, the query's performance also has to generalize. While it is sufficient to keep a hold-out document collection sample for eval-uation of generalization ability, for updating of systematic reviews it is most interesting to evaluate performance on future as yet unseen documents. Fourth, documents might undergo concept drift over time. The performance of weights might be good on a collection at a given time, but can erode over time, because future documents might use different terminology or for other similar reasons [Cohen et al., 2004]. Rather than assuming that the terms in documents published in the future are independent and identically distributed, to some extent, concept drift can be made part of the training process to address this issue. If documents are grouped into training and validation datasets based on chronology, query candidates can be derived from the training documents and the resultant query weight assignment chosen that is best for the validation documents, possibly including a concept drift. The learnt query weights should hence generalize regardless of concept drift, as long as the concept drift itself does not change over time by way of some second-order effect. The latter could be tested on a test collection made up of documents published after the search for the review has been performed. 6.2.2 Weight Training An optimal similarity function should assign higher scores to all relevant documents than to any irrelevant one. This condition is sufficient to describe a perfect ranking for a given doc-ument collection; however, we are more interested in its performance on unseen documents. If training and test documents are considered to be drawn randomly from the same popu-lation (leading to independent document sets with identically distributed term statistics), maximizing the margin between the scores assigned to relevant and irrelevant documents is known to lead to better generalization performance and to a unique, optimal score assign-ment [Hastie et al., 2003]. Intuitively, a larger margin reduces the risk that score variations between training and test documents of the same relevance class lead to irrelevant docu-ments being ranked higher than relevant ones, or vice versa. Hence, considering the output 143 6.2. A WEIGHTING METHOD FOR P -NORM QUERIES o of a similarity function in the interval [0, 1], the ideal score assignment for a query would be to assign 1 to all relevant documents, and 0 otherwise. Consequentially, a loss function L, measuring the error in the score assignments to documents for a query, should assign a loss of 0 to an optimal query. A common choice for such a function is squared error loss: L(θ) = 1 2 n∑ k=1 αk (sk(θ)− rk)2 , (6.2) where n is the number of training instances, a parameter αk is associated with every training instance k to control its influence on the overall error, θ is the set of all weights in the query, and rk is the binary relevance of training instance k. The constant factor is conveniently chosen to cancel out in cases where derivatives of the loss function are used. For now, we do not distinguish between documents and training instances. We also assume all αk to be equal, in particular to be 1; but α could take any prior distribution over the training instances. To account for class imbalance between the number of relevant and irrelevant docu-ments, it is necessary to increase the impact of errors on relevant documents (typically the much smaller class) with respect to irrelevant ones (typically the dominant class). Oth-erwise, the optimization might ignore the minority class and try to reduce the error for the majority class only. Because our interest focuses on the scores of both classes, this is clearly an undesirable behavior. We hence recover class balance by adjusting αk of every relevant training instance k (having rk = 1) with the factor ∑n i=1{αi : ri = 0}∑n i=1{αi : ri = 1} . This is, in effect, equivalent to replication of relevant documents such that there is the same number of documents for both relevance classes, but without explicit materialization of duplicates as training instances so that fractional duplication becomes possible. To minimize the loss L(θ), we have to find a better assignment of query weight param-eters θ. Because our chosen loss function itself and the EBR scoring functions are both differentiable, gradient descent can be used to optimize the loss, an efficient technique no-tably used in neural network training [Bishop, 1996]. In fact, weighted EBR queries can 144 CHAPTER 6. P -NORM QUERY OPTIMIZATION YYY ZZZ · · · · · · · · · s1,Y smY ,Y s1,Z smZ ,Z s oY oZ w1,Y wmY ,Y w1,Z wmZ ,Z · · · · · · · · · · · · · · · · · · Figure 6.2: Schematic representation of two nodes of a general query tree, presented as a neural network with query terms on the left and the query root node Z right. be seen as a restricted form of neural network, forming a tree instead of a fully connected graph and using different activation functions. With gradient descent, the loss function L(θ) is differentiated with regard to the query weights θ, which are then updated in the direction of the negative gradient, −∇L(θ). If the magnitude of this update is not too large and the loss function is well behaved, this method should lead to a loss decrease in each iteration. Figure 6.2 describes the basic model by way of which the algorithm is described in detail. In every node N , i ∈ 1, . . . ,mN input scores si,N and weights wi,N are combined using Equation 6.1 to produce the node's output score oN , which is either used as the input to another node, or leads to the overall similarity score s. Query terms, being leaves in the query tree, seed the calculation by provision of initial input scores. We concentrate on binary term scores throughout this thesis, however, arbitrary real-valued input scores in [0, 1] could be used in the algorithm. Using the chain rule, the partial derivatives of L with regard to the weights wi,Z of the query root node Z can be expressed as ∂L ∂wi,Z = ∂L ∂oZ ∂oZ ∂wi,Z = n∑ k=1 αk (oZ,k − rk)︸ ︷︷ ︸ δZ ∂oZ,k ∂wi,Z , (6.3) 145 6.2. A WEIGHTING METHOD FOR P -NORM QUERIES where δZ refers to the error in the output of node Z for a particular training instance k (subscript omitted for sake of clarity). The term ∂oZ,k∂wi,Z depends on the type of node Z and can be derived from Equation 6.1 as ∂o ∂wi =  ∂fOR(w, s, p) ∂wi = 1 p [∑m j=1w p j s p j∑m j=1w p j ] 1 p −1 pwp−1i s p i ∑m j=1w p j − ∑m j=1w p j s p jw p i(∑m j=1w p j )2 ∂fAND(w, s, p) ∂wi = −∂f OR(w, 1− s, p) ∂wi ∂fNOT(s) ∂wi = 0 . (6.4) We are now able to calculate the slopes on the error surface for the weights used in the query root node and the current assignment of θ. In the following, we generalize this result for all other weights. For any node Y and its weights wi,Y , located in the previous level of the query tree, the chain rule extends to ∂L ∂wi,Y = ∂L ∂oZ ∂oZ ∂oY ∂oY ∂wi,Y = n∑ k=1 αk (oZ,k − rk)︸ ︷︷ ︸ δZ ∂oZ,k ∂oY,k︸ ︷︷ ︸ δY ∂oY,k ∂wi,Y , (6.5) where δY refers to the fraction of the overall error for training instance k that is due to the output of node Y . More generally, for any pair of child (Y ) and parent (Z) nodes we determine δY = δZ ∂oZ ∂oY (6.6) with respect to some training instance k. Since the output oY of some node Y is one of the score inputs si of its parent node Z, the partial derivative ∂oZ ∂oY can be derived by analogy to Equation 6.4 and is equal to 146 CHAPTER 6. P -NORM QUERY OPTIMIZATION ∂o ∂si =  ∂fOR(w, s, p) ∂si = 1 p [∑m j=1w p j s p j∑m j=1w p j ] 1 p −1 wpi ps p−1 i∑m j=1w p j ∂fAND(w, s, p) ∂si = ∂fOR(w, 1− s, p) ∂si ∂fNOT(s) ∂si = −1 . (6.7) The overall error for a query on a training instance successively propagates down the query tree to the output of each intermediate operator node, at which point we are able to calculate the partial derivative with regard to each of its weights and hence, the overall gradient ∇L(θ). A range of methods to update weights have been proposed in the literature [Hastie et al., 2003]. A straightforward approach is to update the weights in the direction of the negative gradient proportional to its magnitude: θ ← θ − η∇L(θ) , (6.8) where η controls the step size for the iteration. Since the gradient of a function L(θ) is a point-wise estimate for the direction and magnitude of the steepest ascent at position θ, linear interpolation of Equation 6.8 is likely to be accurate only in the neighborhood of this point, so that η has to be chosen not too large. Otherwise, the optimization might oscillate around a minimum. However, if η is too small, it might require many iterations to arrive at a local minimum, particularly because the step size η decreases with the gradient when a local minimum is approached. In order to choose an optimal η for each update, a line search in the direction of the negative gradient can be performed; this, however, requires repeated loss calculation for each η that is tried. The Rprop (resilient backpropagation) method is a simpler and often faster heuristic that ignores the magnitude of the gradient and updates each individual weight based on the sign of its partial derivative: ∀j : wj ← wj − η sgn(∇Lj) , (6.9) 147 6.2. A WEIGHTING METHOD FOR P -NORM QUERIES so that η becomes a weight-wise, fixed step size. Further, η can successively be decreased when sign changes in the gradients of consecutive iterations signal oscillation; or increased, when the direction of the gradients does not change [Riedmiller and Braun, 1993]. The addition of momentum terms in the update formula can also be helpful in smoothing the trajectory in which the optimization proceeds. For functions that satisfy specific proper-ties, Newton's method and conjugate gradient descent are able to find minima faster than without a momentum term. However, these methods are more demanding in computational and memory requirements, and if the properties of the function that is to be optimized are unknown, it is often best to choose a simple and efficient, yet robust method. The optimization can be stopped based on any of several criteria. Since ||∇L|| ap-proaches 0 in the proximity of a (local) minimum, the optimization can be stopped once ||∇L|| falls below a small threshold �. Convergence is also reached once L does not sig-nificantly decrease over a number of iterations. Finally, the optimization can be stopped prematurely, before a minimum on the training dataset is reached, if the performance on the validation data starts to degrade. Being an optimization method that is only guaranteed to converge to local minima, it is generally a good idea to repeat the optimization from different starting points to increase the chance of finding a global minimum. This can be done by multiple random initial weight assignments, for instance, normally distributed around a mean of 1 with some small standard deviation σ. However, in practice, we are not interested in finding the best weights with regard to some performance measure on the training data, but one that also generalizes well. A simpler setup is to start the optimization from the baseline weight assignment and stop it according to its performance on the validation dataset, possibly even before a (local) minimum on the training dataset is reached. Otherwise, the complexity of the trained model has to be controlled to guard against over-fitting on the training data. This might be done via introduction of a regularization term and an associated hyper-parameter into Equation 6.2, by way of which training error could be traded in versus model complexity, an approach that is also called weight decay [Hastie et al., 2003]. The best hyper-parameter then has to 148 CHAPTER 6. P -NORM QUERY OPTIMIZATION Algorithm 5: Query weight training algorithm Input : Assignment of query tree nodes to consecutive indices 1, . . . , l via breadth-first search, arrays of size l: w, initial weight assignment, where w[j] is the weight applied to the output of node Nj , p, p[j] returns the index of the parent node of node Nj , C, C[j] returns the set of child node indices of node Nj , training document set T with k documents, and relevance judgements r for T . Data : Working arrays of size l: ∇L, o, δ. Output: Optimized query weights w∗. 1 Evaluate query baseline performance 2 repeat 3 for j ← 1 to l do ∇L[j]← 0 . initialization 4 for k ← 1 to n do . determine gradient for training instance k // Forward step: 5 for j ← l to 1 do . set or calculate output scores for every query node 6 if Nj is term then 7 o[j]← { 1 if term Nj occurs in document k 0 otherwise . populated from inv. list 8 else 9 Compute o[j] for node Nj on the basis of o[i] and w[i], with i ∈ C[j] 10 . Equation 6.1 11 end 12 end // Backward step: 13 δ[1]← o[1]− rk . Equation 6.3 14 for j ← 2 to l do δ[j]← δ[p[j]]∂op[j]∂oj . Equations 6.6 & 6.7 15 for j ← 1 to l do . update training set gradient for clauses of every query node 16 foreach i ∈ C[j] do ∇L[i]← ∇L[i] + αkδ[j] ∂oj∂wi,j . Equations 6.4 & 6.5 17 end 18 end 19 for j ← 2 to l do w[j]← w[j]− η sgn(∇L[j]) . Equation 6.9 20 Evaluate query performance 21 until stopping criterion is satisfied 22 return w be found using the validation dataset, but it is not obvious how to regularize the complexity of a weighted EBR query. The full learning process is presented as Algorithm 5. The optimization performs mul-tiple iterations, in each of which all training instances are inspected (the loop beginning at Line 4). For each training instance, first a forward step is performed, in which the out-put for each query node and finally, the overall query is calculated (Lines 5–12). In the 149 6.2. A WEIGHTING METHOD FOR P -NORM QUERIES corresponding backward step, the error for the query output is propagated back to query child nodes (Lines 13–14). Further, given the error for the output of each query node, the instance-based partial derivative with regard to the weight of each clause is calculated. Once this is done for all weights, we obtain an instance-based gradient; and after we iterate and aggregate over all training instances, we obtain the overall gradient (Lines 15–17). Then, all weights are updated (Line 19) and the performance of the new weighted query evaluated (Line 20). Finally, an appropriate stopping criterion is employed to determine when to stop the optimization (Line 21). 6.2.3 Efficiency For each iteration, Algorithm 5 has a basic time complexity of O(l·n), being linear in the number of training instances n and the size of the query in terms of the number of query tree nodes l. The ordering of the loops has purposefully been chosen in such a way that memory consumption is minimized. All that is required are arrays of size l, with l being typically much smaller than n. Because documents are iterated sequentially, they can be stored externally, an approach that allows the algorithm to scale to large disk-resident document sets. The number of performed iterations is another (linear) factor that determines overall optimization time. With appropriate choice of update formula and parameters, however, a local minimum can be reached with just a few iterations. Moreover, a maximum number of iterations can be enforced to ensure timely termination of the method, giving the possibility to repeat the process with adapted parameter settings or to terminate the process and return the best weight assignment found so far. If the number of iterations is bounded, it effectively becomes a constant, and hence offers less opportunity for efficiency gains. However, several optimizations can be applied to improve on the baseline efficiency of each iteration. The computation of the output scores in the forward step (Lines 5–12) can be performed using Algorithm 3, proposed in Chapter 5 on page 114. Instead of initializing all weights, only active query tree nodes (non-zero outputs) need to be iterated. Because many docu-ments only contain a small number of query terms, speedups in the order of those observed 150 CHAPTER 6. P -NORM QUERY OPTIMIZATION in the experiments in Section 5.3.3 are likely. For clarity, we present the calculation of error deltas (Line 14) separately in Algorithm 5, this can however be done as part of the loop in Line 15 and only for intermediate query tree nodes. Similarly, the evaluation of query performance (on the training documents) can be interleaved with the iteration over the documents. Note that we only have to consider documents that contain at least one query term be-cause all other documents would always have a score of 0, no matter the weight assignment. We cannot learn anything from these documents and therefore exclude them from the train-ing set. Unfortunately, due to the usage of frequent meta-data terms such as Humans/ in biomedical queries, even queries with only a few terms are likely to match a large fraction of the document collection. In contrast to general IR, this optimization is therefore not very effective for typical biomedical collections. The use of binary term weights allows for another important consideration: identical calculations are performed for different documents during each iteration over the training data if documents contain the same subset of query terms and have the same relevance class. Such documents can instead be unified into one training instance with a larger prior αk, giving this training instance more influence. Instead of using a prior of 1, αk can be initialized to the number of documents with the same subset of query terms and relevance class. Because every term can either appear or not, because we do not consider the com-bination where no query term occurs in a document and because we have two relevance classes, the theoretical maximum for the number of training instances after unification is min(2(2t − 1), n), where t is the number of terms in the query and n the number of docu-ments matching at least one term. However, as shown in Figure 6.3 empirically, the actual number of combinations is typically much smaller in practice. The graph shows the number of training instances before (baseline) and after unification (unique) for the queries of the Cochrane query set as a function of the number of query terms t. While the number of matching documents is relatively independent of the number of query terms and large even for the smallest queries, the number of unified training instances is effectively bounded for small t and in practice increases gracefully with query size. For all queries in the Cochrane 151 6.3. EXPERIMENTS F re q u e n c y Number of Query Terms 5 10 20 50 100 200 10 1 10 2 10 3 10 4 10 5 10 6 10 7 Baseline (# of Documents) Unique (# of Training Instances) Unique (Theoretical Maximum) Figure 6.3: The number of training instances before and after unification of identical documents with regard to each query of the Cochrane query set over the number of query terms. Per systematic review, the collection has been limited to the set of training documents published more than four years prior to the search being performed. query set, this optimization reduces the number of training instances by several orders of magnitude, and allows query weights to be learned quickly even on large collections such as Medline, in which often up to 10 million documents can match a typical query. If more complex, document-dependent term weights are used, a similar effect could possibly be achieved by discretization of these weights, or by unification of documents that have very similar term weights. And if the size of the training set can still not be significantly reduced, many calculations (for query subtrees) will be redundant, meaning that caching of intermediate results would be a promising alternative. 6.3 Experiments This section describes experiments that seek to validate the proposed approach empirically. First, the ability of the described setup is tested to arrive at queries that have better perfor-mance based on the measure that is optimized and where this performance indeed general-izes to future, new document sets. Second, the quality of optimized rankings is compared to baseline rankings on the basis of widely used information retrieval measures. Section 6.3.4 152 CHAPTER 6. P -NORM QUERY OPTIMIZATION −8 −4 +4 t Actual: Experimental: Review Search Review Update Training Validation Usage Training Validation Test Figure 6.4: Actual and experimental dataset split. to simulate the update of a systematic review. gives an interpretation of the learned weights for an example query. Finally, a number of observations from the experiments are discussed and other approaches proposed. 6.3.1 Experimental Setup The experiments focus on the Cochrane query set using a snapshot of Medline. The query set consists of 50 complex structured queries each with dozens of query terms, as typi-cally used for the retrieval of a seed document set in systematic reviews produced by the Cochrane Collaboration (see Section 3.1.2). Published studies that have been included in the review or otherwise found to be relevant are considered relevant, and all other docu-ments are considered irrelevant. For each query, the document collection is split up on the basis of document publication dates with regard to the reported review search date. We simulate an update frequency of ∆t = 4 years, as suggested by French et al. [2005]. This is longer than currently practiced by the Cochrane collaboration, but is advantageous for evaluation of the proposed methods because over a longer period of time the likelihood of relevant documents appearing is increased and we are ultimately interested in higher ranks for relevant documents. With comprehensive relevance judgements only available up to the time at which the search has been performed for each of the systematic reviews, we simulate review updates for experimental purposes on documents published before the search (see Figure 6.4). The test dataset consists of documents, published up to 4 years before the search for the review has been performed. A further 4 years of documents pub-lished before that point in time are assigned to the validation dataset. All remaining earlier 153 6.3. EXPERIMENTS documents are then used as the training set, and the collection documents published after the review search date are ignored. Two of the 50 queries did not have relevant documents in their training sets and were thus ignored. This setup simulates the actual review update situation 4 years after the last search for a review, in which query weights would be trained and validated on documents before the review search date and then used for retrieval of newly published documents. One caveat with this experimental setup is that the queries could possibly be biased by knowledge about the documents in the test set. Hence, absolute performance on the test set might not be reliable, but this setup still allows for relative com-parison of the approach because both the optimized queries as well as the baseline would have this advantage. Throughout the experiments, each query is executed on each of the three datasets spe-cific to its update situation, and only documents containing at least one query term are considered. Documents in the training set are then unified as described in Section 6.2.3 to generate the final training instances and the αk are updated to reflect this and to account for class imbalance. During training, Rprop with a constant learning step size of η = 0.025 has been used. Each step was smoothed with a momentum term. Queries have been trained until their performance degraded on the validation dataset and at most for 200 iterations to limit training time for a small subset of queries. To ensure that the experimental results do not depend on the choice of any particular parameter setting, the experiments have been repeated with a selection of different p-values (1, 1.5, 2, 5, and 10), each time assigned to all of the operators. 6.3.2 Generalization & Effectiveness The first experiment investigates the learning process, in particular, the ability of the ap-proach to learn weights that are performing better with regard to the optimized measure than the obvious baseline of unweighted queries. Furthermore, it is of interest if the learned queries do generalize, that is, if similar performance improvements can be achieved on fu-ture, on as yet unknown documents. Training is performed on the training dataset only, however, performance is also evaluated and tracked on the other two datasets. A decision 154 CHAPTER 6. P -NORM QUERY OPTIMIZATION Iteration L o s s ( n o rm a liz e d ) 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 considered optimum 0 5 10 15 20 25 30 Training Validation Test Figure 6.5: Loss L, normalized by ∑ k αk, on the three different datasets over the iterations for one typical query. The normalization leads to a similar range of loss values, but the starting points for the datasets differ because their score distributions are not identically distributed. about the best query – the one that is supposed to be used by this approach to retrieve a set of future documents – is based on the performance achieved on the validation dataset. Actual evaluation of that query is carried out on the test data, which represents "future" documents that are not used in any way during the training and validation process. This process is illustrated in Figure 6.4. Figure 6.5 depicts development of query performance in terms of loss over the training iterations on the different datasets for one query, and is typical of the behaviour observed for many queries. To make the absolute values of the losses comparable between the datasets of different size, we normalize by the sum of all alphas in each dataset. This is equivalent to normalization by the number of documents when all alphas are equal to 1. The loss on the training dataset quickly approaches 0, meaning that all relevant and irrelevant documents have scores close to 1 and 0 respectively. To achieve this, query weights are likely to increase for terms and concepts that happen to match on relevant documents and to decrease otherwise. An exception to this rule are weights for terms and concepts that are below a negation operator. The performance on the validation dataset reveals that the weight changes performed in the first iterations indeed not only reduce the loss on the training data, but that weights are learned that reduce the loss on new, future documents (generalization including concept drift). However, after that point the query weights start to overfit to the particularities of the 155 6.3. EXPERIMENTS Training Validation Test p Baseline Optimized Baseline Optimized Baseline Optimized 1 0.115±0.04 0.054±0.04† 0.121±0.08 0.055±0.05† 0.111±0.08 0.056±0.06† 1.5 0.101±0.04 0.038±0.04† 0.108±0.07 0.043±0.05† 0.099±0.07 0.049±0.05† 2 0.091±0.03 0.034±0.03† 0.098±0.06 0.038±0.04† 0.090±0.07 0.043±0.05† 5 0.078±0.05 0.025±0.03† 0.089±0.07 0.031±0.05† 0.078±0.07 0.040±0.05† 10 0.084±0.06 0.027±0.04† 0.097±0.09 0.032±0.05† 0.081±0.09 0.042±0.06† Table 6.1: Loss L, normalized by ∑ k αk and averaged over the queries, and standard deviation for selected p-values on the Cochrane dataset. The superscript † refers to a statistical significant improvement based on a paired Wilcoxon test at the 0.001 alpha level. training documents. An important observation in this regard is that the performance on the validation dataset correlates with the one for the test dataset. They both have their minimal loss using the same or similar weights, assigned at about the same iteration. This is key to the training and effectively guards against failure in instances, where any optimization (possibly due to too few relevant training documents) is harmful. In these cases, the opti-mization simply stopped at the first iteration and used the baseline weight assignment, that is, the unweighted query. Many other queries did not overfit and continued to approach a minimum until the limit of 200 on the number of iterations has been reached. In these instances, the number of parameters or the distribution of the query terms over relevant and irrelevant documents seems to inherently constrain overfitting. An indication for this is that these queries often converge to a loss larger than 0 on the training data. For the query shown in Figure 6.5, the suggested training setup is functional and effectively guards against overfitting. Next, the investigation is extended to the whole query set. Table 6.1 reports aggregated results over all 48 queries that have relevant documents in the training dataset, for a selection of p-values and for each of the training, validation and test datasets. The losses have again been normalized to make their magnitudes compara-ble between the datasets. An interpretation of a normalized mean loss of around 0.1 for the baseline queries would be that a document has an average difference of √ 0.2 between its as-sociated score and its class. However, note that we virtually duplicated relevant documents 156 CHAPTER 6. P -NORM QUERY OPTIMIZATION by means of αk so that this interpretation might be biased by individual losses on relevant documents. Throughout, optimized queries exhibit on average a decreased loss over their unweighted baseline, statistically strongly significant even when we do not assume normal distribu-tion of the score differences. However, the standard deviations are high for the optimized queries, and sometimes higher than the average. This highlights the diversity in the queries. The absolute values for either the baseline or the optimized query are on a similar level for the different datasets. This is encouraging because concept drift does not seem to be as strong as the patterns present in past documents. A confounding factor in this analysis, however, might be that the used queries might have been reformulated based on knowledge of the documents present in the validation and test datasets (see Figure 6.4). While this could lead to better absolute performance of the EBR queries, both the baseline as well as the optimized queries would have this advantage. Larger p-values generally lead to a smaller loss, for both the baseline and optimized queries. This reflects one property of the p-norm model, resulting in document scores close to the binary values 0 and 1 for high p-values. That this property becomes evident in Ta-ble 6.1 means that the queries have indeed good baseline performance, assigning document scores that are close to their true class. Otherwise, higher p-values would lead to a loss increase if the query would assign a score closer to the wrong class. The best results have been achieved with p = 5. With p-values larger than this (p = 10), average loss and stan-dard deviation appear to increase. This is evident on the baseline and the optimized queries. Higher p-values lead to steeper slopes on the loss function. Hence, assuming constant step size, the optimization can find a minimum faster, but also becomes more susceptible to an inappropriate step size. Much larger p-values might also lead to numerical stability prob-lems in the calculation, but no such issues have been noted for the choice of p-values used throughout this thesis. While Table 6.1 presents average performance over a representative sample of queries, reviewers typically only deal with one particular review and its query. In practice, it is thus important to get an indication of how likely a method is to fail on any particular query. 157 6.3. EXPERIMENTS Baseline O p ti m iz e d 0 0.1 0.2 0.3 Training 0 0.1 0.2 0.3 Validation 0 0.1 0.2 0.3 Test 0 0.1 0.2 0.3 Figure 6.6: Loss L, normalized by ∑ k αk, before (Baseline) and after (Optimized) training for p = 5 on the Cochrane dataset. Each mark corresponds to one query; points below the diagonal line show improvements. Figure 6.6 shows every query's performance before and after query optimization, for each of the three datasets, and with p = 5. The paired sign tests already provided evidence that a majority of queries are improved by the method, but the pairwise examination reveals that the loss could be decreased significantly in all datasets. Even on the test dataset, all but two reviews show either significant loss reductions or unchanged performance. It turns out that for these reviews the performance on the test set behaves conversely to the validation set. The performance in terms of the optimized loss indicates the feasibility of the approach, and that it is indeed possible to automatically derive improved query node weightings. How-ever, improvements in retrieval performance are ultimately of interest, only if they rank relevant documents higher than without the optimization. The next section examines the position of relevant documents in the rankings produced by the baseline and trained queries. 6.3.3 Impact on Rankings Numerous evaluation measures have been proposed to evaluate the effectiveness of ranked retrieval systems. To compare the positions of all relevant documents in a ranking concisely, any aggregate of their positions would suffice. An interpretable measure has however the advantage of quantifying the extent of the change in retrieval effectiveness. To be meaningful in the context of systematic reviews, a measure should take into account that users are likely to look at many documents, say 100 or 1000. Rank-biased 158 CHAPTER 6. P -NORM QUERY OPTIMIZATION Training Validation Test Measure p Baseline Optimized Baseline Optimized Baseline Optimized RBPp=0.99 1 0.0453 0.0481 0.1010 0.1015 0.1457 0.1446 1.5 0.0635 0.0628 0.1058 0.1043 0.1509 0.1497 2 0.0704 0.0681 0.1082 0.1065 0.1529 0.1514 5 0.0669 0.0730 0.1074 0.1066 0.1517 0.1522 10 0.0648 0.0781† 0.1070 0.1085 0.1511 0.1532 RBPp=0.999 1 0.0120 0.0126 0.0871 0.0870 0.1294 0.1293 1.5 0.0147 0.0143 0.0878 0.0874 0.1302 0.1298 2 0.0154 0.0150 0.0880 0.0876 0.1304 0.1302 5 0.0153 0.0160 0.0878 0.0879 0.1303 0.1304 10 0.0150 0.0169† 0.0878 0.0881 0.1303 0.1304 AUC 1 0.962 0.974† 0.960 0.963 0.948 0.956 1.5 0.969 0.976† 0.973 0.968 0.959 0.955 2 0.970 0.978 0.975 0.972 0.961 0.957 5 0.972 0.979† 0.975 0.976 0.959 0.959 10 0.972 0.977† 0.975 0.979† 0.959 0.965 Table 6.2: Effectiveness of the optimization in terms of rank-biased precision (RBP) and the area under the ROC curve (AUC), averaged over the queries of the Cochrane dataset and for selected p-and pRBP-values. The superscript † refers to a statistical significant improvement based on a paired Wilcoxon test at the 0.05 alpha level. precision [Moffat and Zobel, 2008] is parameterized with a parameter pRBP determining the persistence of a user to look at more documents. For a given pRBP-value, a population of users is assumed that look at particular numbers of top-ranked documents following a dis-tribution over these numbers. For instance, a pRBP-value of 0.99 refers to a distribution of users with an expected value of 100 documents, and pRBP = 0.999 similarly corresponds to 1,000 documents. It hence reflects the way users are likely to behave. Another widely used measure in this context is the area under the ROC curve, which was motivated in detail in Section 2.3.2. Table 6.2 shows that performance in the loss measure used as the basis of the opti-mization does not translate into general improvements in information retrieval performance measures such as RBP or AUC. RBP has been calculated on the basis of the usually large number of documents that match on at least one query term. Only one query (5) returned 159 6.3. EXPERIMENTS fewer than 15,000 documents over all datasets, resulting in a noticeable residual of uncer-tainty about the final RBP evaluation score for this query. Otherwise, residuals were well below the number of significant digits that are presented. Table 6.2 reports thus only the RBP base scores. The returned documents are the same for the baseline and optimized queries so that residuals can be ignored for the purpose of system comparison. Although in some cases, the optimized queries show average increases in the measures, this is neither a common trend nor statistically significant. Average absolute query performance in terms of AUC is relatively high. On average, more than 95% of irrelevant documents that match the query in at least one term are ranked below each relevant document. For a query that only matches on a relatively small number of documents, say a thousand documents, this means that the average position of relevant documents are around rank 50. But the prospect of reviewing all relevant documents for queries that match a much larger fraction of today's collections is less promising. As seen for loss L, there is a slight trend to increased effectiveness for higher p-values. Optimizing squared error loss does not improve RBP or AUC for all of the queries, hence, the correlation between AUC and the loss L is not very high. Cortes and Mohri [2003] show that different AUC values can map to the same value on a different measure for which it is known how to directly optimize for it. This result means that it could well be that, by optimizing mean squared error on all queries, some might have increased and others decreased performance on other measures of interest, such as RBP. To investigate this further, a drill-down to the query level is necessary. Figure 6.7 shows the difference in the RBP values between the baseline and the opti-mized query for each of the 48 topics. RBP is improved for around a third of the queries, almost unchanged for another third and worse for the rest of the queries. It is noticeable that, in contrast to Figure 6.6, not all topics show improvements on the training and val-idation dataset, demonstrating that optimizing squared loss does not guarantee improved RBP. Another observation can be made by way of sorting the queries by decreasing RBP on the validation dataset. Although a different measure is considered, the performance on the datasets appear to be correlated. 160 CHAPTER 6. P -NORM QUERY OPTIMIZATION Topics R B P I m p ro v e m e n t −0.005 0.000 0.005 0.010 0.015 −0.004 −0.002 0.000 0.002 0.004 −0.003 −0.002 −0.001 0.000 0.001 0.002 7 2 5 1 4 6 1 6 1 7 4 1 9 3 3 3 5 4 7 3 9 4 8 2 1 2 9 3 8 3 6 4 0 4 2 1 1 1 4 1 5 1 9 2 0 2 7 3 2 3 4 4 3 4 4 2 3 4 5 5 2 4 4 9 2 8 3 1 3 7 1 2 1 0 4 2 1 8 8 2 2 1 3 2 6 3 0 3 T ra in in g V a lid a tio n T e s t Figure 6.7: Per-topic improvements on the datasets as per RBP with a persistence parameter of pRBP= 0.999, sorted by decreasing RBP on the validation dataset. These are results using a p-value of 5. Figure 6.8 presents RBP improvements on the validation versus the test dataset by way of which four cases can be computed based on the (signed) difference between the before and after RBP scores. Queries for which the optimization on mean squared loss led to increased RBP on both validation and test dataset (Quadrant I), such for which both show decreased RBP (Quadrant III) and such for which validation and test datasets do not exhibit the same behavior (Quadrant II and IV). Since the aim is improved query performance on the test dataset, cases III and IV are detrimental and should be avoided. Fortunately, there is a strong correlation between RBP improvements on the validation and test dataset (Pearson correlation coefficient of 0.57). This can be used as a second safe-guard: the weighted version of a query is only to be used if its performance is improved with regard to an evaluation measure such as RBP on the validation dataset, otherwise the unweighted baseline query is to be used. This method effectively brings the reduced test dataset performance in Quadrant III back to 0, but also reduces the applicability of query weight optimization to a subset of the queries. This cautious use of weighted queries also leads to errors (Quadrant II): queries are not employed for which optimized versions 161 6.3. EXPERIMENTS Validation T e s t −0.003 −0.002 −0.001 0.000 0.001 0.002 III III IV −0.006 −0.004 −0.002 0.000 0.002 0.004 0.006 Figure 6.8: Correlation between RBP improvements on validation and test datasets. Results are for an assumed persistence of pRBP= 0.999 and a p-value of 5. Training Validation Test p Baseline Optimized Baseline Optimized Baseline Optimized 1 0.0120 0.0128† 0.0871 0.0875† 0.1294 0.1298† 1.5 0.0147 0.0150 0.0878 0.0880† 0.1302 0.1304† 2 0.0154 0.0156 0.0880 0.0882† 0.1304 0.1306† 5 0.0153 0.0162† 0.0878 0.0883† 0.1303 0.1305† 10 0.0150 0.0167† 0.0878 0.0884† 0.1303 0.1305† Table 6.3: Average RBP for pRBP = 0.999 and selected p-values for 48 queries from the Cochrane dataset when queries that show inconsistent behavior on the validation datasets for loss L and RBP revert to the baseline query. The superscript † refers to a statistical significant improvement based on a paired Wilcoxon test at the 0.05 alpha level. perform better. This method is also not able to guard against case IV, but only one query in the examined query sample is present in this quadrant and does exhibit only minor effec-tiveness decrease on the test dataset. When the optimized queries are used conditionally on the basis of their performance in terms of RBP on the validation dataset, the results in Table 6.3 are achieved. It is now notable that the optimized query approach has statistically significant increased RBP for all p-values and datasets. The absolute improvement is, however, minor and practical sig-nificance is hence questionable. Nevertheless, the learned weights might be of value as a measure of importance of individual terms and concepts in the query. 162 CHAPTER 6. P -NORM QUERY OPTIMIZATION Query Tree Nodes (Grouped with Siblings) W e ig h t 0.7 0.8 0.9 1.0 1.1 1.2 AN D − 0 0 O R − 1 O R − 2 1 AN D − 8 PT :ra nd om iz ed _c on tro lle d_ tri al − 3 SH :ra nd om iz ed _c on tro lle d_ tri al s − 4 M eS H :ra nd om −a llo ca tio n − 5 M eS H :d ou bl e− bl in d− m et ho d − 6 M eS H :s in gl e− bl in d− m et ho d − 7 2 M eS H :c or on ar y_ di se as e − 9 M eS H :c er eb ro va sc ul ar _d is or de rs − 1 0 8 M eS H :h um an s − 12 O R − 1 1 11 AN D − 1 5 PT :c lin ic al _t ria l − 1 3 AN D − 1 6 M eS H :c lin ic al _t ria l − 1 4 TI :ra nd om − 1 9 TI :p la ce bo − 1 8 M eS H :p la ce bo s − 17 15 TI :tr ia l$ − 2 1 AL L: cl in $ − 20 16 AB :tr ia l$ − 2 3 AL L: cl in $ − 22 Figure 6.9: The distribution of learned term weights for p = 5 and Query 48 of the Cochrane dataset. The clauses of operator nodes are listed in breadth-first order from left to right, individually sorted within each group by decreasing weight. Each operator denotes its parent's node index in its header. 6.3.4 Term Importance One advantage of the proposed approach is that the learned weights can be interpreted and might inform users about the importance of terms and whole concepts present in the query. Figure 6.9 gives an example of an optimized query. Note that by means of the normalization in the weight formulation, the absolute values always average to 1. Clause 1 ("OR – 1") received a higher weight than its sibling clause 2 ("OR – 2"). While the latter contains concepts specific to the systematic review in question, the former con-tains query parts of general type that constrain retrieved documents to high-quality research results such as those stemming from trials and having randomized and blind research meth-ods. These concepts have been investigated before and found to decrease result set size (that is, increase precision) but still retain most relevant documents useful in systematic re-views (that is, high recall) [Royle and Waugh, 2005]. It is hence not surprising that they received higher weights, leading them to be ranked higher than other documents. Even so, it is immensely reassuring to see that the automatic process reached the same conclusion. The weight for the MeSH term Humans/ is an important property of relevant document for this review, but documents missing this term have the chance to compensate for this with the presence of other terms, such as those found as clauses of node 11. Finally, the appear-163 6.3. EXPERIMENTS ance of the term trial$ in the title of a Medline abstract appears to be more indicative for a relevant document than if it would occur in the abstract (concluded by comparison of the clauses of node 15 and 16). By inspection of multiple queries, meta-data terms seem to be weighted higher than free-text. This is evidence for the quality of the indexing performed in the medical domain, and reassurance that their use by the expert searchers generating these queries is justified. 6.3.5 Discussion While the approach generalizes and leads to rankings where relevant documents tend to be ranked higher, the extent of position changes of relevant documents in the ranking appears to be small. A number of observations can be made from detailed analysis of the baseline and trained queries. First, the learned weights of the trained queries remain close to 1. This is not surprising and is due to the proposed training method, which starts from 1 and is conservative in deviating from the starting point by means of the early stopping criterion. Although this was effective to retain performance at least as good as the baseline queries, it might have restricted the choice and combination of weight values. Second, the rankings for most queries only have localized changes in the order of doc-uments. This is especially pronounced in cases where documents have had the same score with regard to the unweighted query and hence, an arbitrary order. But, after the train-ing, an order has been introduced for these documents placing those containing terms and concepts of relevant documents higher in the ranking. Whereas the nodes of unweighted queries generate the same output score no matter which of their term clauses are present in a document as long as the same number of terms are present, weighted clauses emphasize the importance of specific terms and thus, introduce order between these documents. Third, typical ranking score distributions have high and distinct scores for top-ranked documents and a long tail containing many documents with the same score. This obser-vation is an explanation why the optimization, even if it leads mostly only to localized changes, does not have the same influence on the top ranked documents and hence, does 164 CHAPTER 6. P -NORM QUERY OPTIMIZATION not lead to larger increases in RBP. Furthermore, the normalization of the query scores to the range [0, 1] further constrains the influence that weights in lower levels of the query tree can have on the overall query similarity score. Finally, many query terms are manual expansion terms of concepts and do not (yet) appear in relevant documents. As a consequence, training assigns smaller weights to these terms, and the tracking of the performance on the validation set might possibly stop the optimization prematurely. By way of the discussed implementation approaches, execution times for the optimiza-tion of queries were in the order of seconds and minutes which is negligible compared to the search efforts that are spend on the update of a systematic review. 6.3.6 Other approaches A number of other approaches can be taken in order to improve query performance without sacrificing repeatability and scrutability. It might be possible to directly optimize on a task-relevant evaluation measure instead of optimizing mean squared error. This has the potential to eliminate the additional safe-guard employed here to use the method only on queries that show consistent behaviour over multiple evaluation measures. Applicability of the method could thus possibly be increased. A first step in this direction is the work of Herschtal and Raskutti [2004] who directly opti-mized an approximation of AUC using gradient descent. Instead of optimizing query weights, there are a number of other ways in which queries can be adapted. First, a diversity of p-values might lead to better retrieval results than choosing a uniform value for all query node operators. Second, additional terms might be introduced that separate relevant from irrelevant documents. Third, while the queries' structure has been normalized in this thesis, different aggregation of the terms and concepts might lead to superior performance. The method proposed here employs early stopping to safeguard against overfitting. An-other approach is to start from multiple initializations instead of the unweighted baseline and employ some form of regularization that penalizes the extreme query weight assignments 165 6.4. SUMMARY that are more likely to lead to overfitting. Also, other update strategies could have been employed, some of which might converge faster. Efficiency considerations however are of minor importance for the task of updating a systematic review. Finally, instead of following a time interval based update strategy, relevance feedback could be employed directly into the review creation process. The work of Wallace et al. [2010a] investigates active learning for this purpose and proposes ways to overcome biases that might result from the order and properties of the documents fed back into the system. 6.4 Summary Weighted queries allow important terms and concepts to be stressed for particular informa-tion needs. The proposed method optimizes mean squared error and shows improvements for the majority of queries. However, the results are mixed on typical effectiveness mea-sures used in information retrieval and for classification. By cautious use of the optimized version of the queries for only the reviews that show consistent behavior on these more task-related measures, better rankings could be achieved, validating the feasibility of the approach. Nevertheless, it appears that for the examined queries and retrieval model the introduced weights change the rankings only minimally. 166 Chapter 7 Conclusions This thesis has presented a new approach to systematic review search. A ranked retrieval alternative in the form of the p-norm extended Boolean retrieval (EBR) model was shown to provide advantages over the current Boolean approach, but to also satisfy important task-specific requirements. Using this retrieval method, a greater number of relevant studies than with the current Boolean approach can be identified for inclusion in systematic reviews, reducing the dependence on potentially biased secondary discovery methods to determine them. The method is close to current practice and hence requires minimal changes to the current process, which should facilitate its adoption. In the background chapter, retrieval models, basic IR methodology and systematic re-viewing practice have been introduced. Besides, requirements for systematic review search have been collated. The retrieval models have then been evaluated in Chapter 4 for their appropriateness in the context of systematic review search, and the family of extended Boolean retrieval models found to be a particularly promising alternative. Moreover, a method was suggested for the transformation of Boolean queries into queries suited to the p-norm approach. That the best retrieval results have been found for high p-values facilitates the property of the model to generalize the Boolean model at these values. 167 CONTRIBUTIONS To mitigate the drawback of these retrieval methods – namely, their execution cost – efficient algorithms were proposed in Chapter 5. Consequently, the p-norm and similar models became a much more viable and competitive alternative in practice. Finally, a query optimization method was proposed in Chapter 6 that makes use of the information about relevant documents available after creation of an initial systematic review to improve retrieval results during the update of systematic reviews. The method has been found to provide valuable feedback about document properties in the collection, however, the rankings for optimized queries turned out to be too similar to the unoptimized baselines for optimized queries to be of practical significance. Contributions This thesis describes improved methods for two important information retrieval tasks that facilitate evidence-based medicine: the initial preparation, and then the update, of system-atic reviews. Particular contributions of this thesis are: • Alternative ranked retrieval methods are evaluated in Chapter 4 for their appropriate-ness in the face of systematic review requirements. The p-norm approach to extended Boolean retrieval, originally proposed almost 30 years ago, is identified as a partic-ularly interesting method and its effectiveness (re-)validated for systematic review search. The ranked EBR method is shown to be as good as the current Boolean approach, but possesses the additional advantages of ranking: any number of docu-ments can consciously be chosen for inspection, and the increased density of relevant documents at the top of the ranking facilitates query reformulation. • In Chapter 5, it was demonstrated that efficient query processing methods available for ranked keyword retrieval can be adapted and supplemented to be applicable for the p-norm and similar EBR models, that use structured queries and monotonic score aggregation functions. The resulting efficiency gains make these models a viable alternative. 168 CHAPTER 7. CONCLUSIONS • By means of the scoring method, presented in Section 5.2.1, the inverted list iteration strategy has been decoupled from the query structure, allowing free optimization of the former. At the same time, the number of query nodes processed is reduced by means of small additional sorting overhead which does not dominate over the gains in practical measurements. • The new scoring method further enables ideas from the Max-Score optimization to be applicable in the context of the p-norm EBR model. Moreover, new techniques are described to attain complementary score bounds that can either be used on their own, or in combination with the adaptation of Max-Score. • The query optimization method described in Chapter 6 applies a gradient descent training method to weighted p-norm EBR queries. While the overall gains are not practically significant in the context of the particular experimental setup, the method has shown to be feasible and with further refinement, additional gains might be possible. • The measurement methodology, proposed in Section 3.3, allows assessment of the impact of a change to an IR system on disk and computational costs using a sim-ple setup. The publication of separate measurements is more expressive and hence informative for readers. • The experimental query parser and the operator implementations are significant im-plementations that have been reused in further research outside of the scope of this thesis, for instance, in a user study performed by Karimi et al. [2011]. For the soft-ware, it can be referred to the BioTALA project of NICTA.1 • All proposed methods are validated with experiments, using a large, representative real-world document collection, Medline, which is freely available for scientific use. Moreover, the results are compared against Ovid, a widely-used state-of-the-art bio-medical search engine implementation. 1http://www.nicta.com.au/research/projects/BioTALA/ 169 http://www.nicta.com.au/research/projects/BioTALA/ APPLICATIONS • Three query sets have been generated as part of this thesis, most significantly the COCHRANE query set, which contains 50 complex queries and relevance judgements. It has been made publicly available to facilitate further research by other researchers. Applications The contributions of this thesis are applicable in at least the following contexts. Other IR tasks and application domains, in which Boolean retrieval is still the primary retrieval mechanism, might also be amenable to ranked retrieval methods and the methods suggested in this thesis. Legal retrieval tasks are particular examples that are very similar to those in systematic review search and are worth exploring. It is hoped that this thesis mo-tivates researchers to question current approaches to other important retrieval tasks beyond systematic review search. The proposed efficient retrieval methods are not limited to the p-norm model. Several retrieval systems use structured queries, but differ in the applied retrieval functions. Many of them could benefit from the suggested methods. The practicability of the proposed time-limited method showcases a particular type of optimization which is worthwhile to explore for other long-running computations. The suggested gradient descent optimization method can be used in context of any differentiable retrieval function, most significantly those of structured queries. Using the original query as starting point in conjunction with early stopping is effective in situations where it is important to retain the effectiveness level of the original query. The measurement methodology presented in Chapter 3 is generally applicable to effi-ciency experiments that involve disk accesses and can easily be adopted for any other de-composable measurements. Finally, the curated query set might be a valuable resource in other research contexts despite that it was derived from systematic reviews for the purpose of studying the perfor-mance of information retrieval methods. 170 CHAPTER 7. CONCLUSIONS Future Directions As typical for most scientific endeavours, the research results of this thesis raise new ques-tions that provide ample opportunity for future work. First, the investigation into the p-norm EBR model can be extended. In the experi-ments in Section 4.4, the same p-value was assigned to all query operators. While this is a common approach, better retrieval results might be attained if different p-values can deter-ministically be assigned. This is particularly interesting in the context of systematic review updates, for which previous relevance judgements can be used to evaluate the p-value as-signments. Moreover, while the two term-weight choices for the p-norm model, suggested in the literature, have been examined, there might be better weighting schemes, in partic-ular those that perform well on citation databases. Richer document representations might also offer opportunities to use weighting schemes that are not as beneficial for retrieval on basis of citation entries. For systematic review search, the only requirement is that these weights are independent of collection statistics. Furthermore, all operators are currently mapped to one of the three basic (extended) Boolean operators in the query preprocessing step. Instead of relaxing the constraints imposed by, for instance, proximity operators, it is worth exploring how to extend the p-norm EBR model with additional operators. Also, the presence of too many term-level negations might have an adverse effect on the efficient query processing methods suggested in Chapter 5. This did not become a problem within the scope of this thesis, but for general queries a theory is desirable by way of which the gains of using the optimization can be estimated which could then be used to determine which implementation alternative to use. Then, the idea of conditional score bounds could be taken further and methods investigated that determine a particular term to condition the score bounds on. Finally, the query optimization method in Chapter 6 effectively mini-mized a surrogate loss, but the effect on evaluation measures of interest were inconsistent. The optimization of different losses, or possibly even directly of the evaluation measures, might lead to more consistent and significant improvements. 171 FUTURE DIRECTIONS Second, other ranked retrieval models could be adapted to make them applicable for systematic review search and then evaluated. A promising candidate is the inference net-work model [Turtle, 1991], but effective collection-independent weighting schemes would have to be devised. 172 Bibliography V. Anh and A. Moffat. Index compression using fixed binary codewords. In Proceedings of the 15th International Australasian Database Conference, pages 61–67, Jan. 2004. V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Informa-tion Retrieval, pages 372–379, Seattle, WA, USA, Aug. 2006a. ACM. V. N. Anh and A. Moffat. Structured index organizations for high-throughput text querying. In Proceedings of the 13th International Symposium on String Processing and Information Retrieval (SPIRE), volume 4209 of Lecture Notes in Computer Science (LNCS), pages 304–315, Glasgow, Scotland, UK, Oct. 2006b. Springer. T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. Has adhoc retrieval improved since 1994? In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Devel-opment in Information Retrieval, pages 692–693, Boston, MA, USA, July 2009. ACM. D. Atkins, K. Fink, and J. Slutsky. Better information for better health care: The evidence-based practice center program and the agency for healthcare research and quality. Annals of Internal Medicine, 142(12 Part 2):1035 –1041, June 2005. A. Avenell, H. H. Handoll, and A. M. Grant. Lessons for search strategies from a systematic review, in the Cochrane Library, of nutritional supplementation trials in patients after hip fracture. The American Journal of Clinical Nutrition, 73(3):505–510, Mar. 2001. PMID: 11237924. R. Baeza-Yates, F. Junqueira, V. Plachouras, and H. F. Witschel. Admission policies for caches of search engine results. In Proceedings of the 14th International Symposium on String Processing and Information Retrieval (SPIRE), volume 4726 of Lecture Notes in Computer Science (LNCS), pages 74–85, Santiago, Chile, Oct. 2007. Springer. R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999. ISBN 020139829X. 173 BIBLIOGRAPHY D. Bamber. The area above the ordinal dominance graph and the area below the receiver operating characteristic graph. Journal of Mathematical Psychology, 12(4):387–415, Nov. 1975. H. Bast and I. Weber. Type less, find more: Fast autocompletion search with a succinct index. In Pro-ceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 364–371, Seattle, WA, USA, Aug. 2006. ACM. H. Bast, D. Majumdar, R. Schenkel, M. Theobald, and G. Weikum. IO-Top-k: Index-access op-timized top-k query processing. In Proceedings of the 32nd International Conference on Very Large Data Bases, pages 475–486, Seoul, Korea, Sept. 2006. ACM. M. J. Bates. Indexing and access for digital libraries and the internet: Human, database, and domain factors. Journal of the American Society for Information Science, 49(13):1185–1205, 1998. C. C. Beahler, J. J. Sundheim, and N. I. Trapp. Information retrieval in systematic reviews: Chal-lenges in the public health arena. American Journal of Preventive Medicine, 18(4 Suppl):6–10, May 2000. PMID: 10793275. M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. In Proceedings of the 2009 Workshop on Web Search Click Data, pages 8–14, Barcelona, Spain, Feb. 2009. ACM. B. Billerbeck and J. Zobel. Questioning query expansion: An examination of behaviour and param-eters. In Proceedings of the 15th Australasian Database Conference (ADC), volume 27, pages 69–76, Dunedin, New Zealand, 2004. Australian Computer Society, Inc., Darlinghurst, Australia. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1st edition, Jan. 1996. ISBN 0198538642. C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 1st ed. 2006., corr. 2nd printing edition, Oct. 2007. ISBN 0387310738. P. Boldi and S. Vigna. Compressed perfect embedded skip lists for quick inverted-index lookups. In Proceedings of the 12th International Symposium on String Processing and Information Retrieval (SPIRE), volume 3772 of Lecture Notes in Computer Science (LNCS), pages 25–28, Buenos Aires, Argentina, Nov. 2005. Springer. F. Boudin, J.-Y. Nie, and M. Dawes. Clinical information retrieval using document and PICO struc-ture. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the 174 BIBLIOGRAPHY North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 822–830, Los Angeles, CA, USA, June 2010. The Association for Computational Linguistics. M. Braschler and C. Peters. Cross-Language Evaluation Forum: Objectives, results, achievements. Information Retrieval, 7:7–31, 2004. A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Y. Zien. Efficient query evaluation using a two-level retrieval process. In Proceedings of the 12th International Conference on Information and Knowledge Management (CIKM), pages 426–434, New Orleans, LA, USA, Nov. 2003. ACM. D. E. Brown and J. C. French. A classification approach to Boolean query reformulation. Journal of the American Society for Information Science, 48(8):694–706, 1997. C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proceed-ings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 25–32, Sheffield, UK, July 2004. ACM. V. Bush. As we may think. The Atlantic Monthly, 176(1):101–108, 1945. S. Büttcher. Multi-User File System Search. PhD thesis, University of Waterloo, Waterloo, Ontario, Canada, Aug. 2007. S. Büttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 Terabyte Track. In Proceedings of the 15th Text REtrieval Conference (TREC 2006), Gaithersburg, MD. USA, Nov. 2006. NIST Special Publication 500-272. S. Campinas, R. Delbru, and G. Tummarello. SkipBlock: Self-indexing for block-based inverted list. In Proceedings of the 33rd European Conference on Information Retrieval (ECIR), pages 555–561, Dublin, Ireland, Apr. 2011. Springer. I. Chalmers and B. Haynes. Systematic reviews: Reporting, updating, and correcting systematic reviews of the effects of health care. British Medical Journal, 309(6958):862–865, Oct. 1994. PMID: 7950620. J. Choi, M. Kim, and V. V. Raghavan. Adaptive relevance feedback method of extended Boolean model using hierarchical clustering techniques. Information Processing and Management, 42(2): 331–349, Mar. 2006. 175 BIBLIOGRAPHY C. W. Cleverdon. The significance of the Cranfield tests on index languages. In Proceedings of the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3–12, Chicago, IL, USA, Oct. 1991. ACM. A. L. Cochrane. 1931–1971: A critical review, with particular reference to the medical profession. In Medicines for the year 2000, pages 1–11. London: Office of Health Economics, 1979. A. M. Cohen. Optimizing feature representation for automated systematic review work prioritiza-tion. In Proceedings of the 2008 Annual Symposium of the American Medical Informatics Asso-ciation (AMIA), pages 121–125, Washington, DC, USA, Nov. 2008. AMIA. PMID: 18998798. A. M. Cohen, R. T. Bhupatiraju, and W. R. Hersh. Feature generation, feature selection, classifiers, and conceptual drift for biomedical document triage. In Proceedings of the 13th Text REtrieval Conference (TREC 2004), Gaithersburg, MD, USA, Nov. 2004. NIST Special Publication 500-261. A. M. Cohen, W. R. Hersh, K. Peterson, and P.-Y. Yen. Reducing workload in systematic review preparation using automated citation classification. Journal of the American Medical Informatics Association, 13(2):206–219, 2006. PMID: 16357352. A. M. Cohen, K. Ambert, and M. McDonagh. Cross-topic learning for work prioritization in sys-tematic review creation and update. Journal of the American Medical Informatics Association, 16 (5):690–704, 2009. PMID: 19567792. W. S. Cooper. Expected search length: A single measure of retrieval effectiveness based on the weak ordering action of retrieval systems. American Documentation, 19(1):30–41, Jan. 1968. C. Cortes and M. Mohri. AUC optimization vs. error rate minimization. In S. Thrun, L. Saul, and B. Schölkopf, editors, Advances in Neural Information Processing Systems, volume 16, Vancou-ver, BC, Canada, 2003. MIT Press. B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison-Wesley Publishing Company, USA, 1st edition, 2009. ISBN 0136072240. J. S. Culpepper and A. Moffat. Efficient set intersection for inverted indexing. ACM Transactions on Information Systems, 29(1):1–25, Dec. 2010. 176 BIBLIOGRAPHY J. R. Dabrowski and E. V. Munson. Is 100 milliseconds too fast? In Proceedings of the ACM Conference on Human Factors in Computing (CHI), pages 317–318, Seattle, WA, USA, Mar. 2001. ACM. F. Davidoff, B. Haynes, D. Sackett, and R. Smith. Evidence based medicine. British Medical Journal, 310(6987):1085–1086, Apr. 1995. PMID: 7742666. A. P. de Vries, G. Kazai, and M. Lalmas. Tolerance to irrelevance: A user-effort evaluation of retrieval systems without predefined retrieval unit. In Recherche dInformation Assiste par Ordi-nateur (Computer Assisted Information Retrieval), pages 463–473, Avignon, France, Apr. 2004. K. Dickersin, R. Scherer, and C. Lefebvre. Systematic reviews: Identifying relevant studies for systematic reviews. British Medical Journal, 309(6964):1286–1291, 1994. M. Dostert and D. Kelly. Users' stopping behaviors and estimates of recall. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 820–821, Boston, MA, USA, July 2009. ACM. S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, and D. C. Robbins. Stuff I've seen: A system for personal information retrieval and re-use. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 72–79, Toronto, Canada, Aug. 2003. ACM. M. Eccles, N. Freemantle, and J. Mason. Using systematic reviews in clinical guideline develop-ment. In M. Egger, G. D. Smith, and D. G. Altman, editors, Systematic Reviews in Health Care: Meta-Analysis in Context, chapter 21, pages 400–409. British Medical Journal Publishing Group, London, UK, 2nd edition, 2008. A. Einstein. On the method of theoretical physics. Philosophy of Science, 1(2):163–169, Apr. 1934. A. Eisinga, N. Siegfried, and M. Clarke. The sensitivity and precision of search terms in phases I, II and III of the Cochrane Highly Sensitive Search Strategy for identifying reports of randomized trials in Medline in a specific area of health care – HIV/AIDS prevention and treatment interven-tions. Health Information and Libraries Journal, 24(2):103–109, June 2007. PMID: 17584213. D. Evans. Hierarchy of evidence: A framework for ranking evidence evaluating healthcare interven-tions. Journal of Clinical Nursing, 12(1):77–84, 2003. 177 BIBLIOGRAPHY E. Fox, S. Betrabet, M. Koushik, and W. Lee. Extended Boolean models. In W. B. Frakes and R. Baeza-Yates, editors, Information Retrieval: Data Structures and Algorithms, pages 393–418. Prentice-Hall, Upper Saddle River, NJ, USA, 1992. E. A. Fox. Extending the Boolean and vector space models of information retrieval with p-norm queries and multiple concept types. PhD thesis, Cornell University, Ithaca, NY, USA, 1983a. E. A. Fox. Some considerations for implementing the SMART information retrieval system under UNIX. Technical Report TR83-560, Cornell University, Ithaca, NY, USA, Sept. 1983b. E. A. Fox. Improved retrieval using a relational thesaurus for automatic expansion of Boolean logic queries. In M. W. Evens, editor, Relational Models of the Lexicon: Representing Knowledge in Semantic Networks, pages 199–210. Cambridge University Press, Cambridge, UK, 1988. E. A. Fox and M. B. Koll. Practical enhanced Boolean retrieval: Experiences with the SMART and SIRE systems. Information Processing and Management, 24(3):257–267, 1988. E. A. Fox and S. Sharan. A comparison of two methods for soft Boolean operator interpretation in information retrieval. Technical Report TR-86-01, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA, Jan. 1986. E. A. Fox, R. K. France, E. Sahle, A. M. Daoud, and B. E. Cline. Development of a modern OPAC: From REVTOLC to MARIAN. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 248–259, Pittsburgh, PA, USA, July 1993. ACM. V. I. Frants, J. Shapiro, V. G. Voiskunskii, and I. Taksa. Boolean search: Current state and perspec-tives. Journal of the American Society for Information Science, 50(1):86–95, 1999. S. French, S. McDonald, J. McKenzie, and S. Green. Investing in updating: How do conclusions change when Cochrane systematic reviews are updated? BMC Medical Research Methodology, 5 (33), Oct. 2005. PMID: 16225692. N. Fuhr, N. Gövert, G. Kazai, and M. Lalmas. INEX: Initiative for the evaluation of XML retrieval. In Proceedings of the 1st Workshop of the INitiative for the Evaluation of XML Retrieval (INEX), Dagstuhl, Germany, Dec. 2002. Springer. 178 BIBLIOGRAPHY S. Garcia, N. Lester, F. Scholer, and M. Shokouhi. RMIT University at TREC 2006: Terabyte Track. In Proceedings of the 15th Text REtrieval Conference (TREC 2006), Gaithersburg, MD, USA, Nov. 2006. NIST Special Publication 500-272. J. M. Glanville, C. Lefebvre, J. N. V. Miles, and J. Camosso-Stefinovic. How to identify randomized controlled trials in MEDLINE: Ten years on. Journal of the Medical Library Association, 94(2): 130–136, Apr. 2006. PMID: 16636704. G. V. Glass. Primary, secondary, and meta-analysis of research. Educational Researcher, 5(10):3–8, Nov. 1976. L. A. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of user behavior in WWW search. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 478–479, Sheffield, UK, July 2004. ACM. W. R. Greiff, W. B. Croft, and H. Turtle. Computationally tractable probabilistic modeling of Boolean operators. SIGIR Forum, 31(Special Issue):119–128, July 1995. W. R. Greiff, W. B. Croft, and H. Turtle. PIC matrices: A computationally tractable class of proba-bilistic query operators. Transactions on Information Systems, 17(4):367–405, Oct. 1999. A. Gulli and A. Signorini. The indexable web is more than 11.5 billion pages. In Proceedings of the 14th International Conference on World Wide Web: Special Interest Tracks and Posters (WWW 2005), pages 902–903, Chiba, Japan, May 2005. ACM. T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, July 2003. ISBN 0387952845. D. Helmer, I. Savoie, C. Green, and A. Kazanjian. Evidence-based practice: Extending the search to find material for the systematic review. Bulletin of the Medical Library Association, 89(4): 346–52, Oct. 2001. PMID: 11837256. S. Héman. Super-scalar database compression between RAM and CPU-cache. MS thesis, Centrum voor Wiskunde en Informatica (CWI), Amsterdam, The Netherlands, July 2005. A. Herschtal and B. Raskutti. Optimising area under the ROC curve using gradient descent. In Pro-ceedings of the 21st International Conference on Machine Learning (ICML), volume 69, Banff, Alberta, Canada, July 2004. ACM. 179 BIBLIOGRAPHY W. Hersh. Information Retrieval: A Health and Biomedical Perspective. Springer, 3rd edition, Nov. 2008. ISBN 038778702X. W. Hersh, S. Price, D. Kraemer, B. Chan, L. Sacherek, and D. Olson. A large-scale comparison of Boolean vs. natural language searching for the TREC-7 Interactive Track. In Proceedings of the 7th Text REtrieval Conference (TREC 1998), pages 491–500, Gaithersburg, MD, USA, Nov. 1998. NIST Special Publication 500-242. W. R. Hersh, R. T. Bhupatiraju, L. Ross, A. M. Cohen, D. Kraemer, and P. Johnson. TREC 2004 Genomics Track overview. In Proceedings of the 13th Text REtrieval Conference (TREC 2004), pages 16–19, Gaithersburg, MD, USA, Nov. 2004. NIST Special Publication 500-261. J. R. Herskovic, L. Y. Tanaka, W. Hersh, and E. V. Bernstam. A day in the life of PubMed: Analysis of a typical day's query log. Journal of the American Medical Informatics Association, 14(2): 212–220, 2007. PMID: 17213501. J. P. T. Higgins and S. Green, editors. Cochrane Handbook for Systematic Reviews of Interventions. Version 5.0.2 [updated Sept. 2009]. The Cochrane Collaboration, 2009. Available from http: //www.cochrane-handbook.org. E. M. Hodson, C. A. Jones, G. F. M. Strippoli, A. C. Webster, and J. C. Craig. Immunoglobulins, vaccines or interferon for preventing cytomegalovirus disease in solid organ transplant recipi-ents. Cochrane Database of Systematic Reviews, Issue 2, 2007. Art. No.: CD005129. PMID: 17443573. S. Hopewell, M. Clarke, L. Stewart, and J. Tierney. Time to publication for results of clinical trials. Cochrane Database of Systematic Reviews, Issue 2, 2007. Art. No.: MR000011. PMID: 17443632. D. J. Hoppe, E. H. Schemitsch, S. Morshed, I. Tornetta, Paul, and M. Bhandari. Hierarchy of evidence: Where observational studies fit in and why we need them. The Journal of Bone and Joint Surgery, 91(Supplement 3):2–9, 2009. X. Huang, J. Lin, and D. Demner-Fushman. Evaluation of PICO as a knowledge representation for clinical questions. In Proceedings of the Annual Symposium of the American Medical Informatics Association (AMIA 2006), pages 359–363, Washington, DC, USA, Nov. 2006. AMIA. PMID: 17238363. 180 http://www.cochrane-handbook.org http://www.cochrane-handbook.org BIBLIOGRAPHY L. Hunter and K. B. Cohen. Biomedical language processing: What's beyond PubMed? Molecular Cell, 21(5):589–594, Mar. 2006. K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422–446, Oct. 2002. J. Kamps, M. Koolen, and M. Lalmas. Locating relevant text within XML documents. In Proceed-ings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 847–848, Singapore, Singapore, July 2008. ACM. N. Kando, K. Kuriyama, T. Nozue, K. Eguchi, H. Kato, and S. Hidaka. Overview of IR tasks at the first NTCIR workshop. In Proceedings of the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition, pages 11–44, Tokyo, Japan, Aug. 1999. S. Karimi, J. Zobel, S. Pohl, and F. Scholer. The challenge of high recall in biomedical systematic search. In Proceeding of the 3rd International Workshop on Data and Text Mining in Bioinfor-matics (DTMBIO 2009), pages 89–92, Hong Kong, China, Nov. 2009. ACM. S. Karimi, S. Pohl, F. Scholer, L. Cavedon, and J. Zobel. Boolean versus ranked querying for biomedical systematic reviews. BMC Medical Informatics and Decision Making, 10(58), Oct. 2010. PMID: 20937152. S. Karimi, F. Scholer, A. Clark, and S. Kharazmi. Domain expert topic familiarity and search behavior. In Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Beijing, China, July 2011. ACM. M. Kaszkiel, J. Zobel, and R. Sacks-Davis. Efficient passage ranking for document databases. ACM Transactions on Information Systems, 17(4):406–439, 1999. D. Kelly, K. Gyllstrom, and E. W. Bailey. A comparison of query and term suggestion features for interactive searching. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 371–378, Boston, MA, USA, July 2009. ACM. B. Kitchenham. Procedures for performing systematic reviews. Joint technical report, Keele Uni-versity (TR/SE-0401),Keele, UK; and NICTA (0400011T.1), Eversleigh, NSW, Australia, July 2004. 181 BIBLIOGRAPHY D. H. Kraft and W. G. Waller. A bayesian approach to user stopping rules for information retrieval systems. Information Processing and Management, 17(6):349–361, 1981. J. Kunz. Index Medicus: A century of medical citation. Journal of the American Medical Associa-tion, 241(4):387–390, Jan. 1979. PMID: 364084. J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for in-formation retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 111–119, New Orleans, LA, USA, Sept. 2001. ACM. J. H. Lee. Properties of extended Boolean models in information retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 182–190, Dublin, Ireland, July 1994. Springer. J. H. Lee. Analyzing the effectiveness of extended Boolean models in information retrieval. Tech-nical Report TR95-1501, Cornell University, Ithaca, NY, USA, Mar. 1995. J. H. Lee, W. Y. Kin, M. H. Kim, and Y. J. Lee. On the evaluation of Boolean operators in the ex-tended Boolean retrieval framework. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 291–297, Pittsburgh, PA, USA, June 1993. ACM. W. C. Lee and E. A. Fox. Experimental comparison of schemes for interpreting Boolean queries. Technical Report TR-88-27, Computer Science, Virginia Polytechnic Institute and State Univer-sity, Blacksburg, VA, USA, 1988. N. Lester, A. Moffat, W. Webber, and J. Zobel. Space-limited ranked query evaluation using adap-tive pruning. In Proceedings of the 6th International Conference on Web Information Systems Engineering, volume 4726 of Lecture Notes in Computer Science (LNCS), pages 470–477, New York, NY, USA, Nov. 2005. Springer. J. Lin and D. Demner-Fushman. The role of knowledge in conceptual retrieval: A study in the domain of clinical medicine. In Proceedings of the 29th Annual International ACM SIGIR Con-ference on Research and Development in Information Retrieval, pages 99–106, Seattle, WA, USA, Aug. 2006. ACM. 182 BIBLIOGRAPHY Z. Lu, W. J. Wilbur, J. R. McEntyre, A. Iskhakov, and L. Szilagyi. Finding query suggestions for PubMed. In Proceedings of the 2009 Annual Symposium of the American Medical Informat-ics Association (AMIA), pages 396–400, San Francisco, CA, USA, Nov. 2009. AMIA. PMID: 20351887. W. Magdy and G. J. Jones. PRES: A score metric for evaluating recall-oriented information re-trieval applications. In Proceeding of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 611–618, Geneva, Switzerland, July 2010. ACM. U. Manber and S. Wu. GLIMPSE: A tool to search through entire file systems. In Proceedings of the USENIX Winter 1994 Technical Conference, pages 23–32, San Francisco, CA, USA, Jan. 1994. H. B. Mann and D. R. Whitney. On a test of whether one of two random variables is stochastically larger than the other. The Annals of Mathematical Statistics, 18(1):50–60, Mar. 1947. D. Martinez, S. Karimi, L. Cavedon, and T. Baldwin. Facilitating biomedical systematic reviews using ranked text retrieval and classification. In Proceedings of the 13th Australasian Document Computing Symposium, pages 3–10, Hobart, Tasmania, Australia, Dec. 2008. J. McGowan and M. Sampson. Systematic reviews need systematic searchers. Journal of the Medical Library Association, 93(1):74–80, Jan. 2005. PMID: 17487451. K. A. McKibbon, N. L. Wilczynski, and R. B. Haynes. Retrieving randomized controlled trials from medline: A comparison of 38 published search filters. Health Information and Libraries Journal, 26(3):187–202, Sept. 2009. PMID: 19712211. F. McLellan. 1966 and all that – when is a literature search done? The Lancet, 358(9282):646, Aug. 2001. PMID: 11530164. D. Metzler. Beyond bags of words: Effectively modeling dependence and features in information retrieval. PhD thesis, University of Massachusetts Amherst, Amherst, MA, USA, Jan. 2007. Paper AAI3289243. D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735–750, Sept. 2004. 183 BIBLIOGRAPHY P. C. Mitchell. A note about the proximity operators in information retrieval. In Proceedings of the 1973 Meeting on Programming Languages and Information Retrieval, pages 177–180. ACM, Nov. 1973. A. Moffat and J. Zobel. Self-indexing inverted files for fast text retrieval. ACM Transactions on Information Systems, 14(4):349–379, Oct. 1996. A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Transactions on Information Systems, 27(1):1–27, Dec. 2008. D. Moher, D. J. Cook, S. Eastwood, I. Olkin, D. Rennie, and D. F. Stroup. Improving the quality of reports of meta-analyses of randomised controlled trials: The QUOROM statement. The Lancet, 354(9193):1896–1900, Nov. 1999. PMID: 10584742. D. Moher, J. Tetzlaff, A. C. Tricco, M. Sampson, and D. G. Altman. Epidemiology and reporting characteristics of systematic reviews. PLoS Medicine, 4(3):e78, Mar. 2007a. PMID: 17388659. D. Moher, A. Tsertsvadze, A. C. Tricco, M. Eccles, J. Grimshaw, M. Sampson, and N. Barrowman. A systematic review identified few methods and strategies describing when and how to update systematic reviews. Journal of Clinical Epidemiology, 60(11):1095–1104, Nov. 2007b. PMID: 17938050. D. Moher, S. Hopewell, K. F. Schulz, V. Montori, P. C. Gøtzsche, P. J. Devereaux, D. Elbourne, M. Egger, and D. G. Altman. CONSORT 2010 explanation and elaboration: Updated guidelines for reporting parallel group randomised trials. British Medical Journal, 340(c869), Mar. 2010. G. Navarro and V. Mäkinen. Compressed full-text indexes. ACM Computing Surveys, 39(1), Apr. 2007. D. W. Oard, B. Hedin, S. Tomlinson, and J. R. Baron. Overview of the TREC 2008 Legal Track. In Proceedings of the 17th Text REtrieval Conference (TREC 2008), Gaithersburg, MD, USA, Nov. 2008. NIST Special Publication 500-277. C. D. Paice. Soft evaluation of Boolean search queries in information retrieval systems. Information Technology Research Development Applications, 3(1):33–41, Jan. 1984. L. A. H. Paris and H. R. Tibbo. Freestyle vs. Boolean: A comparison of partial and exact match retrieval systems. In Information Processing and Management, volume 34, pages 175–190, Tar-rytown, NY, USA, 1998. Pergamon Press, Inc. 184 BIBLIOGRAPHY M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency-sorted indexes. Journal of the American Society for Information Science, 47:749–764, Sept. 1996. B. Piwowarski and G. Dupret. Evaluation in (xml) information retrieval: expected precision-recall with user modelling (eprum). In Proceedings of the 29th Annual International ACM SIGIR Con-ference on Research and Development in Information Retrieval, pages 260–267, Seattle, WA, USA, Aug. 2006. ACM. B. Piwowarski, P. Gallinari, and G. Dupret. Precision recall with user modeling (PRUM): Applica-tion to structured information retrieval. ACM Transactions on Information Systems, 25(1), Feb. 2007. S. Pohl and A. Moffat. Term-frequency surrogates in text similarity computations. In Proceed-ings of the 13th Australasian Document Computing Symposium, pages 3–10, Hobart, Tasmania, Australia, Dec. 2008. S. Pohl and A. Moffat. Measurement techniques and caching effects. In Proceedings of the 31st European Conference on Information Retrieval (ECIR), pages 691–695, Toulouse, France, Apr. 2009. Springer. S. Pohl, J. Zobel, and A. Moffat. Extended Boolean retrieval for systematic biomedical reviews. In B. Mans and M. Reynolds, editors, Proceedings of the 33rd Australasian Computer Science Conference, volume 102 of Conferences in Research and Practice in Information Technology (CRPIT), pages 117–125, Brisbane, Queensland, Australia, Jan. 2010. Australian Computer So-ciety. S. Pohl, A. Moffat, and J. Zobel. Efficient extended Boolean retrieval. IEEE Transactions on Knowledge and Data Engineering, 2011. To appear. J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceed-ings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275–281, Melbourne, Victoria, Australia, Aug. 1998. ACM. S. J. Puglisi, W. F. Smyth, and A. Turpin. Inverted files versus suffix arrays for locating patterns in primary memory. In Proceedings of the 13th International Symposium on String Processing and Information Retrieval (SPIRE), volume 4209 of Lecture Notes in Computer Science (LNCS), pages 122–133, Glasgow, Scotland, UK, Oct. 2006. Springer. 185 BIBLIOGRAPHY T. Radecki. Fuzzy set theoretical approach to document retrieval. Information Processing and Management, 15(5):247–259, 1979. B. A. N. Ribeiro and R. Muntz. A belief network model for IR. In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 253–260, Zurich, Switzerland, Aug. 1996. ACM. M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In IEEE International Conference on Neural Networks, volume 1, pages 586– 591, San Francisco, CA , USA, Mar. 1993. S. Robertson, S. Walker, M. M. Beaulieu, M. Gatford, and A. Payne. Okapi at TREC-4. In Proceed-ings of the 4th Text REtrieval Conference (TREC 1995), pages 73–96, Gaithersburg, MD, USA, Nov. 1995. NIST Special Publication 500-236. S. E. Robertson. The probability ranking principle in IR. Journal of Documentation, 33(4):294–304, 1977. S. E. Robertson and I. Soboroff. The TREC 2002 Filtering Track report. In Proceedings of the 11th Text REtrieval Conference (TREC 2002), Gaithersburg, MD, USA, 2002. Online Proceedings http://trec.nist.gov/pubs/trec11/t11_proceedings.html. S. E. Robertson and C. L. Thompson. Weighted searching: The CIRT experiment. In Informatics 10: Prospects for Intelligent Retrieval, pages 75–89, King's College, Cambridge, UK, Mar. 1990. S. E. Robertson, C. J. van Rijsbergen, and M. F. Porter. Probabilistic models of indexing and search-ing. In Proceedings of the 3rd Annual ACM SIGIR Conference on Research and Development in Information Retrieval, pages 35–56, Cambridge, England, June 1981. Butterworths. S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In Proceedings of the 3rd Text REtrieval Conference (TREC-3), pages 109–126, Gaithersburg, MD, USA, Nov. 1994. NIST Special Publication 500-225. S. E. Robertson, S. Walker, and M. Hancock-Beaulieu. Okapi at TREC-7: Automatic Ad hoc, Filter-ing, VLC and Interactive Track. In Proceedings of the 7th Text REtrieval Conference (TREC-7), pages 253–261, Gaithersburg, MD, USA, Nov. 1998. NIST Special Publication 500-242. 186 http://trec.nist.gov/pubs/trec11/t11_proceedings.html BIBLIOGRAPHY J. J. Rocchio. Relevance feedback in information retrieval. In The SMART Retrieval System – Experiments in Automatic Document Processing, pages 313–323, Englewood Cliffs, NJ, USA, 1971. Prentice Hall, Inc. W. Rosenberg and A. Donald. Evidence based medicine: An approach to clinical problem-solving. British Medical Journal, 310(6987):1122–1126, Apr. 1995. P. Royle and N. Waugh. A simplified search strategy for identifying randomised controlled trials for systematic reviews of health care interventions: A comparison with more exhaustive strategies. BMC Medical Research Methodology, 5(23), July 2005. I. Ruthven. Re-examining the potential effectiveness of interactive query expansion. In Proceed-ings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 213–220, Toronto, Canada, July 2003. ACM. L. Rychetnik, P. Hawe, E. Waters, A. Barratt, and M. Frommer. A glossary for evidence based public health. Journal of Epidemiology and Community Health, 58(7):538–545, July 2004. G. Salton. The SMART Retrieval System – Experiments in Automatic Document Processing. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, Jan. 1971. G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–523, 1988. G. Salton and M. E. Lesk. The SMART automatic document retrieval systems – An illustration. Communications of the ACM, 8(6):391–398, June 1965. G. Salton and M. E. Lesk. Computer evaluation of indexing and text processing. Journal of the ACM, 15(1):8–36, Jan. 1968. G. Salton and E. Voorhees. Automatic assignment of soft Boolean operators. In Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 54–69, Montreal, Canada, June 1985. ACM. G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620, Nov. 1975. G. Salton, E. A. Fox, C. Buckley, and E. M. Voorhees. Boolean query formulation with relevance feedback. Technical Report TR83-539, Cornell University, Ithaca, NY, USA, Jan. 1983a. 187 BIBLIOGRAPHY G. Salton, E. A. Fox, and E. M. Voorhees. Advanced feedback methods in information retrieval. Technical Report TR83-570, Cornell University, Ithaca, NY, USA, Aug. 1983b. G. Salton, E. A. Fox, and H. Wu. Extended Boolean information retrieval. Communications of the ACM, 26(12):1022–1036, Dec. 1983c. G. Salton, E. M. Voorhees, and E. A. Fox. A comparison of two methods for Boolean query relevance feedback. Information Processing and Management, 20(5-6):637–651, July 1984. M. Sampson and J. McGowan. Errors in search strategies were identified by type and frequency. Journal of Clinical Epidemiology, 59(10):1057.e1–1057.e9, Oct. 2006. PMID: 16980145. M. Sampson, N. J. Barrowman, D. Moher, T. P. Klassen, B. Pham, R. Platt, P. D. S. John, R. Viola, and P. Raina. Should meta-analysts search Embase in addition to Medline? Journal of Clinical Epidemiology, 56(10):943–955, Oct. 2003. PMID: 15718124. M. Sampson, N. J. Barrowman, D. Moher, T. J. Clifford, R. W. Platt, A. Morrison, T. P. Klassen, and L. Zhang. Can electronic search engines optimize screening of search results in systematic reviews: An empirical study. BMC Medical Research Methodology, 6(7), Feb. 2006a. PMID: 16504110. M. Sampson, L. Zhang, A. Morrison, N. J. Barrowman, T. J. Clifford, R. W. Platt, T. P. Klassen, and D. Moher. An alternative to the hand searching gold standard: Validating methodological search filters using relative recall. BMC Medical Research Methodology, 6(33), July 2006b. PMID: 16848895. M. Sampson, J. Mcgowan, C. Lefebvre, D. Moher, and J. Grimshaw. PRESS: Peer review of elec-tronic search strategies. Technical report, Ottawa: Canadian Agency for Drugs and Technologies in Health, Ottawa, Ontario, Canada, Jan. 2008a. M. Sampson, J. McGowan, J. Tetzlaff, E. Cogo, and D. Moher. No consensus exists on search reporting methods for systematic reviews. Journal of Clinical Epidemiology, 61(8):748–754, Aug. 2008b. PMID: 18586178. M. Sampson, J. McGowan, E. Cogo, J. Grimshaw, D. Moher, and C. Lefebvre. An evidence-based practice guideline for the peer review of electronic search strategies. Journal of Clinical Epidemi-ology, 62(9):944–952, 2009. PMID: 19230612. 188 BIBLIOGRAPHY M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247–375, 2010. I. Savoie, D. Helmer, C. J. Green, and A. Kazanjian. Beyond Medline. International Journal of Technology Assessment in Health Care, 19(1):168–178, 2003. F. Scholer, H. E. Williams, J. Yiannis, and J. Zobel. Compression of inverted indexes for fast query evaluation. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 222–229, Tampere, Finland, Aug. 2002. ACM. P. Shekelle, M. P. Eccles, J. M. Grimshaw, and S. H. Woolf. When should clinical guidelines be updated? British Medical Journal, 323(7305):155–157, 2001. K. G. Shojania, M. Sampson, M. T. Ansari, J. Ji, S. Doucette, and D. Moher. How quickly do systematic reviews go out of date? A survival analysis. Annals of Internal Medicine, 147(4): 224–233, Aug. 2007. PMID: 17638714. M. E. Smith. Aspects of the P-Norm Model of Information Retrieval: Syntactic Query Generation, Efficiency, and Theoretical Properties. PhD thesis, Cornell University, Ithaca, NY, USA, May 1990. A. Spink, D. Wolfram, M. B. J. Jansen, and T. Saracevic. Searching the web: The public and their queries. Journal of the American Society for Information Science and Technology, 52(3):226–234, 2001. M. Staples and M. Niazi. Experiences using systematic review guidelines. Journal of Systems and Software, 80(9):1425–1437, Sept. 2007. T. Strohman and W. B. Croft. Efficient document retrieval in main memory. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 175–182, Amsterdam, The Netherlands, July 2007. ACM. T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In Proceed-ings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 219–225, Salvador, Bahia, Brazil, Aug. 2005. ACM. M. Theobald, G. Weikum, and R. Schenkel. Top-k query evaluation with probabilistic guarantees. In Proceedings of the 30th International Conference on Very Large Data Bases (VLDB), pages 648–659, Toronto, Ontario, Canada, Aug. 2004. VLDB Endowment. 189 BIBLIOGRAPHY D. Tunkelang. Faceted Search. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2009. H. Turtle. Natural language vs. Boolean query evaluation: A comparison of retrieval performance. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Devel-opment in Information Retrieval, pages 212–220, Dublin, Ireland, July 1994. Springer. H. Turtle and W. B. Croft. Inference networks for document retrieval. In Proceedings of the 13th Annual International ACM SIGIR Conference on Research and Development in Information Re-trieval, pages 1–24, Brussels, Belgium, Sept. 1990. ACM. H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans-actions on Information Systems, 9(3):187–222, July 1991. H. Turtle and J. Flood. Query evaluation: Strategies and optimizations. Information Processing and Management, 31(6):831–850, Nov. 1995. H. R. Turtle. Inference Networks for Document Retrieval. PhD thesis, University of Massachusetts Amherst, Amherst, MA, USA, Feb. 1991. V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995. ISBN 0-387-94559-8. E. von Elm, P. Schoettker, I. Henzi, J. Osterwalder, and B. Walder. Pre-hospital tracheal intubation in patients with traumatic brain injury: systematic review of current evidence. British Journal of Anaesthesia, 103(3):371–386, Sept. 2009. PMID: 19648153. E. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of Cross-Language Information Retrieval Systems, volume 2406 of Lecture Notes in Computer Science, pages 143– 170. Springer, 2002. E. M. Voorhees. The TREC-8 Question Answering Track report. In Proceedings of the 8th Text REtrieval Conference (TREC-8), pages 77–82, Gaithersburg, MD, USA, Nov. 1999. Online pub-lication http://trec.nist.gov/pubs/trec8/t8_proceedings.html. E. M. Voorhees and D. K. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005. ISBN 0262220733. B. C. Wallace, K. Small, C. E. Brodley, and T. A. Trikalinos. Active learning for biomedical citation screening. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge 190 http://trec.nist.gov/pubs/trec8/t8_proceedings.html BIBLIOGRAPHY Discovery and Data Mining, pages 173–181, Washington, DC, USA, July 2010a. ACM. PMID: 20102628. B. C. Wallace, T. Trikalinos, J. Lau, C. Brodley, and C. Schmid. Semi-automated screening of biomedical citations for systematic reviews. BMC Bioinformatics, 11(55), 2010b. PMID: 20102628. W. G. Waller and D. H. Kraft. A mathematical model of a weighted Boolean retrieval system. Information Processing and Management, 15(5):235–245, 1979. H. E. Williams, J. Zobel, and D. Bahle. Fast phrase querying with combined indexes. ACM Trans-actions on Information Systems, 22(4):573–594, Oct. 2004. I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann, May 1999. ISBN 1-55860-570-3. A. Yoshii, D. A. Plaut, K. A. McGraw, M. J. Anderson, and K. E. Wellik. Analysis of the reporting of search strategies in Cochrane systematic reviews. Journal of the Medical Library Association, 97(1):21–29, Jan. 2009. PMID: 19158999. C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 334–342, New Orleans, LA, USA, Sept. 2001. ACM. L. Zhang, I. Ajiferuke, and M. Sampson. Optimizing search strategies to identify randomized con-trolled trials in MEDLINE. BMC Medical Research Methodology, 6(23), May 2006. Y. Zhang, L. Park, and A. Moffat. Click-based evidence for decaying weight distributions in search effectiveness metrics. Information Retrieval, 13(1):46–69, 2010. Y. Zhu, L. Zhao, J. Callan, and J. Carbonell. Structured queries for legal search. In Proceedings of The 16th Text REtrieval Conference (TREC 2007), Gaithersburg, MD, USA, Nov. 2007. NIST Special Publication 500-274. J. Zobel and A. Moffat. Exploring the similarity space. SIGIR Forum, 32:18–34, Apr. 1998. J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys, 38(2):6, July 2006. 191 BIBLIOGRAPHY J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted files versus signature files for text indexing. ACM Transactions on Database Systems, 23(4):453–490, Dec. 1998. J. Zobel, A. Moffat, and L. Park. Against recall: Is it persistence, cardinality, density, coverage, or totality? SIGIR Forum, 43(1):3–15, June 2009. 192 Minerva Access is the Institutional Repository of The University of Melbourne Author/s: POHL, STEFAN Title: Boolean and ranked information retrieval for biomedical systematic reviewing Date: 2012 Citation: Pohl, S. (2012). Boolean and ranked information retrieval for biomedical systematic reviewing. PhD thesis, Department of Computer Science and Software Engineering, The University of Melbourne. Persistent Link: http://hdl.handle.net/11343/37031 File Description: Boolean and ranked information retrieval for biomedical systematic reviewing Terms and Conditions: Terms and Conditions: Copyright in works deposited in Minerva Access is retained by the copyright owner. The work may not be altered without permission from the copyright owner. Readers may only download, print and save electronic copies of whole works for their own personal non-commercial use. Any use that exceeds these limits requires permission from the copyright owner. Attribution is essential when quoting or paraphrasing from these works. 1 Introduction 2 Background & General Concepts 2.1 Information Retrieval Models 2.1.1 Boolean Retrieval 2.1.2 Ranked Keyword Retrieval 2.1.3 Ranking with Structured Queries 2.2 Efficient Implementation 2.2.1 Inverted Indexes 2.2.2 Query Processing 2.3 Information Retrieval Evaluation 2.3.1 Framework 2.3.2 Effectiveness Measures 2.3.3 Efficiency Measures 2.4 Evidence-Based Medicine 2.4.1 Systematic Reviews 2.4.2 Current Retrieval Methods 2.5 Summary 3 Evaluation Methodology 3.1 Data Sets 3.1.1 Document Collections 3.1.2 Query Sets 3.2 Implementations 3.2.1 Indexing 3.2.2 Query Processing 3.3 Evaluation 3.4 Summary 4 Ranking for Systematic Review Search 4.1 Motivation 4.2 Ranking Approaches 4.2.1 Ranking Boolean Result Sets 4.2.2 Ranked Keyword Retrieval 4.2.3 Ranking with Structured Queries 4.2.4 Summary 4.3 Ranking with the p-Norm EBR Model 4.4 Experiments 4.4.1 Experimental Setup 4.4.2 Reproducibility 4.4.3 Ranking Effectiveness 4.4.4 Relative Performance 4.5 Summary 5 Efficient Query Processing 5.1 Implementation Approaches 5.2 A Query Processing Method for p-Norm Queries 5.2.1 Document Scoring 5.2.2 Scoring Fewer Documents 5.2.3 Short-Circuiting Evaluation 5.3 Experiments 5.3.1 Experimental Setup 5.3.2 Simulations 5.3.3 Timings 5.4 Summary 6 p-Norm Query Optimization 6.1 Related Work 6.2 A Weighting Method for p-Norm Queries 6.2.1 Challenges 6.2.2 Weight Training 6.2.3 Efficiency 6.3 Experiments 6.3.1 Experimental Setup 6.3.2 Generalization & Effectiveness 6.3.3 Impact on Rankings 6.3.4 Term Importance 6.3.5 Discussion 6.3.6 Other approaches 6.4 Summary 7 Conclusions Bibliography Log in to post comments
Comments
Submitted by Elizabeth Wood ... on
Mon, 2015-12-21 09:37
We have finished implementing the ADHD case and control algorithms and have a set of 80 cases, and 1580 controls. Baed on chart reviews of 98 potential cases defined by the algorithm, 80 cases were confirned for a PPV of 81.6%. Based on chart review of a random 30 cases defined by the control algorithm, 29 controls were confirmed for a PPV of 96.7%. We can send you a dataset with eMERGE ID, case/control status, race/ethnicity, and gender. Are there other variables that you need?  Should we submit this file on the PheKB site, or send it to you by email.  Beth
 
An algorithm to identify inpatients who have had an acute episode of drug induced liver injury (DILI).
Summary of drug-induced liver injury algorithm
Inclusion criteria
A. Suspect DILI? (NOTE: baseline population is institution specific.  See institution implementation details)
1.     Liver injury AND Exposure to drug (NOTE: medications are institution specific. See institution implementation details)
2.     Temporal relationship of exposure to drug and liver injury diagnosis.
Owner Phenotyping Groups: 
Data Modalities and Methods Used
Owner Phenotyping Groups
July 8, 2016, Cincinnati Children's Hospital Medical Center
A new online open-access database has been developed by scientists to allow the clinical responses of more than 5 million patients to all FDA-approved drugs to be used to identify unexpected clinical harm, benefits and alternative treatment choices for individual patients, according to a study appearing July 8 in Nature Biotechnology.
Developed by scientists in the Division of Biomedical Informatics and the Clinical and Translational Sciences Program at Cincinnati Children's Hospital Medical Center, the database has the potential to help reduce negative side effects from prescription drugs and identify opportunities to reposition existing drugs for new uses, report the study's authors.
Calling their new database AERSMine, researchers said the tool allows anyone from physicians to the general public to rapidly find, combine and analyze the growing volume of drug information stored in the U.S. Food and Drug Administration's Adverse Reporting System (FAERS).
"AERSMine offers an open aperture approach that can reveal unexpectedly better or worse clinical outcomes associated with different drug regimens for some groups of patients, and to facilitate the ultimate goal of protecting patients by improving therapeutic selections and monitoring strategies," said Mayur Sarangdhar, PhD, study first author and a research associate in the Division of Biomedical Informatics at Cincinnati Children's. "It also conserves valuable therapeutics by minimizing harmful interaction choices."
Although the FDA's database offers an extensive array of reports and related data on the clinical use of drugs and adverse effects, study authors said that current methods for information retrieval and analysis are difficult -running the risk of losing statistical power and the ability to detect important differential effects.
AERSMine is designed for easy use and supports analysis of millions of clinical records containing details on drug exposures, disease indications, and clinical outcomes. Researchers said the tool is expected to strengthen access to and analysis of drug safety and effectiveness data. It also has the potential to discover improved uses of individual drugs or drug combinations and develop novel treatments for diseases.
"One of the capabilities that makes AERSMine different from any other clinical data mining system is its ability to use knowledge frameworks - ontologies -to form the groupings of patients, medications, and outcomes and gain what we believe is an unprecedented power to explore and identify both unexpectedly negative and positive drug effects. Doing this has the potential to uncover new uses for drugs and drug regimen combinations," said Bruce Aronow, PhD, senior study author and co-director of the Computational Medicine Center at Cincinnati Children's.
Using the database
The researchers evaluated the effectiveness and utility of the data mining tool by running a series of analyses involving both known side effects as well as more complex scenarios focused on improved use of three important classes of drugs and clinical disorders: 1) lithium - used to treat manic depression/bipolar disorder; 2) anti-tumor necrosis factor (anti-TNF) drugs - used to treat inflammatory conditions such as rheumatoid arthritis; 3) NSAIDs - non-steroidal anti-inflammatory drugs used for pain management.
Their analysis of lithium, for example, showed that 22,575 patients had used lithium with a total of 4,180 adverse drug events . AERSMine-dissected data shows that 327 adverse events significantly correlated with patients that use lithium, including aggression, anger, suicidal tendencies, tremors, irritability, etc., have a significantly reduced rate of occurrence in patients taking angiotensin receptor blocks (ARBs, usually taken to control hypertension). The intriguing possibility suggested by AERSMine analysis then is that ARBs could result in a reduction of these sometimes lethal outcomes of bipolar patients taking lithium.
When the authors studied the use of NSAIDs for arthritis, chronic pain, etc., they were able to see differential rates of adverse clinical events depending on if people used propionic acid derivatives (like ibuprofen) or so-called cox-2 inhibitors known as coxibs, and patient groups for whom the risks of NSAID adverse events are much lower than others (for example patients with chronic arthritis).
"Next generation functions of AERSMine, and part of what we believe provides its underlying power is based on the functions of networks of genes that link the co-occurring clinical phenotypes and drug mechanisms of action to the functions of biological systems," said Anil Jegga, DVM, MRes, study co-author and a computational biologist in the Division of Biomedical Informatics. "This allows the molecular basis for drug-related adverse events to be detected, leveraged, and used to identify opportunities for repositioning drugs for new clinical uses."
DEVELOPMENT AND PERFORMANCE OF TEXT-MINING ALGORITHMS TO EXTRACT SOCIOECONOMIC STATUS FROM DE-IDENTIFIED ELECTRONIC HEALTH RECORDS BRITTANY M. HOLLISTER Vanderbilt Genetics Institute, Vanderbilt University, 519 Light Hall, 2215 Garland Ave. South Nashville, TN, 37232, USA Email: Brittany.M.Hollister@Vanderbilt.edu NICOLE A. RESTREPO Institute for Computational Biology, Department of Epidemiology and Biostatistics, Case Western Reserve University, Wolstein Research Building, 2103 Cornell Road, Cleveland, OH 44106, USA Email: nrestrepo@case.edu ERIC FARBER-EGER Vanderbilt Institute for Clinical and Translational Research, Vanderbilt University Medical Center, 2525 West End Avenue, Suite 600, Nashville, TN 37203, UA Email: eric.h.farber-eger@vanderbilt.edu DANA C. CRAWFORD Institute for Computational Biology, Department of Epidemiology and Biostatistics, Case Western Reserve University, Wolstein Research Building, 2103 Cornell Road, Suite 2527, Cleveland, OH 44106, USA Email: dana.crawford@case.edu MELINDA C. ALDRICH† Department of Thoracic Surgery and Division of Epidemiology, Vanderbilt University Medical Center, 1313 21st Avenue South, 609 Oxford House, Nashville, TN 37232, USA Email: melinda.aldrich@vanderbilt.edu AMY NON† Department of Anthropology, University of California, San Diego, 9500 Gilman Drive #0532 La Jolla, CA 92093, USA Email: alnon@ucsd.edu † Co-Senior authors Pacific Symposium on Biocomputing 2017 230 mailto:Brittany.M.Hollister@Vanderbilt.edu mailto:nrestrepo@case.edu mailto:eric.h.farber-eger@vanderbilt.edu mailto:dana.crawford@case.edu mailto:melinda.aldrich@vanderbilt.edu mailto:wspc@wspc.com Socioeconomic status (SES) is a fundamental contributor to health, and a key factor underlying racial disparities in disease. However, SES data are rarely included in genetic studies due in part to the difficultly of collecting these data when studies were not originally designed for that purpose. The emergence of large clinic-based biobanks linked to electronic health records (EHRs) provides research access to large patient populations with longitudinal phenotype data captured in structured fields as billing codes, procedure codes, and prescriptions. SES data however, are often not explicitly recorded in structured fields, but rather recorded in the free text of clinical notes and communications. The content and completeness of these data vary widely by practitioner. To enable gene-environment studies that consider SES as an exposure, we sought to extract SES variables from racial/ethnic minority adult patients (n=9,977) in BioVU, the Vanderbilt University Medical Center biorepository linked to de-identified EHRs. We developed several measures of SES using information available within the de-identified EHR, including broad categories of occupation, education, insurance status, and homelessness. Two hundred patients were randomly selected for manual review to develop a set of seven algorithms for extracting SES information from de-identified EHRs. The algorithms consist of 15 categories of information, with 830 unique search terms. SES data extracted from manual review of 50 randomly selected records were compared to data produced by the algorithm, resulting in positive predictive values of 80.0% (education), 85.4% (occupation), 87.5% (unemployment), 63.6% (retirement), 23.1% (uninsured), 81.8% (Medicaid), and 33.3% (homelessness), suggesting some categories of SES data are easier to extract in this EHR than others. The SES data extraction approach developed here will enable future EHR-based genetic studies to integrate SES information into statistical analyses. Ultimately, incorporation of measures of SES into genetic studies will help elucidate the impact of the social environment on disease risk and outcomes. 1. Introduction 1.1. Socioeconomic status and health Socioeconomic status (SES) is a major determinant of variation in health outcomes worldwide1. SES is typically defined as a combination of income or wealth, educational achievement, and occupation2,3 and be can assessed at the individual, household, or neighborhood level. Health outcomes within the United States, ranging from cancer to hypertension, vary by socioeconomic levels, regardless of how they are measured4. Multiple measures of SES have been previously associated with health outcomes, including income5, years of education6,7, occupational prestige2,8,9, insurance coverage10, and homelessness11. SES likely affects health through various pathways including access to healthcare services, knowledge of health behaviors, exposure to environmental stressors and hazards, limited financial resources, and social support2. The relationship between SES and health is also highly entangled with race/ethnicity, as SES may covary with race and contribute in part to the existence of racial disparities in health4,12. Though these pathways are difficult to distinguish and could affect different populations to varying degrees, it is important to consider SES as a representation of these potential pathways in studies of human health. Despite the overwhelming evidence that SES affects health outcomes, SES measures are often not included in genetic studies of disease. Neglect of SES data may be due to a lack of available SES information in existing cohorts, as well as the additional time and resources it takes to collect SES data in new studies. Despite these difficulties, it is vital to include measurements of SES in genetic association studies of racial disparities in health. In addition to the possible confounding that may occur due to the association of race/ethnicity with both SES and health13, SES has the potential to modify the effect of genetic variants on health outcomes14. Therefore, the etiology of disease is likely to be misunderstood without the inclusion of SES data in association studies. Although prior genetic association studies have found some gene variants that may explain a small Pacific Symposium on Biocomputing 2017 231 portion of racial disparities in disease prevalence and risk15, SES factors are likely to play an even larger role in racial health disparities6,7. 1.2. SES data within electronic health records The use of electronic health records (EHRs) for research purposes is becoming increasingly prevalent. With the announcement of the Precision Medicine Initiative and its goal of recruiting one million participants with biological and EHR data, the research use of EHRs is anticipated to increase16. EHRs provide an attractive resource for biomedical researchers for many reasons, including their rich phenotypic and longitudinal data, as well as the lower cost of participant recruitment versus a traditional prospective cohort study. Additionally, clinical biobanks that contain biological samples linked to EHRs are becoming an invaluable resource for conducting genetic epidemiology studies. Despite the potential for EHRs in research settings, these clinical data repositories currently have noted deficits in the availability and completeness of important social and environmental data17, including SES, that are known to contribute independently to health status and could modify genetic effects18. Recognizing the importance of formally and consistently capturing social and behavioral measures in the EHR, the Institute of Medicine (IOM) recently recommended SES measures, specifically educational attainment, financial resource strain, and neighborhood median household income be included in the EHR19. The committee also recommended that a plan be developed by the NIH to expand the research use of EHRs to include social and behavioral data19. Adoption of these recommendations will take time, and may not be universal across medical centers; therefore, there is a need to develop approaches and methods to access existing unstructured SES data within the EHR for research purposes. SES data are almost entirely found within the free text clinical notes from providers and the clinical communications between providers. Currently, there are no published algorithms available to extract SES data from EHRs. In this study, we developed an approach for extracting available SES information from the free text of a de-identified EHR. These algorithms will facilitate the immediate extraction of key SES information from clinical biobanks for incorporation into future biomedical research. 1.3. BioVU BioVU is a DNA biobank of the Vanderbilt University Medical Center (VUMC) linked to de-identified EHRs. DNA samples are extracted from discarded blood samples drawn for routine clinical care20. DNA samples are linked to the Synthetic Derivative (SD), the de-identified version of the VUMC EHR, by a unique study ID. Medical records within the SD are scrubbed of all HIPAA identifiers such as names, locations, zip codes, and social security numbers. Dates within each SD record are shifted to prevent re-identification of the records. Date shifting is consistent within a single patient's record. As previously described21, data from BioVU are de-identified in accordance with provisions of Title 45, Code of Federal Regulations, part 46 (45 CFT 46); consequently, this study is considered non-human subjects research by the Vanderbilt University Institutional Review Board. Pacific Symposium on Biocomputing 2017 232 2. Methods 2.1. Population The study population included all racial/ethnic minority patients >18 years old participating in BioVU as of 201122. The EHRs used for the development of the algorithms were updated in 2015 to include current information. Race/ethnicity is administratively reported in BioVU and strongly correlated with genetic ancestry23,24. The majority (81%) of patients in the dataset are black individuals with an average age of 50 years (Table 1). The mean number of clinic visits within a patient's EHR record is 40.45 visits, and the mean number of days between patients' first and last visit within the EHR is 2,340 days (Table 1). 2.2. Development of algorithms We sought to develop algorithms to extract SES data from structured and unstructured data in the de-identified EHRs. We developed seven algorithms for the extraction of SES information including education level, occupation, unemployment, retirement, insurance status, Medicaid status, and homelessness (Table 2). The initial development of the SES algorithms began with a manual review of both structured and unstructured data within the de-identified EHR of 200 randomly selected patients within this minority population dataset to identify the following: 1) the categories of SES information most frequently mentioned, 2) where in the EHR this information is noted, and 3) the semantic language used by clinical providers for socioeconomic information (Figure 1). The manual review revealed that the SES data were found exclusively within the unstructured free text of the clinical notes, social history, and clinical communications of this EHR. It was also noted that the most frequently mentioned semantic categories were employment, education, insurance status, and homelessness, and thus these categories were chosen for extraction. Semantic tags for each category were selected if they appeared more than once within the 200 development records. 2.2.1. Employment Employment information was extracted using three different algorithms designed to capture data on occupation, unemployment, and retirement. The occupation algorithm extracts the occupation Table 1. Vanderbilt BioVU racial/ethnic minority population characteristics Characteristic n= 9,977 Sex Male Female 3,568 (36%) 6,409 (64%) Race/ethnicity Black Hispanic Asian 8,078 (81%) 1,049 (10.5%) 850 (8.5%) Age (mean, years ± SD) 49.8 ± 18.1 Number of clinic visits (mean ± SD) 40.5 ± 55.0 Number of days between visits (mean ± SD) 2,340 ± 1,793.1 Pacific Symposium on Biocomputing 2017 233 mentioned in a patient's record and translates it to an occupational prestige score (scale of 0-100). This score represents how well-respected an occupation is within a society (i.e., subjective socioeconomic position). Occupational prestige scores were determined from a National Opinion Research Center (NORC) survey where respondents were asked to rank occupations according to their prestige25. The occupation tags utilized for the occupation algorithm were adopted from the most recent NORC report. The algorithm's occupation tags were shortened to 678 occupations from the original NORC list of 860 occupations given that some of the occupations were highly specific with repetitive occupational prestige scores. As an example, "teacher, elementary school" and "teacher, secondary school" were collapsed to "teacher." We next used the occupation algorithm to search the unstructured data of the original 200 patients for the initial occupation tags. This search identified a large number of false positives, where the algorithm tagged occupation-related words that were not indicative of the patient's occupation. In order to filter these false positives, additional prefix language such as "works as," "is a/n," "employed" was added to a subset of occupations to filter out non-relevant terms, which greatly improved the algorithm. Unemployment data were extracted using semantic tags for unemployment (e.g., "unemployed," "does not work," "hasn't worked since"). The unemployment algorithm was then tested on the unstructured data from the 200 records used for development, and a high number of false positives were returned. These false positives were often in reference to medications. Therefore the tags "if this does not work" and "if that does not work" were excluded to filter false positives. Unemployment was classified as ever/never (Table 2). Retirement was also extracted from the EHR using the tag "retired" and classified as ever/never (Table 2). 2.2.2. Education The education algorithm was designed to assign education level to a patient based on the highest education achieved and recorded in the EHR. Education levels were assigned to each relevant tag Table 2. Variables extracted by socioeconomic status (SES) algorithms applied to de-identified electronic health records Semantic category Format of algorithm output Occupational prestige 0-100 Unemployment Ever/never Retirement Ever/never Education -Never attended -Less than high school -High school graduate/GED -Associate's degree -Bachelor's degree -Master's degree -Professional degree -Doctoral degree Uninsured Ever/never Medicaid Ever/ never Homelessness Ever/never Pacific Symposium on Biocomputing 2017 234 word or phrase found in the unstructured text of the EHR (Table 2). Sixty-two semantic tags were utilized and the highest level of education was determined for each patient. These tags were exclusive to an assigned education level. For example, the high school degree category of education level included tags such as "high school graduate" and "completed 12th grade," while the bachelor's degree category included terms such as "BS degree" and "completed college." The levels of education were based on U.S. census definitions with one modification such that all grade levels below high school graduate were collapsed into a "less than high school" category. We searched through the unstructured text of the 200 records used for development to determine if further filtering or modification was needed. Fifteen additional tags were used to filter false positive results related to types of medical education (e.g. "diet education," "dialysis education") and Vanderbilt Medical School students (e.g., "medical student," "pharmacy student," "student nurse"). Figure 1. Overview of the development process for the SES algorithms 2.2.3. Insurance status The extraction process for insurance status required two algorithms. The first algorithm was used to determine if there was any time point in the EHR when the patient did not have insurance based on the presence of five semantic tags (Table 2). These tags included "no insurance" and "does not have insurance" and excluded some language that was used in a standard discharge letter and Pacific Symposium on Biocomputing 2017 235 therefore appeared frequently in the EHR. A second insurance algorithm extracted Medicaid information using specific phrases or keywords such as "Medicaid" and "TennCare" and was classified as ever/never in order to determine if a patient was ever on Medicaid in their EHR (Table 2). 2.2.4. Homelessness Homelessness information was extracted using the tags "homeless" and "shelter" among the 200 development EHRs. After this search, several false positives were returned relating to patients who worked or volunteered at homeless shelters. Therefore, exclusion tags were added such as "volunteer at homeless shelter," "works at homeless shelter," "works with homeless," and "animal shelter." Homelessness was classified as ever/never (Table 2). 2.3. Evaluation of algorithm performance To evaluate the performance of these SES algorithms, results were compared to findings from a manual review of 50 randomly selected patients. These 50 individuals were selected using random sampling without replacement. Two independent reviewers manually reviewed the clinical record of each patient and any discrepancies were resolved by discussion between the two reviewers. Comparison of results from the two independent reviewers was quantified using percent positive agreement, percent negative agreement, and kappa statistics for each of the seven categories and subcategories: education level, occupation, unemployment, retirement, uninsured, Medicaid, and homelessness. The manual review of 50 records was then compared to the algorithm results for each of the seven categories and subcategories. Sensitivity, specificity, and positive predictive value were estimated. The chi-square statistic was used to determine if the algorithms performed differently in different populations. 3. Results 3.1. Population characteristics Among the total study population (n=9,977), we were able to extract at least one type of SES information from 8,282 (83.0%) individuals. We extracted education information for 3,780 individuals and occupation information for 7,296 individuals (Table 3). For the remaining Table 3. Percent of records within the study population with algorithm-identified SES characteristics Characteristics Race Black (n=8,078) Hispanic (n=1,049) Asian (n=850) Total (n=9,977) % with occupation 76.0 57.1 65.4 73.1 % unemployed 21.4 13.0 13.4 19.8 % retired 19.8 4.9 11.2 17.5 % with education 39.1 28.7 37.9 37.9 % uninsured 19.5 15.6 11.5 18.4 % on Medicaid 20.5 13.9 7.9 18.7 % homeless 3.7 1.3 1.0 3.2 Pacific Symposium on Biocomputing 2017 236 categories, we were able to determine if an individual was unemployed, retired, uninsured, on Medicaid, or homeless at any point in his or her record. Of the total population for which we were able to extract SES data (n=8,282), 1,978 individuals were unemployed, 1,742 individuals were retired, 1,839 individuals were uninsured, 1,865 were on Medicaid, and 318 were homeless at least one time in their EHR (Table 3). For each of the seven categories, the algorithms returned SES information for a higher percentage of black patients than Hispanic or Asian patients (p<0.00001). The five most frequently extracted occupations among those having occupation information (n=7,296) were manager, nurse, Army, manufacturer, and restaurant employee. Within the population with education information (n=3,780), the vast majority of individuals had a high school degree (n=2,066), followed by individuals without a high school degree (n=492), and individuals with a bachelor's degree (n=446). 3.2. Algorithm Performance Prior to evaluating algorithm performance, the manual review results from the randomly selected records of 50 patients were compared between the two reviewers and any conflicts were resolved. The percent positive agreement between reviewers ranged from 98.0% to 100.0% and the percent negative agreement ranged from 94.7% to 100.0%. The Kappa statistic between reviewers ranged from 0.94 to 1.0. Once all reviewer discrepancies were resolved, the manual review results were used as the gold standard and compared to the algorithm results. All the algorithms, with the exception of occupation, had very high specificity levels >78%. The lower specificity for occupation (40%) is due to six of the ten individuals who did not have occupation information (as identified by manual review) but were identified as having occupation information by the algorithm. All the algorithms also had high sensitivity levels (above 70%), with the exception of education level (66.7%) (Table 4). The lower sensitivity for education is driven by eight individuals who have an education level that was identified by manual review but not by the algorithm. The lower sensitivity for unemployment is due to the six individuals who were identified as unemployed by manual review but not by the algorithm. PPV values across the algorithms ranged from 23.1%-87.5%. The lower PPV for the retirement algorithm (63.6%) is due to the four individuals identified as retired by the Table 4. Comparison of manual review with algorithm results for each SES algorithm in a subset of randomly selected individuals (n=50) Semantic Category Records with SES information (%) Sensitivity (%) Specificity (%) PPV (%) Education level 48.0 66.7 84.5 80.0 Occupation 80.0 87.5 40.0 85.4 Unemployment 40.0 70.0 93.3 87.5 Retirement 14.0 100.0 90.7 63.6 Uninsured 8.00 75.0 78.3 23.1 Medicaid 18.0 100.0 95.1 81.8 Homelessness 2.00 100.0 95.9 33.3 Pacific Symposium on Biocomputing 2017 237 algorithm but not retired by the manual review. (Table 4). The low PPV for the uninsured algorithm (23.1%) is due to the ten individuals who were identified as uninsured by the algorithm, but not by manual review. The low PPV for homelessness (33.3%) was a result of the fact that the manual review only identified one patient with homelessness in their record, whereas the algorithm misidentified two others. 3.2.1. Missing data Of the total population (n=9,977), the algorithm was not able to extract any SES information for 1,695 individuals (17.0%). Of this group, there were 1,193 blacks, 309 Hispanics, and 193 Asians. Missing SES data were more common among Hispanic and Asian individuals, than among black individuals (p<0.001). The Hispanic and Asian populations represent 10.5% and 8.5% of the total dataset, respectively; however, these groups represent 18.2% and 11.4%, respectively, of the individuals with missing SES data. Males represent 35.8% of the study population and 28.0% of those without extracted SES data. The mean age for the total population is 49.9 years, and the mean age for the group without extracted SES information is 46.7 years. 4. Conclusion Socioeconomic status is considered a fundamental cause of disease, because it affects so many proximate risk factors and disease outcomes26. SES has been consistently associated with health outcomes such as mortality, cancer, and cardiovascular disease27,28. Despite these consistent associations, SES data are typically not included in genetic studies of health outcomes. For studies that utilize biobanks, the lack of SES data is likely related to the difficulty in accessing these data within the EHR, where they are not usually recorded in structured fields. The algorithms described in this study are the first to extract these important data from EHRs for research purposes. The SES algorithms described here focus on the extraction of data related to four semantic categories: occupation, education, insurance status, and homelessness. The occupation algorithms extracted and classified data as occupational prestige, unemployment (ever/never), and retirement (ever/never). The occupational prestige algorithm had a strong sensitivity and PPV; however it had a low specificity of 40%, reflective of the difficulty in filtering the occupation information. Although we took steps to remove false positives, it was difficult to completely eliminate all false positives without removing a large amount of accurate data. Our unemployment and retirement algorithms had high sensitivity (70% and 100%) and specificity (93.3% and 90.7%). While the unemployment algorithm had a high PPV, the retirement algorithm had a low PPV. Both unemployment and retirement were classified as ever/never because the EHR only captures a snapshot of time when the patient visits the clinic. It was not possible to accurately capture the length of time for unemployment or retirement as the patient's visits to the clinic may not reflect the length of time he or she was unemployed or retired. The sensitivity of the unemployment algorithm was affected by the varying language used to describe unemployment, which was identified in manual review but not consistently recognized by the algorithm ("does not work outside the home", "used to work in a restaurant"). The quality of the retirement algorithm was Pacific Symposium on Biocomputing 2017 238 affected by false positives related to the identification of words related to retirement that were used in a context outside of the patient's retirement from an occupation. The education algorithm identified the highest level of education that a patient achieved over the course of their EHR. This algorithm had a high specificity and PPV, but a low sensitivity. The low sensitivity was due to the inability of the algorithm to detect variations in education level compared with the manual review. The variation in language used by clinical providers made it difficult to include every mention of education while still maintaining some level of precision. For example, some of the Vanderbilt Medical School students were excluded ("medical student," "pharmacy student") because of the frequent mention of these terms in the EHR related to patient care, rather than education level. The reviewers were able to infer education level based on occupation and context clues as well as identify the medical school students, while the algorithm was not able to so. The algorithm that identified patients who were uninsured at some point in his or her record as well as the homelessness algorithm each had high sensitivity and specificity, but low PPV. Uninsured patients are the smallest portion of patients within VUMC, making up only 4.7% of the patient population in 201529. The low PPV of these algorithms may be due to a low prevalence of uninsured patients and homeless individuals within the VUMC patient population. Within our randomly selected minority patient population used for evaluation, only four individuals were uninsured and one was homeless. These categories had the lowest prevalence within our evaluation dataset. The Medicaid algorithm was the highest performing algorithm, with a high sensitivity, specificity, and PPV. The major challenges in utilizing EHR data in a research setting include missing data and the inconsistencies in the recording of SES data by clinical providers. While the majority of individuals within the study population had some SES information, a notable percentage of individuals did not have any SES information within their records (17.0%). The missing SES data could be a result of the lack of recording of information by the provider, either due to SES factors not being discussed in conversation with the patient, a low number of visits in the patient's EHR, or the willingness of the patient to provide SES information. Additionally, when variables are missing within a patient's record, we cannot distinguish whether it is due to negative data or just missing data. For example, if a patient does not have an occupation listed, we cannot assume that they are unemployed because it may have not been discussed with the provider or recorded by the provider. The higher level of missing data observed for Hispanic and Asian individuals in this dataset could be a reflection of the fact that the algorithms are optimized for the largest racial/ethnic population within the dataset (i.e., black patients). The inconsistencies in the recording of the SES data are typical for social and environmental exposure data contained within free clinical text17. In the development of these algorithms, we noted that providers, in general, do not follow patterns when recording SES data within their notes in the EHR. The lack of consistent language and the numerous variations used to describe the SES information made extracting this information challenging. Furthermore, algorithms could also be limited by the accuracy of the selected filters and tags, rather than the information available within the EHR. While the aim of the algorithms was to include all possible semantic tags, there is a possibility that some information was missed by the algorithms or that information was captured inaccurately due to the limitations of the filtering process. Pacific Symposium on Biocomputing 2017 239 In addition to these general limitations, the algorithms developed here have specific limitations regarding portability. Even within the same dataset, we have noted a difference in tag retrieval for the SES categories queried across the three major racial/ethnic groups. Additional studies are required to improve the algorithms' performances and retrieval of semantic tags in multiple populations as well as within different study sites. Indeed, some of the tags developed here (such as "TennCare" in reference to Medicaid) are specific to Tennessee and will require modification to ensure portability regardless of the state in which the algorithms are deployed. Furthermore, these algorithms were created in a de-identified EHR, which required the development of a free text algorithm for insurance status, as the structured insurance information is considered identifying information. An identified EHR may have this insurance information within the structured text. However, the other categories of SES information are likely to only be found within the free text of an identified EHR. Despite the many challenges faced with the extraction of SES data from the EHR, these algorithms were able to successfully extract a large amount of data not previously accessible for research purposes. The sensitivities, specificities, and PPVs for the algorithms were high considering the limitations of the SES data within the current EHR. Overall, these algorithms represent a first important step in incorporating SES data from EHRs into precision medicine research, as envisioned by the Institute of Medicine and others. 5. Resources Semantic tag and filter lists for each algorithm can be found on the Vanderbilt University Medical Center TREAT Lung Cancer Research Program website (https://medschool.vanderbilt.edu/treat-lung-cancer-program/) and the Institute for Computational Biology website (http://www.icompbio.net/?page_id=1654 ). The code which was used to run the algorithms is available in GitHub. 6. Acknowledgements This work was supported in part by NIH grants U01 HG004798 and its ARRA supplements (DCC) and 1K07CA172294 (MCA). BMH was supported by the NIH/NIGMS Genetics Predoctoral Research Training Program 5T32GM080178-07. The dataset(s) used for the analyses described were obtained from Vanderbilt University Medical Center's BioVU which is supported by institutional funding and by the Vanderbilt CTSA grant funded by the National Center for Research Resources, Grant UL1 RR024975-01, which is now at the National Center for Advancing Translational Sciences, Grant 2 UL1 TR000445-06. References 1. Poverty: Assessing the Distribution of Health Risks by Socioeconomic Position at National and Local Levels. (2004). 2. T. Seeman et al., Social Science & Medicine 66, 72-87 (2008). 3. P. Braveman et al., Public Health Reports 129 Suppl 2, 19-31 (2014). 4. National Center for Health Statistics, Health, United States, 2011: With Special Feature on Socioeconomic Status and Health (2012). Pacific Symposium on Biocomputing 2017 240 https://medschool.vanderbilt.edu/treat-lung-cancer-program/ https://medschool.vanderbilt.edu/treat-lung-cancer-program/ http://www.icompbio.net/?page_id=1654%20) 5. V. Carrieri et al., Health Econ, (2016). 6. A. L. Non et al., American Journal of Public Health 102, 1559-1565 (2012). 7. M. C. Aldrich et al., American Journal of Public Health 103, e73-80 (2013). 8. R. Hauser et al., Sociological Methodology 27, 177-298 (1997). 9. K. Fujishiro et al., Social Science & Medicine 71, 2100-2107 (2010). 10. in Kaiser Commission on Medicaid and the Uninsured T. H. J. K. F. Foundation, Ed. (Washington, D.C., 2012). 11. D. S. Morrison, International Journal of Epidemiology 38, 877-883 (2009). 12. National Center for Health Statistics Health, United States, 2015: With Special Feature on Racial and Ethnic Health Disparities (2016). 13. T. J. VanderWeele et al., Epidemiology 25, 473-484 (2014). 14. S. Cakmak et al., Journal of Environmental Management 177, 1-8 (2016). 15. J. S. Kaufman et al., American Journal of Epidemiology 181, 464-472 (2015). 16. F. S. Collins et al., The New England Journal of Medicine 372, 793-795 (2015). 17. I. S. Kohane, Nature Reviews. Genetics 12, 417-428 (2011). 18. J. Basson et al., American journal of hypertension 27, 431-444 (2014). 19. IOM (Institute of Medicine), Capturing Social and Behavioral Domains and Measures in Electronic Health Records: Phase 2 (2014). 20. D. M. Roden et al., Clinical Pharmacology and Therapeutics 84, 362-369 (2008). 21. J. Pulley et al., Clinical and Translational Science 3, 42-48 (2010). 22. D. C. Crawford et al., Human Heredity 79, 137-146 (2015). 23. J. B. Hall et al., PloS one 9, e99161 (2014). 24. L. Dumitrescu et al., Genetics in Medicine : official journal of the American College of Medical Genetics 12, 648-650 (2010). 25. NORC, Measuring Occupational Presitge on the 2012 General Social Survey (2014). 26. B. G. Link et al., J Health Soc Behav Spec No, 80-94 (1995). 27. T. N. Bethea et al., Ethnicity & Disease 26, 157-164 (2016). 28. A. Rawshani et al., JAMA Internal Medicine, (2016). 29. "2015 Financial Report " (Vanderbilt University, Nashville, TN. , 2015). Pacific Symposium on Biocomputing 2017 241 PY  - 2015
Y1  - 2015
N2  - In the health domain, the adoption of computer systems introduces better services, reduces human errors, and provides reliable services with nearly zero down time. In general, data in computer systems is stored in coded format; however, certain data, like user comments, cannot be coded. Hence, it is stored in the form of free text. Based on the results of the performed literature review, it was identified that the free text contains invaluable information; however, extracting such information is a challenging task due to the complexity of the stored data. In this paper, a Latent Semantic Indexing (LSI) algorithm is developed and applied on The Health Improvement Network (THIN). The algorithm utilizes the computational power provided by the multi-processor/multi-core system in performing the IR process. Further to that, the paper investigates the representation of the patient's data in the Term Document Matrix (TDM) to enhance the accuracy of the retrieved information.
AB  - In the health domain, the adoption of computer systems introduces better services, reduces human errors, and provides reliable services with nearly zero down time. In general, data in computer systems is stored in coded format; however, certain data, like user comments, cannot be coded. Hence, it is stored in the form of free text. Based on the results of the performed literature review, it was identified that the free text contains invaluable information; however, extracting such information is a challenging task due to the complexity of the stored data. In this paper, a Latent Semantic Indexing (LSI) algorithm is developed and applied on The Health Improvement Network (THIN). The algorithm utilizes the computational power provided by the multi-processor/multi-core system in performing the IR process. Further to that, the paper investigates the representation of the patient's data in the Term Document Matrix (TDM) to enhance the accuracy of the retrieved information.
KW  - electronic health records
Three Questions about Clinical Information Retrieval Stephen Wu, James Masanz, Ravikumar K.E., Hongfang Liu Mayo Clinic, Rochester, MN 1 Introduction Electronic Medical Records (EMRs) have greatly expanded the potential for the evidence-based im-provement of clinical practice by providing a data source for computable medical information. The Text REtrieval Conference 2012 Medical Records Track (TREC-med) explored how information retrieval may support clinical research by providing an efficient means to identify cohorts for clinical studies. A shared task called participants to find cohorts of rel-evant patients for 50 different topic queries. The users in TREC-med information retrieval sys-tems would be medical experts who are searching for cohorts. In our previous work, we have col-laborated with such experts on specific queries; the assortment of 50 queries makes this competition a standardized benchmark task. Thus, techniques that have shown case-by-case improvement can be tested against a much larger number of queries. We have taken this opportunity to investigate three core ques-tions around which many of our algorithms are de-signed: 1. What is the relative value of structured data (e.g., fields in EMRs, or document metadata) compared to clinical text? 2. Are extensive information extraction (IE) efforts any benefit when we consider the applied ques-tion of information retrieval (IR)? 3. Can distributional semantics help supply missing information in a query? For each of these three questions, we have ex-tended Apache Lucene1 with pre-existing techniques and tested on the TREC-med cohort identification task. In testing these independently, we aim to find generalizable principles for cohort identification in other documents collections and queries. The rest of this paper describes the TREC 2012 Medical Records task, describes Mayo Clinic's run submissions in detail, and reports evaluation results with subsequent discussion. 1See lucene.apache.org 2 Background The TREC 2012 Medical Records track was arranged as a follow-up to the 2011 track [1], with nearly iden-tical setup. The data to be retrieved lay in the University of Pittsburgh's BLU repository, which in-cludes the free text portions of medical records (see report text below). Each patient at the Univer-sity of Pittsburgh would have one or more medical records (documents) associated with him or herself. Each record was given in XML format, and included both structured data and the unstructured text. <?xml version=''1.0'' encoding=''UTF-8'' standalone=''no''?> <report> <checksum>20060201ER-Fs2xiJYPXwVE-848-1341620775 </checksum> <subtype>EVAL</subtype> <type>ER</type> <chief complaint>DENTAL PAIN</chief complaint> <admit diagnosis>521.00</admit diagnosis> <discharge diagnosis>525.9,E917.9, </discharge diagnosis> <year>2007</year> <downlaod time>2008-02-06</downlaod time> <update time/> <deid>v.6.22.06.0</deid> <report text>[Report de-identified (Safe-harbor compliant) by De-ID v.6.22.06.0] . . . </report text> </report> Records are uniquely identified by their checksum. Note that each record contains a note type and subtype; in the example, the note comes from an Emergency Room/Department. The chief complaint section is a helpful textual sum-mary of what the record is about from the patient's perspective, but is not present for every record. The admit diagnosis and discharge diagnosis serve 1 a similar function but are also not always present. They are given as ICD-9 codes, a medical terminol-ogy frequently used for billing purposes. Finally, no-tice that the notes were de-identified, so that any protected health information has been replaced with surrogates. The records were grouped into visits — a physical visit to the hospital. The unit of retrieval was defined as a patient visit. In total, there were 95,702 records that corresponded to 17,198 visits. The largest visit was 418 records, but the mean visit was 5.56 records. Participants from 24 institutions were given a set of 50 hypothetical topics (queries) developed by experts at the Oregon Health Sciences University (OHSU). Each topic query is given in a form such as Number: 143 Patients who have had a carotid endarterec-tomy These topics defined patient profiles that might be in-volved in a clinical trial. For each topic, participants retrieved a list of patient visits in order of relevance to the topic. For evaluation and ranking, retrieved records from participants' runs were given to assessors at OHSU. These assessors rendered relevance judgments on a stratified pool of visits — the top 15 of each submit-ted run, and a random sample of the remaining top 100 in each run. The nature of each topic and its correspondence with the given dataset varied greatly. For example, 4 topics were discarded for purposes of evaluation because no records were assessed as being relevant to the query topic; on the other hand, other topics likely had many relevant visits that were never assessed. 3 Methods We tested 3 focused questions that lay ground-work for future patient identification systems. We evaluated the usefulness of document metadata, contextually-aware information extraction results, and distributional semantic query expansion. As a baseline, we used a standard Lucene index, and each of the other runs was built directly on this baseline. 3.1 Baseline Apache Lucene is perhaps the most widely used in-formation retrieval framework. Lucene indexes a collection of documents for extremely efficient text search. Similar to rows in a database, documents are the granularity of a collection in Lucene; similar to columns, fields in Lucene contain values for each document that are considered strings by default. We took each medical record from the BLU repository and stored fields corresponding to 10 useful parts of the XML document: Document ID (checksum), Visit ID (stands in for Patient ID), Date (from checksum), Type, Subtype, Chief Complaint, Admit diagnosis (ICD-9 codes), Discharge diagnosis (ICD-9 codes), Year, and Content (the clinical text). As mentioned, each of the fields in Lucene can be indexed. Behind the scenes, by default, Lucene uses an Analyzer pipeline for the text that includes tok-enization, normalization, lowercasing words, and stop word removal; it then creates an inverted index for each token in each field. Unlike the mapping im-plied in the fields (from documents to tokens), an inverted index maps from tokens to documents, and this makes it easy to find terms that match a query. For our baseline methodology, we included variants of "patient" as a stop word, and only searched the Content field. When searching for a term, Lucene effectively uses the following equation to rank which documents are most relevant: score(q, d) = coord(q, d) · qNorm(q) · ∑ t∈q ( tf(t) · idf(t)2 · t.boost() · norm(t, d) ) (1) where q is the query, d is a document in the collection, t is a term, and the following functions hold: score the document's score coord weight for # term matches btwn. q & d qNorm normalizes similarities between queries tf square root of term frequency idf 1 + log(D/(df(t) + 1)), df() is doc. freq. t.boost weight for query terms norm weights for fields & terms in a document In this baseline approach, we strictly used the text of the original TREC topic as the query, without any special weighting for t.boost(). It should be noted, however, that we used Lucene's same Standard Ana-lyzer on both the query and the collection. Likewise, we searched within an unaltered Content field (i.e., the text itself), and thus norm() had no effect on the final weighting. Since Lucene ranks top documents rather than top visits (i.e., patients), we consider the most rel-evant document to represent the whole visit. This maximum-document assumption for each patient is not neceessarily a good one, but we have focused on testing other aspects of the retrieval framework. We report the top 1,000 unique visits for each query. Evaluations in TREC-med 2011 were on a smaller set of 35 topics, and reported results used bpref; the performance on TREC-med 2011 topics was bpref=0.4249. 3.2 Test 1: Including Structured Metadata (MayoMetadata) First, we performed a controlled test of the value of EMR structured data on retrieval. For practition-ers and researchers who use EMR data, ICD-9 codes are the first line of defense in cohort identification. They are frequently used in lieu of more sophisticated cohort identification procedures. We accounted for these ICD-9 codes by mapping to their textual rep-resentations using the UMLS Metathesaurus. Term lists for each ICD-9 code were then stored in addi-tional Lucene index fields: Admit Diagnosis Terms and Discharge Diagnosis Terms. This is not strictly the same as a boolean search for matching ICD-9 codes, which requires person or process to code the query into ICD-9 codes. How-ever, in our experience, users of an IR system will typically think of terms of interest, then consult a reference material to find relevant codes, then search the structured data for codes. Indexing and search-ing the textual representations of ICD-9 codes is thus a reasonable automatic method for retrieving ICD-9 codes. With the query unchanged from the baseline, we searched over 4 fields: the text of these two diag-noses terms, the Chief Complaint, and the Content field. Since we expected these coded representations to be highly relevant for retrieval, we weighted the structured data sections higher (in proportion to the much shorter field length) than the text itself, by using the norm() function. Performance on the 35 TREC 2011 topics improved significantly using this addition (bpref=0.4541). 3.3 Test 2: Weighting IE Output (MayoPayload) Second, we performed a controlled test (i.e., ignoring metadata) of the value of using the results of infor-mation extraction to inform the scoring procedures. We used a recent Information Extraction (IE) sys-tem developed at Mayo Clinic, MedTagger, due to its speed. MedTagger uses a lexicon of terms and variants that have been attested in a large corpus of clinical text [2, 3], and looks for these terms in the document collection (here, the BLU NLP repository) as its means of Named Entity Recognition. Addition-ally, MedTagger applies the NegEx [4] and ConText [5] algorithms, which discover whether these named entities were negated, hypothetical, historical, or ex-perienced by someone other than the patient. After being found in the BLU text, these named entities and their attributes were brought into the Lucene index. For each named entity, the last to-ken in that named entity carried a Payload — addi-tional data attached to a token within Lucene. Our custom-defined payload included a normalized form of the named entity, the semantic group, status (any hedging of a statement), polarity (whether the state-ment was negated), and the experiencer (subject of a statement, typically the patient). We used a simple heuristic to down-weight tokens if its attributes cast any doubt that the named entity was associated with the patient. Values were chosen by manually testing against the 2011 query topics: polarity = "negated" → .10w status = "history of" → .75w "fam. history" → .10w "probable" → .25w experiencer = not "patient" → .10w (2) This weight w is an incremental part of the calcula-tion for norm(t, d). By augmenting the index with the ability to downweight on these mentions, query terms finding negated or hedged matches would be dispreferred. This simple NLP-driven addition improved perfor-mance from the baseline on TREC-med 2011 topics, with bpref=0.4730. 3.4 Test 3: Query Expansion with Se-mantic Vectors (MayoExpanded) Finally, we performed a controlled test (i.e., ignor-ing metadata and negation) of the value of query expansion through distributional semantics. While the baseline and previous approaches directly used the text of the queries (with stop word removal) to search documents, here we modified the queries. We used Random Indexing [6] to build distribu-tional semantic representations (i.e., vectors) of terms from a large corpus of Mayo Clinic clinical notes. Near-neighbor terms (often multiple tokens each) were selected for each topic. We constructed ex-panded queries whose terms had a t.boost() weighting based on the frequency of tokens in the near-neighbor synonymous term lists. We included it for diversity in the result pool, despite slightly decreased perfor-mance on TREC 2011 topics (bpref=0.4097). infNDCG σ2 infNDCG infAP bpref R-prec P@10 MayoLucene 0.3694 0.2310 0.1359 0.2771 0.2583 0.4043 MayoMetaData 0.3222 0.2125 0.1175 0.2474 0.2072 0.3553 MayoPayload 0.4119 0.0634 0.1590 0.2981 0.2807 0.4319 MayoExpanded 0.3587 0.1422 0.1239 0.2652 0.2321 0.4043 Table 1: MayoClinicNLP results for TREC-med 2012 2011 topics 2012 topics bpref ∆ bpref ∆ MayoLucene 0.4249 – 0.2771 – MayoMetaData 0.4541 +6.87% 0.2474 -10.72% MayoPayload 0.4730 +11.32% 0.2981 +7.58% MayoExpanded 0.4097 -3.58% 0.2652 -4.29% Table 2: Comparison of methods on 2011 topics vs. 2012 topics. Lucene MetaData Payload Expanded Lucene – 0.6451 0.9632 0.8125 MetaData 0.6451 – 0.6336 0.6279 Payload 0.9632 0.6336 – 0.7770 Expanded 0.8125 0.6279 0.7770 – Table 3: Correlations between the 2012 runs. 4 Evaluation Official TREC results on the baseline and variants are shown in Table 1. In 2011, bpref was used for the official evaluation rankings, due to problems in calcu-lating the inferred measures. The inferred measures [7] are now available in 2012. The baseline (MayoLucene) is improved upon by the IE-influenced retrieval (MayoPayload) across all metrics. Both ICD-9 codes (MayoMetaData) and se-mantic vector query expansion (MayoExpanded) ac-tually decrease the performance across all metrics. For comparison, Table 2 shows the performance of these techniques on both the 35 topics from 2011 and the 50 topics from 2012. The ∆ columns highlight the % difference from baseline associated with each of the three techniques being tested. Finally, in Table 3 we show the correlations be-tween our baseline and 3 other runs. 5 Discussion At the beginning, we set out to answer three ques-tions, which we revisit here. 5.1 Structured clinical data in IR? (MayoMetaData) The evaluation results for 2012 topics suggest that structured data does not uniformly improve perfor-mance. This is especially interesting given that per-formance did improve significantly for 2011 topics when structured data was added, and given that the median R-precision scores for 2011 and 2012 were very similar. In 2012, 32 of the 47 topics (68%) were hurt by including ICD-9 codes, showing that the detri-mental effects of the structured data were relatively widespread. MayoMetaData had the highest stan-dard deviation (0.2125) among the three tested sys-tems. All this may indicate that topics vary widely, and structured data is not always a good match for what an end user is looking for. Alternatively, a possible explanation is that the 2012 topics are harder to en-capsulate in diagnosis codes. 5.2 IE results in IR? (MayoPayload) Here, we find a clear indication that Information Ex-traction methods do benefit Information Retrieval. This is shown with positive results across both 2011 and 2012 topic sets. For 2012 topics, 35 of 47 topics (74%) were im-proved by including this type of contextual informa-tion. This is an encouraging result, showing that ex-tensive IE research has practical benefit in IR sys-tems. MayoPayload also has the smallest standard deviation (0.0634), showing the consistency of the ap-proach. 5.3 Distributional semantic query ex-pansion? (MayoExpanded) The flavor of query expansion that we have proposed shows an overall drop in performance, showing that query expansion through distributional means cannot necessarily be relied upon. There are indeed cases where query expansion outperforms plain queries (25 of 47, or 53% of cases), but the opposite is also true (22 of 47, or 47%). One contributing factor could be that the semantic vectors were trained on a different distribution of data (Mayo EDT) than the test data (Pittsburgh BLU). Since most TREC 2012 topics had few relevant visits and the goal of query expansion is to aid in increasing recall, it seems that increases in performance due to recall are more than balanced out by the cost in precision. 5.4 Other questions As shown in Table 3, the correlative relationships be-tween the different approaches confirm existing find-ings. Overall, MayoLucene is a strong baseline, and large deviations from it tended to come from lower-performing runs. In particular, Because MayoMeta-Data searches (and highly weights) fields that are not present in the other approaches, it is the least corre-lated with the others. Two related, untested questions are whether inter-active user input and structured (or faceted) queries would be of benefit. This is part of our future work. 6 Conclusion The TREC 2012 Medical Records track competi-tion provided an opportunity to test three focused questions about Information Retrieval in the clinical domain. Structured data and query expansion can sometimes be helpful, but information extraction re-sults can be used effectively to greatly increase IR performance. References [1] E. Voorhees and R. Tong. Overview of the trec 2011 medical records track. In The Twentieth Text REtrieval Conference Proceedings TREC, 2011. [2] Stephen Wu and Hongfang Liu. Semantic Char-acteristics of NLP-extracted Concepts in Clinical Notes vs. Biomedical Literature. In Proceedings of AMIA 2011, 2011. [3] Stephen Wu, Hongfang Liu, Dingcheng Li, Cui Tao, Mark Musen, Christopher Chute, and Nigam Shah. UMLS Term Occurrences in Clinical Notes: A Large-scale Corpus Analysis. In Proceedings of the AMIA Joint Summit on Clinical Research Informatics, 2012. [4] W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, and B.G. Buchanan. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical In-formatics, 34(5):301–310, 2001. [5] H. Harkema, J.N. Dowling, T. Thornblade, and W.W. Chapman. Context: An algorithm for determining negation, experiencer, and temporal status from clinical reports. Journal of Biomedi-cal Informatics, 42(5):839–851, 2009. [6] D. Widdows and K. Ferraro. Semantic vectors: a scalable open source package and online tech-nology management application. Proceedings of the Sixth International Language Resources and Evaluation (LREC'08), pages 1183–1190, 2008. [7] E. Yilmaz, E. Kanoulas, and J.A. Aslam. A sim-ple and efficient sampling method for estimating ap and ndcg. In Proceedings of the 31st annual in-ternational ACM SIGIR conference on Research and development in information retrieval, pages 603–610. ACM, 2008. December 1, 2015
Format: Kindle Edition
According to William Hersh, the main goal of Information Retrieval: A Health and Biomedical Perspective, third edition, "is to provide an understanding of the theory, implementation, and evaluation of information retrieval (IR) systems in health and biomedicine" (p. ix). The author's method for accomplishing this goal is to "introduce all the necessary theories to allow coverage of the implementation and evaluation of IR systems in health and medicine" (p. ix-x). The book is comprised of 3 sections: "Basic Concepts," "State of the Art," and "Research Directions" and 9 chapters. In the first section Hersh covers basic definitions of IR, the evaluation of IR systems, complexities associated with health IR (as compared to other fields involving IR), meaning and generation of information in the health and biomedical fields, and information needs of people working in health and biomedical fields. In part 2, Hersh covers various systems containing health and biomedical information, indexing, ontologies and controlled vocabularies, retrieval, and digital libraries. In the final section of the text, evaluation of IR in health and biomedical domains, research areas in IR, information extraction and topics related to IR are covered.
As mentioned previously, the book covers a broad range of topics in the area of IR as well as topics related to IR. The first chapter of the book provides an excellent overview of IR and how health and biomedical fields have some unique requirements that set it apart from normal IR uses. From the rest of the book, the best portions for IR in the health and biomedical domains are the "Indexing" and "Retrieval" chapters from section 2 and the "Evaluation" and "System and User Research" chapters from section 3. While not going into extensive technical nor algorithmic details, Hersh provides a good general overview of common methods for indexing content followed by a general overview of techniques for determining the best document to be retrieved for a given query. The topic of methods for evaluating IR systems is covered with specific use cases related to health care fields but doesn't provide any convincing evidence that evaluation methods of health and biomedical related IR is unique in any way from IR evaluations in other use cases. The "System and User Research" chapter covers more advanced concepts for both indexing and retrieval but also dives into additional evaluation methods as well as discussing the ideas that have come out of some Text Retrieval Conferences (aka TREC).
While the text provides a general overview of IR and specific examples, uses, and evaluations in health and biomedical domains, it has many shortcomings. As an example, while this is a book on IR,  chapter 2, "Health and Biomedical Information," provides an excessively deep dive into the specifics of information generation via research methods, peer review processes, publication, and how to evaluate other research - topics that are best covered in research methods texts. Many other texts exist such as Foundations of Clinical Research by Portney et al. that provide a better understanding into the needs and nuances of health and biomedical research. As another example, the author spends a lot time detailing from a user perspective how systems such as Medline, PubMed, MESH, and UMLS can be used to retrieve information. While it is useful to be familiar with common health and biomedical IR systems, repeated examples from each of the systems across multiple chapters make the book feel like a user's guide to health and biomedical information retrieval systems. Furthermore, while Hersh attempts to put IR in a health and biomedical perspective, many examples, comparisons, and evaluations are pre 2005, with a many from the 1990's and before. While some historical understanding is valuable, Hersh spends too much time reviewing dated and very early IR systems.
Overall while the books goal "is to provide an understanding of the theory, implementation, and evaluation of information retrieval (IR) systems in health and biomedicine" its many flaws result in a text that would require a major overhaul to be detailed and focused enough for people working in the IR field as well as being too outdated for those wanting to be IR system end users or evaluators. While the book is now in its third edition its many dated references and use of terms and technology that are no longer relevant or even available make it difficult to apply directly today in an IR setting. Furthermore excessive detail in fields related to IR, excessive detail on health/biomedical items not directly relevant to IR, as well as the dilution of key IR topics such as indexing and retrieval across multiple non-consecutive chapters make it difficult to move from a beginning understanding to more advanced IR topics. If a reader wanted to understand the field of IR including theory and specifics for implementation they would be better served by reading other IR texts such as the freely available Introduction to Information Retrieval combined with reading some recent health and biomedical IR articles easily found via PubMed to understand some of the unique requirements for IR in the health and biomedical domains. Similarly, if a user wanted to understand how to use or evaluate IR systems, they would be better served by going to more recent texts or currently published research articles.
References
Portney LG, Watkins MP. Foundations of Clinical Research: Applications to Practice. 3rd edition. Upper Saddle River, N.J: Prentice Hall; 2008.
Manning CD, Raghavan P, Schütze H. Introduction to Information Retrieval, Cambridge University Press. 2008.
National University of Sciences and Technology (NUST), H-12, Islamabad 44000,  Pakistan
Received 5 May 2015; Revised 17 June 2015; Accepted 29 June 2015
Academic Editor: Xiaojun Yao
Copyright © 2015 Abdul Wahab Muzaffar  et al. This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.
Abstract
The information extraction from unstructured text segments is a complex task. Although manual information extraction often produces the best results, it is harder to manage biomedical data extraction manually because of the exponential increase in data size. Thus, there is a need for automatic tools and techniques for information extraction in biomedical text mining. Relation extraction is a significant area under biomedical information extraction that has gained much importance in the last two decades. A lot of work has been done on biomedical relation extraction focusing on rule-based and machine learning techniques. In the last decade, the focus has changed to hybrid approaches showing better results. This research presents a hybrid feature set for classification of relations between biomedical entities. The main contribution of this research is done in the semantic feature set where verb phrases are ranked using Unified Medical Language System (UMLS) and a ranking algorithm. Support Vector Machine and Naïve Bayes, the two effective machine learning techniques, are used to classify these relations. Our approach has been validated on the standard biomedical text corpus obtained from MEDLINE 2001. Conclusively, it can be articulated that our framework outperforms all state-of-the-art approaches used for relation extraction on the same corpus.
1. Introduction
With the massive information and knowledge hidden in the biomedical field, in the form of publications, that is growing exponentially, it is not possible for researchers and practitioners to keep themselves updated with all the developments in any specific field [ 1 , 2 ]. The emphasis of biomedical research is shifting from individual entities to whole systems, with the demand of extracting relationships between entities, for example, protein-protein interaction, diseases genes from biomedical text to generate knowledge [ 3 , 4 ]. Manual effort to transform unstructured text into structured is a laborious process [ 5 ]. Automatic techniques for relation extraction provide a solution to the problem [ 6 ].
A number of relation extraction techniques for biomedical text have been proposed [ 7 – 10 ]. These techniques are broadly categorized into four groups, that is, cooccurrence based, pattern-based, rule-based, and machine learning based approaches.
The simplest approach to identify/extract relations between entities is cooccurrence that identifies cooccurring entities in a sentence, abstract, or document [ 11 ]. Pattern based systems rely on a set of patterns to extract relations; these patterns can be defined manually as well as automatically. Manual patterns are defined by domain experts, which are a time-consuming process and have low recall [ 12 ]. To increase the recall of manually generated patterns, automatic pattern generation is used. Automatic pattern generation can use bootstrapping [ 13 ] or generate directly from corpora [ 14 ]. In rule-based systems, a set of rules can be built to extract relations [ 15 , 16 ]. Rule-based systems can be defined in both ways, that is, manually and automatically. When the annotated corpora on biomedical is available, machine learning based approaches become more effective and ubiquitous [ 17 , 18 ]. Most approaches use supervised learning, in which relation extraction tasks are modeled as classification problems. Broadly, any relation extraction system consists of three generalized modules that is, text preprocessing, parsing, and relation extraction.
This paper presents a detailed feature set to extract the relations between disease and treatment from biomedical text. This representation model is a hybrid as it uses the bag of word, natural language processing, and semantic representation to extract biomedical relations. Our framework is validated on the standard corpus form [ 19 ]. By presenting the implementation for three relations in the corpus that is, cure, prevent, and side effect relations.
2. Literature Review
Generally, the relation classification in biomedical domain is done by three methods, that is, supervised, semisupervised, and unsupervised. Major work in biomedical domain is done on protein-protein interaction, or protein and gene relation, while not a lot of work has been cited in disease-treatment relations.
Huang et al. [ 20 ], the authors, proposed a new hybrid approach to extract protein-protein (P2P) relations from biomedical scientific papers. This approach is a combination of shallow parsing and pattern matching. Based on shallow parsing analysis, using syntactic and semantic constraints, appositive and coordinative structures are interpreted. Long and complex sentences are then divided into smaller ones. In the end, the greedy pattern matching algorithm has been used to extract relations from shorter sentences, and patterns are generated automatically. This technique achieved an average
-score of 80% on individual verbs and 66% on all verbs. As stated by the use of shallow parsing analysis, remarkable improvement in pattern matching is noticed. Author mentioned 7% improvement of both
-score and precision of their approach compared to traditional pattern matching algorithms and achieved a performance which is comparable to the best of these systems. This approach was complex and designed for a small domain of protein-protein interaction, as patterns were developed to extract proteins only.
Frunza and Inkpen [ 21 ] used the integration of biomedical and medical knowledge for the discovery of semantic relations, from biomedical sentences, which occurs between diseases and treatments. Cure, prevent, and side effect relations are the semantic relations considered to be extracted between entities (disease, treatment). The authors claimed better results compared to the previous studies done on this topic. Results showed different figures for each of the three relations mentioned: accuracy for cure relation is 95%, prevent relation has 75% accuracy, and 46% accuracy for side effect relation has been claimed. The approach lacks detailed analysis as it does not give the
-score.
Sharma et al. [ 22 ] primarily focused on the task of identification and extraction of relations between entities present in biomedical literature. The paper proposed a verb-centric algorithm, unlike cooccurrence patterns or manual syntactic rules, as done in previous biomedical relation extraction work. No rule-based approaches are required, as algorithm identifies the main verbs in the sentences. The entities involved in relations are then identified using a dependency parse tree with syntactic and linguistic features. As claimed by the authors, this technique can extract the relations from complex sentence structures effectively. The algorithm is evaluated on multiple biomedical datasets prepared using MEDLINE, and the average
-score achieved is almost 90%. The main problem with this approach was that it caters only to words from the part of speech. No preprocessing was done, that is, stop word removal, or stemming, and so forth was done. Also, no standard dataset was used to evaluate the technique.
Ben Abacha and Zweigenbaum [ 23 ] explained the extraction of semantic relations from medical text. The scope of relation extraction is only between disease and treatment entities. The authors propose an approach, which is a hybrid in nature; that is, it employs two different techniques to extract the semantic relations. In the first technique, relations are extracted by patterns based on human expertise whereas, in the second one, relations are extracted by a machine learning technique based on Support Vector Machine classification. This new hybrid approach mainly relies on manual patterns when available relation examples are less, while feature values are used more when the number of available relations examples is sufficient. The authors claimed an overall
-measure of 94.07% for cure, prevent, and side effect relation extraction. Due to pattern based approach, the domain specific results are generated. Also, since the feature extraction was based on dataset, it may not perform well on disease-treatment relation for some other datasets.
Ben Abacha and Zweigenbaum [ 24 ] present a platform, MeTAE, for identification of medical entities and medical relations linking those entities. The approach is based on linguistic patterns and domain knowledge. The proposed approach contains two main parts. First part deals with the medical entities' recognition and in the second part exact semantic relations between any two identified medical entities are extracted. The identification of medical entities is achieved by an extended use of MetaMap. The results with the simple use of MetaMap and extended use of MetaMap are compared and the latter improved the precision by 19.59%. The extraction of medical relations is based on linguistic patterns which are constructed semiautomatically from a corpus chosen using semantic criteria. 16 types of medical entities are identified to evaluate the ability of the proposed system. In order to assess the system, the extraction of treatment relations between a medication and a disease is also taken into account. The results claimed by this research are encouraging as compared to similar research works in the literature with a precision of 75.72% and a recall of 60.64%. The problem with this approach is that named entity recognition phase does not extract all the named entities for which relations are to be extracted, so it further decreases the precision and recall of the approach.
Yang et al. [ 25 ] proposed a system with four main modules. The first module deals with the named entity recognition; here authors extract five entity types which are foods, chemicals, diseases, proteins, and genes. The second one is the relation extraction module to extract binary relationships between the entities; this is based on the verb-centric approach. The third module focuses on the polarity and strength analysis of relationships. In order to capture the syntactic, semantic, and structural aspects of relations, unique features have been constructed by authors. Support Vector Machine and support vector regression are used in this step. A user interface has been developed to integrate and visualize extracted relationships to intuitively observe and explore the previously extracted relations. Evaluation of the first three modules of this system exhibits its efficiency. The named entity recognition has an
-score of 89% with equal precision and recall, relationship extraction task has an
-score of 90.5%, and the accuracy of relationship polarity was 91%, while 96% strength level was rated in relationships. This technique lacks as it did not use a standard dataset, rather they built their own dataset and also no proof of results was stated in the paper.
Kadir and Bokharaeian [ 26 ] proposed a new technique to extract relations between biological and medical entities from biomedical documents. This technique is hybrid in nature, combines different relation extraction approaches, and extracts simple as well as complex relations between the pair of entities. The proposed hybrid approach combines rule-based, kernel based, and cooccurrence based methods. Combiner and classifier are additional components. The authors did not evaluate the approach in terms of results and just gave an idea of how this approach will work. The main drawback of this paper is that it did not present any evaluation or results as a proof of concept for their approach.
Rosario and Hearst [ 19 ] compared five generative graphical models and a neural network with lexical, syntactic, and semantic features. This paper examines the distinction among seven different relation types that exist between the disease and treatment entities. The accuracy achieved for three main relations is 92.6% for the cure, 38.5% for prevent, and 20% for side effect relations.
Analysis of previous approaches on relation extraction is summarized in Table 1 .
Table 1: Analysis of the existing literature on biomedical relation extraction.
3. Dataset
We used the standard text corpus that is obtained from [ 19 ]. This corpus/dataset contains eight possible types of relationships, between treatment and disease. This dataset was collected from MEDLINE 2001 abstracts. Relations are annotated from sentences taken from titles and abstracts. Table 2 presents the original dataset, as published in previous research showing relationships and number of sentences. This dataset was collected from MEDLINE 2001 abstracts (The corpus detail and download available at the following link: http://biotext.berkeley.edu/dis_treat_data.html ).
Table 2: Original dataset description from [ 21 ].
4. Proposed Framework
The framework we propose is partially inspired by Frunza and Inkpen [ 21 ], the feature set and corpus they used. However, our framework additionally uses UMLS to rank the verb phrases in the corpus, instead of only relying on noun phrase ranking. The ranking of the verb phrases using verb based biomedical lexical resource is the first implementation of this idea, to the best of our knowledge. Ben Abacha and Zweigenbaum [ 23 ] mentioned verb-related semantic resources for the medical domain, but they believe that there was no resource available at that time. Our framework for relation classification is a stepwise approach where each step deals with one module of the overall method. There are five major modules of relation extraction framework: corpus preprocessing, natural language processing, UMLS based ranking of noun and verb phrase, creation of
-dimensional vector space, and classification of entities. Figure 1 shows our comprehensive, detailed proposed framework for relation extraction.
Figure 1: Hybrid feature set based relation extraction framework.
4.1. Corpus Preprocessing
This step is designed to preprocess the corpus by applying four text processing steps: tokenization, sentence splitting, part of speech (POS) tagging, and morphological analyzer.
General Architecture for Text Engineering (GATE) [ 27 , 28 ] for text preprocessing is used, which is open source and widely used by many research communities. The purpose of the process is to transform the text that can be used for further text engineering activities. A Nearly-New Information Extraction System (ANNIE) [ 28 ] is the information extraction application available in GATE and used in our preprocessing process with default options.
The input to this step is the text corpus/dataset which is used for the task of relation classification and the output of this step is the set of unigram features that will be further used for the feature set. The preprocessing activities used in this process are(i)Tokenization: the tokenizer splits the text into small tokens, that is, different type of words, punctuation, and numbers [ 29 ]. For example, "disease and diagnosis" has 6 tokens, that is, (disease), (space), (and), (space), (diagnoses), and (.).(ii)Sentence splitting: the sentence splitter splits the text that is required for taggers into sentences. The sentence splitter uses a dictionary list of abbreviations to differentiate between full stops and other token types [ 30 ]. Sentence splitter takes the (.) to split one sentence from another. For example, "disease and diagnosis" is a single sentence.(iii)Part of speech (POS) tagging: this module produces a part-of-speech tag and annotates each word or symbol in the text. Part of speech tags can be a verb, noun, adverb, or adjective. Tagger [ 31 ] can be customized by changing the rule set given to it. For example, "disease and diagnosis" (disease) is a noun, (and) is an article word, and (diagnosis) is also a noun.(iv)Morphological Analysis: the morphological analyzer takes as input a tokenized GATE document. It identifies the lemma and an affix of each token by considering token's part of speech tag, one at a time. These values will then be added as features on the token annotation. Morpher is based on certain regular expression rules [ 32 ]. This module is used to identify the common root of words in the text. GATE morphological analyzer is used with ANNIE to consider the root of each word, instead of the original string in feature extraction. For example, "smoking causes disease," the word (smoking) will be converted to (smoke) and (causes) in its base form (cause).
4.2. Natural Language Processing
The syntactical or natural language processing (NLP) information is the second representation. It has noun phrases and verb phrases that are very important for relation classification. We consider these phrasal features with the unigrams. GENIA tagger [ 33 ] is used to extract the syntactical information from the data. GENIA tagger is specially designed for biomedical text, such as, MEDLINE abstracts. The tagger takes an English sentence as input and provides the base forms, part-of-speech tags, chunk tags, and named entity tags as outputs.
Example sentence from our dataset: "Only two protein subunits, Pop1p and Pop4p, specifically bind the RNA subunit."
The full output of the GENIA tagger and Open NLP Chunker to extract the output of the sentence, taken as an example from our dataset, is shown in Table 3 .
Table 3: GENIA tagger output on the example sentence.
The noun phrases and verb phrases recognized by the tagger are considered to be the syntactical features for our technique. We preprocessed these features before finalizing the feature set, removed features that contain only punctuation, removed the stop word, and did morphological analysis to identify the root word. The purpose is to identify the base form of the word that has multiple inflected forms.
4.3. Noun and Verb Phrase Ranking Based on UMLS
The concept ranking module is the core contribution area of our framework, with the ranking of the verb phrases obtained in the previous phase (NLP module). The ranking uses the mapping function of MetaMap, in order to get the concept variants of noun phrases and verb phrases. Noun phrase ranking has already been used as a feature in the literature by Frunza and Inkpen [ 21 ] and got promising results. In our framework, we implemented the idea of verb phrase ranking along with noun phrases, because verb phrases are the main clue to find relation-based words in any text. The main purpose of phrasal ranking is to get domain specific concepts and to avoid noisy and irrelevant features. This domain specific phrasal ranking is the key to improve the performance of our relation extraction framework. The noun and verb phrases obtained in the NLP module of the framework are processed, in order to make them better representative feature for our relation classification.
We used UMLS [ 34 ] to rank both noun and verb phrases. This was done to measure the similarity of the concepts in the original corpus with the concept in the UMLS. MetaMap is a tool created by NLM that is used for mapping free text to medical concepts in the UMLS. UMLS consists of three knowledge sources, that is, The Metathesaurus, the Semantic Network, and the SPECIALIST Lexicon. For concept similarity or concept ranking, MetaMap uses its Metathesaurus as a knowledge source. With MetaMap API, we sent the list of noun phrases and verb phrases, which were obtained in the NLP phase of our framework. The authors in [ 21 ] also used MetaMap to rank concepts. The difference is that their ranking limits itself to noun phrases, while our method maps both noun phrases and verb phrases. Our rationale behind the use of verb phrases is very obvious, as verbs are the first indication of relation into the text. For each noun and verb phrase, variant noun and verb phrases are generated through MetaMap API. The variants or inflected forms of noun and verb phrases are then evaluated. This evaluation is done using the algorithm, which takes the list of variants generated by the MetaMap mapping function, processes them, and provides the best-ranked variant as output. The single top-ranked concept with the maximum score generated by our ranking algorithm is finally selected as a feature for our classifier. The pseudocode of our ranking algorithm is given below.
4.3.1. Concept Rank Algorithm
Our algorithm consists of three major steps as follows.
Step 1. Extract related concepts for all the noun and verb phrases from UMLS.
Step 2. Filter the extracted concept for two reasons: firstly, to address the diversity of corpus, and secondly, to filter out phrases that were not related to classes on the basis of three criteria:(1)the score of each concept which is given by MetaMap. A threshold of the concept Meta Mapping score is set as follows:(a)for noun phrase concepts threshold score ≥600,(b)for verb phrase concept threshold score ≥700,(2)the type of each concept, that is, Therapeutic or Preventive Procedure, Functional Concept, Qualitative Concept, and so forth,(3)if
dimensions of the phrase are mapped to a MetaMap concept,
dimensions of a phrase must exist in the MetaMap space.
Step 3. All the concepts filtered in Step 2 were selected as features for our classification algorithms.
For example the noun phrase "efficacy and safety" is extracted by the algorithm in Step 1 .
The three filters in Step 2 on "efficacy and safety" will be processed as given below.
The MetaMap score of both efficacy and safety is 1000, which qualifies the 1st filter: score ≥ 600 for noun phrases.
Types of the concepts selected were [Qualitative Concept] for efficacy and [Human-caused Phenomenon or Process] for safety, and it passed the 2nd filter in Step 2 .
In the 3rd filter, the phrase (efficacy and safety) has 3 words/dimensions. The words (efficacy) and (safety) match, while the word (and) does not match the MetaMap: here
, so
dimensions are matched. Hence, this phrase qualifies the
filter.
Step 3 of the algorithm will choose "efficacy and safety" as a feature for classification.
The top two noun phrases and verb phrase obtained from noun and verb phrase chunker further processed by MetaMap API are shown in Table 4 .
Table 4: Output of MetaMap API to rank the noun phrases and verb phrases.
4.4. Vector Representation (Vector Space Model)
The vector space model is a representation of the documents and concepts as vectors, in a multidimensional space. Its dimensions are the terms in the documents that are used to build an index. If a term occurs infrequently in the whole collection, but frequently in the document, a high-ranking score can be assigned to that document. This phase mainly focuses on conversion of text into vectors, so it can be later used for classification. Vector is represented with the use of features, and one important decision at this stage is to select each feature weight. Feature weight also affects the classification performance as shown from literature. Bag of word (BOW) model mostly uses three feature value representations:(1)term presence or binary feature values: if the feature is present in an instance the value will be 1 and 0 otherwise;(2)frequency feature values: the value is the number of times a feature appears in an instance or 0 if it did not appear;(3)Term Frequency-Inverse Document Frequency (TFIDF): it is a relative weight of the feature with its document frequency and requires more calculation to form a vector.
We used the term presence, because it is easy to formulate a document or text vector. It requires less processing and computation in machine learning tasks and better classification results [ 35 ]. We used the term presence using the formula:
4.5. Relation Classification
This phase mainly focuses on the classification of all those relations which exist between disease and treatment entities in the text corpora. Our main focus is to extract three main relations, that is, cure, prevent, and side effect relations. Support Vector Machine and Naïve Bayes algorithms are used for the classification of relations.
4.5.1. Support Vector Machine Algorithm
Support Vector Machines (SVMs) belong to the group of supervised learning used for the data analysis and pattern recognition. They offer a direct and open engineering solution for classification problems. Support Vector Machines (SVMs) have also been widely used in the protein-protein interaction extraction task and have shown competitive results over other learning methods ([ 36 , 37 ]). Relation extraction is a text classification problem. We used Support Vector Machine (SVM), as SVMs have already been used to yield higher accuracy on related tasks like text categorization [ 38 ]. The original implementation of SVM was designed for binary classification, while relation extraction can be a binary as well as multiclass classification problem. We have to extend the SVM for multiclass classification, for which a Library for Support Vector Machines (LIBSVM) [ 39 ] is used that is integrated software for support vector classification. We used the LIBSVM in both settings, that is, linear kernel and Radial Based Function (RBF) kernel. For linear kernel, the best results are obtained at
, while other parameters are on default settings. For RBF kernel, best results are when
and
.
When an SVM is used for classification, it is important that an appropriate kernel function is chosen. For classification tasks such as relation extraction, where the number of feature set is large, it has been reported [ 40 ] that a linear kernel is typically the most suitable. The authors of paper [ 41 ] also compared three types of SVM kernels (linear, quadratic, and cubic kernels) for relation extraction task. In their comparison, they reported that, with all the features, the linear kernel is better than both the quadratic and cubic ones.
The dataset used in our research consists of high-dimensional feature space. It has been reported [ 42 ] that SVMs build a separating hyper plane in a high-dimensional feature space in order to maximize the separability. This hyper plane uses the small training vectors, called the support vectors, in the original space. For a given finite set of learning patterns, the optimum separation hyper plane is the linear classifier with a maximum margin between the positive and the negative samples. The problem of relation extraction normally is not a binary classification task. So, before using the SVMs, it is mandatory to first reduce the relation extraction to a binary or multiclass classification process.
4.5.2. Naïve Bayes Algorithm
It is a probabilistic classifier which applies Bayes' theorem with strong independence assumptions on features and the order is irrelevant. Thus, the presence of one feature does not affect other features in classification tasks. Due to the precise nature of the probability model, the Naïve Bayes classifiers can be efficiently trained by the comparatively less amount of data to estimate the classification parameters. Due to the independence of variables, for each class, only the variances of the variables need to be determined, not the entire covariance matrix. We use the Naïve Bayes algorithm in our experimentation because the prevent and side effect relations have a very small amount of training data that is the main advantage of the Naïve Bayes classifier. The simplicity of Naïve Bayes also makes it attractive in numerous tasks with reasonable performance. Recently, [ 43 ] reported significant results for text classification by Naïve Bayes with SVM. Below are the general equations of the Naïve Bayes classifier:
Naive Bayes performs well on numeric and textual data, it is easy to implement and computationally simple as compared to other algorithms. However, due to conditional independence assumption, its performance is affected when data has correlated features.
5. Experimental Design
The settings are also taken from [ 21 ]; the reason for reusing these settings was ensuring the authenticity and comparison of results.
Setting 1. In this setting, we set up three models and two classes labeled as positive and negative. In the first model, positive class is cure, while negative class is Disonly, Treatonly. For the second model positive class is prevent relation, negative class once again is Disonly, Treatonly. For the third model, side effect is positive and Disonly, Treatonly are negative.
Setting 2. Here, we built three models again, each focused on one relation that can distinguish sentences that contain the relation, from sentences that do not contain any relevant information. The first model has cure as positive class and vague as negative class. In the second model, prevent relation is positive, while vague is negative whereas, in the third model, side effect is positive while vague is negative again.
Setting 3. Once again three models were built that distinguish the three relations: we have cure as positive, and we have prevent and side effect relations as negative. Prevent relation is positive and side effect is negative and side effect is positive and prevent relation is taken as negative class. This is a special setting in which all the three meaningful relation sentences (cure, prevent, and side effect) are considered as positive and negative alternatively, in order to check the performance of our feature set, so that the resultant relation is chosen accurately.
The detail of each setting is explained as in Table 5 .
Table 5: Detail of experimental settings.
6. Results and Discussion
Table 6 presents the results of our framework on the dataset for cure, prevent, and side effect relations. We chose these three relations from the dataset because these are the only meaningful relation types that exist in the dataset. Our dataset is designed by [ 19 ] and already used in research [ 21 , 23 ] with different settings for cure, prevent, and side effect relations. Ben Abacha and Zweigenbaum [ 23 ] used three different settings for each type of relation; the same settings are being used in our case. Our framework mainly focused on four types of features that have been extracted from the sentences in the datasets. This feature set consists of bag of word features (unigram), natural language processing features (verb and noun phrases), filtration of noun phrases features using UMLS, and filtration of verb phrases features from the UMLS.
Table 6: Results of all feature sets using classification algorithms.
We also used SVM and Naïve Bayes algorithms to classify our feature set because of the following reasons: SVM is used by many researchers as a baseline classifier for text classification task, and it is the most widely used classifier for text classification [ 43 ]. Naïve Bayes is used particularly for those example sets in data that have very less participation in the overall dataset. In our case, prevent and side effect relations have only 63 and 30 sentences available. Both of these relations showed exceptional results for Naïve Bayes, as it requires small training data to learn the classifier. We swapped the role of data; that is, the data used for training will be used for testing, and the data used for testing will be used for training. We used 10-fold cross validation when there is a limited amount of data for training and testing. 10-fold means, we split the data into 10 equal partitions and the process for 10 times, while ensuring that each partition is used for testing at once, such that 10% data is for testing and 90% is for training. Average the performance results of 10x iterations to get the final results. Table 6 shows the results of our framework in detail.
Table 6 is showing the results of all three settings with the combination of different feature sets. Accuracy, precision, recall, and
-score are the metrics used to evaluate the performance of our approach.
For cure relation, our approach achieved the best
-scores in settings 1, 2, and 3 as 85.10% (SVM linear) with BOW representation, 98.05% (SVM-RBF kernel) with our proposed UMLS (NP + VP) feature set, and 97.58% (SVM-RBF kernel) with UMLS (NP), respectively. It can be observed that, for cure relation, the overall best results of
-score are achieved with our proposed feature set.
For prevent relation, we achieved best
-scores in settings 1, 2, and 3 as 81.42% (SVM linear) with UMLS (NP), 93.55% (NB) with our proposed UMLS (NP + VP) feature set, and 91.06% (NB) with BOW representation, respectively. Again in this case the overall best
-score for prevent relation is achieved with our proposed feature set.
For side effect relation, the achieved best
-scores in settings 1, 2, and 3 as 30.43% (NB) with NLP, 74.20% (NB) with BOW representation, and 88.89% (NB) with our proposed UMLS (NP + VP), respectively. Once again, it can be observed that for side effect relation the overall best results of
-score are achieved with our proposed feature set. All the results for precision, recall metrics can be analyzed from Table 6 .
Table 7 presents a comparison of the accuracy results obtained in the previous work by [ 19 , 21 ] and our proposed approach. As we can see from the table, our technique has a major edge over previous results for all three relations. Our results are very consistent in terms of accuracy for all three relations. The improvement for cure relation is 1.19 percent points; prevent improved by 22.45 percent points, and side effect improved by 50.49 percent points in terms of accuracy.
Table 7: Classification of accuracy comparison with state-of-the-art approaches.
Table 8 compares our
-measure results with state-of-the-art approaches applied on the same corpus. Our approach clearly outperforms the work presented by [ 23 ] and the work of [ 21 ]. Frunza and Inkpen [ 21 ] reported good results on the same corpus in setting 3. However, these results are not reproducible using 10-fold cross validation. We used the same settings and got those results by training the classifier on 90% data and testing it on 10%. This shows that the results quoted in [ 21 ] are looking biased due to training and testing ratio.
Table 8:
-measure comparison with state-of-the-art approaches.
Our results mainly improved due to the reason that we used verb-based concept ranking from UMLS. Verb phrases are the main features for the relations in the text. All other features used in this approach are already used for text classification, except the ranking of verb phrases using UMLS. We ranked both the noun and verb phrase concepts using an algorithm in which we used filters to rank all the concepts on the basis of three different criteria. This improved our results significantly compared to the previous approaches on the same dataset. Our results are effective due to 10-fold cross validation and efficient in terms of
-measure and accuracy.
7. Conclusion and Future Directions
This research was aimed to build a relation extraction framework between medical entities from biomedical texts. We mainly focused on the extraction of semantic relations between treatments and diseases. The proposed approach relies on a hybrid feature set, which consists of (1) bag of word model, (2) natural language processing features, (3) lexical features, and (4) semantic features based on UMLS concepts. We used the supervised learning methods that used SVM and NB classifier to evaluate our feature set. We conducted experiments on this approach and compared it with the previous approaches [ 19 , 21 , 23 ]. The obtained results showed that our approach clearly outperformed the previous techniques and provided an alternative to improve the accuracy and
-measure of relation extraction in the biomedical domain: few training examples are available.
In future, we intend to test our approach with other types of relations and different corpora; we will also work on multistage classifier to enhance the performance of relation extraction. Some unsupervised techniques can be introduced for relation classification.
Conflict of Interests
The authors declare that there is no conflict of interests regarding the publication of this paper.
Acknowledgments
The authors are thankful to National Library of Medicine for creating UMLS and making it available for research and development. They are also thankful to National University of Sciences and Technology (NUST) for providing the opportunity and platform and mentoring and support which led to the accomplishment of the current research objectives.
References
L. J. Jensen, J. Saric, and P. Bork, "Literature mining for the biologist: from information retrieval to biological discovery," Nature Reviews Genetics, vol. 7, no. 2, pp. 119–129, 2006. View at Publisher · View at Google Scholar · View at Scopus
S. Ananiadou, D. B. Kell, and J.-I. Tsujii, "Text mining and its potential applications in systems biology," Trends in Biotechnology, vol. 24, no. 12, pp. 571–579, 2006. View at Publisher · View at Google Scholar · View at Scopus
W. W. Chapman and K. B. Cohen, "Current issues in biomedical text mining and natural language processing," Journal of Biomedical Informatics, vol. 42, no. 5, pp. 757–759, 2009. View at Publisher · View at Google Scholar · View at Scopus
P. Zweigenbaum, D. Demner-Fushman, H. Yu, and K. B. Cohen, "Frontiers of biomedical text mining: current progress," Briefings in Bioinformatics, vol. 8, no. 5, pp. 358–375, 2007. View at Publisher · View at Google Scholar · View at Scopus
M. E. Cusick, H. Yu, A. Smolyar et al., "Literature-curated protein interaction datasets perspective," Nature Methods, vol. 6, pp. 39–46, 2009. View at Publisher · View at Google Scholar
R. A.-A. Erhardt, R. Schneider, and C. Blaschke, "Status of text-mining techniques applied to biomedical text," Drug Discovery Today, vol. 11, no. 7-8, pp. 315–325, 2006. View at Publisher · View at Google Scholar · View at Scopus
D. Zhou and Y. He, "Extracting interactions between proteins from the literature," Journal of Biomedical Informatics, vol. 41, no. 2, pp. 393–407, 2008. View at Publisher · View at Google Scholar · View at Scopus
A. Airola, S. Pyysalo, J. Björne, T. Pahikkala, F. Ginter, and T. Salakoski, "All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning," BMC Bioinformatics, vol. 9, no. 11, article 52, 2008. View at Publisher · View at Google Scholar · View at Scopus
H. Kilicoglu and S. Bergler, "Adapting a general semantic interpretation approach to biological event extraction," in Proceedings of the BioNLP Shared Task Workshop, pp. 173–182, 2011.
W. A. Baumgartner Jr., K. B. Cohen, and L. Hunter, "An open-source framework for large-scale, flexible evaluation of biomedical text mining systems," Journal of Biomedical Discovery and Collaboration, vol. 3, article 1, 2008. View at Publisher · View at Google Scholar · View at Scopus
Y. Garten, A. Coulet, and R. B. Altman, "Recent progress in automatically extracting information from the pharmacogenomic literature," Pharmacogenomics, vol. 11, no. 10, pp. 1467–1489, 2010. View at Publisher · View at Google Scholar · View at Scopus
J. Hakenberg, Mining relations from the biomedical literature [Ph.D. thesis], 2009.
H.-C. Wang, Y.-H. Chen, H.-Y. Kao, and S.-J. Tsai, "Inference of transcriptional regulatory network by bootstrapping patterns," Bioinformatics, vol. 27, no. 10, Article ID btr155, pp. 1422–1428, 2011. View at Publisher · View at Google Scholar · View at Scopus
H. Liu, R. Komandur, and K. Verspoor, "From graphs to events: a subgraph matching approach for information extraction from biomedical text," in Proceedings of the BioNLP Shared Task 2011 Workshop, pp. 164–172, 2011.
H. Jang, J. Lim, J.-H. Lim, S.-J. Park, K.-C. Lee, and S.-H. Park, "Finding the evidence for protein-protein interactions from PubMed abstracts," Bioinformatics, vol. 22, no. 14, pp. e220–e226, 2006. View at Publisher · View at Google Scholar · View at Scopus
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi, "Automated extraction of information on protein-protein interactions from the biological literature," Bioinformatics, vol. 17, no. 2, pp. 155–161, 2001. View at Publisher · View at Google Scholar · View at Scopus
J.-J. Kim, Z. Zhang, J. C. Park, and S.-K. Ng, "BioContrasts: extracting and exploiting protein-protein contrastive relations from biomedical literature," Bioinformatics, vol. 22, no. 5, pp. 597–605, 2006. View at Publisher · View at Google Scholar · View at Scopus
C. Giuliano, A. Lavelli, L. Romano, and V. Sommarive, "Exploiting shallow linguistic information for relation extraction from biomedical literature," EACL, vol. 18, pp. 401–408, 2006. View at Google Scholar
B. Rosario and M. A. Hearst, "Classifying semantic relations in bioscience texts," in Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL '04), Article 430, Barcelona, Spain, July 2004. View at Publisher · View at Google Scholar
M. Huang, X. Zhu, and M. Li, "A hybrid method for relation extraction from biomedical literature," International Journal of Medical Informatics, vol. 75, no. 6, pp. 443–455, 2006. View at Publisher · View at Google Scholar · View at Scopus
O. Frunza and D. Inkpen, "Extraction of disease-treatment semantic relations from biomedical sentences," in Proceedings of the Workshop on Biomedical Natural Language Processing (ACL '10), pp. 91–98, Uppsala, Sweden, July 2010.
A. Sharma, R. Swaminathan, and H. Yang, "A verb-centric approach for relationship extraction in biomedical text," in Proceedings of the 4th IEEE International Conference on Semantic Computing (ICSC '10), pp. 377–385, IEEE, Pittsburgh, Pa, USA, September 2010. View at Publisher · View at Google Scholar · View at Scopus
A. Ben Abacha and P. Zweigenbaum, "A hybrid approach for the extraction of semantic relations from medline abstracts," in 12th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing '11), Tokyo, Japan, February 2011, vol. 6609 of Lecture Notes in Computer Science, part II, pp. 139–150, Springer, Berlin, Germany, 2011. View at Google Scholar
A. Ben Abacha and P. Zweigenbaum, "Automatic extraction of semantic relations between medical entities: application to the treatment relation," Journal of Biomedical Semantics, vol. 2, supplement 5, p. S4, 2011. View at Google Scholar
H. Yang, R. Swaminathan, A. Sharma, V. Ketkar, and J. D'Silva, "Mining biomedical text towards building a quantitative food-disease-gene network," in Learning Structure and Schemas from Documents, vol. 375 of Studies in Computational Intelligence, pp. 205–225, Springer, Berlin, Germany, 2011. View at Publisher · View at Google Scholar
R. A. Kadir and B. Bokharaeian, "Overview of biomedical relations extraction using hybrid rule-based approaches," Journal of Industrial and Intelligent Information, vol. 1, no. 3, pp. 169–173, 2013. View at Publisher · View at Google Scholar
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva, "Getting more out of biomedical documents with gate's full lifecycle open source text analytics," PLoS Computational Biology, vol. 9, no. 2, Article ID e1002854, 2013. View at Publisher · View at Google Scholar · View at Scopus
H. Cunningham, D. Maynard, K. Bontcheva et al., Text Processing with GATE (Version 6), Department of Computer Science, University of Sheffield, 2011.
in the case of long queries) and
(although small values can sometimes report improvements).
Finding the set of optimal parameters is costly to compute, since they have local maxima that are singularity values [ 35 ]. Hence, we are using a simplistic optimization approach. The best values obtained in our tests with the Cystic Fibrosis corpora are
,
(see Table 5 ).
Table 5: Testing values of the parameters for BM25. MAP is used as the evaluation measure.
For the TF-IDF weighting algorithm with Okapi TF formula, the parameters obtained for Okapi BM25 are the best approach, using the correspondence shown in Table 3 . Moreover, the values obtained with the Log TF and Raw TF formulas without parameters were studied, verifying that they are worse than the approximation BM25 (see Table 6 ).
Table 6: MAP values for the TF-IDF BM25, Raw TF, and
formulas.
Many researchers use the BM25 algorithm in articles, steering their studies to retrieve information in several fields, not only in the Abstract [ 31 , 36 , 37 ]. By this assumption, we test how the MAP measure increases if we look for documents related to the queries in the Abstract, Title, and Mesh fields using the BM25, TF-IDF BM25, TF-IDF Log TF and TF-IDF Raw TF formulas (see Table 7 ).
Table 7: MAP values to retrieving in Abstract, Title, and MeSH fields using different weighting algorithms.
The results obtained with Okapi BM25 are consistent with those presented by Trotman in [ 36 , 37 ], which show a value of
in the MAP measure with the same collection.
In Tables 6 and 7 we can see that the best MAP results are obtained using TF-IDF BM25 TF formula, so we continue our study with this approximation.
3.3. Query Expansion Processes Testing
3.3.1. Pseudorelevance Feedback
To improve the results of the previous processes, we proceed to make pseudo relevance feedback using the Rocchio algorithm implemented in Lemur, explained in Section 2 .
For the first time, it is necessary to parameterize the algorithm. The parameters for Rocchio are
number of documents in feedback
,
number of terms selected in feedback
, and
Table 8 shows the best values of the parameters obtained for the Cystic Fibrosis collection.
Table 8: Finding the best values of the parameters for TF-IDF BM25 feedback.
After the parameterization, we test how the MAP increases if we look for documents related to the queries in the Abstract, Title, and Mesh fields, using the TF-IDF BM25 weighting algorithm (see Table 9 ). These results are comparable with those obtained by Shin and Han in their expansion system presented in [ 8 ], which achieved a maximum value of
for R-prec, with our R-prec being greater than
.
Table 9: Evaluation measures using the pseudorelevance feedback in Abstract, Title, and MeSH fields.
3.3.2. Use of MeSH to Expand Queries
As many authors have already worked with MeSH fields to retrieve information, we focus this part of the research on testing its efficiency in query expansion. Our method is based on the work of Kwangcheol Shin and Han [ 8 ], which proves the advantage of using MeSH Headings to expand the queries instead of working with terms not related to the MeSH fields.
The new strategy consists of expanding the queries with the MeSH Headings related to its terms. For the first time, we analyze the query and extract its keywords. Each keyword is automatically introduced at the PubMed online tool to match it with the Entry terms stored in MEDLINE. If the keyword is an Entry Term, we search the associated MeSH Headings (major and minor). Finally, the query is reformulated adding the descriptors contained in the MeSH fields extracted in the previous step.
In Table 10 the results of the experiment with the proposed strategy are shown. The query expansion is applied in the Abstract, Title, and MeSH fields of the documents and the weighting algorithm is TF-IDF BM25. As shown, the results are similar to those obtained with pseudo relevance feedback (Table 9 ), and are comparable with those obtained by Shin and Han in their expansion system [ 8 ].
Table 10: Evaluation measures using query expansion with descriptors, applied in MeSH, Title, and Abstract fields.
Finally, a Recall-Precision graph (see Figure 2 ) is included showing the improvement obtained with the query expansion methods using the MeSH, Abstract, and Title fields (Tables 9 and 10 ) with respect to the document retrieval using only the Abstract field with TF-IDF BM25 (see Table 7 ). The figure shows that curves of the query expansion algorithms for the combination Porter stemmer-NLM stopwords are closest to the upper right hand corner of the graph (where recall and precision are maximized), which indicates the best performance.
Figure 2: Recall-Precision curve obtained with the query expansion methods using the MeSH, Abstract, and Title fields.
4. Conclusions
We have developed and evaluated preprocessing and query expansion techniques for retrieving documents in several fields of biomedical articles belonging to the corpus Cystic Fibrosis, a corpus of MEDLINE documents. We test the benefit of using stemming and stopwords in the preprocessing of documents and queries, following the investigations of other authors.
Studies were carried out to compare the weighting algorithms Okapi BM25 and TF-IDF available in the Lemur tool, concluding that TF-IDF with
formula given by BM25 approximation is superior in its results.
Document retrieval based on Abstract, MeSH, and Title fields seems more effective than looking at each of these fields individually. In addition, the use of relevance feedback, a technique widely used by researchers in this field, produces a great improvement in the retrieval of scientific documents. The Rocchio algorithm allows obtaining good results, improving MAP and other measures.
Finally, we perform a study to improve searching expanding queries with MeSH terms. For this, we have enhanced queries locating Entry terms in them and obtaining MeSH Headings in PubMed in order to expand the original query and to map it with the documents. The results are good, making use of the Title, Abstract and MeSH fields to improve the list of documents retrieved, compared to baseline methods.
In this paper, authors have used a very simplistic approach to determine the BM25 parameters values. Tuning the BM25 free parameters (
,
, and
) is a difficult and computationally expensive problem that requires advanced multidimensional optimization techniques. Retrieval accuracy can be improved using more advanced parameterization methods.
Conflict of Interests
The authors declare that there is no conflict of interests regarding the publication of this paper.
Acknowledgment
This work has been funded from the European Union Seventh Framework Programme (FP7/REGPOT-2012-2013.1) under Grant agreement no. 316265, BIOCAPS.
References
R. A. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley Longman, 1999.
C. D. Maning, P. Raghavan, and H. Schtze, An Introduction to Information Retrieval, Cambridge University Press, Cambridge, UK, 2008.
D. Hiemstra and D. van Leeuwen, Creating A Dutch information Retrieval Test Corpus, 2002.
A. A. Chang, K. M. Heskett, and T. M. Davidson, "Searching the literature using medical subject headings versus text word with PubMed," Laryngoscope, vol. 116, no. 2, pp. 336–340, 2006. View at Publisher · View at Google Scholar · View at Scopus
M. H. Coletti and H. L. Bleich, "Medical subject headings used to search the biomedical literature," Journal of the American Medical Informatics Association, vol. 8, no. 4, pp. 317–323, 2001. View at Google Scholar · View at Scopus
J. O. Ebbert, D. M. Dupras, and P. J. Erwin, "Searching the medical literature using PubMed: a tutorial," Mayo Clinic Proceedings, vol. 78, no. 1, pp. 87–91, 2003. View at Google Scholar · View at Scopus
L. V. Gault, M. Shultz, and K. J. Davies, "Variations in Medical Subject Headings (MeSH) mapping: from the natural language of patron terms to the controlled vocabulary of mapped lists," Journal of the Medical Library Association, vol. 90, no. 2, pp. 173–180, 2002. View at Google Scholar · View at Scopus
K. Shin and S. Y. Han, "Improving information retrieval in medline by modulating mesh term weights," in Natural Language Processing and Information Systems, F. Meziane and E. Mtais, Eds., vol. 3136, Lecture Notes in Computer Science, pp. 137–166, Springer, Berlin, Germany, 2004. View at Google Scholar
M. F. Porter, "An algorithm for suffx stripping," Program, vol. 14, no. 3, pp. 130–137, 1980. View at Google Scholar
J. Leveling and G. J. F. Jones, "Sub-word indexing and blind relevance feedback for English, Bengali, Hindi, and Marathi IR," ACM Transactions on Asian Language Information Processing, vol. 9, no. 3, article 12, 2010. View at Publisher · View at Google Scholar · View at Scopus
G. K. Zipf, Human Behavior and the Principle of Least Effort, Addison-Wesley, Reading, Mass, USA, 1949.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford, Okapi at Trec-3, 1996.
S. E. Robertson and K. S. Jones, "Relevance weighting of search terms," Journal of the American Society for Information Science, vol. 27, no. 3, pp. 129–146, 1976. View at Google Scholar · View at Scopus
J. Ramos, Using Tf-Idf to Determine Word Relevance in Document Queries, 2003.
S. Robertson, "Understanding inverse document frequency: on theoretical arguments for IDF," Journal of Documentation, vol. 60, no. 5, pp. 503–520, 2004. View at Publisher · View at Google Scholar · View at Scopus
S. Gauch, J. Wang, and S. M. Rachakonda, "A corpus analysis approach for automatic query expansion and its extension to multiple databases," ACM Transactions on Information Systems, vol. 17, no. 3, pp. 250–269, 1999. View at Google Scholar · View at Scopus
J. Guo, G. Xu, X. Cheng, and H. Li, "Named entity recognition in query," in Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '09), J. Allan, J. A. Aslam, M. Sanderson, C. Zhai, and J. Zobel, Eds., pp. 267–274, ACM, 2009.
S. Karimi, J. Zobel, and F. Scholer, "Quantifying the impact of concept recognition on biomedical information retrieval," Information Processing and Management, vol. 48, no. 1, pp. 94–106, 2012. View at Publisher · View at Google Scholar · View at Scopus
C. J. Crouch and B. Yang, "Experiments in automatic statistical thesaurus construction," in Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '92), pp. 77–88, New York, NY, USA, June 1992. View at Scopus
Y. Qiu and H. P. Frei, "Concept based query expansion," in Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 160–169, July 1993. View at Scopus
E. D. Liddy and S.-H. Myaeng, "Tipster panel—dr-link's linguistic-conceptual approach to document detection," in TREC, pp. 113–130, 1992. View at Google Scholar
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller, "Introduction to wordnet: an on-line lexical database," International Journal of Lexicography, vol. 3, no. 4, pp. 235–244, 1990. View at Publisher · View at Google Scholar · View at Scopus
L. Borrajo, R. Romero, E. L. Iglesias, and C. M. Redondo Marey, "Improving imbalanced scientific text classification using sampling strategies and dictionaries," Journal of Integrative Bioinformatics, vol. 8, no. 3, p. 176, 2011. View at Google Scholar · View at Scopus
L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia, "Overview of BioCreAtIvE: critical assessment of information extraction for biology," BMC Bioinformatics, vol. 6, supplement 1, article S1, 2005. View at Publisher · View at Google Scholar · View at Scopus
G. D. Zhou, "Recognizing names in biomedical texts using hidden markov model and SVM plus sigmoid," in Proceedings of the COLING International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP '04), N. Collier, P. Ruch, and A. Nazarenko, Eds., pp. 1–7, Geneva, Switzerland.
R. Apweiler, A. Bairoch, C. H. Wu et al., "UniProt: the universal protein knowledgebase," Nucleic Acids Research, vol. 32, pp. D115–D119, 2004. View at Google Scholar · View at Scopus
M. Mitra, A. Singhal, and C. Buckley, "Improving automatic query expansion," in Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 206–214, ACM Press, 1998.
S. Abdou, "Evaluation of stemming, query expansion and manual indexing approaches for the genomic task," in Proceedings of the 15th Text REtrieval Conference Proceedings (TREC '05), pp. 863–871, 2005.
Z. Yang, H. Lin, Y. Li, B. Liu, and Y. Lu, "Trec genomics track experiments at dutai," in Text REtrieval Conference, 2005.
T. Brants and  Google Inc, "Natural language processing in information retrieval," in Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands, pp. 1–13, 2004.
Y. Fan, X. Huang, and  York University at TREC, "Enterprise email discussion search," in Proceedings of the 15th Text Retrieval Conference (TREC '06), M. Ellen Voorhees and P. Lori Buckland, Eds., vol. 500-272, National Institute of Standards and Technology (NIST), 2006.
S. E. Robertson and S. Walker, "Okapi/keenbow at trec-8," 1999.
E. Greengrass, "Information retrieval: a survey," Tech. Rep. tr-r52-008-001, united States Department of Defense, 2001. View at Google Scholar
S. E. Robertson and S. Walker, Microsoft Cambridge at Trec-9: Filtering Track, 2001.
S. Robertson and H. Zaragoza, "The probabilistic relevance framework: BM25 and beyond," Foundations and Trends in Information Retrieval, vol. 3, no. 4, pp. 333–389, 2009. View at Publisher · View at Google Scholar · View at Scopus
A. Trotman, An Artificial Intelligence Approach to Information Retrieval, 2004.
A. Trotman, Learning to Rank. Information Retrieval, vol. 8, 2005.
Pharma, medicine and veterinary
Medical and veterinary information in STN
We offer our experience and access to wide range of medical and veterinary databases in STN International, where over 30 databases in the field of medicine, pharmacy and veterinary medicine is available. The best known and most commonly used are:  Medline, Embase, Derwent Drug File, ADIS, Biosis, Pascal, IMS R&D, Kosmet, Scisearch, Toxcenter, Drugmonog, Biotechno, VETU, CABA.
 
We offer single retrospective search on selected topics - e.g. to compare the efficacy and safety of treatment, suitable literature for expert reports, literature for PSURs, publications about clinical trials. For veterinary medicine - information about residues of substances in the body/animal products, etc. Increasingly, we also provide regular weekly or monthly monitoring for pharmacovigilance purposes.
 
We would like to offer you free of charge trial (first preview) to any issue, recommend the most appropriate databases and based on the number of adequate publications and records, we will prepare a price offer.
 
Tobias Sing and Martin Däumer.
Introduction: selection of antiretroviral therapy
Physicians treating HIV-1-infected patients are faced with selecting an optimal new regimen upon therapy failure. This task is highly complex because of the increasing number of available antiretroviral drugs, significant cross-resistance and the likely presence of archived drug-resistant viral variants selected by previous regimens. Parameters with potential impact on treatment decisions include the plasma viral load, CD4 cell count, viral genotype, phenotype and replication capacity, and pharmacological data. Other factors to consider include tolerability, toxicity and the ability to preserve future treatment options.
Interpretation algorithms are designed to assist the treating physician in choosing an optimal drug combination using information from drug-resistance testing. In this context, `interpretation' refers to the task of predicting a specific factor (i.e. drug activity or virological response) from one or more other factors. The word `algorithm' originates from computer science and denotes a set of well-defined instructions for accomplishing a given task. In clinical practice, the most popular interpretation algorithms are rule-based approaches for predicting drug activity from the viral genotype. Published reviews provide in-depth coverage of rule-based approaches [ 1 , 2 ]. However, the field has broadened considerably in scope and methodology, and new tools are currently being developed.
The factors of interest: drug activity, viral phenotype and virological response
Interpretation algorithms vary considerably in goals and methodology. However, due to the lack of a commonly accepted terminology, they are often collectively referred to as `resistance algorithms' or `resistance scores'. To avoid confusion, this chapter distinguishes between algorithms for predicting in vivo antiviral activity of a single drug, in vitro viral phenotypic resistance or replication capacity, and in vivo virological response to a combination of drugs. From the clinical point of view, we are ultimately interested in the latter (measured as the decline in plasma viral load). However, drug activity and viral phenotype can be valuable intermediates in predicting responses to combination therapy. For an appropriate use of interpretation algorithms, it is crucial to understand the differences between these three key terms.
Phenotypic drug resistance and replication capacity refer to the ability of a virus to replicate in the presence and absence of drug, respectively. These parameters are measured using controlled and reproducible experimental assays. Results of phenotypic assays are typically reported as the fold change (FC) in the drug concentration that inhibits viral growth by 50% (IC
50
) relative to a control wild-type virus for each drug. The replication capacity is reported as a percentage relative to the reference strain. It should be kept in mind that the ability of a virus to replicate decreases continually with increasing drug concentration, and that measurements of FC and replication capacity can be used to summarise this dose–response curve.
Replication capacity assays have only recently been introduced and are yet to gain a firm place in clinical practice; however, assays for measuring phenotypic resistance have been in use for about 10 years and are able to predict virological response [ 3 – 5 ]. Nevertheless, this association is not perfect, and several cases of discordance between FC values and virological response in vivo have been reported. For example, the reverse transcriptase (RT ) mutation M184V is associated with resistance to didanosine (ddI) and abacavir (ABC), but in isolation has little or no discernible impact on virological response to these drugs [ 6 ]. Similarly, patients who develop M184V during lamivudine (3TC) monotherapy maintain a plasma viral load of approximately 0.5 log
10
copies/ml lower than at baseline, despite maximal levels of phenotypic resistance to the drug [ 7 ]. Based on these observations, the clinical use of phenotypic resistance data has been said to suffer from `serious limitations as the association between in vitro resistance levels and virological response is often not well characterized' [ 6 ]. However, it is conceivable that these apparent shortcomings can be alleviated by taking into account additional features of the dose–response curve. For example, replication capacity assays show a marked replicative defect associated with the M184V mutation [ 8 ], thus providing a possible explanation for the observed virological suppression despite phenotypic resistance. In summary, phenotypic drug-resistance data alone cannot explain the full range of in vivo virological response effects.
The concept of in vivo antiviral activity of a single drug is slightly more abstract than in vitro phenotype and in vivo virological response. The objective of this measurement is to capture information about the antiviral effect of a single drug. However, drugs are generally given in combination, and synergistic and antagonistic effects between the drugs in a regimen play a significant role in determining virological response; an up-to-date review can be found in Reference [ 9 ], and the University of Liverpool provides an informative on-line resource [ 10 ]. Thus, it is conceptually problematic to evaluate in isolation a model of single-drug activity because the effects induced by the drug are not independent from the effects of co-administered drugs. Consequently, models of drug activity are often evaluated in the context of combination therapy, where an additional algorithm is used to combine the activity of individual drugs into a score for the drug combination. While this suggests a straightforward evaluation procedure, it is important to emphasise that it is the integration of a drug activity model and an activity combination step that is evaluated here, and not an activity model in isolation. Nevertheless, different activity models can be compared by keeping the combination algorithm fixed. To summarise, the concept of in vivo drug activity has proven to be a useful intermediate between in vitro measurements and the virological response to a combined regimen.
Predicting in vivo antiviral activity of individual drugs
Algorithms for predicting in vivo drug activity can be either derived from automatic, data-driven statistical procedures or developed by panels of experts. They can be formulated as simple tables or as complex computational procedures. Finally, they can be based on genotype, phenotype or genotype with predicted phenotype as an intermediate step, and can also take into account additional relevant factors, such as pharmacokinetic data.
Predicting in vivo drug activity from the viral genotype
As genotyping is used widely, algorithms for predicting in vivo activity from the genotype provide valuable tools to support decision-making in clinical practice. The main limitations in relating the viral genotype to drug activity include paucity of monotherapy data and the diversity and complexity of mutational patterns. Currently, more than 80 mutations have been implicated in drug resistance in vitro or reduced drug activity in vivo [ 11 ], and the number is likely to grow as more patients are exposed to existing agents in varying combinations [ 12 – 14 ], increasing attention is devoted to non-B HIV-1 subtypes (see Chapter 14 ), new compounds are introduced and new resistance mutations are identified from large databases.
The most popular genotype interpretation algorithms include: HIVdb (Stanford University Medical Center, CA, USA); RetroGram (Virology Networks BV, Utrecht, the Netherlands); RegaInst (Rega Institute for Medical Research and University Hospitals Leuven, Katholieke Universiteit Leuven, Belgium); ANRS AC11 (Agence Nationale de Recherches sur le SIDA); and HIV-GRADe (developed by a German standardisation initiative). The genotyping assays ViroSeq HIV-1 Genotyping System (Abbott) and TruGene GuideLines (Bayer Diagnostics) are each provided with one of the two commercial algorithms approved by the US Food and Drug Administration. These algorithms are updated regularly and most of the non-commercial ones can be used via convenient web services ( Table 1 ). The remainder of this section will briefly compare the scoring schemes, reported levels of drug activity, and the design and evaluation of these different approaches.
: Selection of available systems for predicting in vivo activity, in vitro phenotypic resistance and virological response to combination therapy from genotype.
Scoring schemes
At the heart of most genotype interpretation algorithms are simple scoring schemes, based on the presence or absence of specific mutations. For example, one of the rules for predicting resistance to ddI in the current version of the Rega algorithm is based on the scheme [K65R+T69G/N+T69 insertion+L74V+Q151M]. Thus, a genotype with mutation K65R and L74V would be assigned a score of 2. Scores are often turned into a fixed set of activity levels (such as active versus non-active) via the choice of a cut-off (see below). Our example would be classified as `ddI inactive' by the Rega algorithm because the scheme comes with a cut-off of 1. Scores may also take into account resensitisation effects induced by specific mutations. For example, one of the ANRS rules for predicting ddI resistance is based on the scheme [M41L +T69D+L74V+T215Y/F+K219E/Q–K70R–M184V/I], where the presence of K70R or M184V/I leads to a subtraction from the score.
In reality, not all mutations contribute equally to reduced activity of a drug. The HIVdb algorithm accommodates this observation by assigning a weight to each mutation. Again, negative weights indicate resensitisation effects of varying strength. For example, the current HIVdb ddI scoring scheme is based on 51 mutations with weights ranging from +2 to +50. When limited to the mutations given in the abovementioned REGA score, the HIVdb score reads as follows: [30·K65R+20·T69G+40·T69 insertion+50·L74V+50·Q151M]. Non-linear interactions between specific mutations are usually ignored in these models.
Levels of drug activity
Although drug activity is a continuum, for ease of usage, scores are typically split into a small number of drug activity levels. The most popular approaches employ three classes of drug activity: active, inactive and intermediately active. Unfortunately, most activity algorithms use the terms susceptible and resistant which should be restricted to approaches for predicting in vitro drug resistance. For example, the ANRS score for tenofovir [M41L+E44D+D67N+T69D/N/S+L74V+L210W+T215F/Y] is categorised using the genotypic cut-offs `≤3 mutations' (active), `3–5 mutations' (partially active) and `≥6 mutations' (inactive). The ANRS algorithm assigns only two levels of activity to non-nucleoside reverse transcriptase inhibitors (NNRTIs) and the fusion inhibitor enfuvertide, in contrast to the situation with nucleoside reverse transcriptase inhibitors (NRTIs) and protease inhibitors (PIs). Other approaches that use three activity levels include GuideLines, RegaInst and ViroSeq. Four levels are used by HIV-GRADe and RetroGram (which have an additional special class `insufficient data' for refraining from a prediction), whereas HIVdb has five distinct activity levels. Although these categories of activity may facilitate interpretation, the use of raw scores may be useful when predicting response to drug combinations [ 5 ].
Design and evaluation
Statistical approaches for deriving drug activity from genotype and treatment response data are summarised in Reference [ 6 ]. Crucial decisions include the way the response is measured and the time of analysis. Typically, relevant mutations are first identified by using a feature-selection method, such as simple univariate statistical tests, followed by a multivariate model-building procedure. This approach has been used for example in the ANRS Narval study [ 15 ]. However, most methods are not based on statistical analysis, but rather on consensus from expert panels based on current literature. Several comparative evaluations of these models have been performed [ 4 , 16 ], and further initiatives are on the way [ 17 ].
Predicting in vivo drug activity from the viral phenotype
At first sight, predicting drug activity from FC values obtained in phenotypic assays may seem easier than interpreting genotypic results. However, despite this apparent simplicity, there is no gold standard for interpreting phenotypic results. The bulk of work on this topic has focused on the choice of the appropriate FC cut-offs to categorise drugs into `active' or `non-active'. The use of cut-offs is based on the assumption that drugs have a binary activity profile, being completely active below, and completely inactive above, a given level of phenotypic resistance. However, for most antiretroviral drugs, the relation between phenotypic resistance and in vivo activity is not an all-or-nothing phenomenon. Distinct levels of activity can be derived from FC data by setting two cut-offs, one for diminished activity (the lower cut-off) and one for abolished activity (the higher cut-off). For example, the VircoTYPE system (Virco BVBA) has recently adopted two cut-offs for each drug, categorising the continuous drug activity as full, reduced by 20% or reduced by 80% [ 18 ].
Remarkably, recent studies indicate that the use of models based on cut-offs does not necessarily improve prediction of virological response. Swanstrom et al. [ 5 ] compared activity derived from a single cut-off with a continuous activity score in which FC values below 2.5 and above 10 were mapped to 0 (completely inactive) and 1 (completely active), respectively, with linear interpolation in between. The model using continuous activity score, but not that with cut-offs, was significantly associated with virological response.
An interesting combination of a dichotomous model with a continuous score is described by Beerenwinkel et al. [ 19 ]. Based on the observation that the distribution of predicted resistance factors closely follows a mixture of two Gaussians (the two components accounting for susceptible and resistant samples), activity of a drug is defined as the probability that the corresponding FC value belongs to the susceptible population.
Predicting in vitro resistance and replication capacity from the viral genotype
Phenotypic assays are more expensive and laborious than genotypic assays, and thus less suitable for routine clinical use. There is also evidence that phenotype predictions derived from genotypic data might be more accurate than a single phenotypic measurement as a result of the inherent variability of phenotypic assays [ 20 ]. Moreover, genotypic testing can detect the presence of resistance mutations that signal emerging resistance before a significant effect on phenotype is measurable in vitro. For these reasons, a considerable amount of work has been devoted to developing systems for accurately predicting the viral phenotype from genotypic data.
To date, only two comprehensive systems are available, the VirtualPhenotype [ 21 ] provided by Virco BVBA and geno2pheno [ 22 – 24 ], which is freely available (see Table 1 ). Both systems are based on statistical learning methodology [ 25 ], which constructs predictive models from `training' data in the form of matched genotype–phenotype pairs. Despite this common background, the models are very different in nature: the VirtualPhenotype is derived from an instance-based method, also known as nearest-neighbour learning. From a query sequence, a drug-specific mutational profile is derived, based on predefined sequence positions. All samples with this profile are then retrieved from a large database of experimentally determined genotype–phenotype pairs. The predicted phenotype is the mean FC of these matched samples. geno2pheno is based on a much smaller database (~900 samples per drug) than the VirtualPhenotype. Whereas the latter is essentially model-free, relying only on averaging experimentally determined phenotypes, geno2pheno is based on a mathematical model known as the support vector machine (SVM). In the training step, sequences with known phenotype are mapped into a high-dimensional vector space. In this space, a hyperplane is computed which optimally approximates the genotype–phenotype relation. The final SVM models of geno2pheno are high-dimensional linear models that assign weights to individual mutations, which are then added to yield the predicted FC, similar to in the HIVdb model. The VirtualPhenotype has been shown to be superior to standard phenotyping (when using a single measurement), whereas the prediction for geno2pheno is highly correlated with phenotypic resistance for most drugs.
Other approaches have been considered based on a variety of statistical learning approaches, including linear discriminant analysis [ 26 ], linear regression with [ 27 ] or without interaction terms [ 28 ], a novel non-parametric statistical method [ 29 ], decision trees [ 23 ] and artificial neural networks [ 30 ], in addition to a semi-supervised approach based on self-organising maps using predicted structural features of the enzyme–ligand complex [ 31 ].
All approaches based on statistics or statistical learning crucially rely on `training' data in the form of matched genotype–phenotype pairs. However, such data are hard to obtain for novel drugs, thus delaying the development of genotype–phenotype models. A possible approach to deriving genotype–phenotype models without training data is structure-based phenotyping. This method is based on the assumption that resistance can be predicted from computing changes in binding energy using molecular modelling. The only prerequisite is the availability of at least one crystal structure of the target molecule, which is then adapted computationally to specific mutant genotypes. For example, Cengent Therapeutics and Quest Diagnostics have developed structure-based phenotyping models for six PIs and, in comparing the models to the PhenoSense or Antivirogram assays, report squared correlation coefficients between 0.34 and 0.83, depending on drug and data source. Whereas the correlation is lower than that of the data-driven approaches, structure-based phenotyping may be useful at stages when genotype–phenotype data are scarce. Intrinsic drawbacks include the dependence on an adequate crystal structure (which can be problematic in the case of the envelope glycoprotein gp120 or the transmembrane glycoprotein gp41) and the inability to capture effects that take place across large distances in the enzyme or that happen far from the active site.
As mentioned above, drug activity is a function of multiple factors, including resistance. One of these factors, which can also be measured in vitro, is the viral replication capacity. Models for predicting activity from FC and replication capacity might be superior to approaches based on FC alone. Thus, as in the case of phenotypic resistance, approaches for predicting replication capacity from genotype may prove useful in routine clinical practice. To date, two approaches have been suggested. Segal et al. have used decision trees and random forests [ 32 ] and a custom `deletion/substitution/addition' regression algorithm has been developed by Birkner et al. [ 33 ]. In both models, the M184V mutation was predictably found to be associated with reduced replication capacity, along with a number of other mutations. A comprehensive screening for protease and RT positions associated with changes in replication capacity has been performed [ 34 ], based on almost 10,000 matched genotype–replication capacity pairs (the position list is available in the supporting on-line material in Reference [ 35 ]).
Predicting in vivo virological response to combination therapy
The most common approach to predicting virological response to a combination of drugs proceeds in two steps. The activities of the individual drugs in the regimen are predicted using one of the models described above, and combined into an overall response score for the whole regimen. One method simply sums up all predicted activities for the drugs in the regimen. Depending on the data source, this approach leads to the calculation of a genotypic (GSS) or phenotypic (PSS) susceptibility score [ 3 ]. For example, given a viral population with the RT mutation K65R and the protease mutations V32I and I47A, in addition to using the ANRS algorithm for predicting activity, a regimen comprising ABC, 3TC and ritonavir-boosted lopinavir would be assigned a GSS of 0.5+0.5+0=1, whereas the combination of zidovudine (ZDV), ddI and efavirenz (EFV) would attain a score of 1+0.5+1=2.5. Likewise, given phenotypic measurements, scores of 0 and 1 (or intermediate scores) are assigned based on FC cut-offs. Susceptibility scores can also be weighted, for example with different drug potencies, or can be derived from continuous activity models. Indeed, Swanstrom et al. [ 5 ] showed that a PSS based on continuous activity is superior to a score based on a cut-off.
Both the GSS and PSS suffer from ignoring drug–drug interactions. An alternative to these susceptibility scores has been suggested based on a very conservative model of drug–drug interactions [ 19 ]. Whereas susceptibility scores assume that the effect of single drugs combines additively both within and across drug classes, the alternative model assumes that only the most active drug from each drug class determines the response to a drug combination. Using this model, the combination of ZDV, ddI and EFV in the aforementioned example (RT, K65R; protease, V32I and I47A; evaluated with ANRS) would be scored as 1 (NRTIs)+1 (NNRTIs)=2 because ddI is less effective than ZDV and thus ignored. At first sight, this approach implies no immediate benefit of having more than one drug from a class in a particular regimen as only the most active drug in each class is scored. However, in Reference [ 19 ], not only is the current state of the viral population (as represented by the genotype) considered in scoring a drug combination, but also – by traversing the evolutionary neighbourhood of the current genotype – possible escape mutants. The most active drug from each class will usually vary across these escape mutants, which leads to `indirect' positive interactions between compounds from the same class. These intraclass synergies are demonstrated using the example of a wild-type strain and the combination of ZDV/3TC, which differ significantly in their phenotypic resistance profiles. When taking into account only the current genotype (no consideration of escape mutants), the same score is assigned to the combination of both drugs as to the drugs in isolation. However, when the evolutionary neighbourhood is traversed and possible escape mutants are considered, the benefits of the combination can be observed. For example, in the case of two escape mutants, one with a thymidine analogue mutation and the other with K65R, 3TC and ZDV, respectively, will still be active, but the other drug will show reduced activity. Thus, the combination will be active on both escape mutants, whereas the single-drug regimens would only be active on one of them.
As summarised above, scoring the mutational neighbourhood can improve response prediction. It seems intuitive that the neighbourhood search probes the genetic barrier to the drug combination and, as we have seen, the combination of ZDV/3TC has a higher genetic barrier than the individual drugs. However, the search, as it is performed by Beerenwinkel et al. [ 19 ], corresponds to a simple notion of genetic barrier, which is based on counting the number of substitutions necessary to acquire resistance (or lose activity). This simple definition ignores the different probabilities and timings at which these mutations occur. Thus, a more realistic assumption relies on defining the genetic barrier as the probability that the virus will not develop resistance within a given time period. An estimation method for this `probabilistic' genetic barrier is described in Reference [ 36 ]. The application of genetic barriers in predicting response to combination therapy is described in References [ 37 , 38 ], and implemented in the THEO (THErapy Optimizer) system [ 39 ].
Another two-step approach for predicting response to combination therapy has been suggested based on the use of fuzzy logic [ 40 ], but there are also several approaches that try to relate baseline parameters directly with response to a drug combination. For example, the HIV Resistance Response Database Initiative is experimenting with neural networks, using mutations, drugs and viral load as direct inputs [ 41 , 42 ]. Recently, another `direct' approach consisting of a nearest neighbour classifier has been suggested [ 43 ]: given baseline genotype, viral load and a drug combination, similar cases with known outcome are retrieved from a clinical database and the average of the outcomes is taken as the predicted response (as with the VirtualPhenotype). The success of the method crucially depends on the similarity measure being used. Certainly, comparative evaluations are needed to identify the advantages and limitations of all these approaches.
Summary
This chapter outlines the currently available interpretation algorithms for supporting the use of antiretroviral therapy. Several rule-based algorithms for predicting in vivo drug activity from the viral genotype are regularly updated by panels of experts and represent the most widely known tools to support decision-making. The label `resistance interpretation algorithm' commonly assigned to these systems is misleading because all of them partially or completely rely on in vivo response data and should therefore be termed `drug activity algorithms'.
Drug resistance, defined as the ability of the virus to replicate in the presence of drug, and measurable using phenotypic assays, is a major determinant of in vivo response. Consequently, approaches with and without cut-offs have been developed for predicting virological response from phenotypic resistance data. Although phenotypic resistance and viral replication capacity can be measured using experimental assays, these are more laborious and expensive than standard genotyping and thus less suitable for routine clinical use. Thus, it is desirable to be able to accurately predict both quantities from genotypic data and considerable advances have been made in this field.
Finally, in this chapter, the current approaches for predicting virological response to a drug combination have been reviewed. This is without doubt the `holy grail' of the field, and we are still far from satisfying solutions. The mainstream approach is to use a susceptibility score in which the in vivo response to a drug combination is computed by adding the activities of the individual components. Alternatives have been proposed with the aim of modelling drug–drug interactions in non-additive ways, or of directly relating genotype to response with combination therapy, without resorting to individual drug activity models.
The future
Several topics are expected to play a crucial role as the field of interpretation algorithms moves forwards.
Reference datasets and standardised evaluation schemes
One important obstacle to progress in the field is the lack of large and publicly available clinical outcome data that can serve as a standard benchmark for the evaluation and comparison of interpretation algorithms. An additional problem is that the evaluation procedures differ between studies. As a consequence, it is difficult to compare reported performance measures (such as correlation coefficients or error rates) across different studies. Although comparisons of different methods are being published, they fall short of their objectives as long as they do not reach maximal reproducibility by publishing the underlying data, for example, via the Stanford HIV Drug Resistance Database [ 44 ].
Decision support for co-receptor antagonists
The introduction of co-receptor inhibitors in clinical practice may require monitoring of co-receptor usage before, during and after treatment. Clinical practice would greatly benefit from reliable genotype-based prediction models for co-receptor usage, as these would be cheaper and faster than phenotypic assays. All genotypic approaches described to date are based on the third hypervariable (V3) loop of the envelope gp120, known as the major determinant of co-receptor usage. The most popular genotypic prediction is based on the classical 11/25 rule, predicting virus that uses the chemokine co-receptor CXCR4 (X4 virus) in the presence of positively charged residues at positions 11 or 25 of the V3 region. However, several statistical learning methods have been shown to improve the sensitivity of detecting X4 virus [ 45 – 47 ] (reviewed in Reference [ 48 ]). Recently, it was shown that the prediction performance can be further improved by incorporating immunological markers (such as CD4 counts) into predictive models as surrogate markers for undetected minority viral variants [ 49 ]. Despite these advances, genotypic prediction of co-receptor usage needs further improvements, for example by considering positions outside the V3 region.
Improved interpretability
Paradoxically, many interpretation algorithms – excluding rule-based algorithms – lack easy interpretability (in the sense that a user can easily understand the rationale for a prediction), and are mainly used as `black box' prediction systems. This is especially true with modern statistical learning methods such as neural networks and SVMs, although advances have been made in this respect [ 50 ]. To be clinically useful, the output of prediction systems should at least be augmented with well-calibrated confidence estimates. Moreover, sensitivity analyses and other techniques should be used to provide a certain degree of interpretability even for highly non-linear models such as neural networks.
Additional baseline parameters for activity and response prediction
Baseline genotype is the most popular piece of information for predicting activity, augmented by baseline viral load for predicting virological response. As more sophisticated models are being developed, the types of input that can be used by these models will expand. This might include (predicted) replication capacity, antiretroviral treatment history [ 41 ], adherence data [ 42 ] or immunological parameters. It remains a major challenge to integrate pharmacokinetic data, as obtained from therapeutic drug monitoring, into algorithms for predicting activity or response. The concept of inhibitory quotient, calculated as the ratio of the minimum plasma concentration (C
min
) for the drug to the level of drug resistance is a first step in this direction.
Incorporating information on salvage options and therapy sequencing into regimen scoring
Current algorithms predict virological response to a given drug combination. However, long-term planning requires considering the future treatment options should the regimen fail. Would there be enough remaining treatment options, or would further options be severely limited by the emergence of cross-resistance? Without doubt, questions related to optimal therapy sequencing will become increasingly important with the ever-rising number of approved anti-HIV-1 drugs and increasing number of individuals on long-term therapy.
Recommendations for clinical practice
Interpretation algorithms are designed to support, not replace, the treating physician. The guidance they provide is further improved by expert opinion.
Given the availability of several systems for different tasks (see Table 1 ), it is advisable not to rely on a single system, but to collect and consider recommendations from a variety of systems.
Rule-based scores for predicting in vivo drug activity from genotype are currently the most widely used interpretation algorithms. However, as activity prediction is difficult, the output should be used in an informed way, along with other treatment-relevant information.
Tools for predicting phenotype from genotype provide reliable predictions and can be used, in addition to other tools, to obtain a comprehensive overview of the current treatment situation.
A variety of tools for predicting response to combination therapy are being developed (and one is already available), but these have not yet matured to the degree of reliability provided by systems for predicting phenotype from genotype.
Acknowledgments
Tobias Sing would like to thank Thomas Lengauer and Niko Beerenwinkel for their support and advice. Work at the Max Planck Institute was performed in the context of the European Union Network of Excellence BioSapiens, grant no. LSHG-CT-2003-503265.
References
1.
Schmidt B, Walter H, Zeitler N, Korn K. Genotypic drug resistance interpretation systems -- the cutting edge of antiretroviral therapy. AIDS Rev. 2002;4:148–156. [ PubMed: 12416449 ]
2.
Zolopa A, Kiazand A and Race E. Resistance tests interpretation. In: HIV Infection: Antiretroviral Resistance. (Clavel F, Soriano V and Zolopa A, eds), Bash Medical Publishing, Paris, 2004, pp 77–92.
3.
DeGruttola V, Dix L, D'Aquila R. et al. The relation between baseline HIV drug resistance and response to antiretroviral therapy: re-analysis of retrospective and prospective studies using a standardized data analysis plan. Antivir Ther. 2000;5:41–48. [ PubMed: 10846592 ]
4.
De Luca A, Cingolani A, Di Giambenedetto S. et al. Variable prediction of antiretroviral treatment outcome by different systems for interpreting genotypic human immunodeficiency virus type 1 drug resistance. J Infect Dis. 2003;187:1934–1943. [ PubMed: 12792870 ]
5.
Swanstrom R, Bosch RJ, Katzenstein D. et al. Weighted phenotypic susceptibility scores are predictive of the HIV-1 RNA response in protease inhibitor-experienced HIV-1-infected subjects. J Infect Dis. 2004;190:886–893. [ PubMed: 15295692 ]
6.
Brun-Vezinet F, Costagliola D, Khaled MA. et al. Clinically validated genotype analysis: guiding principles and statistical concerns. Antivir Ther. 2004;9:465–478. [ PubMed: 15456077 ]
7.
Eron JJ, Benoit SL, Jemsek J. et al. Treatment with lamivudine, zidovudine, or both in HIV-positive patients with 200 to 500 CD4+ cells per cubic millimeter. North American HIV Working Party. N Engl J Med. 1995;333:1662–1669. [ PubMed: 7477218 ]
8.
Devereux HL, Emery VC, Johnson MA, Loveday C. Replicative fitness in vivo of HIV-1 variants with multiple drug resistance-associated mutations. J Med Virol. 2001;65:218–224. [ PubMed: 11536226 ]
9.
Boffito M, Acosta E, Burger D. et al. Therapeutic drug monitoring and drug-drug interactions involving antiretroviral drugs. Antivir Ther. 2005;10:469–477. [ PubMed: 16038472 ]
10.
​.hiv-druginteractions.org (accessed on 3 February 2006).
11.
Johnson VA, Brun-Vezinet F, Clotet B. et al. Update of the drug resistance mutations in HIV-1: 2005. Top HIV Med. 2005;13:51–57. [ PubMed: 15849371 ]
12.
Gonzales MJ, Wu TD, Taylor J. et al. Extended spectrum of HIV-1 reverse transcriptase mutations in patients receiving multiple nucleoside analog inhibitors. AIDS. 2003;17:791–799. [ PMC free article: PMC2573403 ] [ PubMed: 12660525 ]
13.
Wu TD, Schiffer CA, Gonzales MJ. et al. Mutation patterns and structural correlates in human immunodeficiency virus type 1 protease following different protease inhibitor treatments. J Virol. 2003;77:4836–4847. [ PMC free article: PMC152121 ] [ PubMed: 12663790 ]
14.
Ceccherini-Silberstein F, Svicher V, Sing T. et al. Involvement of novel HIV-1 reverse transcriptase mutations in the highly ordered regulation of NRTI resistance. Antivir Ther. 2005;10:S106.
15.
Brun-Vezinet F, Descamps D, Ruffault A. et al. Clinically relevant interpretation of genotype for resistance to abacavir. AIDS. 2003;17:1795–1802. [ PubMed: 12891065 ]
16.
De Luca A, Cozzi-Lepri A, Perno CF. et al. Variability in the interpretation of transmitted genotypic HIV-1 drug resistance and prediction of virological outcomes of the initial HAART by distinct systems. Antivir Ther. 2004;9:743–752. [ PubMed: 15535412 ]
17.
Costagliola D, Cozzi-Lepri A, Dalban C, Cheng B. Initiatives for developing and comparing genotype interpretation systems step 1: external validation of existing rules-based algorithm for abacavir and ddI evaluated on virological response. Antivir Ther. 2005;10:S11.
18.
Bacheler L, Winters B, Nauwelaers D. et al. Estimation of phenotypic clinical cutoffs for VirtualPhenotype through meta analyses of clinical trial and cohort data. Antivir Ther. 2004;9:S154.
19.
Beerenwinkel N, Lengauer T, Daumer M. et al. Methods for optimizing antiviral combination therapies. Bioinformatics. 2003;19(suppl 1):i16–25. [ PubMed: 12855433 ]
20.
Van Houtte M, Vermeiren H, Lecocq P et al. The VirtualPhenotype™ analysis of an HIV-1 genotype provides a more accurate prediction of drug susceptibility than a single phenotype measurement. 13th International Symposium on HIV & Emerging Infectious Diseases, Toulon, 2004, Abstr. OP 8.9.
21.
Larder B, De Vroey V, Dehertogh P. et al. Predicting HIV-1 phenotypic resistance from genotype using a large phenotype-genotype relational database. Antivir Ther. 1999;4(suppl 1):S41.
22.
Beerenwinkel N, Schmidt B, Walter H. et al. Geno2pheno: interpreting genotypic HIV drug resistance tests. IEEE Intelligent Systems. 2001;16:35–41.
23.
Beerenwinkel N, Beerenwinkel N, Schmidt B. et al. Diversity and complexity of HIV-1 drug resistance: a bioinformatics approach to predicting phenotype from genotype. Proc Natl Acad Sci U S A. 2002;99:8271–8276. [ PMC free article: PMC123057 ] [ PubMed: 12060770 ]
24.
Beerenwinkel N, Däumer M, Oette M. et al. Geno2pheno: estimating phenotypic drug resistance from HIV-1 genotypes. Nucleic Acids Res. 2003;31:3850–3855. [ PMC free article: PMC168981 ] [ PubMed: 12824435 ]
25.
Hastie T, Tibshirani R and Friedman J. The Elements of Statistical Learning. Springer, New York, 2001.
26.
Sevin AD, DeGruttola V, Nijhuis M. et al. Methods for investigation of the relationship between drug-susceptibility phenotype and human immunodeficiency virus type 1 genotype with applications to AIDS clinical trials group 333. J Infect Dis. 2000;182:59–67. [ PubMed: 10882582 ]
27.
Vermeiren H, Van den Bulcke T, Van Marck H. et al. Application of multiple linear regression modelling to the quantitative prediction of HIV-1 drug susceptibility phenotype from viral genotype. Antivir Ther. 2004;9:S122.
28.
Wang K, Jenwitheesuk E, Samudrala R, Mittler JE. Simple linear model provides highly accurate genotypic predictions of HIV-1 drug resistance. Antivir Ther. 2004;9:343–352. [ PubMed: 15259897 ]
29.
DiRienzo AG, DeGruttola V, Larder B, Hertogs K. Non-parametric methods to predict HIV drug susceptibility phenotype from genotype. Stat Med. 2003;22:2785–2798. [ PubMed: 12939786 ]
30.
Wang D, Larder B. Enhanced prediction of lopinavir resistance from genotype by use of artificial neural networks. J Infect Dis. 2003;188:653–660. [ PubMed: 12934180 ]
31.
Draghici S, Potter RB. Predicting HIV drug resistance with neural networks. Bioinformatics. 2003;19:98–107. [ PubMed: 12499299 ]
32.
Segal MR, Barbour JD, Grant RM. Relating HIV-1 sequence variation to replication capacity via trees and forests. Stat Appl Genet Mol Biol. 2004;3:Article 2. [ PubMed: 16646798 ]
33.
Birkner MD, Sinisi SE, van der Laan M. Multiple testing and data adaptive regression: an application to HIV-1 sequence data. Stat Appl Genet Mol Biol. 2005;4:Article 8. [ PubMed: 16646861 ]
34.
Bonhoeffer S, Chappey C, Parkin NT. et al. Evidence for positive epistasis in HIV-1. Science. 2004;306:1547–1550. [ PubMed: 15567861 ]
35.
US Department of Veterans Affairs
Additional file
Timeline of genetic and electronic health record-based research. A timeline of major milestones in the development of EHR-derived genetic research.
Footnotes
The authors declare that they have no competing interests.
Contributor Information
Joshua C Denny, Email: ude.tlibrednav@ynned.hsoj .
References
1. Manolio TA. Genomewide association studies and assessment of the risk of disease. N Engl J Med. 2010;363:166–176. doi: 10.1056/NEJMra0905980. [ PubMed ] [ Cross Ref ]
2. Welter D, MacArthur J, Morales J, Burdett T, Hall P, Junkins H, Klemm A, et al.  The NHGRI GWAS Catalog, a curated resource of SNP-trait associations. Nucleic Acids Res. 2014;42:D1001–D1006. doi: 10.1093/nar/gkt1229. [ PMC free article ] [ PubMed ] [ Cross Ref ]
3. Boycott KM, Vanstone MR, Bulman DE, MacKenzie AE. Rare-disease genetics in the era of next-generation sequencing: discovery to translation. Nat Rev Genet. 2013;14:681–691. doi: 10.1038/nrg3555. [ PubMed ] [ Cross Ref ]
4. SIGMA Type 2 Diabetes Consortium. Williams AL, Jacobs SB, Moreno-Macías H, Huerta-Chagoya A, Churchhouse C, et al.  Sequence variants in SLC16A11 are a common risk factor for type 2 diabetes in Mexico. Nature. 2014;506:97–101. [ PMC free article ] [ PubMed ]
5. Consortium GLG, Willer CJ, Schmidt EM. Sengupta S, Peloso GM, Gustafsson S, et al. Discovery and refinement of loci associated with lipid levels. Nat Genet. 2013;45:1274–1283. doi: 10.1038/ng.2797. [ PMC free article ] [ PubMed ] [ Cross Ref ]
6. Weeke P, Muhammad R, Delaney JT, Shaffer C, Mosley JD, Blair M, et al.  Whole-exome sequencing in familial atrial fibrillation. Eur Heart J. 2014;35:2477–2483. doi: 10.1093/eurheartj/ehu156. [ PMC free article ] [ PubMed ] [ Cross Ref ]
7. Haines JL, Hauser MA, Schmidt S, Scott WK, Olson LM, Gallins P, et al.  Complement factor H variant increases the risk of age-related macular degeneration. Science. 2005;308:419–421. doi: 10.1126/science.1110359. [ PubMed ] [ Cross Ref ]
8. The Wellcome Trust Case Control Consortium  Genome-wide association study of 14,000 cases of seven common diseases and 3,000 shared controls. Nature. 2007;447:661–678. doi: 10.1038/nature05911. [ PMC free article ] [ PubMed ] [ Cross Ref ]
9. The Wellcome Trust Case Control Consortium. http://www.wtccc.org.uk/ .
10. Framingham Heart Study. https://www.framinghamheartstudy.org/ .
11. H3Africa Consortium. Rotimi C, Abayomi A, Abimiku A, Adabayeri VM, Adebamowo C, et al.  Research capacity. Enabling the genomic revolution in Africa. Science. 2014;344:1346–1348. doi: 10.1126/science.1251546. [ PMC free article ] [ PubMed ] [ Cross Ref ]
12. Bowton E, Field JR, Wang S, Schildcrout JS, Van Driest SL, Delaney JT, et al.  Biobanks and electronic medical records: enabling cost-effective research. Sci Transl Med. 2014;6:234cm233. doi: 10.1126/scitranslmed.3008604. [ PMC free article ] [ PubMed ] [ Cross Ref ]
13. Kohane IS. Using electronic health records to drive discovery in disease genomics. Nat Rev Genet. 2011;12:417–428. doi: 10.1038/nrg2999. [ PubMed ] [ Cross Ref ]
14. Bachenheimer JF, Brescia BA. Reinventing patient recruitment: revolutionary ideas for clinical trial success. Gower: Aldershot; 2007.
15. Ioannidis JP, Thomas G, Daly MJ. Validating, augmenting and refining genome-wide association signals. Nat Rev Genet. 2009;10:318–329. doi: 10.1038/nrg2544. [ PubMed ] [ Cross Ref ]
16. Spencer CC, Su Z, Donnelly P, Marchini J. Designing genome-wide association studies: sample size, power, imputation, and the choice of genotyping chip. PLoS Genet. 2009;5 doi: 10.1371/journal.pgen.1000477. [ PMC free article ] [ PubMed ] [ Cross Ref ]
17. Visscher PM, Brown MA, McCarthy MI, Yang J. Five years of GWAS discovery. Am J Hum Genet. 2012;90:7–24. doi: 10.1016/j.ajhg.2011.11.029. [ PMC free article ] [ PubMed ] [ Cross Ref ]
18. Mardis ER. A decade's perspective on DNA sequencing technology. Nature. 2011;470:198–203. doi: 10.1038/nature09796. [ PubMed ] [ Cross Ref ]
19. Metzker ML. Sequencing technologies - the next generation. Nat Rev Genet. 2010;11:31–46. doi: 10.1038/nrg2626. [ PubMed ] [ Cross Ref ]
20. DNA Sequencing Costs: Data from the NHGRI Genome Sequencing Program. http://www.genome.gov/sequencingcosts/ .
21. Hennekam RC, Biesecker LG. Next-generation sequencing demands next-generation phenotyping. Hum Mut. 2012;33:884–886. doi: 10.1002/humu.22048. [ PMC free article ] [ PubMed ] [ Cross Ref ]
22. Leipzig M. Implementing an electronic medical record system in ambulatory care. Hosp Technol Ser. 1996;15:2–4. [ PubMed ]
23. Sado AS. Electronic medical record in the intensive care unit. Crit Care Clin. 1999;15:499–522. doi: 10.1016/S0749-0704(05)70068-3. [ PubMed ] [ Cross Ref ]
24. Mikulich VJ, Liu YC, Steinfeldt J, Schriger DL. Implementation of clinical guidelines through an electronic medical record: physician usage, satisfaction and assessment. Int J Med Inform. 2001;63:169–178. doi: 10.1016/S1386-5056(01)00177-0. [ PubMed ] [ Cross Ref ]
25. Scott JT, Rundall TG, Vogt TM, Hsu J. Kaiser Permanente's experience of implementing an electronic medical record: a qualitative study. BMJ. 2005;331:1313–1316. doi: 10.1136/bmj.38638.497477.68. [ PMC free article ] [ PubMed ] [ Cross Ref ]
26. Samoutis G, Soteriades ES, Kounalakis DK, Zachariadou T, Philalithis A, Lionis C. Implementation of an electronic medical record system in previously computer-naive primary care centres: a pilot study from Cyprus. Inform Prim Care. 2007;15:207–216. [ PubMed ]
27. Gleser M. Benefits and obstacles for hospital executives of the electronic medical record. Healthc Inf Manage. 1993;7:32–34. [ PubMed ]
28. McFall EL. An electronic medical record–delivering benefits today. Healthc Inform. 1993;10:76–78. [ PubMed ]
29. Wagner MM, Hogan WR. The accuracy of medication data in an outpatient electronic medical record. J Am Med Inform Assoc. 1996;3:234–244. doi: 10.1136/jamia.1996.96310637. [ PMC free article ] [ PubMed ] [ Cross Ref ]
30. Gill JM, Ewen E, Nsereko M. Impact of an electronic medical record on quality of care in a primary care office. Del Med J. 2001;73:187–194. [ PubMed ]
31. Rollman BL, Hanusa BH, Gilbert T, Lowe HJ, Kapoor WN, Schulberg HC. The electronic medical record. A randomized trial of its impact on primary care physicians' initial management of major depression [corrected] Arch Int Med. 2001;161:189–197. doi: 10.1001/archinte.161.2.189. [ PubMed ] [ Cross Ref ]
32. Kinn JW, Marek JC, O'Toole MF, Rowley SM, Bufalino VJ. Effectiveness of the electronic medical record in improving the management of hypertension. J Clin Hypertens. 2002;4:415–419. doi: 10.1111/j.1524-6175.2002.01248.x. [ PubMed ] [ Cross Ref ]
33. Laing K. The benefits and challenges of the computerized electronic medical record. Gastroenterol Nurs. 2002;25:41–45. doi: 10.1097/00001610-200203000-00002. [ PubMed ] [ Cross Ref ]
34. Hripcsak G, Albers DJ. Next-generation phenotyping of electronic health records. J Am Med Inform Assoc. 2013;20:117–121. doi: 10.1136/amiajnl-2012-001145. [ PMC free article ] [ PubMed ] [ Cross Ref ]
35. Wilke RA, Xu H, Denny JC, Roden DM, Krauss RM, McCarty CA, et al.  The emerging role of electronic medical records in pharmacogenomics. Clin Pharmacol Ther. 2011;89:379–386. doi: 10.1038/clpt.2010.260. [ PMC free article ] [ PubMed ] [ Cross Ref ]
36. Newton KM, Peissig PL, Kho AN, Bielinski SJ, Berg RL, Choudhary V, et al.  Validation of electronic medical record-based phenotyping algorithms: results and lessons learned from the eMERGE network. J Am Med Inform Assoc. 2013;20:e147–e154. doi: 10.1136/amiajnl-2012-000896. [ PMC free article ] [ PubMed ] [ Cross Ref ]
37. Tate AR, Beloff N, Al-Radwan B, Wickson J, Puri S, Williams T, et al.  Exploiting the potential of large databases of electronic health records for research using rapid search algorithms and an intuitive query interface. J Am Med Inform Assoc. 2014;21:292–298. doi: 10.1136/amiajnl-2013-001847. [ PMC free article ] [ PubMed ] [ Cross Ref ]
38. Bailar JC., 3rd The practice of meta-analysis. J Clin Epidemiol. 1995;48:149–157. doi: 10.1016/0895-4356(94)00149-K. [ PubMed ] [ Cross Ref ]
39. Kho AN, Pacheco JA, Peissig PL, Rasmussen L, Newton KM, Weston N, et al.  Electronic medical records for genetic research: results of the eMERGE consortium. Sci Transl Med. 2011;3:79re71. doi: 10.1126/scitranslmed.3001807. [ PMC free article ] [ PubMed ] [ Cross Ref ]
40. Phenotype KnowledgeBase. https://phekb.org/ .
41. Monda KL, Chen GK, Taylor KC, Palmer C, Edwards TL, Lange LA, et al.  A meta-analysis identifies new loci associated with body mass index in individuals of African ancestry. Nat Genet. 2013;45:690–696. doi: 10.1038/ng.2608. [ PMC free article ] [ PubMed ] [ Cross Ref ]
42. Postmus I, Trompet S, Deshmukh HA, Barnes MR, Li X, Warren HR, et al.  Pharmacogenetic meta-analysis of genome-wide association studies of LDL cholesterol response to statins. Nat Commun. 2014;5:5068. doi: 10.1038/ncomms6068. [ PMC free article ] [ PubMed ] [ Cross Ref ]
43. NIH RePORTER. http://projectreporter.nih.gov/reporter.cfm .
44. Tatonetti NP, Denny JC, Murphy SN, Fernald GH, Krishnan G, Castro V, et al.  Detecting drug interactions from adverse-event reports: interaction between paroxetine and pravastatin increases blood glucose levels. Clin Pharmacol Ther. 2011;90:133–142. doi: 10.1038/clpt.2011.83. [ PMC free article ] [ PubMed ] [ Cross Ref ]
45. Hersh WR, Weiner MG, Embi PJ, Logan JR, Payne PR, Bernstam EV, et al.  Caveats for the use of operational electronic health record data in comparative effectiveness research. Med Care. 2013;51:S30–S37. doi: 10.1097/MLR.0b013e31829b1dbd. [ PMC free article ] [ PubMed ] [ Cross Ref ]
46. Henderson GE, Cadigan RJ, Edwards TP, Conlon I, Nelson AG, Evans JP, et al.  Characterizing biobank organizations in the U.S.: results from a national survey. Genome Med. 2013;5:3. doi: 10.1186/gm407. [ PMC free article ] [ PubMed ] [ Cross Ref ]
47. Roden DM, Xu H, Denny JC, Wilke RA. Electronic medical records as a tool in clinical pharmacology: opportunities and challenges. Clin Pharmacol Ther. 2012;91:1083–1086. doi: 10.1038/clpt.2012.42. [ PMC free article ] [ PubMed ] [ Cross Ref ]
48. Delaney JT, Ramirez AH, Bowton E, Pulley JM, Basford MA, Schildcrout JS, et al.  Predicting clopidogrel response using DNA samples linked to an electronic health record. Clin Pharmacol Ther. 2012;91:257–263. doi: 10.1038/clpt.2011.221. [ PMC free article ] [ PubMed ] [ Cross Ref ]
49. Wei WQ, Leibson CL, Ransom JE, Kho AN, Chute CG. The absence of longitudinal data limits the accuracy of high-throughput clinical phenotyping for identifying type 2 diabetes mellitus subjects. Int J Med Inform. 2013;82:239–247. doi: 10.1016/j.ijmedinf.2012.05.015. [ PMC free article ] [ PubMed ] [ Cross Ref ]
50. Denny JC, Crawford DC, Ritchie MD, Bielinski SJ, Basford MA, Bradford Y, et al.  Variants near FOXE1 are associated with hypothyroidism and other thyroid conditions: using electronic medical records for genome- and phenome-wide studies. Am J Hum Genet. 2011;89:529–542. doi: 10.1016/j.ajhg.2011.09.008. [ PMC free article ] [ PubMed ] [ Cross Ref ]
51. Ritchie MD, Denny JC, Crawford DC, Ramirez AH, Weiner JB, Pulley JM, et al.  Robust replication of genotype-phenotype associations across multiple diseases in an electronic medical record. Am J Hum Genet. 2010;86:560–572. doi: 10.1016/j.ajhg.2010.03.003. [ PMC free article ] [ PubMed ] [ Cross Ref ]
52. Million Veteran Program. http://www.research.va.gov/mvp/ .
53. The Kaiser Permanente Research Program on Genes, Environment, and Health. http://www.rpgeh.kaiser.org .
54. Gottesman O, Kuivaniemi H, Tromp G, Faucett WA, Li R, Manolio TA, et al.  The Electronic Medical Records and Genomics (eMERGE) Network: past, present, and future. Genet Med. 2013;15:761–771. doi: 10.1038/gim.2013.72. [ PMC free article ] [ PubMed ] [ Cross Ref ]
55. McCarty CA, Chisholm RL, Chute CG, Kullo IJ, Jarvik GP, Larson EB, et al.  The eMERGE Network: a consortium of biorepositories linked to electronic medical records data for conducting genomic studies. BMC Med Genet. 2011;4:13. [ PMC free article ] [ PubMed ]
56. Carroll RJ, Thompson WK, Eyler AE, Mandelin AM, Cai T, Zink RM, et al.  Portability of an algorithm to identify rheumatoid arthritis in electronic health records. J Am Med Inform Assoc. 2012;19:e162–e169. doi: 10.1136/amiajnl-2011-000583. [ PMC free article ] [ PubMed ] [ Cross Ref ]
57. Kho AN, Hayes MG, Rasmussen-Torvik L, Pacheco JA, Thompson WK, Armstrong LL, et al.  Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes within a genome-wide association study. J Am Med Inform Assoc. 2012;19:212–218. doi: 10.1136/amiajnl-2011-000439. [ PMC free article ] [ PubMed ] [ Cross Ref ]
58. Wei WQ, Feng Q, Weeke P, Bush W, Waitara MS, Iwuchukwu OF, et al.  Creation and validation of an EMR-based algorithm for identifying major adverse cardiac events while on statins. AMIA Jt Summits Transl Sci. 2014;2014:112–119. [ PMC free article ] [ PubMed ]
60. DeCODE genetics. http://www.decode.com .
61. The UK Biobank. http://www.ukbiobank.ac.uk .
62. Sulem P, Gudbjartsson DF, Walters GB, Helgadottir HT, Helgason A, Gudjonsson SA, et al.  Identification of low-frequency variants associated with gout and serum uric acid levels. Nat Genet. 2011;43:1127–1130. doi: 10.1038/ng.972. [ PubMed ] [ Cross Ref ]
63. Gudbjartsson DF, Bjornsdottir US, Halapi E, Helgadottir A, Sulem P, Jonsdottir GM, et al.  Sequence variants affecting eosinophil numbers associate with asthma and myocardial infarction. Nat Genet. 2009;41:342–347. doi: 10.1038/ng.323. [ PubMed ] [ Cross Ref ]
64. Gudmundsson J, Sulem P, Gudbjartsson DF, Masson G, Petursdottir V, Hardarson S, et al.  A common variant at 8q24.21 is associated with renal cell cancer. Nat Commun. 2013;4:2776. doi: 10.1038/ncomms3776. [ PubMed ] [ Cross Ref ]
65. Stefansson H, Meyer-Lindenberg A, Steinberg S, Magnusdottir B, Morgen K, Arnarsdottir S, et al.  CNVs conferring risk of autism or schizophrenia affect cognition in controls. Nature. 2014;505:361–366. doi: 10.1038/nature12818. [ PubMed ] [ Cross Ref ]
66. Helgason A, Hrafnkelsson B, Gulcher JR, Ward R, Stefansson K. A populationwide coalescent analysis of Icelandic matrilineal and patrilineal genealogies: evidence for a faster evolutionary rate of mtDNA lineages than Y chromosomes. Am J Hum Genet. 2003;72:1370–1388. doi: 10.1086/375453. [ PMC free article ] [ PubMed ] [ Cross Ref ]
67. Pukkala E, Andersen A, Berglund G, Gislefoss R, Gudnason V, Hallmans G, et al.  Nordic biological specimen banks as basis for studies of cancer causes and control–more than 2 million sample donors, 25 million person years and 100,000 prospective cancers. Acta Oncol. 2007;46:286–307. doi: 10.1080/02841860701203545. [ PubMed ] [ Cross Ref ]
68. Furu K, Wettermark B, Andersen M, Martikainen JE, Almarsdottir AB, Sørensen HT. The Nordic countries as a cohort for pharmacoepidemiological research. Basic Clin Pharmacol Toxicol. 2010;106:86–94. doi: 10.1111/j.1742-7843.2009.00494.x. [ PubMed ] [ Cross Ref ]
69. Chen Z, Lee L, Chen J, Collins R, Wu F, Guo Y, et al.  Cohort profile: the Kadoorie Study of Chronic Disease in China (KSCDC) Int J Epidemiol. 2005;34:1243–1249. doi: 10.1093/ije/dyi174. [ PubMed ] [ Cross Ref ]
70. Triendl R. Japan launches controversial Biobank project. Nat Med. 2003;9:982. doi: 10.1038/nm0803-982b. [ PubMed ] [ Cross Ref ]
71. Nakamura Y. The BioBank Japan Project. Clin Adv Hematol Oncol. 2007;5:696–697. [ PubMed ]
72. Murphy SN, Weber G, Mendis M, Gainer V, Chueh HC, Churchill S, et al.  Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2) J Am Med Inform Assoc. 2010;17:124–130. doi: 10.1136/jamia.2009.000893. [ PMC free article ] [ PubMed ] [ Cross Ref ]
73. Sun W, Rumshisky A, Uzuner O. Evaluating temporal relations in clinical text: 2012 i2b2 Challenge. J Am Med Inform Assoc. 2013;20:806–813. doi: 10.1136/amiajnl-2013-001628. [ PMC free article ] [ PubMed ] [ Cross Ref ]
74. Uzuner O, Goldstein I, Luo Y, Kohane I. Identifying patient smoking status from medical discharge records. J Am Med Inform Assoc. 2008;15:14–24. doi: 10.1197/jamia.M2408. [ PMC free article ] [ PubMed ] [ Cross Ref ]
75. Wood GC, Still CD, Chu X, Susek M, Erdman R, Hartman C, et al.  Association of chromosome 9p21 SNPs with cardiovascular phenotypes in morbid obesity using electronic health record data. Genomic Med. 2008;2:33–43. doi: 10.1007/s11568-008-9023-z. [ PMC free article ] [ PubMed ] [ Cross Ref ]
76. Kurreeman F, Liao K, Chibnik L, Hickey B, Stahl E, Gainer V, et al.  Genetic basis of autoantibody positive and negative rheumatoid arthritis risk in a multi-ethnic cohort derived from electronic health records. Am J Hum Genet. 2011;88:57–69. doi: 10.1016/j.ajhg.2010.12.007. [ PMC free article ] [ PubMed ] [ Cross Ref ]
77. Crawford DC, Crosslin DR, Tromp G, Kullo IJ, Kuivaniemi H, Hayes MG, et al.  eMERGEing progress in genomics – the first seven years. Front Genet. 2014;5:184. doi: 10.3389/fgene.2014.00184. [ PMC free article ] [ PubMed ] [ Cross Ref ]
78. Chen DP, Weber SC, Constantinou PS, Ferris TA, Lowe HJ, Butte AJ. Novel integration of hospital electronic medical records and gene expression measurements to identify genetic markers of maturation. Pac Symp Biocomput. 2008;243–54. [ PMC free article ] [ PubMed ]
79. Kullo IJ, Ding K, Shameer K, McCarty CA, Jarvik GP, Denny JC, et al.  Complement receptor 1 gene variants are associated with erythrocyte sedimentation rate. Am J Hum Genet. 2011;89:131–138. doi: 10.1016/j.ajhg.2011.05.019. [ PMC free article ] [ PubMed ] [ Cross Ref ]
80. Kullo IJ, Ding K, Jouni H, Smith CY, Chute CG. A genome-wide association study of red blood cell traits using the electronic medical record. PLoS One. 2010;5 doi: 10.1371/journal.pone.0013011. [ PMC free article ] [ PubMed ] [ Cross Ref ]
81. Crosslin DR, Carrell DS, Burt A, Kim DS, Underwood JG, Hanna DS, et al.  Genetic variation in the HLA region is associated with susceptibility to herpes zoster. Genes Immun. 2014;16:1–7. doi: 10.1038/gene.2014.51. [ PMC free article ] [ PubMed ] [ Cross Ref ]
82. Coloma PM, Valkhoff VE, Mazzaglia G, Nielsson MS, Pedersen L, Molokhia M, et al.  Identification of acute myocardial infarction from electronic healthcare records using different disease coding systems: a validation study in three European countries. BMJ Open. 2013;3 doi: 10.1136/bmjopen-2013-002862. [ PMC free article ] [ PubMed ] [ Cross Ref ]
83. Tu K, Mitiku T, Guo H, Lee DS, Tu JV. Myocardial infarction and the validation of physician billing and hospitalization data using electronic medical records. Chronic Dis Can. 2010;30:141–146. [ PubMed ]
84. Kottke TE, Baechler CJ. An algorithm that identifies coronary and heart failure events in the electronic health record. Prev Chronic Dis. 2013;10 [ PMC free article ] [ PubMed ]
85. Liao KP, Cai T, Gainer V, Goryachev S, Zeng-treitler Q, Raychaudhuri S, et al.  Electronic medical records for discovery research in rheumatoid arthritis. Arthritis Care Res. 2010;62:1120–1127. doi: 10.1002/acr.20184. [ PMC free article ] [ PubMed ] [ Cross Ref ]
86. Denny JC. Chapter 13: mining electronic health records in the genomics era. PLoS Comput Biol. 2012;8 doi: 10.1371/journal.pcbi.1002823. [ PMC free article ] [ PubMed ] [ Cross Ref ]
87. Wei WQ, Feng Q, Jiang L, Waitara MS, Iwuchukwu OF, Roden DM, et al.  Characterization of statin dose response in electronic medical records. Clin Pharmacol Ther. 2014;95:331–338. doi: 10.1038/clpt.2013.202. [ PMC free article ] [ PubMed ] [ Cross Ref ]
88. Ramirez AH, Shi Y, Schildcrout JS, Delaney JT, Xu H, Oetjens MT, et al.  Predicting warfarin dosage in European-Americans and African-Americans using DNA samples linked to an electronic health record. Pharmacogenomics. 2012;13:407–418. doi: 10.2217/pgs.11.164. [ PMC free article ] [ PubMed ] [ Cross Ref ]
89. Doshi-Velez F, Ge Y, Kohane I. Comorbidity clusters in autism spectrum disorders: an electronic health record time-series analysis. Pediatrics. 2014;133:e54–e63. doi: 10.1542/peds.2013-0819. [ PMC free article ] [ PubMed ] [ Cross Ref ]
90. Xu H, Aldrich MC, Chen Q, Liu H, Peterson NB, Dai Q, et al.  Validating drug repurposing signals using electronic health records: a case study of metformin associated with reduced cancer mortality. J Am Med Inform Assoc. 2015;22:179–191. [ PMC free article ] [ PubMed ]
91. Overby CL, Pathak J, Gottesman O, Haerian K, Perotte A, Murphy S, et al.  A collaborative approach to developing an electronic health record phenotyping algorithm for drug-induced liver injury. J Am Med Inform Assoc. 2013;20:e243–e252. doi: 10.1136/amiajnl-2013-001930. [ PMC free article ] [ PubMed ] [ Cross Ref ]
92. Birdwell KA, Grady B, Choi L, Xu H, Bian A, Denny JC, et al.  The use of a DNA biobank linked to electronic medical records to characterize pharmacogenomic predictors of tacrolimus dose requirement in kidney transplant recipients. Pharmacogenet Genomics. 2012;22:32–42. doi: 10.1097/FPC.0b013e32834e1641. [ PMC free article ] [ PubMed ] [ Cross Ref ]
93. Kawai VK, Cunningham A, Vear SI, Van Driest SL, Oginni A, Xu H, et al.  Genotype and risk of major bleeding during warfarin treatment. Pharmacogenomics. 2014;15:1973–1983. doi: 10.2217/pgs.14.153. [ PMC free article ] [ PubMed ] [ Cross Ref ]
94. Onitilo AA, McCarty CA, Wilke RA, Glurich I, Engel JM, Flockhart DA, et al.  Estrogen receptor genotype is associated with risk of venous thromboembolism during tamoxifen therapy. Breast Cancer Res Treat. 2009;115:643–650. doi: 10.1007/s10549-008-0264-2. [ PubMed ] [ Cross Ref ]
95. Karnes JH, Cronin RM, Rollin J, Teumer A, Pouplard C, Shaffer CM, et al.  A genome-wide association study of heparin-induced thrombocytopenia using an electronic medical record. Thromb Haemost. 2014;113:772–781. doi: 10.1160/TH14-08-0670. [ PMC free article ] [ PubMed ] [ Cross Ref ]
96. Rzhetsky A, Wajngurt D, Park N, Zheng T. Probing genetic overlap among complex human phenotypes. Proc Natl Acad Sci U S A. 2007;104:11694–11699. doi: 10.1073/pnas.0704820104. [ PMC free article ] [ PubMed ] [ Cross Ref ]
97. Cross-Disorder Group of the Psychiatric Genomics Consortium. Lee SH, Ripke S, Neale BM, Faraone SV, Purcell SM, et al.  Genetic relationship between five psychiatric disorders estimated from genome-wide SNPs. Nat Genet. 2013;45:984–994. doi: 10.1038/ng.2711. [ PMC free article ] [ PubMed ] [ Cross Ref ]
98. Denny JC, Ritchie MD, Basford MA, Pulley JM, Bastarache L, Brown-Gentry K, et al.  PheWAS: demonstrating the feasibility of a phenome-wide scan to discover gene-disease associations. Bioinformatics. 2010;26:1205–1210. doi: 10.1093/bioinformatics/btq126. [ PMC free article ] [ PubMed ] [ Cross Ref ]
99. Denny JC, Bastarache L, Ritchie MD, Carroll RJ, Zink R, Mosley JD, et al.  Systematic comparison of phenome-wide association study of electronic medical record data and genome-wide association study data. Nat Biotechnol. 2013;31:1102–1110. doi: 10.1038/nbt.2749. [ PMC free article ] [ PubMed ] [ Cross Ref ]
100. Hebbring SJ, Schrodi SJ, Ye Z, Zhou Z, Page D, Brilliant MH. A PheWAS approach in studying HLA-DRB1*1501. Genes Immun. 2013;14:187–191. doi: 10.1038/gene.2013.2. [ PMC free article ] [ PubMed ] [ Cross Ref ]
101. Ye Z, Mayer J, Ivacic L, Zhou Z, He M, Schrodi SJ, et al.  Phenome-wide association studies (PheWASs) for functional variants. Eur J Hum Genet. 2015;23:523–529. doi: 10.1038/ejhg.2014.123. [ PMC free article ] [ PubMed ] [ Cross Ref ]
102. Cronin RM, Field JR, Bradford Y, Shaffer CM, Carroll RJ, Mosley JD, et al.  Phenome-wide association studies demonstrating pleiotropy of genetic variants within FTO with and without adjustment for body mass index. Front Genet. 2014;5:250. doi: 10.3389/fgene.2014.00250. [ PMC free article ] [ PubMed ] [ Cross Ref ]
103. Namjou B, Marsolo K, Caroll RJ, Denny JC, Ritchie MD, Verma SS, et al.  Phenome-wide association study (PheWAS) in EMR-linked pediatric cohorts, genetically links PLCL1 to speech language development and IL5-IL13 to eosinophilic esophagitis. Front Genet. 2014;5:401. doi: 10.3389/fgene.2014.00401. [ PMC free article ] [ PubMed ] [ Cross Ref ]
104. Shameer K, Denny JC, Ding K, Jouni H, Crosslin DR, de Andrade M, et al.  A genome- and phenome-wide association study to identify genetic variants influencing platelet count and volume and their pleiotropic effects. Hum Genet. 2014;133:95–109. doi: 10.1007/s00439-013-1355-7. [ PMC free article ] [ PubMed ] [ Cross Ref ]
105. Pendergrass SA, Brown-Gentry K, Dudek S, Frase A, Torstenson ES, Goodloe R, et al.  Phenome-wide association study (PheWAS) for detection of pleiotropy within the Population Architecture using Genomics and Epidemiology (PAGE) Network. PLoS Genet. 2013;9 doi: 10.1371/journal.pgen.1003087. [ PMC free article ] [ PubMed ] [ Cross Ref ]
106. Chan KS, Fowles JB, Weiner JP. Electronic health records and the reliability and validity of quality measures: a review of the literature. Med Care Res Rev. 2010;67:503–527. doi: 10.1177/1077558709359007. [ PubMed ] [ Cross Ref ]
107. Song Y, Skinner J, Bynum J, Sutherland J, Wennberg JE, Fisher ES. Regional variations in diagnostic practices. N Engl J Med. 2010;363:45–53. doi: 10.1056/NEJMsa0910881. [ PMC free article ] [ PubMed ] [ Cross Ref ]
108. Castro VM, Apperson WK, Gainer VS, Ananthakrishnan AN, Goodson AP, Wang TD, et al.  Evaluation of matched control algorithms in EHR-based phenotyping studies: A case study of inflammatory bowel disease comorbidities. J Biomed Inform. 2014;52:105–111. doi: 10.1016/j.jbi.2014.08.012. [ PMC free article ] [ PubMed ] [ Cross Ref ]
109. Wei WQ. The impact of data fragmentation on high-throughput clinical phenotyping: health informatics. Minneapolis: University of Minnesota; 2012.
110. Bourgeois FC, Olson KL, Mandl KD. Patients treated at multiple acute health care facilities: quantifying information fragmentation. Arch Int Med. 2010;170:1989–1995. doi: 10.1001/archinternmed.2010.439. [ PubMed ] [ Cross Ref ]
111. Smith PC, Araya-Guerra R, Bublitz C, Parnes B, Dickinson LM, Van Vorst R, et al.  Missing clinical information during primary care visits. JAMA. 2005;293:565–571. doi: 10.1001/jama.293.5.565. [ PubMed ] [ Cross Ref ]
112. Wei WQ, Leibson CL, Ransom JE, Kho AN, Caraballo PJ, Chai HS, et al.  Impact of data fragmentation across healthcare centers on the accuracy of a high-throughput clinical phenotyping algorithm for specifying subjects with type 2 diabetes mellitus. J Am Med Inform Assoc. 2012;19:219–224. doi: 10.1136/amiajnl-2011-000597. [ PMC free article ] [ PubMed ] [ Cross Ref ]
113. O'Malley KJ, Cook KF, Price MD, Wildes KR, Hurdle JF, Ashton CM. Measuring diagnoses: ICD code accuracy. Health Serv Res. 2005;40:1620–1639. doi: 10.1111/j.1475-6773.2005.00444.x. [ PMC free article ] [ PubMed ] [ Cross Ref ]
114. Moon S, Pakhomov S, Liu N, Ryan JO, Melton GB. A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources. J Am Med Inform Assoc. 2014;21:299–307. doi: 10.1136/amiajnl-2012-001506. [ PMC free article ] [ PubMed ] [ Cross Ref ]
115. Wei WQ, Tao C, Jiang G, Chute CG. A high throughput semantic concept frequency based approach for patient identification: a case study using type 2 diabetes mellitus clinical notes. AMIA Annu Symp Proc. 2010;2010:857–861. [ PMC free article ] [ PubMed ]
116. Apache cTAKES. http://ctakes.apache.org/ .
117. Shortliffe EH, Cimino JJ. Biomedical informatics: computer applications in health care and biomedicine. 4. New York City: Springer; 2014.
118. Denny JC, Smithers JD, Miller RA, Spickard A., 3rd "Understanding" medical school curriculum content using KnowledgeMap. J Am Med Inform Assoc. 2003;10:351–362. doi: 10.1197/jamia.M1176. [ PMC free article ] [ PubMed ] [ Cross Ref ]
119. Uzuner O, South BR, Shen S, DuVall SL. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. J Am Med Inform Assoc. 2011;18:552–556. doi: 10.1136/amiajnl-2011-000203. [ PMC free article ] [ PubMed ] [ Cross Ref ]
120. Nadkarni PM, Ohno-Machado L, Chapman WW. Natural language processing: an introduction. J Am Med Inform Assoc. 2011;18:544–551. doi: 10.1136/amiajnl-2011-000464. [ PMC free article ] [ PubMed ] [ Cross Ref ]
121. McCarty CA, Chapman-Stone D, Derfus T, Giampietro PF, Fost N. Community consultation and communication for a population-based DNA biobank: the Marshfield clinic personalized medicine research project. Am J Med Genet A. 2008;146A:3026–3033. doi: 10.1002/ajmg.a.32559. [ PMC free article ] [ PubMed ] [ Cross Ref ]
122. Avellino S, Colonie R, Carey D. PS2–7: automated ordering and sample collection to leverage electronic medical record based genetic research – the Geisinger MyCode Project. Clin Med Res. 2013;11:2. doi: 10.3121/cmr.2013.1176.ps2-7. [ Cross Ref ]
123. Roden DM, Pulley JM, Basford MA, Bernard GR, Clayton EW, Balser JR, et al.  Development of a large-scale de-identified DNA biobank to enable personalized medicine. Clin Pharmacol Ther. 2008;84:362–369. doi: 10.1038/clpt.2008.89. [ PMC free article ] [ PubMed ] [ Cross Ref ]
124. Sweeney L. Privacy and medical-records research. N Engl J Med Res. 1998;338:1077–1078. [ PubMed ]
125. Malin B, Loukides G, Benitez K, Clayton EW. Identifiability in biobanks: models, measures, and mitigation strategies. Hum Genet. 130:383–92. [ PMC free article ] [ PubMed ]
126. NIH Genomic Data Sharing Policy. http://gds.nih.gov/03policy2.html .
127. Gurwitz D, Pirmohamed M. Pharmacogenomics: the importance of accurate phenotypes. Pharmacogenomics. 2010;11:469–470. doi: 10.2217/pgs.10.41. [ PubMed ] [ Cross Ref ]
128. Marsolo K, Spooner SA. Clinical genomics in the world of the electronic health record. Genet Med. 2013;15:786–791. doi: 10.1038/gim.2013.88. [ PMC free article ] [ PubMed ] [ Cross Ref ]
129. Quality Data Model. http://www.healthit.gov/quality-data-model .
130. Thompson WK, Rasmussen LV, Pacheco JA, Peissig PL, Denny JC, Kho AN, et al.  An evaluation of the NQF Quality Data Model for representing Electronic Health Record driven phenotyping algorithms. AMIA Annu Symp Proc. 2012;2012:911–920. [ PMC free article ] [ PubMed ]
131. SNOMED Clinical Terms. http://www.nlm.nih.gov/research/umls/Snomed/snomed_main.html .
132. openEHR. http://www.openehr.org/ .
133. Clinical Element Model. http://www.clinicalelement.com/ .
134. Saitwal H, Qing D, Jones S, Bernstam EV, Chute CG, Johnson TR. Cross-terminology mapping challenges: a demonstration using medication terminological systems. J Biomed Inform. 2012;45:613–625. doi: 10.1016/j.jbi.2012.06.005. [ PMC free article ] [ PubMed ] [ Cross Ref ]
135. Bodenreider O. Issues in mapping LOINC laboratory tests to SNOMED CT. AMIA Annu Symp Proc. 2008;2008:51–55. [ PMC free article ] [ PubMed ]
136. Kim TY, Hardiker N, Coenen A. Inter-terminology mapping of nursing problems. J Biomed Inform. 2014;49:213–220. doi: 10.1016/j.jbi.2014.03.001. [ PubMed ] [ Cross Ref ]
137. Kuhn M, Campillos M, Letunic I, Jensen LJ, Bork P. A side effect resource to capture phenotypic effects of drugs. Mol Syst Biol. 2010;6:343. doi: 10.1038/msb.2009.98. [ PMC free article ] [ PubMed ] [ Cross Ref ]
138. Wei WQ, Cronin RM, Xu H, Lasko TA, Bastarache L, Denny JC. Development and evaluation of an ensemble resource linking medications to their indications. J Am Med Inform Assoc. 2013;20:954–961. doi: 10.1136/amiajnl-2012-001431. [ PMC free article ] [ PubMed ] [ Cross Ref ]
139. Wei WQ, Cronin RM, Xu H, Lasko TA, Bastarache L, Denny JC. Development of an ensemble resource linking MEDications to their Indications (MEDI) AMIA Jt Summ Transl Sci Proc. 2013;2013:172. [ PubMed ]
140. Wei WQ, Mosley JD, Bastarache L, Denny JC. Validation and enhancement of a computable medication indication resource (MEDI) using a large practice-based dataset. AMIA Annu Symp Proc. 2013;2013:1448–1456. [ PMC free article ] [ PubMed ]
141. Csermely P, Korcsmaros T, Kiss HJ, London G, Nussinov R. Structure and dynamics of molecular networks: a novel paradigm of drug discovery: a comprehensive review. Pharmacol Ther. 2013;138:333–408. doi: 10.1016/j.pharmthera.2013.01.016. [ PMC free article ] [ PubMed ] [ Cross Ref ]
142. Bejan CA, Wei W-Q, Denny JC. Assessing the role of a medication-indication resource in the treatment relation extraction from clinical text. J Am Med Inform Assoc. 2014 [ PubMed ]
143. Rasmussen-Torvik LJ, Stallings SC, Gordon AS, Almoguera B, Basford MA, Bielinski SJ, et al.  Design and anticipated outcomes of the eMERGE-PGx Project: a multicenter pilot for preemptive pharmacogenomics in electronic health record systems. Clin Pharmacol Ther. 2014;96:482–489. doi: 10.1038/clpt.2014.137. [ PMC free article ] [ PubMed ] [ Cross Ref ]
144. Pulley JM, Denny JC, Peterson JF, Bernard GR, Vnencak-Jones CL, Ramirez AH, et al.  Operational implementation of prospective genotyping for personalized medicine: the design of the Vanderbilt PREDICT project. Clin Pharmacol Ther. 2012;92:87–95. doi: 10.1038/clpt.2011.371. [ PMC free article ] [ PubMed ] [ Cross Ref ]
145. Hicks JK, Crews KR, Hoffman JM, Kornegay NM, Wilkinson MR, Lorier R, et al.  A clinician-driven automated system for integration of pharmacogenetic interpretations into an electronic medical record. Clin Pharmacol Ther. 2012;92:563–566. doi: 10.1038/clpt.2012.140. [ PMC free article ] [ PubMed ] [ Cross Ref ]
146. Andre F, Mardis E, Salm M, Soria JC, Siu LL, Swanton C. Prioritizing targets for precision cancer medicine. Ann Oncol. 2014;25:2295–2303. doi: 10.1093/annonc/mdu478. [ PubMed ] [ Cross Ref ]
147. Weitzel KW, Elsey AR, Langaee TY, Burkley B, Nessl DR, Obeng AO, et al.  Clinical pharmacogenetics implementation: approaches, successes, and challenges. Am J Med Genet C Semin Med Genet. 2014;166C:56–67. doi: 10.1002/ajmg.c.31390. [ PMC free article ] [ PubMed ] [ Cross Ref ]
148. Chung WH, Hung SI, Hong HS, Hsih MS, Yang LC, Ho HC, et al.  Medical genetics: a marker for Stevens-Johnson syndrome. Nature. 2004;428:486. doi: 10.1038/428486a. [ PubMed ] [ Cross Ref ]
149. Mallal S, Phillips E, Carosi G, Molina JM, Workman C, Tomazic J, et al.  HLA-B*5701 screening for hypersensitivity to abacavir. N Engl J Med. 2008;358:568–579. doi: 10.1056/NEJMoa0706135. [ PubMed ] [ Cross Ref ]
150. IGNITE Network. http://rt5.cceb.med.upenn.edu/public/ignite_home.html .
151. Collins F. Opportunities and challenges for the NIH – an interview with Francis Collins, Interview by Robert Steinbrook. N Engl J Med. 2009;361:1321–1323. doi: 10.1056/NEJMp0905046. [ PubMed ] [ Cross Ref ]
AMIA Annu Symp Proc. 2011; 2011: 1564–1572.
Published online 2011 Oct 22.
PMCID: PMC3243156
PMID: 22195222
Extracting and Integrating Data from Entire Electronic Health Records for Detecting Colorectal Cancer Cases
Author information ► Copyright and License information ► Disclaimer
Copyright ©2011 AMIA - All rights reserved.
This is an Open Access article: verbatim copying and redistribution of this article are permitted in all media for any purpose
This article has been cited by other articles in PMC.
Abstract
Identification of a cohort of patients with specific diseases is an important step for clinical research that is based on electronic health records (EHRs). Informatics approaches combining structured EHR data, such as billing records, with narrative text data have demonstrated utility for such tasks. This paper describes an algorithm combining machine learning and natural language processing to detect patients with colorectal cancer (CRC) from entire EHRs at Vanderbilt University Hospital. We developed a general case detection method that consists of two steps: 1) extraction of positive CRC concepts from all clinical notes (document-level concept identification); and 2) determination of CRC cases using aggregated information from both clinical narratives and structured billing data (patient-level case determination). For each step, we compared performance of rule-based and machine-learning-based approaches. Using a manually reviewed data set containing 300 possible CRC patients (150 for training and 150 for testing), we showed that our method achieved F-measures of 0.996 for document level concept identification, and 0.93 for patient level case detection.
INTRODUCTION
Electronic health records (EHRs) contain a longitudinal record of patient health, disease, and response to treatment useful for epidemiologic, clinical, genomic, and informatics research. Notable recent examples include those explored by the electronic Medical Records and Genomics (eMERGE) network [ 1 ]. However, accurate identification of cohorts of patients having specific diseases or receiving certain treatments from EHR can be challenging. As much of the detailed patient information is embedded in clinical narratives, a traditional method for case detection is to conduct manual chart review by physicians. However, it is a very costly and time-consuming task to manually collect, find, and abstract all clinical documents of possible patients.
Recent advances in natural language processing (NLP) have offered automated methods to extract information from free text, including clinical narratives. Over the last three decades, a number of clinical NLP systems have been developed, including some earlier systems such as Medical Language Processing (MLP) system from the Linguistic String Project (LSP) [ 2 , 3 ], MedLEE (Medical Language Extraction and Encoding System) [ 4 – 6 ], and SymText/MPlus [ 7 – 9 ], as well as more recent open source systems such as cTAKES [ 10 ] and HiTEX [ 11 ]. Some systems, such as MetaMap [ 12 ] and KnowledgeMap [ 13 ], focus on extracting concepts from biomedical text including biomedical literature and clinical notes. After that, contextual information of those concepts such as assertion status (i.e., is a medical problem present or negated?) can be recognized through additional algorithms or programs, such as NegEx [ 14 ] for negation status, and ConText [ 15 ] for other broad contextual information.
For disease case detection, a number of studies have shown that coded data such as International Classification of Disease (ICD) codes were not sufficient or accurate enough [ 16 – 18 ]. Therefore, many studies involve data extraction from clinical text and NLP has been used for phenotype extraction in various studies. Penz et al. [ 19 ] found that ICD-9 and Current Procedural Terminology (CPT) codes only identified less than 11% of the cases in a study of detecting adverse events related to central venous catheters, while NLP methods achieved a sensitivity of 0.72 and a specificity of 0.80. Li et al. [ 20 ] compared the results of ICD-9 encoded diagnoses and NLP-processed discharge summaries for clinical trial eligibility queries. They concluded that NLP-processed notes provide more valuable data sources for clinical trial pre-screening as they provide past medical histories as well as more specific details about disease that are unavailable in ICD-9 codes. Savova et al. [ 21 ] used cTAKES to discover peripheral arterial disease (PAD) cases from radiology notes, and classified cases into four categories: positive, negative, probable, and unknown. There are a number of studies focused on cancer case detection. For example, Friedlin et al. [ 22 ] found either ICD-9 codes and NLP methods identified pancreatic cancer patient well from a cohort of pancreatic cyst patients. However, the ICD-9 code method had lower specificity and positive predictive value. Wilson et al. [ 23 ] evaluated the ability of an automated system to extract mesothelioma patients' personal and family history of cancer from positive surgical pathological report and when patients are admitted to hospital after mesothelioma diagnosis. They tested two information extraction methods: Dynamic-window and ConText [ 15 ] for building cancer frames, which showed that both the methods performed much better than their human benchmark. Relevant to CRC patients, we have previously compared NLP methods to colonoscopy events to find completed colonoscopies, finding that billing records identified 67% of the events compared to NLP with 92% [ 24 ].
Despite the success of applying informatics approaches to case detection from EHRs, most prior research has focused on a limited number of document types (e.g., chest radiographs for pneumonia). Few studies have explored methods to extract, integrate, and utilize heterogeneous clinical data (including structured and narrative data from various types of notes) in entire EHR, for case detection purposes. In this study, we investigated informatics approaches for detecting CRC patients from entire EHR, in order to support an ongoing epidemiological study at Vanderbilt. Colorectal cancer (CRC) is the fourth most common incident cancer and the second most leading cause of cancer death in the United States [ 25 ], despite significant efforts to increase screening and improve treatment. About 1 in 18 individuals will develop colorectal cancer over their lifetime and 40% will die within 5 years of diagnosis, mainly due to diagnosis at a late stage [ 26 , 27 ]. The goal of the study is to collect all CRC cases, including those with history of treated CRC from Vanderbilt University Hospital's (VUH) EHR. We developed a 2-step approach for CRC case detection, which consists of 1) extraction of CRC concepts from various types of clinical documents, and 2) prediction of CRC cases by integrating evidences from both structured and narrative sources.
METHODS
Overview of the Case Detection Method
The case detection method proposed in this study consists of two components: 1) a module to identify positive CRC concepts from various types of narrative clinical documents (document-level concept identification); and 2) a module to determine CRC cases using aggregated information from both narrative data and coded ICD-9 and CPT data (patient-level case determination). Figure 1 shows an overview of our approach. The document-level concept identification component also consists of two steps. The first step is to identify all CRC concepts in clinical documents using the MedLEE NLP system; while the second step determined if a detected CRC concept is positive or negative. For the patient-level case determination module, we implemented and compared two different methods to combine evidence from heterogeneous clinical data: a heuristic rule-based approach and a ML-based approach.
Overview of the 2-step case detection method from EHR.
Data Set
In this study, we used clinical data from the Synthetic Derivative (SD) database, which is a de-identified copy of the EHR at VUH. Different types of notes include DS – discharge summaries, CC – clinical communications, FORM – clinical forms, RAD – radiology notes, PATH – pathology notes, PL – patient summary lists, and OTHER – Other clinical notes (History & Physicals, clinic notes, progress notes). We limited the scope of this study to a 10-year period (1999–2008), in which there were 1,262,671 patients in total. Given the overall low incidence of CRC (about 5% individuals will develop CRC over their lifetime), we first selected an minimum data set that we expected to contain all patients with CRC. Domain experts defined a broad filter using ICD-9 codes, CPT codes, CRC and related drug keywords, to capture all possible CRC cases and form a data set of interest for this study. Table 1 shows the criteria of the filter. If a patient matched any one of the criteria in Table 1 , he/she was included into the pool of possible CRC cases, which had 17,125 patients.
Table 1.
Criteria for selecting possible CRC patients from EHR.
Inclusion Criteria
# of Patients
CRC related ICD-9 codes, including "153, 153.0, 153.1, 153.2, 153.3, 153.4, 153.5, 153.6, 153.7, 153.8, 153.9, 154.0, 154.1, 45.92, 45.93, 45.94, 48.5, 48.62"
5,797
CRC related CPT codes, including "44160, 44147, 44140, 44145, 44146, 44143, 44144, 44141, 44156, 44158, 44157, 44155, 44151, 44150, 38564, 38562, 38770, 4180F"
2,090
CRC keyword search: any sentence containing both a cancer term in "adenocarcinoma OR cancer OR carcinoma OR neoplasm OR tumor OR tumour" AND a body location term in "appendix OR bowel OR caecum OR cecum OR colon OR colorectal OR intestine OR rectosigmoid OR rectal OR rectum OR sigmoid OR splenic flexture". This search is limited to four types of notes: discharge summaries, pathology notes, radiology notes, and patient summary list for outpatients.
13,241
CRC related drug keyword search, including "irinotecan, cpt11, camptosar, eloxatin, oxaliplatin"
2,475
Open in a separate window
From the pool of possible CRC cases, we randomly selected 300 patients and collected all of their records in EHR. An oncologist (ZF) manually reviewed records of all 300 patients and decided if a patient had colorectal cancer (including those with history of CRC). After we developed our automated case detection system, we applied the system to the same 300 patients. Any discrepancy between the manually reviewed result and system generated result was submitted to another domain expert (NP) for a second review. The final decision was made by NP, with discussion with other physicians in the team. This labeled data set with 300 patients served as the gold standard for this study. We further divided the entire data into a training set of 150 patients, and a test set of 150 patients. The methods described below were developed and optimized using the training set, and evaluated on the independent test set. Figure 2 shows the workflow about the creation of the data sets for this study.
The workflow of creation of the gold standard data set containing 300 patients with labels indicating their CRC status (Yes or No).
Document-level Concept Identification
Based on UMLS, domain experts in our team defined a list of concepts for CRC, including134 UMLS CUIs, manually selecting concepts from related CRC concepts using the KnowledgeMap web application [ 28 ]. The task is then to identify those CRC concepts that are about patients from clinical text, excluding those that are negated, hypothetical, or representing family history. We used the well-established NLP system MedLEE for concept extraction in this study, because it has the capability to detect contextual information associated with a concept. The MedLEE system has shown good coverage on concept extraction in many studies [ 29 ]; therefore our focus in this study was to improve the assertion determination – to decide if a CRC concept is positive or negative. A CRC concept is classified as positive if it is not negated, associated with other people, hypothetical, or possible. As we included all types of clinical notes in this study, the assertion determination was more challenging than focusing on only one type of clinical notes.
The first method for assertion determination was a heuristic rule-based approach. MedLEE detects some contextual information associated with a concept, including "certainty", "status", and "family" modifiers associated with concepts. These attributes can be used to identity negation and the experiencer of a clinical concept. Our manual review showed that MedLEE had good performance on recognizing contextual information overall, but sometimes it missed certain section headers or other context modifiers. For example, sometimes it did not recognize certain section headers indicating family history sections, or it missed the non-patient experiencer information (e.g. "M: Colon Ca" – it means that mother had colon cancer). Therefore, we developed an additional rule-based program to search context around CRC concepts and identify contextual information that were missed or incorrectly extracted by MedLEE. Some rules are specific to certain types of notes. For example, a section header in the Problem List often starts with four dashes, e.g., "----Social History" and MedLEE did not recognize that kind of format of section headers. Therefore we developed rules to remove dashes from section headers in Problem List.
The second method used SVM (Support Vector Machines) to determine assertion. We randomly selected 500 unique sentences containing CRC concepts, from the training set of 150 patients. We manually reviewed them and labeled each CRC concept as "positive" or "negative". Based on this annotated data set, we developed a classifier using SVMs implemented via the LibSVM package [ 30 ]. The features for the classifier include modifier information from MedLEE's output, words and bigram from contextual window around the CRC concept, as well as the distance and direction (left vs. right) of those words. The parameters of the SVM classifier were optimized using a 3-fold cross validation (CV) method on the 500 annotated sentences from the training set, before it was applied to the test set.
To assess the performance of the rule-based and ML-based assertion determination methods for CRC concepts, we developed an independent data set of 300 randomly-selected sentences. These were chosen from all CRC-concept-containing sentences in the test set, and then manually labeled. Performance of both methods on this independent data set was evaluated and reported as well.
Patient-level Case Determination
Once positive CRC concepts were identified from all types of clinical notes, aggregated information about counts of CRC concepts in each type of clinical documents (e.g., DS – discharge summaries) could be obtained (shown in Figure 1 ). Meanwhile, we also collected counts of ICD-9 and CPT codes from EHR. The goal of this step was to assess how such aggregated information could be used to identify CRC cases, as well as how to combine information from both unstructured and structured data to optimize the case detection task. We tested two types of methods here: rule-based and ML-based approaches.
For rule-based methods, we manually review examples in the training set and developed rules to determine if a patient was a CRC case. Concepts identified from the best algorithm in the document-level concept extraction task were used. Simple rules could be based on counts from a single source, e.g., if ICD-9 count >= 1, then it is a case. More complicated rules could take multiple sources into consideration, e.g., if CRC count from text >= 1 and patient has at least one ICD-9 or CPT code, then it is a case. However, we found that it was very difficult to define useful rules by manual review of the aggregated counts CRC concepts from different sources. Therefore we investigated ML-based approaches, which would automatically find useful patterns to determine if a patient is a CRC case or not. The input data for ML algorithms contain 12 columns, including counts of ICD-9 codes, CPT codes, CRC concepts from all types of notes, and CRC concepts from 9 individual types of clinical notes. In addition to raw counts, we also defined normalized counts, which were the ratio between the raw counts of CRC concepts from a single source and the counts of any concepts in the source documents. For example, if MedLEE identified 200 clinical concepts from entire discharge summaries of one patient and 10 of them were CRC concepts, the normalized CRC count would be 10/200=0.05 (raw count was 10). The intension was to normalize the CRC information by the length of available clinical records of a patient. Four different ML algorithms were tested, including Random Forest (RF) [ 31 ], Ripper [ 32 ], Support Vector Machine (SVM), and Logistic Regression (LR) [ 33 ]. RF is implemented via the R package [ 34 , 35 ], Ripper is implemented via the WEKA package [ 36 ], SVM is implemented via the LibSVM package [ 30 ], and LR is implemented via the LibLinear package [ 33 ]. Parameters of each algorithm were optimized using a 3-fold cross validation method on the training set. Finally we trained ML-based models on entire training set and applied them to the test set. We reported both results from the training set and the test set below.
Evaluation
As both the document-level CRC concept identification and patient-level case determination were binary classification tasks, we reported standard classification metrics including precision, recall (sensitivity), specificity, accuracy, and F-measures, by comparing the system output with its corresponding manually annotated gold standard.
RESULTS
Characteristics of Data Sets
Based on the manual annotation, there were 121 CRC cases in the 300 patients who were randomly selected from the pool of 17,125 possible CRC patients. The detailed characteristics of data sets used in this study are shown in Table 2 .
Table 2.
Distribution of cases and non-cases in testing and training data sets.
Data Set
Open in a separate window
Document-level CRC Concept Identification
The document-level CRC concept identification system was developed using 500 annotated sentences in the training set, and evaluated using 300 annotated sentences in the test set. These sentences were randomly selected from all types of notes. The rule-based method achieved high performance on both training and test data sets (F-measure of 0.996), indicating such rules were generalizable. Table 3 shows the results of detecting positive CRC concepts for both rule-based and ML-based methods using both the training and the test sets. The results of ML-based method on the training set were the averages from 3-fold cross validation.
Table 3.
Results of document level CRC concept identification using both rule-based and ML-based methods.
Method
Open in a separate window
Patient-Level CRC Case Determination
Results of patient-level case detection using the ML-based algorithms on the training set are shown in Table 4 . The Ripper and RF methods achieved higher performance than SVM and LR, when normalized counts were used. Therefore, we only applied those two ML methods to the independent test data set. Results of patient-level case detection using the test set are shown in Table 5 . CPT codes alone were not useful at all. When only ICD-9 codes were used (Count >=1), results were poor as well (Precision 0.77, Recall 0.52). When a single source of clinical documents was used (e.g., DS), it usually had good precision (e.g., 0.95 for DS); but its recall was poor (e.g., 0.36 for DS). However, using counts of CRC concepts from all types of clinical text achieved very good performance. The simplest rule ("if CRC count in text >=1, then it is a case") achieved 0.84 precision and 0.97 recall. However, more complicated rules considering coded data (e.g., "if # CRC concept > 2 OR (# CRC concept >=1 AND (# ICD-9 or #CPT) >=1), then case") did not improve much of the performance (precision 0.88, recall 0.91). However, the ML-based methods, both RF and Ripper algorithms using normalized counts achieved the better results. The Ripper algorithm achieved the best F-measure of 0.93 (0.90 precision and 0.97 recall). Also distribution of CRC concepts among different types of notes is shown in Figure 3 .
Distribution of CRC concepts among different types of notes, including DS – discharge summaries, CC – clinical communications, FORM – clinical forms, RAD – radiology notes, PATH – pathology notes, PL – patient summary lists, and OTHER – Other clinical notes (History & Physicals, clinic notes, progress notes).
Table 4.
Results of patient-level case determination using different ML algorithms (values are averages from a 3-fold CV on the training set)
ML Algorithm
Open in a separate window
Table 5.
Results of patient-level case determination on the test set of 150 patients, when different data sources and methods (rule-based vs. ML-based) were used. Abbreviations of data sources: PATH – pathology notes, RAD – radiology notes, PL – patient summary list, DS – discharge summary, CC – clinical communication, and ALL – all types of notes including FORM – clinical forms and OTHER – Other clinical notes (History & Physicals, clinic notes, progress notes).
Data Source
Rule: if # CRC concept >= 1, then case
0.88
Rule: if # CRC concept >= 1, then case
0.88
Rule: if # CRC concept >= 1, then case
0.92
Rule: if # CRC concept >= 1, then case
0.95
Rule: if # CRC concept >= 1, then case
1.0
Rule: if # CRC concept >= 1, then case
0.84
Rule: if # CRC concept >=2, then case
0.84
Rule: if # CRC concept >=3, then case
0.93
Rule: if # CRC ICD-9 > 0, then case
0.77
Rule: if # CRC CPT > 0, then case
0.10
0.56
Mixed
Rule: if # CRC concept > 2 OR (# CRC concept >=1 AND (# ICD-9 or #CPT) >=1), then case
0.88
ML: RF with normalized counts
0.90
ML: Ripper with normalize counts
0.90
Open in a separate window
DISCUSSION
Identification of cases for a disease cohort from EHR is an important task for clinical and biologic research. Use of informatics approaches to identify cases using available EHR data may enable a broad new class of data using real-world patients. In this study, we scanned an entire EHR database for colorectal cancer cases, including both various types of free-text clinical notes and coded data such as ICD-9 and CPT codes. To handle the massive data extracted from EHR, we developed a 2-step framework for case detection, including a document-level concept identification module and a patient-level case determination module. Our evaluation using a collection of 300 possible CRC patients showed that such methods were superior, when compared with methods using a single source (e.g., pathology notes).). Overall, these methods achieved strong results with a case detection F-measure of 0.93.
Our results (see Table 4 ) showed that clinical narratives more reliably identified CRC cases than coded data such as ICD-9 and CPT. When CRC concepts were accurately identified from clinical text, simple rules that rely on the counts of CRC concepts could detect CRC cases with good performance. The generalizability of this finding needs further validation with other diseases. Using all types of notes in the EHR improved recall of CRC case detection and did not decrease precision significantly, when compared to the results from using pathology notes alone (0.88 for PATH and 0.84 for ALL, see Table 5 ). Figure 3 shows the distribution of CRC concepts among different types of clinical notes. Since CRC is largely an outpatient disease with the exception of surgical intervention, this distribution is logical. Indeed, it is notable that relevant CRC concepts were even found in nontraditional types of clinical documentation such as provider-staff interactions and clinic-patient messages through the MyHealthAtVanderbilt.com patient portal (collectively referred to as "clinical communications - CC" in Figure 3 below).
We investigated both rule-based and machine learning based methods for concept assertion case determination. For assertion determination of CRC concepts, the rule-based method achieved higher performance on the independent test set, indicating such customized rules derived from the corpus were generalizable within the given corpus. The well-established MedLEE system was another reason for the high performance of CRC concept identification. For case detection, ML-based methods outperformed rule-based methods, though both methods performed very well. Our experience was that it was very difficult to define effective rules by manual review of aggregated data based on the training set. However, ML algorithm could identify inexplicit patterns, thus improving the case detection. An interesting finding was that the normalization of the raw counts helped with the RF, Ripper, and LR algorithm, but not SVM. This finding indicates that methods used to normalize features are important to ML-based case detection, and that it could be ML algorithm-specific. We plan to look into this issue in the future.
We looked into failures of the rule-based system to accurately identify CRC cases in the test set. Two CRC cases were missed because MedLEE system did not identify any CRC concepts from the notes of those patients. In this study, we applied MedLEE to VUH clinical text without any changes and it showed very good performance on capturing CRC concepts overall (56 out of 58 cases in the test set had at least one CRC concept extracted by MedLEE from their notes). We reviewed clinical notes of those two cases and noticed that there were some scenarios where text was complicated and difficult for NLP systems. For example, MedLEE missed the CRC concept from this sentence "COLON, SPLENIC FLEXURE, PARTIAL COLECTOMY (***PATH-NUMBER[3], 8 SLIDES;**DATE[Jan 07 01]): MODERATELY DIFFERENTIATED ADENOCARCINOMA". In this example, the word "COLON" was far away from the term "ADENOCARCINOMA"; therefore MedLEE did not link "COLON" and "ADENOCARCINOMA." Concept linkage is difficult for any NLP system, and could be addressed with customization for these specific note types. False positives in case detection were often caused by false positives in concept identification phrase, highlighting the importance of accurate negation detection. Various reasons contributed to false positives in concept identification. For example, a misspelling of the word "screening" as "srceeing" resulted in a false positive case. In another case, a physician incorrectly entered "colon cancer" in a note for a lung cancer patient. These problems are likely very challenging for a computer system to resolve.
In this study, we evaluated the framework on colorectal cancer case detection only. More diseases have to be investigated in order to make generalizable conclusions on how to combine different data sources for case detection. Instead of using either rule-based or ML-based methods, we are planning to combine both methods to further improve the performance of case detection methods. Currently, we only used aggregated counts from different sources to determine cases. Future work would consider the temporal sequence of extracted concepts and develop more sophisticated methods for case determination.
CONCLUSION
In this study, we scanned the entire EHR to collect and integrate evidence for accurate identification of CRC patients. NLP techniques identify concepts from EHR entries are more reliable than using coded data like ICD-9 code and CPT code. The two-step case identification framework not only accurately extracts relevant concepts from clinical documents, but also provides a general approach to combine information from heterogeneous data sources in EHR for case detection.
Acknowledgments
This study was supported in part by NCI grant R01CA141307. The datasets used were obtained from Vanderbilt University Medical Center's Synthetic Derivative, which is supported by institutional funding and by the Vanderbilt CTSA grant 1UL1RR024975-01 from NCRR/NIH. We also thank Dr. Carol Friedman at Columbia University for providing MedLEE, which is supported by NLM grants LM008635 and LM010016.
References
1. eMERGE Network. https://www.mc.vanderbilt.edu/victr/dcc/projects/acc/index.php/Main_Page .
2. Sager N, et al.  The analysis and processing of clinical narrative. MedInfo. 1986
3. Sager N, Friedman C, Lyman M. Medical language processing: computer management of narrative data. Reading, MA: Addison-Wesley; 1987.
4. Hripcsak G, et al.  Unlocking clinical data from narrative reports: a study of natural language processing. Ann Intern Med. 1995;122(9):681–8. [ PubMed ]
5. Friedman C, et al.  A general natural-language text processor for clinical radiology. J Am Med Inform Assoc. 1994;1(2):161–74. [ PMC free article ] [ PubMed ]
6. Hripcsak G, et al.  Use of natural language processing to translate clinical information from a database of 889,921 chest radiographic reports. Radiology. 2002;224(1):157–63. [ PubMed ]
7. Haug PJ, et al.  Experience with a mixed semantic/syntactic parser. Proc Annu Symp Comput Appl Med Care. 1995:284–8. [ PMC free article ] [ PubMed ]
8. Fiszman M, et al.  Automatic identification of pneumonia related concepts on chest x-ray reports. Proc AMIA Symp. 1999:67–71. [ PMC free article ] [ PubMed ]
9. Haug PJ, et al.  A natural language parsing system for encoding admitting diagnoses. Proc AMIA Annu Fall Symp. 1997:814–8. [ PMC free article ] [ PubMed ]
10. Savova GK, et al.  Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. J Am Med Inform Assoc. 2010;17(5):507–13. [ PMC free article ] [ PubMed ]
11. Zeng QT, et al.  Extracting principal diagnosis, co-morbidity and smoking status for asthma research: evaluation of a natural language processing system. BMC Med Inform Decis Mak. 2006;6:30. [ PMC free article ] [ PubMed ]
12. Aronson AR, Lang FM. An overview of MetaMap: historical perspective and recent advances. J Am Med Inform Assoc. 2010;17(3):229–36. [ PMC free article ] [ PubMed ]
13. Denny JC, et al.  Development and evaluation of a clinical note section header terminology. AMIA Annu Symp Proc; 2008. pp. 156–60. [ PMC free article ] [ PubMed ]
14. Chapman WW, et al.  A simple algorithm for identifying negated findings and diseases in discharge summaries. J Biomed Inform. 2001;34(5):301–10. [ PubMed ]
15. Harkema H, et al.  ConText: an algorithm for determining negation, experiencer, and temporal status from clinical reports. J Biomed Inform. 2009;42(5):839–51. [ PMC free article ] [ PubMed ]
16. Birman-Deych E, et al.  Accuracy of ICD-9-CM codes for identifying cardiovascular and stroke risk factors. Medical care. 2005;43(5):480–5. [ PubMed ]
17. Kern EF, et al.  Failure of ICD-9-CM codes to identify patients with comorbid chronic kidney disease in diabetes. Health services research. 2006;41(2):564–80. [ PMC free article ] [ PubMed ]
18. Schmiedeskamp M, et al.  Use of International Classification of Diseases, Ninth Revision, Clinical Modification codes and medication use data to identify nosocomial Clostridium difficile infection. Infection control and hospital epidemiology : the official journal of the Society of Hospital Epidemiologists of America. 2009;30(11):1070–6. [ PubMed ]
19. Penz JF, Wilcox AB, Hurdle JF. Automated identification of adverse events related to central venous catheters. J Biomed Inform. 2007;40(2):174–82. [ PubMed ]
20. Li L, et al.  Comparing ICD9-encoded diagnoses and NLP-processed discharge summaries for clinical trials pre-screening: a case study. AMIA Annu Symp Proc; 2008. pp. 404–8. [ PMC free article ] [ PubMed ]
21. Savova GK, et al.  Discovering peripheral arterial disease cases from radiology notes using natural language processing. AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium; 2010. pp. 722–6. [ PMC free article ] [ PubMed ]
22. Friedlin J, et al.  Comparing methods for identifying pancreatic cancer patients using electronic data sources. AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium; 2010. pp. 237–41. [ PMC free article ] [ PubMed ]
23. Wilson RA, et al.  Automated ancillary cancer history classification for mesothelioma patients from free-text clinical reports. J Pathol Inform. 2010;1:24. [ PMC free article ] [ PubMed ]
24. Denny JC, et al.  Extracting timing and status descriptors for colonoscopy testing from electronic medical records. J Am Med Inform Assoc. 2010;17(4):383–8. [ PMC free article ] [ PubMed ]
25. Ries LAG, EM, Kosary CL, Hankey BF, Miller BA. SEER Cancer Statistics Review. 2004
26. Jemal A, TA, Murray T, Thun M. Cancer statistics, 2002. CA Cancer J Clin. 2002;52:23–47. [ PubMed ]
27. Hardy RG, MS, Jankowski JA. ABC of colorectal cancer. Molecular basis for risk factors. 2000;321:886–889. BMJ. [ PMC free article ] [ PubMed ]
28. Denny J, et al. "Where do we teach what?" Finding broad concepts in the medical school curriculum. J Gen Intern Med. 2005;20(10):4. [ PMC free article ] [ PubMed ]
29. Meystre SM, et al.  Extracting information from textual documents in the electronic health record: a review of recent research. Yearb Med Inform. 2008:128–44. [ PubMed ]
30. Chang C, Lin C. {LIBSVM}: a library for support vector machines. 2001.
31. Breiman L. Random Forests. Machine Learning. 2001;45(1):5–32.
32. Cohen W. Learning Trees and Rules with Set-valued Features. 1996. pp. 709–716.
33. Fan R-E, et al.  LIBLINEAR: A Library for Large Linear Classification. J. Mach. Learn. Res. 2008;9:1871–1874.
34. Kleiber A.Z.a.F.L.a.K.H.a.C. An {R} Package for Testing for Structural Change in Linear Regression Models. An {R} Package for Testing for Structural. 2002;7(2)
35. Hornik K. The {R} {FAQ} 2011.
36. Witten I, Frank E. Data Mining: Practical Machine Learning Tools and Techniques. 2005.
Articles from AMIA Annual Symposium Proceedings are provided here courtesy of American Medical Informatics Association
Formats:
Copyright ©2013 AMIA - All rights reserved.
This article has been cited by other articles in PMC.
Abstract
The database of Genotypes and Phenotypes (dbGaP) is archiving the results of different Genome Wide Association Studies (GWAS). dbGaP has a multitude of phenotype variables, but they are not harmonized across studies. We proposed a method to standardize phenotype variables by classifying similar variables based on semantic distances. We first extracted variables description, enriched them using domain knowledge, and computed the distances among them. We used clustering techniques to classify the most similar variables. We used domain experts to audit clusters, annotated the clusters with appropriate labels, and used re-clustering to build a semantically-driven Genotypes and Phenotypes (sdGaP) ontology using the UMLS semantic network and metathesaurus. The sdGaP ontology allowed us to expand user queries and retrieve information using a semantic metric called density measure (DM). We illustrated the potential improvement of information retrieval using the sdGaP ontology in one search scenario using the variables from the Cleveland Family Study.
Introduction
Phenotypes are observable physical or biochemical characteristics of an organism, as determined by both genetic variants and environmental influences. Genome-Wide Association Studies (GWAS) investigate many common genetic variants to check whether they are associated with diseases. Several researchers have produced interesting findings with the help of phenotype mapping and harmonization [ 7 , 8 , 9 ], including PheWAS [ 4 ]. In PheWAS, the association between a number of common genetic variations and a wide variety of phenotypes are systematically characterized. The database of Genotypes and Phenotypes (dbGaP) is the main repository of genotypes and phenotypes. The problem with phenotype variables in dbGaP is that they are not harmonized and semantically equivalent variables have several identifiers that make it almost impossible to effectively find them accurately. Furthermore, semantic relations between phenotype variables are not specified in dbGaP. Thus, it is not possible to retrieve variables semantically based on researcher queries. To achieve this goal, we propose to structure phenotypes in dbGaP using custom built software that retrofits existing data sets into information models. This information model based representation formalizes the semantics of phenotype variables. We propose to develop an information model semi-automatically, based on expert human review and NLP-based cluster analysis of variable attributes. Later, we use the semantic hierarchy for expanding and retrieving answers to researcher queries. We use a semantic metric called Density Measure (DM) for information retrieval. The rest of this paper is organized as follows. In section 1, we present our method to solve the stated problem including the technical details of classifying phenotype variables. In section 2, details of sdGaP ontology development are presented. In section 3, we explain how the sdGaP ontology affects information retrieval. In section 4, we present preliminary experimental results in a selected query within the variables from the Cleveland Family Study (dbGaP Study Accession: phs000284.v1.p1). Finally, the conclusion is presented in section 5.
1.   Methods
To build a phenotype data model for dbGaP, namely sdGaP, we computed the semantic hierarchy of dbGaP variables using data mining techniques and background domain knowledge, as depicted in Figure 1 . That is, we used the UMLS semantic network as the basis for our sdGaP ontology. We classified dbGaP variables into clusters and domain experts cleaned up these clusters. They relocated the variables that were incorrectly grouped into certain clusters. They then combined some existing clusters or created new clusters. Later, we assigned labeled clusters to the corresponding concepts in the UMLS Metathesaurus and updated sdGaP ontology. Section 1.1 below explains the details of operations for clustering using distances. We computed the semantic hierarchy between phenotype variables as described in section 1.2.
Appl Clin Inform. 2014; 5(1): 191–205.
Published online 2014 Mar 5. doi:  10.4338/ACI-2013-08-CR-0065
PMCID: PMC3974255
PMID: 24734133
An Information Retrieval System for Computerized Patient Records in the Context of a Daily Hospital Practice: the Example of the Léon Bérard Cancer Center (France)
Find articles by P. Biron
M.H. Metzger
Find articles by M.H. Metzger
C. Pezet
Find articles by C. Pezet
C. Sebban
Find articles by C. Sebban
E. Barthuet
Find articles by E. Barthuet
T. Durand
Find articles by T. Durand
1Léon Bérard Cancer Center, Lyon, France
2Université Lyon I – CNRS-UMR 5558, Lyon, France
3SWORD, Saint Didier au Mont d'Or, France
MH Metzger, Unité d'Hygiène et d'Épidémiologie, Hôpital de la Croix-Rousse, 103 Grande-Rue de la Croix-Rousse, 69317 Lyon CEDEX 04, France
This article has been cited by other articles in PMC.
Summary
Background
A full-text search tool was introduced into the daily practice of Léon Bérard Center (France), a health care facility devoted to treatment of cancer. This tool was integrated into the hospital information system by the IT department having been granted full autonomy to improve the system.
Objectives
To describe the development and various uses of a tool for full-text search of computerized patient records.
Methods
The technology is based on Solr, an open-source search engine. It is a web-based application that processes HTTP requests and returns HTTP responses. A data processing pipeline that retrieves data from different repositories, normalizes, cleans and publishes it to Solr, was integrated in the information system of the Leon Bérard center. The IT department developed also user interfaces to allow users to access the search engine within the computerized medical record of the patient.
Results
From January to May 2013, 500 queries were launched per month by an average of 140 different users. Several usages of the tool were described, as follows: medical management of patients, medical research, and improving the traceability of medical care in medical records. The sensitivity of the tool for detecting the medical records of patients diagnosed with both breast cancer and diabetes was 83.0%, and its positive predictive value was 48.7% (gold standard: manual screening by a clinical research assistant).
Conclusion
The project demonstrates that the introduction of full-text-search tools allowed practitioners to use unstructured medical information for various purposes.
Key words: Electronic health records, data mining, information storage and retrieval, user-computer interface
1. Introduction
In the last 20 years, the development of search engines has dramatically changed the techniques of information retrieval on the Internet, allowing users to instantly find relevant information by typing keywords into a search interface. However, the implementation of these techniques to search for medical information in computerized patient records is still very new and is essentially experimental, most often referred to research in university hospitals [ 1 , 2 ]. Few studies have described the value and utility that full-text search tools might have in real time in healthcare environments [ 3 ]. Projects to develop such tools are underway, but these efforts are still mostly limited to research projects [ 4 ]. A team at the University of Michigan Comprehensive Cancer Center confirmed that the use of a search engine optimizes the recovery of medical data in the free text sections of computerized patient records, and may allow physicians to save time [ 5 ]. Such a tool can facilitate collaboration by allowing users to analyze and share different types of information between people and across disciplines [ 6 ].
Léon Bérard Cancer Center (France) wanted to provide practitioners with an IT solution that supported full-text search of patient medical records using natural language. The tool has two different functions: firstly, it allows users to search within a single electronic medical record; and secondly, it allows queries on all electronic medical records in the hospital database.
The objective of this paper is to describe the methods used to develop and deploy the solution, and explain how the solution is used in the daily hospital practice of a healthcare facility.
2. Methods
2.1 Computerized patient records at the Léon Bérard Cancer Center
2.1.1 Pioneer in computerization of patient records
The Léon Bérard Cancer Center (CLB), based in Lyon, is the regional center for diagnosis and care of cancer in the Rhône-Alpes region (France). The CLB is a referral center for oncology care for patients in the region, and is also involved in the research and development of innovative treatments at national and international level. In 1989, the CLB Management Board decided to computerize patient records. Computerization began in January 1993, with paper versions of medical records not used after July 2002.
This patient record system was developed by the CLB's internal IT and medical teams using web technology. Contrary to the recommendations of the French government, which at the time promoted specific IT packages, the CLB favored internal development to allow greater responsiveness to the changing requirements of the medical staff. Consequently, the IT department was able to design all aspects of the full-text search engine for computerized patient records, allowing this project to move forward.
2.1.2 Description of the computerized patient record system
The computerized patient record system is shown in ▶ Figure 1 . It consists of a data acquisition module, a storage module, and a web form module for data query.
Fig. 1
Hospital Information System (Léon Bérard Cancer Center, Lyon, France)
The data acquisition module is composed of different systems that correspond to diagnostic and care activity. Practitioners enter the required data, depending on the medical procedure that was performed (e.g., data from medical consultation, pathology data, chemotherapy, etc.), ensuring the traceability of these procedures in the database. In addition, documents produced outside of the institution (e.g., letters from treating physicians) are digitized and subjected to character recognition.
For reasons related to volume and performance, data entered into the medical record are stored in one of two instances of the database, according to their origin: one instance for biological data and the other instance for other data in computerized patient records. As of April 20, 2013, the SQL Server database contained 700,000 biological results.
Textual data
Two types of textual data are stored in the medical record system:
Natural language data are stored on an SQL Server database (Windows Server 2008 R2; SQL Server 2008: 64-bit; .NET Framework 3.5 sp1). These reports are classified according to their type: consultation, pathology (including reports of pathology, cytology, and molecular biology), surgery, endoscopy, imaging, hospital discharge summary, and miscellaneous. As of April 20, 2013, the database contained 4.4 million documents.
Letters written by physicians participating in the care of the patient outside the CLB are stored on a file server (network-attached storage). As of April 20, 2013, the database stored 1 million of these documents.
Structured data
A number of structured data describing diseases and their management are available in the medical record system.
ADICAP codes: cyto-pathological-anatomical coding (French language), published by the Association for the Development of Information Technology in Anatomy and Pathological Cytology [ 7 ]. These structured data are entered into computerized medical records by pathologists.
Codes of the International Classification of Diseases for Oncology, Third Edition (ICD-O-3): These structured data are entered into computerized patient records by clinicians.
Chemotherapy codes: These codes are organized in a thesaurus of chemotherapy, developed by the CLB. These structured data are entered into computerized medical records by clinicians and nurses.
A web form allows the user to query computerized patient records using a full-text search engine. ▶ Figure 2 shows an example of a query for structured data:
Example of a query on the web form (multi-record search)
"What is the number of patient records in the database of the cancer center with:
a moderately differentiated squamous cell carcinoma, more or less keratinizing (code ADICAP „E7T3")
located on the border of tongue (Code: C02.1)
with the stage of T1 N0 M0 (TNM Classification of Malignant Tumors)?"
▶ Figure 3 shows an example of a query for keywords in the patient record database; a screenshot of the response is shown. Medical documents containing the keywords are selected. For each search result, an extract of the document (specifically, the portion of the document containing the queried keyword) is displayed, and the type of document is specified (hospital discharge report, letter to a practitioner, etc.). By clicking on the search result, the user can view the full corresponding document.
Example of a query (keywords) on the web form (multi-record search)
2.2 Development of a full-text search tool for computerized patient records
2.2.1 Project design
To make better use of data available in computerized patient records, the IT department proposed to work with clinicians to incorporate semantic analysis tools designed to extract and enrich medical data written in natural language. A multidisciplinary project team was established, consisting of a large panel of users, especially medical radiologists, physicians, clinical research assistants, computer specialists, the medical consultant for the development of computerized patient records, and the chairman of the medical committee of the institution.
The IT department solicited SWORD Group, an IT services company with experience of developing solutions in the field of full-text and semantic analysis, and with extensive experience of medical IT. These various solutions were presented to the medical team during brainstorming sessions aimed at selecting the most relevant functionalities for medical activities. The multidisciplinary nature of these brainstorming sessions was very effective. Indeed, although the proposed IT functionalities were based on complex semantic technologies, clinicians formulated much simpler needs, envisioning tools close to modern Internet search engines.
Two goals emerged from these brainstorming sessions. The first goal was to enable a search for medical information within a computerized patient record during the patient's stay in the hospital. The second goal was to allow a search for medical information on all (or a subset of) medical records in the CLB database, in order to retrieve data for evaluation analysis, research, financial analysis, etc.
The goal finally adopted was the development of an application to search for terms found in any text document, possibly in combination with structured data in the medical record.
2.2.2 Technical description
Overall architecture
The technology used for text mining in our healthcare facility is based on Solr, an open-source search engine which indexes content sources, processes query requests, and returns search results. Solr is a web-based application that processes HTTP requests and returns HTTP responses. The Solr search engine ( http://lucene.apache.org/solr/ ) is provided by the Apache Software Foundation, and is based on the Apache Lucene library ( http://lucene.apache.org/core/ ), which provides language analysis functionalities and core search features.
To integrate this tool into the information systems of the CLB, it was necessary to use a data-processing pipeline to retrieve data from different repositories, normalize and clean it, enrich it, and publish it to Solr. This pipeline is similar to an Extract, Transform, and Load (ETL) tool, which performs a data integration process for transferring raw data from a source server to a data warehouse on a target server. However, ETLs usually handle structured data only, whereas this pipeline also manipulates content from unstructured documents. This component, which was previously developed by SWORD Group, made the implementation of the search platform much easier and quicker. It was also necessary to develop user interfaces to allow users to access the search engine within the computerized medical record of a patient. This development was conducted by the CLB IT department.
Data indexing
Three data sources are indexed by the search engine, as follows: computerized medical records, stored in an SQL Server database; PDF documents generated inside the CLB; and PDF documents from practitioners outside the CLB. Some of the indexed PDF documents are PDF images, from which it is not possible to directly extract text for indexing in the search engine. In these cases, optical character recognition (OCR) is performed on the scanned documents when they are integrated into the medical record, and the textual output of OCR is indexed by the search engine. The search engine indexes changes every night and those records and documents that have been created or updated during the day are (re-)indexed at night so that the changes are available in the search engine the next day.
A full indexing is performed every weekend. During this process, the search engine's content is deleted and the contents of all data sources are indexed. Any documents and records that have been deleted since the previous indexing are deleted from the index.
Query workflow
▶ Figure 4 details the query workflow. The user accesses the web client by clicking on a button („full-text search") available on the computerized medical record screen.
Fig. 4
Data repository search engine and user interface (Léon Bérard Cancer Center, Lyon, France)
The user makes a request in the web form and the queries entered into the web interface are submitted via HTTP to Solr.
The search engine returns XML documents containing the results corresponding to the query criteria. These XML documents are used to display results to users within the usual medical record interface.
Data security and confidentiality
A procedure was implemented to assure data security and confidentiality. This procedure defined the rights of authorized users regarding use of the search engine in a single medical record or within different medical records of the facility. Any query to the search engine requires the user to enter a password. All queries are traced in a database (including the name of the user, the search criteria, and the date and time of the search). A reminder screen regularly displays the confidentiality rules. A search within a patient record is open to all users authorized to view that record, whereas a search on the entire database of patient records (multi-record search) is only permitted to members of the medical board of the hospital, or to those granted special dispensation by the hospital management in exceptional situations. To prevent data leaks, exporting of query results (e.g., into Excel format) is only available to physicians of the Department of Medical Information who have been individually authorized.
2.2.3 Functional description
To query a single patient record, the solution returns a list of documents from the computerized patient record in which the selection criteria were found. Some descriptive data are displayed along with the query response: number of selected text documents, document type (consultation letter, pathology report, etc.), and dates of the procedures described in the report (e.g., surgery, consultation, etc.).
To query the database of patient records, the solution returns a list of patients with the documents that match the criteria.
Some descriptive elements regarding the group of patients are displayed as facets: age, living/dead status, and housing department. The user can then navigate through each document in the record of each patient selected. Different operators, filters, and facets are available, and these functionalities are detailed in ▶ Table 1 .
Table 1
Available operators, filters, and facets for queries of the medical record database
Type of functionality
Search records containing the expression "Word 1"
Search records containing the expression "Word 1" and the expression"Word 2"
Search records containing the expression „Word 1" or the expression "Word 2"
Search records containing an exact expression, "Word 1 Word 2"
Filters
Date of completion of the event
Pathological code
Time between the effective date of the event and the date of the report
Age of the patient at the time of report
Age of patient (when alive)
Living status: Alive / Dead
Department: Department of Housing patient
Type of text document (consultation report, pathology report, etc.)
Doctor who signed the report
All queries are stored by users, enabling them to easily repeat a previous query. This also ensures the traceability of queries issued.
2.2.4 Logistical aspects of the implementation
The ability of the IT department to act with full autonomy, along with the partnership with SWORD Group, allowed the rapid implementation of this project, which was completed in six months at a low cost (€15,000). The software development process consisted of three steps: gathering requirements and designing the architecture (2 months); development of the tool and writing the documentation (2 months); and deployment, bug corrections, and elaboration of the maintenance process (2 months). Development costs were broken down as follows: €5,000 for compensation of two trainees over 5 months (one for CLB and one for SWORD); €7,000 for a CLB developer to finish the integration project; and €3,000 for the purchase of a new server. The Solr licenses were free.
At the time of launch, demonstration sessions were offered, on a voluntary basis, to the future users of the full-text search tool on single medical records. In parallel, the user manual was distributed by e-mail to all authorized users of electronic medical records (physicians, nurses, and other paramedical professions). For future users of the full-text search of the database of patient records, a demonstration was given at a meeting of the CLB's medical board. The user manual for multi-record search was distributed by e-mail to all members of the CLB's medical board.
When new staff members arrive at the hospital (especially physicians and medical students), training sessions are held regarding the use of the electronic medical records, including the use of the full-text search tool.
2.3 Evaluation of the quality of search results
The quality of query results was assessed by evaluating the sensitivity and positive predictive value of the full-text search tool for the detection of patients eligible for an epidemiological study, ISICA [ 8 ]. This study required the selection of patients suffering from breast cancer and diabetes. The gold standard for assessing the sensitivity was the identification of eligible cases by manual search of the computerized records by a clinical research assistant. Working time required for the conventional selection of eligible cases was compared to the time required for selection using the full-text search tool.
3. Results
3.1 Description of types of use
From January to May 2013, 500 queries concerning individual patient records were launched per month by an average of 140 different users, and 210 different expressions were searched per month. Among all healthcare professionals authorized to use the computerized medical records for medical care, 50% of search engine users who queried individual patient records were healthcare professionals other than medical practitioners (e.g., nurses and other paramedical professionals).
In the same period (January to May 2013), 25% of users of medical records who were entitled to make requests on the entire database launched at least one request. On average, 114 multi-record requests are made per month.
3.1.1 Tool for the medical care of the patient
The most common use of the full-text search tool is in consultation or at the bedside. In such cases, a medical practitioner needs additional information regarding the medical care of the patient, and launches an appropriate query of the computerized patient record. The most frequently searched expressions are drug names, different steps involved in medical care (radiotherapy, chemotherapy, metastasis, multidisciplinary consultation meetings, etc.), comorbidities (allergy, diabetes, etc.), and test results (myelogram, KRAS gene, etc.).
For example, a patient was monitored for chronic lymphocytic leukemia for several years at the center. The medical practitioner could not remember whether blood cytogenetics have been performed so he made a request in the computerized patient record, entering the term „karyotype." Instantly, the date and the test results were found in the computerized patient record, avoiding unnecessary new prescriptions and procedures.
Under the Multidisciplinary Consultation Meetings, this tool is particularly useful when discussing a case of orphan disease, a difficult case, or new therapies still rarely used in the situation under consideration. Furthermore, the tool allows quick retrieval of records of patients whose medical care has already been discussed. Earlier decisions traced in the database aid with discussions of the most suitable medical management for that patient.
The tool is also used by the Pharmacy for the validation of drug prescriptions.
3.1.2 Tool for clinical research
This tool also aids with identifying patients eligible for clinical trials, a process far quicker than manual search. For example, suppose that a patient is being considered for pre-selection for a clinical trial for chronic lymphocytic leukemia, in which an exclusion condition is the absence of prior treatment. To verify this exclusion condition, the doctor queries the patient's record by entering the term „chlorambucil". The search results include a document indicating that the patient received this treatment several years ago for 2 months. Therefore, the patient must be excluded from the clinical trial.
Full-text search also facilitates the rapid generation of a database in response to a study protocol (e.g., in a thesis, or in preparation of a scientific publication). For instance, full-text search allowed exhaustive retrieval of patients treated for a very rare clinical entity (NK/T-cell lymphoma) and easy collection of data for a retrospective multicenter study of French institutions [ 9 ].
3.1.3 Improving the traceability of medical care in medical records
The Department of Medical Information and medical assistants use the tool to ensure good traceability of medical care in patients' medical records and the pricing of medical procedures.
3.2 Qualitative evaluation
The ISICA study [ 8 ] was a case-control study based on data collected in 92 centers that manage breast cancer in France, the United Kingdom and Canada. In the context of this study, the Medical Information Department selected patients diagnosed with breast cancer at the CLB between 1 January 2008 and 30 June 2009, using ICD-10 coding for diagnosis-related groups (DRGs).
A clinical research assistant analyzed each patient's medical records to assess their diabetic status. Of the 2,129 patients diagnosed with breast cancer over this period, 88 patients had diabetes and were selected for inclusion in the study. The screening of the 2,129 files required 304 hours of work (on average, seven cases handled per hour).
In parallel, the screening of 2,129 records was performed using the full-text search tool. Using the combinations „DIABETES" OR „INSULIN" or „trade name insulin treatment" or „trade name treatment with oral antidiabetic agents," the full-text search identified 150 patients.
Screening by the clinical research assistant of the 150 preselected patients identified 73 of the 88 patients selected by the traditional method, corresponding to a sensitivity of 83.0% and a positive predictive value (PPV) of 48.7%.
The work time required was 2 hours for the Medical Information Department (conception of the query, programming the query on the server, formatting results) vs. 22 hours for screening by the clinical research assistant. If this method had been used for the study, the gain in work time of the clinical research assistant would have been 92.8%. However, in order to follow the protocol of this multicenter study, this tool has not been chosen for the selection of eligible study subjects.
4. Discussion
Thirty years ago, the CLB Cancer Center made the strategic choice to develop its own computerized patient record system and gradually develop additional tools for its use. This decision allowed the deployment of a full-text search tool to be used directly by medical practitioners in the care of their patients. When patient records were stored on paper, practitioners seeking very specific information about a patient needed to review each sheet, one after another. One of the first advances related to the computerization of medical records was the use of metadata, including the characteristics of each document (surgical report, pathology report, etc.), which limited searches to subsets of medical records. The introduction of a tool for full-text search for direct use within computerized patient records provides an additional method for searching unstructured data within medical documents. The functionalities offered by the full-text search tool have significantly changed the use of computerized patient records by practitioners. Indeed, these functionalities allow health professionals to obtain query results in a few seconds, significantly improving their daily practice as demonstrated in this article.
There are four prerequisites for implementing full-text search of computerized patient records.
Health information consists of confidential data to which access is regulated, taking into account the rights of the patient. A procedure that describes various aspects of data security and confidentiality (see "Data Security", above) must be implemented in order to enforce these rights.
It is necessary for patient records to be fully computerized and for health professionals to support this by not using a paper record.
It is necessary to work with software engineers who have technical mastery of the full-text search tool and search-engine indexing.
It is necessary for the developer of the computerized patient record to be willing to invest in this area.
Full-text search is particularly efficient at retrieving poorly structured data from medical records, due to its ability to search for words in documents without any constraint on the data structure. Uncommon medical data such as karyotypes, HLA typing, particular histories, and former treatments are examples of data for which the full-text search tool can help to retrieve information very rapidly. The searches are indeed extremely fast (a few seconds at most), saving time and providing medical information that reduces the number of redundant prescriptions and procedures (e.g., biological or radiological examinations that were already prescribed or performed recently), both of which are important parameters for improving the quality of medical care. This empirical observation has been demonstrated in the health information technology literature, which has shown that this health information technology enhances monitoring and surveillance activities, reduces medication errors, and reduces rates of utilization of potentially redundant or inappropriate prescriptions [ 10 ].
This project was met with strong approval by the CLB medical community, possibly attributed to the involvement of physicians during the initial "needs gathering" phase at the time of project startup. Because this type of tool is very new to physicians, they asked that the first implementation phase should focus on simple functionalities such as full-text keyword-based search of medical records. More complex functionalities, such as synonym searches for medical concepts, giving similar concepts (e.g., generic names of drugs), were judged as "nice to have" additional features for the next step, but the initial features were judged to be good enough for the first step. Now that the implementation phase has been completed successfully, users have seen the limits of the basic features initially introduced and now want to refine the search capabilities of the tool. Indeed, the limitations of the current version of the tool represent areas for improvement in the coming years. For example, because the tool is not based on a semantic analysis technique, it is not possible to identify negations [ 11 ]. This leads to a significant number of false positives, as shown by the relatively low PPV (48.7%) (See "Results", above). A considerable proportion of the false positives are due to differential diagnoses listed and discussed in medical records. Moreover, in the absence of semantic analysis, no time labeling of medical entities takes place. For example, if the user searches for patients with non-Hodgkin's lymphomas located on the ovary, a large number of false positives (66 of 72 records in the current database) will be selected by the tool, because it cannot discriminate between information about the ovaries relating to the patient's history (including family) and information related to the current management of lymphoma.
Time labeling of extracted medical information is, therefore, an important feature that should be introduced to clarify the relevance of detected entities [ 12 ]. Depending on the type of query, the degree of precision of time labeling will be more or less important. The minimum precision for time labeling would permit discrimination between medical information extracted from the „history", „reason for seeking care", and „evolution" sections.
In the short term, to overcome this lack of precision, the developers decided to combine full-text search with the use of the metadata structuring computerized patient records, allowing users to exclude entities identified in the „history" section. Moreover, the information processing required for medical practice seems more sophisticated than that offered by full-text search tools found on the Internet [ 13 ]. More complex algorithms remain to be performed [ 14 ]. In addition, because of the number and diversity of potential users of health data, implementation of search modules should facilitate development of algorithms tailored to these different types of queries [ 15 ].
Development prospects in the coming months are:
Increasing the number of types of documents on which a search is possible (medical notes during hospitalization stays; end-of-life wishes; resuscitation information; information given to the patient; information provided to third parties; reports on multidisciplinary consultation meetings; orders issued from computerized patient records; certificates issued from computerized patient records).
Implementing advanced functionality, including NLP (for detection of negations and of documents headers), concept extraction (used to identify medical terms such as names of drugs, diseases, and multidrug-resistant bacteria), use of thesauri to extend searches to synonyms and drug equivalents, and implementation of reasoning rules to categorize the patient profile. Along with other French cancer centers, the CLB is a member of the Consore project, which will provide these functionalities. The text-analysis service developed for Consore will be integrated into the CLB search engine during the first semester of 2014. This service uses an ontology that integrates several international terminologies (ICD-10; the 'morphology' and 'disease' branches of SNOMED; ATC, a list of commercial drugs names; the Cosmic gene list; and CCAM, which contains French terminology for therapeutic procedures) that are used for concept annotation.
Connecting this advanced functionality to aid in DRG coding, in order to optimize the cost of hospitalizations, including taking better account of comorbidities.
Narrowing down the rules of access and use of sensitive data to facilitate their use, while maintaining strict confidentiality of patient data.
Generating automatic alerts from pre-built queries (e.g., sending an alert when a new document mentions a patient with multidrug-resistant bacteria).
5. Conclusion
In France, hospital IT tools that can influence medical practice are uncommon. Indeed, the most frequently deployed tools are based on an old conception (from the 1990s) and have focused more on the needs of hospital institutions than on the requirements of medical activities.
The project described here, consisted in integrating a full-text search engine in the hospital information system. Using an open source search engine Solr which indexes content sources, processes query requests and returns search results in electronic health records, this project demonstrates that the evolution of computer technology in the health sector has the potential to significantly change daily medical practice in patient care.
Footnotes
Clinical relevance statement
The primary purposes of integrating a search engine into electronic health records are to save medical time and facilitate the use of unstructured medical information for various endeavors. This has a direct impact on the medical care, changes the practice of patient management, and reduces the time required to select patients and collect data for research activities.
Conflicts of interest
The search engine integration and configuration was performed by SWORD, under the direction of E. Barthuet. The other authors declare that they have no conflicts of interest in this research.
Protection of human and animal subjects
Human and/or animal subjects were not included in the project.
References
1. Gregg W, Jirjis J, Lorenz iN, Giuse D. StarTracker: an integrated, web-based clinical search engine. AMIA Annu Symp Proc 2003: 855. [ PMC free article ] [ PubMed ]
2. Hanauer DA. EMERSE: the Electronic Medical Record Search Engine. AMIA Annu Symp Proc 2006:941. [ PMC free article ] [ PubMed ]
3. Erinjeri J, Picus D, Prior F, Rubin D, Koppel P. Development of a Google-Based Search Engine for Data Mining Radiology Reports. Journal of Digital Imaging 2009; 22(4): 348–356 [ PMC free article ] [ PubMed ]
4. Thiessard F, Mougin F, Diallo G, Jouhet V, Cossin S, Garcelon N, Campillo B, Jouini W, Grosjean J, Massari P, Griffon N, Dupuch M, Tayalati F, Dugas E, Balvet A, Grabar N, Pereira S, Frandji B, Darmoni S, Cuggia M. RAVEL: retrieval and visualization in electronic health records. Stud Health Technol Inform 2012; 180: 194–198 [ PubMed ]
5. Seyfried L, Hanauer DA, Nease D, Albeiruti R, Kavanagh J, Kales HC. Enhanced identification of eligibility for depression research using an electronic medical record search engine. Int J Med Inform 2009; 78(12): e13–e18 [ PMC free article ] [ PubMed ]
6. Zheng K, Mei Q, Hanauer DA. Collaborative search in electronic health records. J Am Med Inform Assoc 2011; 18(3): 282–291 [ PMC free article ] [ PubMed ]
7. Association pour le Développement de l'Informatique en Anatomie et Cytologie Pathologiques Thésaurus de la codification ADICAP. 2009
8. Grimaldi-Bensouda L, Marty M, Pollack M, Cameron D, Riddle M, et al. The International Study of Insulin and Cancer. Lancet 2010; 376: 769–770 [ PubMed ]
9. Chauchet A, Michallet A, Berger F, Bedgedjian I, Deconinck E, Sebban C, Antal D, Orfeuvre H, Corront B, Petrella T, Hacini M, Bouteloup M, Salles G, Coiffier B. Complete remission after first-line radio-chemotherapy as predictor of survival in extranodal NK/T cell lymphoma. J Hematol Oncol 2012; 5: 27. [ PMC free article ] [ PubMed ]
10. Chaudhry B, Wang J, Wu S, Maglione M, Mojica W, Roth E, Morton SC, Shekelle PG. Systematic review: impact of health information technology on quality, efficiency, and costs of medical care. Ann Intern Med 2006; 144(10): 742–752 [ PubMed ]
11. Hagège C, Marchal P, Gicquel Q, Darmoni S, Pereira S, Metzger M. Linguistic and Temporal Processing for Discovering Hospital Acquired Infection from Patient Records. In: David Riaño AtT, Silvia Miksch, Mor Peleg, editors. , Knowledge Representation for Health-Care – ECAI 2010 Workshop KR4HC 2010, Lisbon, Portugal, August 17, 2010, Revised Selected Papers Lectures Notes in Artificial Intelligence. Vol. 6512Springer, 2010; 70–84
12. Proux D, Hagège C, Gicquel Q, Kergourlay I, Pereira S, Rondeau G, Darmoni S, Segond F, Metzger M. ALADIN : Développement d'un outil sémantique d'analyse des documents textuels médicaux pour la détection d'infections associées aux soins IRBM, Ingenierie et Recherche BioMedicale 2012;33(2):137–142
13. Giustini D. How Google is changing medicine. BMJ 2005; 331: 1487. [ PMC free article ] [ PubMed ]
14. Yang L, Mei Q, Zheng K, Hanauer DA. Query log analysis for an electronic health record search engine AMIA Annu Symp Proc 2011: 915–924 [ PMC free article ] [ PubMed ]
15. Natarajan K, Stein D, Jain S, Elhadad N. An analysis of clinical queries in an electronic health record search utility. Int J Med Inform 2010; 79(7): 515–522 [ PMC free article ] [ PubMed ]
Articles from Applied Clinical Informatics are provided here courtesy of Thieme Medical Publishers
Formats:
F-scores for groups: 0.76, outcomes: 0.42, group sizes: 0.80, outcome numbers: 0.71
Kiritchenko et al. (2010) [ 36 ]
Eligibility criteria, sample size, drug dosage, primary outcomes
50 full-text journal articles with 1050 test instances
SVM classifier to recover relevant sentences, extraction rules for correct solutions
Concept
Full text
P5 precision for the classifier: 0.88, precision and recall of the extraction rules: 93 and 91 %, respectively
Lin et al. (2010) [ 39 ]
Intervention, age group of the patients, geographical area, number of patients, time duration of the study
93 open access full-text literature documenting oncological and cardio-vascular studies from 2005 to 2008
Linear chain, conditional random fields
Concept
Full text
Precision of 0.4 for intervention, 0.63 for age group, 0.44 for geographical area, 0.43 for number of patients and 0.83 for time period
Restificar et al. (2012) [ 37 ]
Eligibility criteria
44,203 full-text articles with clinical trials
Latent Dirichlet allocation along with logistic regression
Concept
75 and 70 % accuracy based on similarity for inclusion and exclusion criteria, respectively.
De Bruijn et al. (2008) [ 40 ]
Eligibility criteria, sample size, treatment duration, intervention, primary and secondary outcomes
88 randomized controlled trials full-text articles from five medical journals
SVM classifier to identify the most promising sentences; manually crafted weak extraction rules for the information elements
Sentence, concept
Full text
Precision for eligibility criteria: 0.69, sample size: 0.62, treatment duration: 0.94, intervention: 0.67, primary outcome: 1.00, secondary outcome: 0.67
Zhu et al. (2012) [ 41 ]
Subject demographics: patient age, gender, disease and ethnicity
50 randomized controlled trials full-text articles
Manually crafted rules for extraction from the parse tree
Concept
Disease extraction: for exact matching, the F-score was 0.64. For partially matched, it was 0.85.
Marshall et al. (2014) [ 27 ]
Risk of bias concerning sequence generation, allocation concealment and blinding
2200 clinical trial reports
Soft-margin SVM for a joint model of risk of bias prediction and supporting sentence extraction
Sentence
Full text
For sentence identification: F-score of 0.56, 0.48, 0.35 and 0.38 for random sequence generation, allocation concealment, blinding of participants and personnel, and blinding of outcome assessment
Open in a separate window
The accuracy of most (N = 18, 69 %) studies was measured using a standard text mining metric known as F-score, which is the harmonic mean of precision (positive predictive value) and recall (sensitivity). Some studies (N = 5, 19 %) reported only the precision of their method, while some reported the accuracy values (N = 2, 8 %). One study (4 %) reported P5 precision, which indicates the fraction of positive predictions among the top 5 results returned by the system.
Studies that did not implement a data extraction system
Dawes et al. [ 12 ] identified 20 evidence-based medicine journal synopses with 759 extracts in the corresponding PubMed abstracts. Annotators agreed with the identification of an element 85 and 87 % for the evidence-based medicine synopses and PubMed abstracts, respectively. After consensus among the annotators, agreement rose to 97 and 98 %, respectively. The authors proposed various lexical patterns and developed rules to discover each PECODR element from the PubMed abstracts and the corresponding evidence-based medicine journal synopses that might make it possible to partially or fully automate the data extraction process.
Studies that identified sentences but did not extract data elements from abstracts only
Kim et al. [ 13 ] used conditional random fields (CRF) [ 15 ] for the task of classifying sentences in one of the PICO categories. The features were based on lexical, syntactic, structural, and sequential information in the data. The authors found that unigrams, section headings, and sequential information from preceding sentences were useful features for the classification task. They used 1000 medical abstracts from PIBOSO corpus and achieved micro-averaged F-scores of 91 and 67 % over datasets of structured and unstructured abstracts, respectively.
Boudin et al. [ 16 ] utilized a combination of multiple supervised classification techniques for detecting PICO elements in the medical abstracts. They utilized features such as MeSH semantic types, word overlap with title, number of punctuation marks on random forests (RF), naive Bayes (NB), support vector machines (SVM), and multi-layer perceptron (MLP) classifiers. Using 26,000 abstracts from PubMed, the authors took the first sentence in the structured abstracts and assigned a label automatically to build a large training data. They obtained an F-score of 86 % for identifying participants (P), 67 % for interventions (I) and controls (C), and 56 % for outcomes (O).
Huang et al. [ 17 ] used a naive Bayes classifier for the PICO classification task. The training data were generated automatically from the structured abstracts. For instance, all sentences in the section of the structured abstract that started with the term "PATIENT" were used to identify participants (P). In this way, the authors could generate a dataset of 23,472 sentences. Using 23,472 sentences from the structured abstracts, they obtained an F-score of 91 % for identifying participants (P), 75 % for interventions (I), and 88 % for outcomes (O).
Verbeke et al. [ 18 ] used a statistical relational learning-based approach (kLog) that utilized relational features for classifying sentences. The authors also used the PIBOSO corpus for evaluation and achieved micro-averaged F-score of 84 % on structured abstracts and 67 % on unstructured abstracts, which was a better performance than Kim et al. [ 13 ].
Huang et al. [ 19 ] used 19,854 structured extracts and trained two classifiers: one by taking the first sentences of each section (termed CF by the authors) and the other by taking all the sentences in each section (termed CA by the authors). The authors used the naive Bayes classifier and achieved F-scores of 74, 66, and 73 % for identifying participants (P), interventions (I), and outcomes (O), respectively, by the CF classifier. The CA classifier gave F-scores of 73, 73, and 74 % for identifying participants (P), interventions (I), and outcomes (O), respectively.
Hassanzadeh et al. [ 20 ] used the PIBOSO corpus for the identification of sentences with PIBOSO elements. Using conditional random fields (CRF) with discriminative set of features, they achieved micro-averaged F-score of 91 %.
Robinson [ 21 ] used four machine learning models, 1) support vector machines, 2) naive Bayes, 3) naive Bayes multinomial, and 4) logistic regression to identify medical abstracts that contained patient-oriented evidence or not. These data included morbidity, mortality, symptom severity, and health-related quality of life. On a dataset of 1356 PubMed abstracts, the authors achieved the highest accuracy using a support vector machines learning model and achieved an F-measure of 86 %.
Chung [ 22 ] utilized a full sentence parser to identify the descriptions of the assignment of treatment arms in clinical trials. The authors used predicate-argument structure along with other linguistic features with a maximum entropy classifier. They utilized 203 abstracts from randomized trials for training and 124 abstracts for testing and achieved an F-score of 76 %.
Hara and Matsumoto [ 23 ] dealt with the problem of extracting "patient population" and "compared treatments" from medical abstracts. Given a sentence from the abstract, the authors first performed base noun-phrase chunking and then categorized the base noun-phrase into one of the five classes: "disease", "treatment", "patient", "study", and "others" using support vector machine and conditional random field models. After categorization, the authors used regular expression to extract the target words for patient population and comparison. The authors used 200 abstracts including terms such as "neoplasms" and "clinical trial, phase III" and obtained 91 % accuracy for the task of noun phrase classification. For sentence classification, the authors obtained a precision of 80 % for patient population and 82 % for comparisons.
Studies that identified only sentences but did not extract data elements from full-text reports
Zhao et al. [ 24 ] used two classification tasks to extract study data including patient details, including one at the sentence level and another at the keyword level. The authors first used a five-class scheme including 1) patient, 2) result, 3) intervention, 4) study design, and 5) research goal and tried to classify sentences into one of these five classes. They further used six classes for keywords such as sex (e.g., male, female), age (e.g., 54-year-old), race (e.g., Chinese), condition (e.g., asthma), intervention, and study design (e.g., randomized trial). They utilized conditional random fields for the classification task. Using 19,893 medical abstracts and full-text articles from 17 journal websites, they achieved F-scores of 75 % for identifying patients, 61 % for intervention, 91 % for results, 79 % for study design, and 76 % for research goal.
Hsu et al. [ 25 ] attempted to classify whether a sentence contains the "hypothesis", "statistical method", "outcomes", or "generalizability" of the study and then extracted the values. Using 42 full-text papers, the authors obtained F-scores of 86 % for identifying hypothesis, 84 % for statistical method, 90 % for outcomes, and 59 % for generalizability.
Song et al. [ 26 ] used machine learning-based classifiers such as maximum entropy classifier (MaxEnt), support vector machines (SVM), multi-layer perceptron (MLP), naive Bayes (NB), and radial basis function network (RBFN) to classify the sentences into categories such as analysis (statistical facts found by clinical experiment), general (generally accepted scientific facts, process, and methodology), recommendation (recommendations about interventions), and rule (guidelines). They utilized the principle of information gain (IG) as well as genetic algorithm (GA) for feature selection. They used 346 sentences from the clinical guideline document and obtained an F-score of 98 % for classifying sentences.
Marshall et al. [ 27 ] used soft-margin support vector machines in a joint model for risk of bias assessment along with supporting sentences for random sequence generation, allocation concealment, blinding of participants and personnel, and blinding of outcome assessment, among others. They utilized presence of unigrams in the supporting sentences as features in their model. Working with full text of 2200 clinical trials, the joint model achieved F-scores of 56, 48, 35, and 38 % for identifying sentences corresponding to random sequence generation, allocation concealment, blinding of participants and personnel, and blinding of outcome assessment, respectively.
Studies that identified data elements only from abstracts but not from full texts
Demner-Fushman and Lin [ 28 ] used a rule-based approach to identify sentences containing PICO. Using 275 manually annotated abstracts, the authors achieved an accuracy of 80 % for population extraction and 86 % for problem extraction. They also utilized a supervised classifier for outcome extraction and achieved accuracy from 64 to 95 % across various experiments.
Kelly and Yang [ 29 ] used regular expressions and gazetteer to extract the number of participants, participant age, gender, ethnicity, and study characteristics. The authors utilized 386 abstracts from PubMed obtained with the query "soy and cancer" and achieved F-scores of 96 % for identifying the number of participants, 100 % for age of participants, 100 % for gender of participants, 95 % for ethnicity of participants, 91 % for duration of study, and 87 % for health status of participants.
Hansen et al. [ 30 ] used support vector machines [ 31 ] to extract number of trial participants from abstracts of the randomized control trials. The authors utilized features such as part-of-speech tag of the previous and next words and whether the sentence is grammatically complete (contained a verb). Using 233 abstracts from PubMed, they achieved an F-score of 86 % for identifying participants.
Xu et al. [ 32 ] utilized text classifications augmented with hidden Markov models [ 33 ] to identify sentences about subject demographics. These sentences were then parsed to extract information regarding participant descriptors (e.g., men, healthy, elderly), number of trial participants, disease/symptom name, and disease/symptom descriptors. After testing over 250 RCT abstracts, the authors obtained an accuracy of 83 % for participant descriptors: 83 %, 93 % for number of trial participants, 51 % for diseases/symptoms, and 92 % for descriptors of diseases/symptoms.
Summerscales et al. [ 34 ] used a conditional random field-based approach to identify various named entities such as treatments (drug names or complex phrases) and outcomes. The authors extracted 100 abstracts of randomized trials from the BMJ and achieved F-scores of 49 % for identifying treatment, 82 % for groups, and 54 % for outcomes.
Summerscales et al. [ 35 ] also proposed a method for automatic summarization of results from the clinical trials. The authors first identified the sentences that contained at least one integer (group size, outcome numbers, etc.). They then used the conditional random field classifier to find the entity mentions corresponding to treatment groups or outcomes. The treatment groups, outcomes, etc. were then treated as various "events." To identify all the relevant information for these events, the authors utilized templates with slots. The slots were then filled using a maximum entropy classifier. They utilized 263 abstracts from the BMJ and achieved F-scores of 76 % for identifying groups, 42 % for outcomes, 80 % for group sizes, and 71 % for outcome numbers.
Studies that identified data elements from full-text reports
Kiritchenko et al. [ 36 ] developed ExaCT, a tool that assists users with locating and extracting key trial characteristics such as eligibility criteria, sample size, drug dosage, and primary outcomes from full-text journal articles. The authors utilized a text classifier in the first stage to recover the relevant sentences. In the next stage, they utilized extraction rules to find the correct solutions. The authors evaluated their system using 50 full-text articles describing randomized trials with 1050 test instances and achieved a P5 precision of 88 % for identifying the classifier. Precision and recall of their extraction rules was found to be 93 and 91 %, respectively.
Restificar et al. [ 37 ] utilized latent Dirichlet allocation [ 38 ] to infer the latent topics in the sample documents and then used logistic regression to compute the probability that a given candidate criterion belongs to a particular topic. Using 44,203 full-text reports of randomized trials, the authors achieved accuracies of 75 and 70 % for inclusion and exclusion criteria, respectively.
Lin et al. [ 39 ] used linear-chain conditional random field for extracting various metadata elements such as number of patients, age group of the patients, geographical area, intervention, and time duration of the study. Using 93 full-text articles, the authors achieved a threefold cross validation precision of 43 % for identifying number of patients, 63 % for age group, 44 % for geographical area, 40 % for intervention, and 83 % for time period.
De Bruijn et al. [ 40 ] used support vector machine classifier to first identify sentences describing information elements such as eligibility criteria, sample size, etc. The authors then used manually crafted weak extraction rules to extract various information elements. Testing this two-stage architecture on 88 randomized trial reports, they obtained a precision of 69 % for identifying eligibility criteria, 62 % for sample size, 94 % for treatment duration, 67 % for intervention, 100 % for primary outcome estimates, and 67 % for secondary outcomes.
Zhu et al. [ 41 ] also used manually crafted rules to extract various subject demographics such as disease, age, gender, and ethnicity. The authors tested their method on 50 articles and for disease extraction obtained an F-score of 64 and 85 % for exactly matched and partially matched cases, respectively.
Risk of bias across studies
In general, many studies have a high risk of selection bias because the gold standards used in the respective studies were not randomly selected. The risk of performance bias is also likely to be high because the investigators were not blinded. For the systems that used rule-based approaches, it was unclear whether the gold standard was used to train the rules or if there were a separate training set. The risk of attrition bias is unclear based on the study design of these non-randomized studies evaluating the performance of NLP methods. Lastly, the risk of reporting bias is unclear because of the lack of protocols in the development, implementation, and evaluation of NLP methods.
Discussion
Summary of evidence
Extracting the data elements
Participants—Sixteen studies explored the extraction of the number of participants [ 12 , 13 , 16 – 20 , 23 , 24 , 28 – 30 , 32 , 39 ], their age [ 24 , 29 , 39 , 41 ], sex [ 24 , 39 ], ethnicity [ 41 ], country [ 24 , 39 ], comorbidities [ 21 ], spectrum of presenting symptoms, current treatments, and recruiting centers [ 21 , 24 , 28 , 29 , 32 , 41 ], and date of study [ 39 ]. Among them, only six studies [ 28 – 30 , 32 , 39 , 41 ] extracted data elements as opposed to highlighting the sentence containing the data element. Unfortunately, each of these studies used a different corpus of reports, which makes direct comparisons impossible. For example, Kelly and Yang [ 29 ] achieved high F-scores of 100 % for age of participants, 91 % for duration of study, 95 % for ethnicity of participants, 100 % for gender of subjects, 87 % for health status of participants, and 96 % for number of participants on a dataset of 386 abstracts.
Intervention—Thirteen studies explored the extraction of interventions [ 12 , 13 , 16 – 20 , 22 , 24 , 28 , 34 , 39 , 40 ], intervention groups [ 34 , 35 ], and intervention details (for replication if feasible) [ 36 ]. Of these, only six studies [ 28 , 34 – 36 , 39 , 40 ] extracted intervention elements. Unfortunately again, each of these studies used a different corpus. For example, Kiritchenko et al. [ 36 ] achieved an F-score of 75–86 % for intervention data elements on a dataset of 50 full-text journal articles.
Outcomes and comparisons—Fourteen studies also explored the extraction of outcomes and time points of collection and reporting [ 12 , 13 , 16 – 20 , 24 , 25 , 28 , 34 – 36 , 40 ] and extraction of comparisons [ 12 , 16 , 22 , 23 ]. Of these, only six studies [ 28 , 34 – 36 , 40 ] extracted the actual data elements. For example, De Bruijn et al. [ 40 ] obtained an F-score of 100 % for extracting primary outcome and 67 % for secondary outcome from 88 full-text articles. Summerscales [ 35 ] utilized 263 abstracts from the BMJ and achieved an F-score of 42 % for extracting outcomes.
Results—Two studies [ 36 , 40 ] extracted sample size data element from full text on two different data sets. De Bruijn et al. [ 40 ] obtained an accuracy of 67 %, and Kiritchenko et al. [ 36 ] achieved an F-score of 88 %.
Interpretation—Three studies explored extraction of overall evidence [ 26 , 42 ] and external validity of trial findings [ 25 ]. However, all these studies only highlighted sentences containing the data elements relevant to interpretation.
Objectives—Two studies [ 24 , 25 ] explored the extraction of research questions and hypotheses. However, both these studies only highlighted sentences containing the data elements relevant to interpretation.
Methods—Twelve studies explored the extraction of the study design [ 13 , 18 , 20 , 24 ], study duration [ 12 , 29 , 40 ], randomization method [ 25 ], participant flow [ 36 , 37 , 40 ], and risk of bias assessment [ 27 ]. Of these, only four studies [ 29 , 36 , 37 , 40 ] extracted the corresponding data elements from text using different sets of corpora. For example, Restificar et al. [ 37 ] utilized 44,203 full-text clinical trial articles and achieved accuracies of 75 and 70 % for inclusion and exclusion criteria, respectively.
Miscellaneous—One study [ 26 ] explored extraction of key conclusion sentence and achieved a high F-score of 98 %.
Related reviews and studies
Previous reviews on the automation of systematic review processes describe technologies for automating the overall process or other steps. Tsafnat et al. [ 43 ] surveyed the informatics systems that automate some of the tasks of systematic review and report systems for each stage of systematic review. Here, we focus on data extraction. None of the existing reviews [ 43 – 47 ] focus on the data extraction step. For example, Tsafnat et al. [ 43 ] presented a review of techniques to automate various aspects of systematic reviews, and while data extraction has been described as a task in their review, they only highlighted three studies as an acknowledgement of the ongoing work. In comparison, we identified 26 studies and critically examined their contribution in relation to all the data elements that need to be extracted to fully support the data extraction step.
Thomas et al. [ 44 ] described the application of text mining technologies such as automatic term recognition, document clustering, classification, and summarization to support the identification of relevant studies in systematic reviews. The authors also pointed out the potential of these technologies to assist at various stages of the systematic review. Slaughter et al. [ 45 ] discussed necessary next steps towards developing "living systematic reviews" rather than a static publication, where the systematic reviews can be continuously updated with the latest knowledge available. The authors mentioned the need for development of new tools for reporting on and searching for structured data from clinical trials.
Tsafnat et al. [ 46 ] described four main tasks in systematic review: identifying the relevant studies, evaluating risk of bias in selected trials, synthesis of the evidence, and publishing the systematic reviews by generating human-readable text from trial reports. They mentioned text extraction algorithms for evaluating risk of bias and evidence synthesis but remain limited to one particular method for extraction of PICO elements.
Most natural language processing research has focused on reducing the workload for the screening step of systematic reviews (Step 3). Wallace et al. [ 48 , 49 ] and Miwa et al. [ 50 ] proposed an active learning framework to reduce the workload in citation screening for inclusion in the systematic reviews. Jonnalagadda et al. [ 51 ] designed a distributional semantics-based relevance feedback model to semi-automatically screen citations. Cohen et al. [ 52 ] proposed a module for grouping studies that are closely related and an automated system to rank publications according to the likelihood for meeting the inclusion criteria of a systematic review. Choong et al. [ 53 ] proposed an automated method for automatic citation snowballing to recursively pursue relevant literature for helping in evidence retrieval for systematic reviews. Cohen et al. [ 54 ] constructed a voting perceptron-based automated citation classification system to classify each article as to whether it contains high-quality, drug-specific evidence. Adeva et al. [ 55 ] also proposed a classification system for screening articles for systematic review. Shemilt et al. [ 56 ] also discussed the use of text mining to reduce screening workload in systematic reviews.
Research implications
No standard gold standards or dataset
Among the 26 studies included in this systematic review, only three of them use a common corpus, namely 1000 medical abstracts from the PIBOSO corpus. Unfortunately, even that corpus facilitates only classification of sentences into whether they contain one of the data elements corresponding to the PIBOSO categories. No two other studies shared the same gold standard or dataset for evaluation. This limitation made it impossible for us to compare and assess the relative significance of the reported accuracy measures.
Separate systems for each data element
Few data elements, which are also relatively straightforward to extract automatically, such as the total number of participants (14 overall and 5 for extracting the actual data elements), have a relatively higher number of studies aiming towards extracting the same data element. This is not the case with other data elements. There are 27 out of 52 potential data elements that have not been explored for automated extraction, even if for highlighting the sentences containing them; seven more data elements were explored just by one study. There are 38 out of 52 potential data elements (>70 %) that have not been explored for automated extraction of the actual data elements; three more data elements were explored just by one study. The highest number of data elements extracted by a single study is only seven (14 %). This finding means that not only are more studies needed to explore the remaining 70 % data elements, but that there is an urgent need for a unified framework or system to extract all necessary data elements. The current state of informatics research for data extraction is exploratory, and multiple studies need to be conducted using the same gold standard and on the extraction of the same data elements for effective comparison.
Limitations
Our study has limitations. First, there is a possibility that data extraction algorithms were not published in journals or that our search might have missed them. We sought to minimize this limitation by searching in multiple bibliographic databases, including PubMed, IEEExplore, and ACM Digital Library. However, investigators may have also failed to publish algorithms that had lower F-scores than were previously reported, which we would not have captured. Second, we did not publish a protocol a priori, and our initial findings may have influenced our methods. However, we performed key steps, including screening, full-text review, and data extraction in duplicate to minimize potential bias in our systematic review.
Future work
"On demand" access to summarized evidence and best practices has been considered a sound strategy to satisfy clinicians' information needs and enhance decision-making [ 57 – 65 ]. A systematic review of 26 studies concluded that information-retrieval technology produces positive impact on physicians in terms of decision enhancement, learning, recall, reassurance, and confirmation [ 62 ]. Slaughter et al. [ 45 ] discussed necessary next steps towards developing "living systematic reviews" rather than a static publication, where the systematic reviews can be continuously updated with the latest knowledge available. The authors mention the need for development of new tools for reporting on and searching for structured data from published literature. Automated information extraction framework that extract data elements have the potential to assist the systematic reviewers and to eventually automate the screening and data extraction steps.
Medical science is currently witnessing a rapid pace at which medical knowledge is being created—75 clinical trials a day [ 66 ]. Evidence-based medicine [ 67 ] requires clinicians to keep up with published scientific studies and use them at the point of care. However, it has been shown that it is practically impossible to do that even within a narrow specialty [ 68 ]. A critical barrier is that finding relevant information, which may be located in several documents, takes an amount of time and cognitive effort that is incompatible with the busy clinical workflow [ 69 , 70 ]. Rapid systematic reviews using automation technologies will enable clinicians with up-to-date and systematic summaries of the latest evidence.
Conclusions
Our systematic review describes previously reported methods to identify sentences containing some of the data elements for systematic reviews and only a few studies that have reported methods to extract these data elements. However, most of the data elements that would need to be considered for systematic reviews have been insufficiently explored to date, which identifies a major scope for future work. We hope that these automated extraction approaches might first act as checks for manual data extraction currently performed in duplicate; then serve to validate manual data extraction done by a single reviewer; then become the primary source for data element extraction that would be validated by a human; and eventually completely automate data extraction to enable living systematic reviews.
Abbreviations
J Biomed Inform. Author manuscript; available in PMC 2016 Jun 1.
Published in final edited form as:
Published online 2015 May 13. doi:  10.1016/j.jbi.2015.05.003
PMCID: PMC4527540
PMID: 25979153
Supporting Information Retrieval from Electronic Health Records: A Report of University of Michigan's Nine-Year Experience in Developing and Using the Electronic Medical Record Search Engine (EMERSE)
1Department of Pediatrics, University of Michigan Medical School, Ann Arbor, MI, USA
2School of Information, University of Michigan, Ann Arbor, MI, USA
Find articles by David A. Hanauer
Qiaozhu Mei
2School of Information, University of Michigan, Ann Arbor, MI, USA
3Department of Electronic Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA
Find articles by Qiaozhu Mei
James Law
Find articles by James Law
Ritu Khanna
Find articles by Ritu Khanna
Kai Zheng
2School of Information, University of Michigan, Ann Arbor, MI, USA
5School of Public Health Department of Health Management and Policy, University of Michigan, Ann Arbor, MI, USA
Find articles by Kai Zheng
1Department of Pediatrics, University of Michigan Medical School, Ann Arbor, MI, USA
2School of Information, University of Michigan, Ann Arbor, MI, USA
3Department of Electronic Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA
4Medical Center Information Technology, University of Michigan Health System, Ann Arbor, MI, USA
5School of Public Health Department of Health Management and Policy, University of Michigan, Ann Arbor, MI, USA
Address correspondence to: David A. Hanauer MD, MS, 5312 CC, SPC 5940, 1500 E Medical Center Drive, Ann Arbor, MI 48109-5940, USA, Phone: +1 (734) 615-0599, Fax: +1 (206) 338-4213, ude.hcimu.dem@reuanah
The publisher's final edited version of this article is available  at J Biomed Inform
See other articles in PMC that cite the published article.
Abstract
Objective
This paper describes the University of Michigan's nine-year experience in developing and using a full-text search engine designed to facilitate information retrieval (IR) from narrative documents stored in electronic health records (EHRs). The system, called the Electronic Medical Record Search Engine (EMERSE), functions similar to Google but is equipped with special functionalities for handling challenges unique to retrieving information from medical text.
Materials and Methods
Key features that distinguish EMERSE from general-purpose search engines are discussed, with an emphasis on functions crucial to (1) improving medical IR performance and (2) assuring search quality and results consistency regardless of users' medical background, stage of training, or level of technical expertise.
Results
Since its initial deployment, EMERSE has been enthusiastically embraced by clinicians, administrators, and clinical and translational researchers. To date, the system has been used in supporting more than 750 research projects yielding 80 peer-reviewed publications. In several evaluation studies, EMERSE demonstrated very high levels of sensitivity and specificity in addition to greatly improved chart review efficiency.
Discussion
Increased availability of electronic data in healthcare does not automatically warrant increased availability of information. The success of EMERSE at our institution illustrates that free-text EHR search engines can be a valuable tool to help practitioners and researchers retrieve information from EHRs more effectively and efficiently, enabling critical tasks such as patient case synthesis and research data abstraction.
Conclusion
EMERSE, available free of charge for academic use, represents a state-of-the-art medical IR tool with proven effectiveness and user acceptance.
Keywords: Electronic Health Records (E05.318.308.940.968.625.500), Search Engine (L01.470.875), Information Storage and Retrieval (L01.470)
Graphical abstract
Open in a separate window
I. Background and Significance
In addition to improving patient care delivery, the widespread adoption of electronic health records (EHRs) in the U.S. has created unprecedented opportunities for increased access to clinical data, enabling multiple secondary use purposes such as quality assurance, population health management, and clinical and translational research. The broader use of clinical data for discovery, surveillance, and improving care provides great potential to transform the U.S. healthcare system into a self-learning vehicle—or a "Learning Health System"—to advance our knowledge in a wide range of clinical and policy domains. 1 , 2
However, the benefits of electronically captured clinical data have yet to be fully realized for a number of reasons. Foremost is the continued popularity of free-text documentation in EHRs. While structured data at the time of entry is desirable, unstructured clinical documentation is likely to persist due to the need by clinicians to express their thoughts in a flexible manner and to preserve the complexity and nuances of each patient. 3 , 4 Recent studies have shown that clinicians often revert to free-text entry even when coding options are provided, 3 , 5 - 7 and that the free text is still needed for complex tasks such as clinical trial recruitment. 8
The challenges to extracting information locked in medical text should not be underestimated. 9 Factors contributing to this complexity include clinicians' frequent use of interchangeable terms, acronyms and abbreviations, 10 as well as negation and hedge phrases. 11 - 13 Ambiguities may also arise due to a lack of standard grammar and punctuation usage 10 and the inherent difficulties for computer systems to process context-sensitive meanings, 14 anaphora and coreferences, 15 and temporal relationships. 16 Even different ways by which clinical notes were created (e.g., via dictation/transcription vs. typing) could result in distinct linguistic properties posing postprocessing challenges. 17 Indeed, a paradox has been noted in the biomedical informatics literature that increased availability of electronic patient notes does not always lead to increased availability of information. 18
Automated data extraction methods, including natural language processing, hold great promise for transforming unstructured clinical notes into a structured, codified, and thus computable format. 19 However, the use of such tools is often associated with considerable upfront costs in software setup and in training the algorithms for optimal performance. Further, despite significant research advancements, the precision and recall of such tools are not yet up to par for meeting the requirements of many sophisticated chart abstraction tasks, and existing tools' lack of generalizability often necessitates customized solutions be built to the specific needs and data characteristics of each problem. 20 - 23
As such, search engines, or information retrieval (IR) systems more generally, offer an effective, versatile, and scalable solution that can augment the value of unstructured clinical data. 24 - 29 Search engines help human reviewers quickly pinpoint where information of interest is located, while leaving some difficult problems that computers are not yet capable of solving to human wisdom. The requirement for end user training is also minimized as healthcare practitioners and researchers are already familiar with how search engines work through their day-to-day interactions with general-purpose web search engines such as Google and literature search tools such as PubMed. This antecedent familiarity is important because in healthcare, clinicians, administrators, and researchers often lack time to attain mastery of informatics tools.
Surprisingly, despite a growing need for IR tools in healthcare settings for both operational and research purposes, very few successful implementations have been reported in the literature. 26 , 27 , 30 In this paper, we describe University of Michigan's (UM) nine-year experience in developing and using a web-based full-text medical search engine designed to facilitate information retrieval from narrative EHR documents. The system, called the Electronic Medical Record Search Engine (EMERSE), has been used by numerous research groups in over 750 clinical and translational studies yielding 80 peer-reviewed publications to date. (e.g., 31 - 35 ). As part of the results validation process several studies explicitly examined the efficacy of EMERSE and concluded that the system was instrumental in ensuring the quality of chart review while significantly reducing manual efforts. 33 , 36 , 37 To the best of our knowledge, this is the first comprehensive description of an EHR search tool that has been used in a production setting by many real end users for multiple years and for a wide variety of clinical, operational, and research tasks. We believe our experience of designing, building, maintaining, and disseminating EMERSE will provide useful insights to the researchers who have a similar need for an HER search engine and will help potential users of such an EHR search engine to understand what types of problems that such tools can help solve.
In the following sections, we first describe the background and architecture of EMERSE, followed by a presentation of various usage metrics and the results of user adoption and effectiveness evaluations. Note that the primary purpose of this paper is to provide a comprehensive description of the design and features of EMERSE, especially those that significantly deviate from traditional IR systems and those that are well received among end users of EMERSE, most of whom are clinicians, healthcare administrators, and clinical researchers. It should also be noted that there is no universally accepted benchmark, or 'gold standard,' with which to measure the performance of a system such as EMERSE that is designed to serve a wide range of purposes in a wide variety of clinical contexts. For example, when trying to identify a subset of potential study subjects among a pool of many possible candidates, precision may be most important. By contrast, when trying to identify all patients affected by a faulty pacemaker then recall may be most important. Therefore, our goal is not to evaluate the IR performance of the EMERSE and compare it to that of other search engines (e.g., PubMed) or algorithms developed for IR competitions (e.g., TREC), but rather to describe EMERSE in the context of the mostly empty landscape of EHR-specific IR tools. Such information is important to developing an enhanced understanding of how to achieve better penetration of IR systems such as EMERSE in everyday healthcare settings.
II. Methods and Materials
A. History and Current Status
EMERSE has been operational since 2005 and has undergone multiple rounds of interface and architectural revisions based on end user feedback, usability testing, and the changing technology environment. The National Center for Advancing Translational Sciences, the National Library of Medicine (NLM), the UM Comprehensive Cancer Center, the Michigan Institute for Clinical and Health Research, and the UM Medical Center Information Technology department provided funding support for the software's continued development and evaluation.
EMERSE was originally designed to work with the University of Michigan Health System's (UMHS) legacy homegrown EHR system, CareWeb, deployed in 1998. In 2012, EMERSE was overhauled so it could integrate data from our newly implemented commercial EHR system, Epic (Epic Systems Corporation, Verona, WI), locally renamed MiChart. During the overhaul, significant efforts were made to ensure EMERSE is as much platform independent and vendor-neutral as possible. At present, users at UMHS can use EMERSE to search through 81.7 million clinical documents: 36.4 million from CareWeb, 10.6 million from Epic (MiChart), 10.4 million radiology reports, 23.2 million narrative pathology reports, and 1.2 million other genres of documents such as electroencephalography and pulmonary function studies.
EMERSE has also been adapted to work with VistA, the Veterans Affairs (VA)'s health IT architecture, and has been used at the VA Hospital in Ann Arbor, Michigan since 2006 to support various VA research initiatives. 38 - 40 This adapted version can be readily adopted by other VA facilities nationwide as they all share the same underlying IT infrastructure. EMERSE is available free of charge for academic use. Note that due to reasons such as local customization, not all features described in this paper are present in all versions of the software. Additional details, as well as a demonstration version of EMERSE, are available at http://project-emerse.org .
B. User Interface Design and Software Architecture
Figure 1 exhibits the main workspace of EMERSE where users construct search queries and subsequently submit the queries to the backend IR engine for processing. Search terms can be entered rapidly as a Quick Search or they can be activated from Search Term Bundles pre-stored in the system.
Figure 1
EMERSE main workspace. Screen capture of EMERSE showing where search terms can be entered. Search keywords can be quickly typed into the Quick Search text box. Available pre-saved collections of search terms (i.e., search term bundles) are listed further down on the same page. In this example, a search term bundle named "Animal bites" has been selected and the terms belonging to the bundle are shown at the top.
Quick Search
Similar to Google, the most common way of using EMERSE is to type keywords into a simple text entry box. Search terms may contain single words or multi-word phrases (e.g., "sick sinus syndrome"), wild cards (e.g., "hyperten*"), and other operators (e.g., ˆ for case sensitivity). In earlier versions of EMERSE, advanced users could also write sophisticated search queries using regular expressions. This function was dropped during the 2012 overhaul due to lack of use. Searches in EMERSE are case insensitive by default, but an option is provided allowing users to enforce the case-sensitivity, such as for distinguishing "FROM" (full range of motion) from the common word "from." Similarly, stop words are preserved in the document indices because many are legitimate acronyms of medical concepts, e.g., OR: operating room; IS: incentive spirometry; IT: intrathecal.
Exclusion criteria can be entered to instruct the system not to include certain words and phrases in the search. This feature has been utilized particularly in handling negations. For example, the UMHS Department of Ophthalmology developed a "search term bundle" (see below) to look in surgeon notes for perioperative complications ( Appendix A.1 ). The query contains only one search term, "complications," while excluding 51 phrases that unambiguously rule out the possibility of perioperative complications (e.g., "without any complications") or that mentioned complications in an irrelevant context (e.g., "diabetes with neurologic complications").
Search Term Bundles and Collaborative Search
EMERSE provides a special "collaborative search" mechanism that allows users to save their search queries as "search term bundles" which can be reused and shared with others. Examples include a bundle that contains 28 search terms enumerating common ways in which apathy or indifference may be described in clinician notes ( Appendix A.2 ), and another that lists 70 concepts for identifying infections in hematopoietic stem cell transplant patients ( Appendix A.3 ). This collaborative search feature was inspired by social information foraging and crowdsourcing techniques found on the Web that leverage users' collective wisdom to perform collaborative tasks such as IR. The resulting search term bundles not only provide a means for end users to preserve and collectively refine search knowledge, but also to ensure the consistent use of standardized sets of search terms by users. In prior work assessing adoption of this feature, we found that about half of the searches performed in EMERSE had used pre-stored search term bundles, of which one-third utilized search knowledge shared by other users. 41
Handling of Spelling Errors and Logic Validation
Medical terminology contains many difficult-to-spell words (e.g., "ophthalmology") which can be challenging even to seasoned clinicians. It is also not uncommon for misspellings to make their way into official patient records. These can be words incorrectly spelled such as "infectoin" instead of "infection," or words that were spelled correctly (thus eluding detection by a spell checker) but were nevertheless incorrect, such as "prostrate" instead of "prostate."
To address these issues, EMERSE incorporates a medical spelling checker to alert users about potentially misspelled words in their search queries. In addition, EMERSE offers an option for users to include potentially misspelled forms of the search terms in the search. The implementation of these features was based on phonetic matching and sequence comparison algorithms provided in open-source spell check APIs (Jazzy in earlier versions of EMERSE and Apache Lucene in the overhauled version), 42 , 43 and a customized dictionary containing about 6,200 common spelling alternatives that we manually curated from the search logs of EMERSE over the years. The dictionary includes, for example, 24 misspelled forms of the word "diarrhea", e.g. "diarrheae" and "diarheea." Besides spelling mistakes, EMERSE also inspects for common logic errors found in user-submitted search queries, e.g., same keywords appearing on both inclusion and exclusion lists.
Query Recommendation
EMERSE users who are tasked with reviewing medical documents do not necessarily possess adequate clinical knowledge (e.g., they may be student research assistants). Through our observations of users and analyses of search logs, we also discovered that even clinicians with extensive clinical experience might have difficulty creating a set of search terms 'minimally necessary' to ensure reasonably inclusive search results. 44 For example, when looking for "myocardial infarction," users often failed to include common synonyms such as "heart attack," "cardiac arrest," and its acronym "MI."
To improve search quality and reduce user variation, significant effort was made to build a query expansion function to recommend alternative terms that users may consider adding to a query ( Figure 2 ). These alternative terms can be acronyms and synonyms of the keywords searched, as well as generic names of a commercial drug or vice versa. The knowledge base underlying this feature, currently consisting of about 78,000 terms representing approximately 16,000 concepts, was derived from multiple sources including medical dictionaries, drug lexicons, and an empirical synonym set manually curated from the search logs of EMERSE and from the pre-stored search term bundles. The query recommendation feature is available with the 'Quick Search' option as well as with the 'Bundles' option.
Figure 2
Screen capture of EMERSE showing the editing of a search terms bundle. In the example, the term "cefuroxime" was added by the user and the system provided additional suggestions. To aid in recognizing terms in the search results, the Bundles feature will highlight synonyms and related concepts using the same color, which can be overridden by users.
Funded by the NLM, we also developed an experimental extension to EMERSE to leverage the nomenclatures included in the Unified Medical Language System® (UMLS®) Metathesaurus for more comprehensive query expansion. This experimental extension also incorporates MetaMap, NLM's named entity recognition engine, 45 to enable the use of additional text features such as term frequency, inverse document frequency, and document length penalization to improve the relevance of document ranking of the search results that EMERSE returns.
Multilevel Data Views with Visual Cues
EMERSE presents search results through multilevel data views and uses visual cues to help users quickly scan through returned documents. The data view at the highest level, the Overview, provides users a succinct summary of patients (rows) and document sources (columns) where the "hits" of a search are found ( Figure 3 ). Cells in the Overview display a color gradation along with both the number of relevant documents found and the total number of documents for the patient, suggesting the 'intensity' of the hits. Clicking on a cell in the Overview takes users to the Summaries view where documents are presented in reverse chronological order with text snippets revealing the context in which the search terms appear ( Figure 4 ). Clicking on a snippet will then reveal the full document in the Documents view.
Figure 3
The Overview screen, presenting the search results for eight terms related to animal bites and wound cleaning. Each row on the Overview screen represents a patient and each column represents a document source. All patient identifiers have been replaced with realistic substitutes.
Figure 4
The document Summaries view where the rows represent each of the documents belonging to a specific patient. Documents that do not contain any of the search terms are also included for viewing if needed.
There are two important design aspects of EMERSE that deviate from a general-purpose search engine with respect to these views. The first is that documents are grouped and displayed by patient, and among each patient they are grouped and displayed by source system (e.g., pathology, radiology, etc.). The second is that all documents for a patient are retrieved and made available to the user, including those without a hit of the query words. This is visible in the top row of the results shown in Figure 4 . On that row, no visible snippets are displayed, demonstrating that the document does not contain any of the search terms used. Retaining this document in the Documents view is still important in many common scenarios of EHR search because even though such documents do not contain the exact search terms, they may still be relevant to the particular task of the user (e.g., patient screening). .
Matched keywords found in returned documents (or in text snippets displayed in the document summary view) are automatically highlighted using a palette of 18 system-assigned colors to ensure that terms stands out from their surroundings ( Figure 2 ). For search terms included in pre-stored bundles, users have additional controls over the choice of colors; for example, they can override system-assigned colors to allow logical groupings by color of similar medical concepts (e.g., all narcotic medications in green and all stool softeners in orange). This feature makes rapid visual scanning of search results easier, especially when many search terms are involved, The use of distinct colors for different concepts is not commonly supported by general-purpose search engines, but this feature has been highly appreciated by our users.
Patient Lists and Date Range
EMERSE allows users to supply a predefined list of patients to limit the scope of search which can be, for example, a cohort of patients that has passed preliminary trial eligibility screening. Such lists may be prepared ad hoc (e.g., of patients currently staying in an inpatient unit) or systematically identified through claims data, disease-specific patient registries, or research informatics tools such as the i2b2 Workbench. 46 We have developed a plug-in for the i2b2 Workbench so that users can directly transfer patient cohorts identified in i2b2 to EMERSE as predefined patient lists ready for searching. Searches in EMERSE can also be bounded with a date range. This feature is used by the UMHS Infection Control and Epidemiology Department to identify cases of post-operative surgical site infections in one-month blocks.
Security
EMERSE is hosted behind the UMHS firewall in a computing environment certified for storing protected health information. Access to EMERSE is limited to authorized personnel who have patient data access privileges or, among those using it for research, have provided evidence of training in responsible research practices and proof of valid institutional review board approvals, including demonstration of a need to review identifiable patient information. At each login users must complete a brief attestation form to document their intent of use. Audit trail logs are kept for each use session. Searching clinical documents for clinical or operational purposes generally does not warrant removal of identifiers. For research, de-identification would be desirable in certain settings, but is not currently mandated by our health system. Nevertheless, we are exploring de-identification of the entire document corpus to reduce privacy concerns.
Technical Implementation
The technical architecture of the most recent version of EMERSE is illustrated in Figure 5 . The system uses a variety of open-source components including Apache Lucene, Spring, and Hibernate, with a code base written in Java. Free-text documents are indexed nightly using Apache Solr without additional processing. Document repositories are recommended for indexing but are not needed at EMERSE runtime. Further, while our institution utilizes Epic as its EHR, EMERSE was built to be vendor-neutral and contains no dependencies on the EHR itself. Additional implementation details can be found in our online Technical Manual ( http://project-emerse.org/software.html ), including directions on how to configure EMERSE to work with local data sources.
Figure 5
The Technical Architecture of EMERSE. Data are captured from the sources of EHR data (right side) mainly via HL7 interfaces and added to locally-maintained document repositories separate from the EHR. The Indexing services server routinely updates the Lucene-based indices used by the main search application from these databases. Note that while having all documents indexed in both a case-sensitive and case-insensitive index increases storage requirements, this approach allows for faster response time for most search scenarios.
III. Results
The most revealing result in evaluating a practical informatics tool such as EMERSE is perhaps whether people use the system in their everyday work. In this section, we present usage statistics of EMERSE, typical use scenarios, and results of several evaluation studies that assessed the IR performance and end user satisfaction of the system.
A. Usage Statistics and Typical Use Scenarios
As of March 2015, EMERSE has had 1,137 registered users who represent nearly all clinical and operational departments at UMHS covering a majority of medical specialties and professions. These users have collectively logged in more than 138,000 times. EMERSE currently stores 961 search term bundles created by users, each on average containing 32 search terms (median 18; range 1 - 774).
Several UMHS operational departments have incorporated EMERSE into their job routine in data synthesis and reporting. For example, the cancer registrars at our cancer center use EMERSE to perform data abstraction of genetic and biomarker testing results for submission to our tumor registry. Coding teams in the Department of Health Information Management use the system as a computer-assisted coding tool to improve the efficiency in evaluating and managing inpatient billing code assignments as well as for regulatory compliance reviews. EMERSE has also supported the coding team in improving hospital reimbursements by aiding in the identification of supporting information (e.g., proof of pneumonia on admission) for reimbursements that were previously rejected by insurers, and for which they lacked the manpower to perform manual chart reviews. Professionals in the Department of Infection Control and Epidemiology use EMERSE to monitor the incidence of surgical site infections, a process that previously required labor-intensive manual chart auditing. 47
In the clinical setting, EMERSE has enabled providers to answer questions such as "for this patient in clinic today, what medication worked well for her severe migraine headache several years ago," without having to read every note. EMERSE has also been used often in obtaining 'prior authorization' clearance for medications initially denied by insurance companies. With the system, clinic staff can rapidly identify supporting evidence as to when and why cheaper, more 'preferable' medications have failed for a patient.
Researchers at UMHS use EMERSE to perform a variety of chart abstraction tasks ranging from cohort identification and eligibility determination to augmenting phenotypic data for laboratory-based translational research linking biomarkers to patient outcomes. 35 , 48 - 50 The research studies that EMERSE has supported span many topic areas in health sciences. 35 , 51 , 52 Several papers explicitly acknowledged the enabling role that EMERSE played in the studies which allowed the investigators to "systematically identify data within the electronic medical record," 33 and helped to accomplish "a standardized, accurate, and reproducible electronic medical record review." 36 , 37 Table 1 exhibits 14 peer-reviewed publications appearing in 2014 alone that were supported by the use of EMERSE. A full list of the publications supported by EMERSE to date, of which we are aware, is provided in Appendix B .
Table 1
Peer-reviewed publications supported by EMERSE, 2014 only
Publication Title
The way EMERSE was used
Journal
PubMed Identifier
Anticoagulant Complications in Facial Plastic and Reconstructive Surgery
Data abstraction from operative and other notes for wound healing, infection, bleeding, return to the operating room, and other factors.
JAMA Facial Plast Surg
25541679
Proton pump inhibitors and histamine 2 blockers are associated with improved overall survival in patients with head and neck squamous carcinoma.
To identify mentions of medications taken by the patients in the study.
Cancer Prev Res
25468899
Transcutaneous biopsy of adrenocortical carcinoma is rarely helpful in diagnosis, potentially harmful, but does not affect patient outcome
To obtain data about staging, reason for biopsy, number of biopsies, institution performing the biopsy, complications, and other factors.
Eur J Endocrinol
Engraftment syndrome after allogeneic hematopoietic cell transplantation predicts poor outcomes
To identify patients with engraftment syndrome using a set of 82 search terms
Biol Blood Marrow Transplant
Osteochondromas after radiation for pediatric malignancies: a role for expanded counseling for skeletal side effects
To identify patient who developed osteochondromas after hematopoietic stem cell transplant and total body irradiation
J Pediatr Orthop
23965908
Unbiased identification of patients with disorders of sex development
Chart reviews to locate features described in the notes that helped identify patients with disorders of sex development
PLoS One
25268640
Changes in characteristics of hepatitis C patients seen in a liver centre in the United States during the last decade
Medical records review which included treatment status and clinical status at the time of presentation
J Viral Hepat
25311830
Symptomatic subcapsular and perinephric hematoma following ureteroscopic lithotripsy for renal calculi
Chart review of clinical notes to identify patients whose surgery was complicated by a symptomatic hematoma
J Endourol
Comparison of Second-Echelon Treatments for Ménière's Disease
Data abstraction for and identification of patients with Ménière's Disease
JAMA Otolaryngol Head Neck Surg
25057891
A Combined Paging Alert and Web-Based Instrument Alters Clinician Behavior and Shortens Hospital Length of Stay in Acute Pancreatitis
Finding data of interest for chart reviews.
Am J Gastroenterol
24594946
Adjuvant therapies, patient and tumor characteristics associated with survival of adult patients with adrenocortical carcinoma
Chart reviews for a variety of clinical parameters including stage, pathology data, evaluation of hormone secretion, surgical approach, and treatment modalities.
J Clin Endocrinol Metab
Population-based incidence and prevalence of systemic lupus erythematosus: The Michigan Lupus Epidemiology & Surveillance (MILES) Program
Data abstraction for a variety of clinical characteristics for patients with systemic lupus erythematosus.
Arthritis Rheum
24504809
The association between race and gender, treatment attitudes, and antidepressant treatment adherence
Eligibility determination based on chart review by considering factors such as clinically significant depression and having a new recommendation for antidepressant treatment.
Int J Geriatr Psychiatry
23801324
Aneurysms in abdominal organ transplant recipients
To identify patients with mention of an arterial aneurysm in the clinical documentation among patients undergoing liver or kidney transplants over an 11 year period.
J Vasc Surg
Open in a separate window
B. Results of Evaluation of IR Performance
We evaluated the IR performance of EMERSE in the context of conducting several clinical or informatics studies. For example, in a psychiatric trial, 53 we assessed the efficiency of eligibility screening with or without the system and showed that the team assisted by EMERSE achieved significant time savings while maintaining accuracy compared to the team doing manual chart review. In another study aiming to improve the process of data submission to the American College of Surgeons National Surgical Quality Improvement Program (ACS NSQIP), we demonstrated that automated data extraction procedures powered by EMERSE attained high sensitivity (100.0% and 92.8%, respectively) and high specificity (93.0% and 95.9%, respectively) in identifying postoperative complications, which compared favorably with existing ACS NSQIP datasets manually prepared by clinical surgical nurses. 54 Finally, in collaboration with pediatric cardiologists at UMHS, we compared the performance of using EMERSE vs. three other specialty surgical registries to identify a rare tachyarrhythmia associated with congenital heart surgery. EMERSE was found to be the best-performing method, yielding the highest sensitivity (96.9%) and had comparable performance on other evaluation dimensions. 55
C. Results of End User Satisfaction Survey
Periodic satisfaction surveys have been conducted as part of an ongoing effort to collect users' feedback regarding the usefulness and usability of EMERSE. The most recent survey received responses from 297 users (60.5% of all active users at the time). According to the survey, the three largest user groups of EMERSE were clinicians with research responsibilities (22.2%), data analysts/managers (21.5%), and full-time researchers (14.4%). Over a quarter of the respondents were specialized in hematology and oncology (26.7%) followed by pediatrics (17.5%), general medicine (8.7%), and general surgery (8.3%). Among the respondents there were also 17 (5.7%) undergraduate students, 17 (5.7%) non-medical graduate students, and 13 (4.4%) medical students approved to participate in clinical research as student research assistants.
The survey questionnaire solicited end users' options about EMERSE such as to what extent the system facilitates data extraction tasks ("solves my problem or facilitates the tasks I face") and how use of the system compares to manual chart review processes ("has expanded my ability to conduct chart reviews to areas previously impossible with manual review"). The results are shown in Table 2 . Across all items, EMERSE users reported a high level of satisfaction with the system and the vast majority believed the system contributed to improved time efficiency and helped them find data that they "might have otherwise missed or overlooked."
Table 2
End user satisfaction survey results
Does not meet (%)
Published online 2015 Apr 24. doi:  10.1136/bmj.h1885
PMCID: PMC4707569
PMID: 25911572
Development of phenotype algorithms using electronic medical records and incorporating natural language processing
1Division of Rheumatology, Immunology and Allergy, Brigham and Women's Hospital, Boston, MA 02115, USA
2Harvard Medical School, Boston
Find articles by Katherine P Liao
Tianxi Cai
Find articles by Tianxi Cai
Guergana K Savova
Find articles by Guergana K Savova
Shawn N Murphy
Find articles by Shawn N Murphy
Elizabeth W Karlson
1Division of Rheumatology, Immunology and Allergy, Brigham and Women's Hospital, Boston, MA 02115, USA
2Harvard Medical School, Boston
Find articles by Elizabeth W Karlson
Ashwin N Ananthakrishnan
Find articles by Ashwin N Ananthakrishnan
Vivian S Gainer
Find articles by Vivian S Gainer
Stanley Y Shaw
Find articles by Stanley Y Shaw
Zongqi Xia
Find articles by Zongqi Xia
Peter Szolovits
Find articles by Peter Szolovits
Susanne Churchill
Find articles by Susanne Churchill
Isaac Kohane
Find articles by Isaac Kohane
1Division of Rheumatology, Immunology and Allergy, Brigham and Women's Hospital, Boston, MA 02115, USA
2Harvard Medical School, Boston
3Department of Biostatistics, Harvard School of Public Health, Boston
4Department of Pediatrics, Children's Hospital of Boston, Boston
5Department of Neurology, Massachusetts General Hospital, Boston
6Department of Gastroenterology, Massachusetts General Hospital, MGH Crohn's and Colitis Center, Boston
7Partners Research Computing, Partners HealthCare System, Boston
8Center for Systems Biology, Massachusetts General Hospital, Boston
9Department of Neurology, Harvard Medical School, Boston
10Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA
Correspondence to: K P Liao gro.srentrap@oailk
Copyright © BMJ Publishing Group Ltd 2015
This article has been cited by other articles in PMC.
Electronic medical records are emerging as a major source of data for clinical and translational research studies, although phenotypes of interest need to be accurately defined first. This article provides an overview of how to develop a phenotype algorithm from electronic medical records, incorporating modern informatics and biostatistics methods.
The increasing use of electronic medical records (EMR), driven mainly by efforts to improve the quality of patient care, have also launched a discipline of research using EMR data. In the past decade, methods and tools specifically used to conduct EMR research have allowed for sophisticated analyses including pharmacovigilance, 1 genetic association, 2 and pharmacogenetic studies. 3 Phenotype algorithms using EMR data to classify patients with specific diseases and outcomes is a foundation of EMR research. Diagnoses or billing codes are typically used in these algorithms, and are examples of structured EMR data. These data are readily available and searchable (fig 1 ​1), ), but vary in accuracy. Recent work has focused on incorporating other informative EMR data to develop robust phenotype algorithms.
Open in a separate window
Fig 1 Overview of the two main types of EMR data, structured and unstructured, and how these data can be integrated for research studies. In this instance, the figure illustrates the development of a phenotype algorithm for rheumatoid arthritis. *Including ICD-9 (international classification of diseases, 9th revision) codes and CPT (current procedural terminology) codes
Beyond billing and diagnoses codes, advanced EMRs contain a variety of structured data such as electronic prescriptions and laboratory values. A substantial portion of clinical data is also embedded in unstructured data in the form of narrative text notes, either typed or dictated by physicians (fig 1). Extracting accurate information from narrative notes is a well known challenge to clinical researchers and is typically obtained through laborious medical record review. Natural language processing (NLP), 4 a specialty of computer science and informatics, has greatly helped researchers extract clinical data from narrative notes in a high throughput manner. While cutting edge NLP technologies have been successfully applied to internet search engines and automatic speech recognition, they are only now being adapted with new methods for biomedical research.
Overall methods for EMR phenotype algorithms, 5 including NLP algorithms, have been specified elsewhere. 6 7 8 However, the implementation of these algorithms with a team of clinical domain experts, bioinformaticians or NLP experts, biostatisticians, EMR informaticians, and genomics researchers has only been analysed tangentially. The focus on this implementation process by a multidisciplinary team was an objective of the Informatics for Integrating Biology and the Bedside (i2b2) project, with the overarching goal to harness the output of the healthcare system for discovery research. As part of the i2b2 project, we applied one general approach to develop several phenotype algorithms: depression, 9 diabetes mellitus (V Kumar, in preparation), inflammatory bowel disease (ulcerative colitis and Crohn's disease), 10 multiple sclerosis, 11 and rheumatoid arthritis. 12 This method was also successfully applied to EMR data at other institutions. 13 In this article, we present a roadmap of the tools and methods used in our approach to develop EMR phenotype algorithms.
Summary points
Successful application of natural language processing (NLP) into a phenotype algorithm developed from electronic medical records (EMR) requires a multidisciplinary team—clinical investigator, biostatisticians, EMR informaticians, and NLP experts—working in close collaboration
In the Informatics for Integrating Biology and the Bedside study, NLP improved the sensitivity of all algorithms, classifying more patients with high accuracy than algorithms using only structured data
Despite other robust methods to develop EMR phenotype algorithms, the positive predictive value, along with the percentage of patients with the phenotype classified by the algorithm, are the best metrics for evaluating the performance of EMR phenotype algorithms, regardless of the method for development
Toolbox: basic components needed to create EMR phenotype algorithms
The research question
The first step in creating an EMR phenotype algorithm is defining the major research objectives and the ideal study design and population. For example, the initial objective of the rheumatoid arthritis study was to determine the genetic risk factors for the disorder. In genetic studies, a clean phenotype is needed to ensure adequate power to detect risk alleles associated with the disease. Thus, we aimed to develop a classification algorithm for rheumatoid arthritis that would identify a sufficient number of patients with a high positive predictive value (PPV>90%) for the disorder.
Research database of structured EMR data
EMRs were developed primarily for patient care; therefore, their data formats are typically not ideal for research studies. Loading and storing the data in a relational database 14 enables investigators to perform queries to obtain preliminary data. For example, an investigator can query the dataset for the number of patients who received an electronic rofecoxib prescription and subsequently had a new code for myocardial infarction from the ICD-9 (international classification of diseases, 9th revision) within five years. At Partners Healthcare (where the i2b2 project was based), structured data included ICD-9 codes, current procedural terminology codes, electronic prescriptions, and laboratory tests, along with the dates of evaluation.
Natural language processing
NLP 4 is a computational method for processing text to extract information using the rules of linguistics. When notes are processed, NLP breaks down sentences and phrases into words, and assigns each word a part of speech—for example, a noun or adjective. The NLP program then applies the rules of linguistics to interpret the possible meaning of the sentence. In creating EMR phenotypes, we relied on the NLP task that identified so-called concepts in narrative clinical text. A concept is a meaning; for example, the terms "atrial fibrillation(s)" and "auricular fibrillation(s)" are different ways of expressing the same concept. 15
Incorporating data extracted by NLP into a phenotype algorithm has several advantages. First, NLP provides data that are not available in the structured data or where the accuracy of the structured data is low. For example, before 2012, no specific ICD-9 code existed for basal cell carcinoma, a common skin condition. 16
Second, NLP can systematically link several terms to a concept. For example, smoking is an important risk factor for many chronic diseases, but most information on smoking status is in a patient's narrative notes. Determining a patient's smoking status can be challenging because it is described in multiple forms, such as "tobacco," "pack-year," or "cigarettes." 17 18 NLP differs from a "find" command because it can recognize that the terms "tobacco," "pack-year," and "cigarettes" are all related to the concept of smoking. This NLP task is made possible by databases that standardize health terminologies, define the terms, and relate terms to each other and to a concept.
Such databases include the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT), which organizes health terminologies into categories (such as body structure or clinical finding), and RxNorm, which links drug names to other drug names in major pharmacy and drug interaction databases (table 1 ​1). ). In RxNorm, simvastatin is linked to its brand name, Zocor, as well as drugs that form a combination pill with simvastatin (such as sitagliptin/simvastatin (Juvisync), niacin/simvastatin (Simcor), and ezetimibe/simvastatin (Vytorin).
Table 1
Useful web resources for EMR phenotype development*
Resource
Dictionary of all concepts: maps concepts to unique concept identifiers
UMLS
Biomedical terminology systems used by NLP: organizes concepts by categories such as drug class, signs and systems, and diagnoses
RxNorm
NLP open sources systems: parses notes to identify medical terms and maps terms to concepts
cTAKES
Multicenter EMR projects applying NLP to EMR phenotype algorithms in clinical and translational studies
eMERGE
Open in a separate window
*This table lists examples of resources used by the i2b2 team for EMR phenotype development or mentioned in this article; it is not a comprehensive list.
Both SNOMED CT and RxNORM are part of the Unified Medical Language System, 19 20 a resource linking standardized biomedical terms together into a concept. Each concept is assigned a unique concept identifier. For example, "atrial fibrillation(s)" and "auricular fibrillation(s)" are both defined under one unique concept identifier, C0004238. Similarly, all forms of simvastatin are represented by the identifier C0074554. As a result, terms expressed differently in clinical notes can link to one concept and one unique concept identifier.
An investigator studying simvastatin can use NLP (linked to RxNorm and the Unified Medical Language System) to process EMR narrative notes to identify patients on any formulation of simvastatin across different notes, patients, and EMR systems. For example, the variable of simvastatin (unique concept identifier C0074554) would have a value of 1 if there was a mention of simvastatin in the medical notes, or a value of zero if there was no mention. The equivalent task would require a manual search for each individual drug, generic name, and trade name using keywords terms. Although searches for individual terms are feasible for some concepts, if the study involved multiple diseases, drugs, and outcomes, the search terms needed could increase exponentially.
The i2b2 project used two open source, NLP software systems to extract concepts: the Health information Text Extraction system and the Apache clinical Text Analysis and Knowledge Extraction System 21 (table 1).
Methods used to develop EMR phenotype algorithms
Creation of a sensitive data mart
The PPV, or accuracy, of an algorithm depends on the prevalence of the disease. The relationship between PPV and prevalence is shown in the following formula:
PPV=(sensitivity×prevalence)/[sensitivity×prevalence+(1−specificity)×(1−prevalence)]
From large, population based epidemiological studies, the prevalence of four of the i2b2 study phenotypes, Crohn's disease, multiple sclerosis, rheumatoid arthritis, and ulcerative colitis, was 1% or less in the general population in the United States. 22 23 24 Developing an algorithm for these phenotypes using all patients in the EMR would substantially limit the PPV, owing to the phenotypes' low expected prevalence in the EMR population. Therefore, as a first development step for all algorithms, we applied a screen selecting for patients with any data suggestive for the phenotype and excluded those with no evidence of the phenotype (fig 2 ​2). ). Patients with any data suggesting that they had the phenotype were included in a highly sensitive data mart. Clinical domain experts—who were the team physician scientists in the i2b2 study—determined the components of the screen. For example, a multiple sclerosis screen would include any patient with an ICD-9 code for "multiple sclerosis," "encephalitis, myelitis, and encephalomyelitis," and "other demyelinating disease of the central nervous system." 11
Fig 2 Overview of methods used to develop EMR phenotype algorithms
Algorithm variables
For each phenotype algorithm, the clinical domain experts created a comprehensive list of potential variables and terms (known as a customized dictionary). For rheumatoid arthritis, this list included "rheumatoid arthritis," "bone erosions," "synovitis," "rheumatoid factor positivity," and first line treatments such as methotrexate. The list was converted to available structured data including ICD-9 and current procedural terminology codes, electronic prescriptions, and laboratory tests. We also identified potential negative predictors, such as phenotypes with similar clinical presentations: for example, a negative predictor for ulcerative colitis was Crohn's disease and vice versa. 10 The clinical domain experts and NLP experts then mapped the list of terms to the concepts and unique concept identifiers using the Unified Medical Language System. In the i2b2 studies, we used NLP to process all clinical text notes including progress notes, discharge summaries, radiology reports, and pathology reports. NLP transformed the narrative data into data that could be readily analyzed—such as the number of times "bone erosions" was mentioned in the radiology reports. The final dataset for analysis included structured data (patient study identifier, number of ICD-9 codes for rheumatoid arthritis, and number of electronic prescriptions for methotrexate), alongside the narrative data (such as smoking status (yes v no) and number of mentions of "bone erosions"; fig 1).
The accuracy of each variable to define the phenotype was not as important as how the variables together in the algorithm could predict the phenotype. For all phenotypes, both the ICD-9 code and NLP concept for the phenotype were among the top five most predictive variables in the algorithm, despite low accuracy (PPV=20% 12 ) for some ICD-9 codes. Thus, although we reviewed sentences labeled by NLP as containing a concept to assess whether the correct concept was identified, we did not systematically validate each potential variable for the algorithm. For example, we reviewed 100 sentences labeled by NLP as discussing cardiac catheterization to ensure that the sentence was not instead describing other forms of catheterization, such as urinary catheterization. After creating a comprehensive list of candidate variables, we relied on these data to inform which variables were most predictive for the phenotype.
Training set
We created a training set by selecting patients from the data mart at random (fig 2). The size of the training set was determined by the number of candidate variables and the prevalence of the phenotype. A phenotype with a substantial number of candidate variables and lower prevalence would need a large training set to achieve a robust classification algorithm. The clinical domain experts reviewed medical records of all patients in the training set and classified each patient as having the phenotype or not based on expert opinion and, when available, validated classification criteria from their respective clinical societies (for example, the American College of Rheumatology and the European League Against Rheumatism Classification Criteria for rheumatoid arthritis 25 ).
Developing the classification algorithm
We identified the predictive variables for the algorithm and their weights using the adaptive LASSO penalized logistic regression 26 method. The final classification algorithm (fig 2) was a logistic regression model, which assigned each patient a probability of having the phenotype based on their values for each variable. A hypothetical classification algorithm for phenotype A (PA) would be as follows:
Logit (probability of PA) = intercept − 0.16(sex) + 0.73 log(1+(NLP PA)) + 0.88 log(1+(ICD-9 PA)) + 0.63(NLP treatment) + ...
In the algorithm above, the input from the phenotype A data mart included a patient's sex (1=female, 0=male), number of mentions of the NLP concept phenotype A (NLP PA) from the narrative notes, number of ICD-9 codes for phenotype A (ICD-9 PA), and whether a treatment for phenotype A was mentioned in any of the narrative notes (NLP treatment; 1=yes, 0=no). The end result of applying the algorithm to the phenotype A data mart was a calculated probability for phenotype A ranging between 0 and 1.0 for each patient.
Patients were classified as having phenotype A or not if their probability was above or below a threshold level, respectively. Unlike a Boolean approach (such as ≥1 ICD-9 code + treatment), this type of algorithm allows the investigator to adjust the threshold based on the scientific question. For the genetic study in rheumatoid arthritis, we found that a specificity of 95% (probability threshold ≥0.53) provided more power to detect an association with potential risk alleles (odds ratio 1.2) than a specificity of 97% (probability threshold ≥0.71). 27 The improved power of the algorithm using a lower specificity threshold was driven largely by the classification of additional patients (from n=3585 to n=4575), with similarly high accuracy. Investigators interested in a pharmacovigilance study could consider setting a lower specificity threshold at 90% to capture additional patients.
Validation
We created a validation set comprising all patients classified with the phenotype, mixed with an additional 50% of random patients from the data mart. The clinical domain experts reviewed the records of all patients in the validation set using the same criteria to define the phenotype in the training set. Reviewers were blinded to the algorithm classification results. The performance of the algorithm was estimated using the validation set.
Other considerations
Use of NLP in phenotype classification algorithms
Incorporation of NLP improved the performance of all the algorithms studied in the i2b2 project. This improvement can be illustrated by the validation results for the algorithms for Crohn's disease, multiple sclerosis, rheumatoid arthritis, and ulcerative colitis. For each phenotype, we compared the performance of a structured and NLP data algorithm with algorithms using only structured data or only data derived using NLP (table 2 ​2). ). For Crohn's disease, multiple sclerosis, and ulcerative colitis, we achieved high accuracy (PPV≥94%) algorithms using structured data alone. NLP improved all algorithms using structured data by increasing the sensitivity while either maintaining or improving the accuracy, because NLP added independent predictive variables to the algorithm. For Crohn's disease, the top two predictors for the phenotype were the number of ICD-9 codes followed by the number of NLP mentions. Therefore, although the structured and NLP variables described the same concept, the information they provided was not the same and were independently predictive of Crohn's disease.
Table 2
Comparison of performance algorithms using different types of data to classify phenotypes*
Phenotype and performance characteristic
Open in a separate window
*Specificity cut-off for all phenotypes was set at 97%.
In absolute numbers, the addition of NLP increased the size of the ulcerative colitis cohort from 4183 to 5522 patients (fig 3 ​3) ) and improved the power for subsequent association studies. 28 29 The addition of NLP to structured data in rheumatoid arthritis substantially improved the accuracy of the algorithm from 88% to 94% and increased the number of patients classified with the disorder from 3046 to 3585. But why did the addition of NLP data not improve accuracy in the algorithms for Crohn's disease, multiple sclerosis, and ulcerative colitis? The accuracy of the structured data might explain some of this difference. Among a ulcerative colitis training set of 600 patients with at least one ICD-9 code for the disorder, 378 (PPV=64%) were confirmed to have the disorder. 10 Among a rheumatoid arthritis training set of 500 patients, only 96 (PPV=19%) were confirmed to have the disorder, 12 limiting the accuracy of an algorithm with structured data. In our experience, NLP had a greater impact on improving algorithms for phenotypes with a low prevalence and low accuracy for the phenotype ICD-9 code.
Open in a separate window
Fig 3 Proportion of patients in data marts for rheumatoid arthritis (n=28 982) and ulcerative colitis (n=14 335) who have been classified to have the phenotype. Numbers over each bar=EMR cohort size
The main limitation of using NLP in an EMR phenotype algorithm was the time and resources needed to identify and extract the variables for the algorithms. Such resources would be affected by the number of notes, number of variables in the customized dictionary, and NLP systems used for processing. For projects with many potential algorithm variables, mapping the clinical terms to NLP concepts was rate limiting, even with the use of available tools that assist with mapping. Several groups are now developing tools to accelerate the tuning process, in which terms are mapped accurately to NLP concepts. This process requires first tuning these tools on larger texts of medical knowledge, or using an automated tuning process that adapts or learns from mapping corrections made by the clinical domain experts. 21 30 31 32 Finally, we note that the general phenotyping method presented in this article were successfully applied to a range of defined diseases and conditions but has not been extensively tested on outcomes such as drug response or adverse events.
EMR platform for clinical and translational studies
A unique aspect of EMR based cohorts is the ability to link clinical data with a biorepository, integrating clinical and genomic data in an EMR research platform for translational studies. 33 With the appropriate infrastructure, EMR phenotype cohorts can be assembled in a relatively short period of time (12-18 months) compared with the years needed to recruit patients for prospective cohort studies, particularly for uncommon diseases. An EMR research platform containing linked data provides opportunities to conduct both hypothesis testing and generating studies. Hypothesis testing includes traditional, clinical, and genetic association studies. In a method unique to EMR research, the Phenome Wide Association Study allows for hypothesis generating studies and can be used as a screen to test for the association between genes or biomarkers and all phenotypes in the EMR. 34 Moreover, the ability to apply EMR phenotype algorithms across institutions allows for the use of one phenotype for collaborative multicenter studies highlighted by the Electronic Medical Records and Genomics network 35 as well as projects from our group. 13
FAQs for clinical investigators
Can any institution with an EMR develop phenotype algorithms?
Any institution with an advanced EMR database—that includes data such as billing codes, electronic prescriptions, laboratory values, and narrative text notes—has the potential to develop phenotype algorithms. However, tapping into these data requires programmers with expertise in transforming the data into a useable format for research, such as a relational database structure. Such data reformatting requires an infrastructure that can support a research copy of the EMR, secure servers, terabytes of hardware space, and programmers who can manage and extract the data.
How would an investigator assemble a team to develop an EMR phenotype algorithm?
Despite many advances in the development of tools to mine EMR data commodity (for example, NLP software for clinical researchers), carrying out these studies presently requires a specialized team. The core team members include a biostatistician, clinical researcher, EMR informatician, and NLP expert. With the growth of the NLP field and its applications to biomedical research, most large academic medical centers have NLP experts on staff. An often missed but essential member of the team is the EMR informatician, who can understand the particularities of healthcare system data, such as differences in the way diagnostic results are reported by various clinics and where the data are stored.
In the i2b2 project, team meetings with all members present (especially at the start of the project) were the most effective way to work through multidisciplinary questions and discuss key concepts from our respective specialties. For example, a simple request can take a few steps, such as extracting data for white blood cell counts. Although the EMR informaticians at our institution know where to obtain the data, they would need to know from the clinical investigators which of the 46 types of laboratory data pertaining to white blood cell counts, grouped in two ways, were the correct fields to extract from the database. In another example, our NLP team presented a smoking module and used what they considered "precision" and "recall" to describe the performance of the algorithms. After some discussion, the clinical investigators and biostatisticians learned that NLP "precision" and "recall" is the same as PPV and sensitivity, respectively.
Notes
We thank the i2b2 team members integral to the development of our EMR algorithms, including Andrew Cagan, programmer, Research Computing, Partners Healthcare; Su-Chun Cheng, senior research scientist, Harvard School of Public Health; Sergey Goryachev, software developer, Ariadne Labs; Vishesh Kumar, postdoctoral fellow, Massachusetts General Hospital; and Robert Plenge, vice president, Merck Laboratories (Boston, MA, USA).
Contributors: All authors participated in the conception and design of the article, worked on the drafting of the article and revised it critically for important intellectual content, and approved the final version to be published.
Funding: This study was funded by grants from the US National Institutes of Health (U54LM008748, AR 060257, K08 K23 DK097142), and the Harold and Duval Bowen Fund.
Competing interests: All authors have completed the ICMJE uniform disclosure form at  www.icmje.org/coi_disclosure.pdf (available on request from the corresponding author) and declare: support from US National Institutes of Health and the Harold and Duval Bowen Fund for the submitted work; KPL is supported by the National Institutes of Health and the Harold and Duval Bowen Fund; GKS is on the Advisory Board of Wired Informatics, which provides services and products for clinical NLP applications; no other relationships or activities that could appear to have influenced the submitted work.
Provenance and peer review: Not commissioned; externally peer reviewed.
Notes
Cite this as: BMJ 2015;350:h1885
References
1. Brownstein JS, Murphy SN, Goldfine AB, Grant RW, Sordo M, Gainer V, et al. Rapid identification of myocardial infarction risk associated with diabetes medications using electronic medical records. Diabetes Care 2010;33:526-31. [ PMC free article ] [ PubMed ]
2. Liao KP, Diogo D, Cui J, Cai T, Okada Y, Gainer VS, et al. Association between low density lipoprotein and rheumatoid arthritis genetic factors with low density lipoprotein levels in rheumatoid arthritis and non-rheumatoid arthritis controls. Ann Rheum Dis 2013;73:1170-5. [ PMC free article ] [ PubMed ]
3. Ramirez AH, Shi Y, Schildcrout JS, Delaney JT, Xu H, Oetjens MT, et al. Predicting warfarin dosage in European-Americans and African-Americans using DNA samples linked to an electronic health record. Pharmacogenomics 2012;13:407-18. [ PMC free article ] [ PubMed ]
4. Jurafsky D, Martin JH. Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition. 2nd ed. Pearson Prentice Hall, 2009.
5. Rasmussen LV, Thompson WK, Pacheco JA, Kho AN, Carrell DS, Pathak J, et al. Design patterns for the development of electronic health record-driven phenotype extraction algorithms. J Biomed Inform 2014;51:280-6. [ PMC free article ] [ PubMed ]
6. Carrell DS, Halgrim S, Tran DT, Buist DS, Chubak J, Chapman WW, et al. Using natural language processing to improve efficiency of manual chart abstraction in research: the case of breast cancer recurrence. Am J Epidemiol 2013;179:749-58. [ PMC free article ] [ PubMed ]
7. Murff HJ, FitzHenry F, Matheny ME, Gentry N, Kotter KL, Crimin K, et al. Automated identification of postoperative complications within an electronic medical record using natural language processing. JAMA 2011;306:848-55. [ PubMed ]
8. Zeng QT, Goryachev S, Weiss S, Sordo M, Murphy SN, Lazarus R. Extracting principal diagnosis, co-morbidity and smoking status for asthma research: evaluation of a natural language processing system. BMC Med Inform Decis Mak 2006;6:30. [ PMC free article ] [ PubMed ]
9. Perlis RH, Iosifescu DV, Castro VM, Murphy SN, Gainer VS, Minnier J, et al. Using electronic medical records to enable large-scale studies in psychiatry: treatment resistant depression as a model. Psychol Med 2011;42:41-50. [ PMC free article ] [ PubMed ]
10. Ananthakrishnan AN, Cai T, Savova G, Cheng SC, Chen P, Perez RG, et al. Improving case definition of Crohn's disease and ulcerative colitis in electronic medical records using natural language processing: a novel informatics approach. Inflamm Bowel Dis 2013;19:1411-20. [ PMC free article ] [ PubMed ]
11. Xia Z, Secor E, Chibnik LB, Bove RM, Cheng S, Chitnis T, et al. Modeling disease severity in multiple sclerosis using electronic health records. PLoS One 2013;8:e78927. [ PMC free article ] [ PubMed ]
12. Liao KP, Cai T, Gainer V, Goryachev S, Zeng-treitler Q, Raychaudhuri S, et al. Electronic medical records for discovery research in rheumatoid arthritis. Arthritis Care Res (Hoboken) 2010;62:1120-7. [ PMC free article ] [ PubMed ]
13. Carroll RJ, Thompson WK, Eyler AE, Mandelin AM, Cai T, Zink RM, et al. Portability of an algorithm to identify rheumatoid arthritis in electronic health records. J Am Med Inform Assoc 2012;19:e162-9. [ PMC free article ] [ PubMed ]
14. Nalichowski R, Keogh D, Chueh HC, Murphy SN. Calculating the benefits of a research patient data repository. AMIA Annu Symp Proc 2006:1044. [ PMC free article ] [ PubMed ]
15. Pradhan S, Elhadad N, South BR, Martinez D, Christensen L, Vogel A, et al. Evaluating the state of the art in disorder recognition and normalization of the clinical narrative. J Am Med Inform Assoc 2015;22:143-54. [ PMC free article ] [ PubMed ]
16. Endicott M. New ICD-9-CM diagnosis codes for FY 2012. J AHIMA 2012;82:60-2;quiz 63. [ PubMed ]
17. McCormick PJ, Elhadad N, Stetson PD. Use of semantic features to classify patient smoking status. AMIA Annu Symp Proc 2008:450-4. [ PMC free article ] [ PubMed ]
18. Savova GK, Ogren PV, Duffy PH, Buntrock JD, Chute CG. Mayo clinic NLP system for patient smoking status identification. J Am Med Inform Assoc 2008;15:25-8. [ PMC free article ] [ PubMed ]
19. US National Library of Medicine. Unified medical language system terminology services. 2014. https://uts.nlm.nih.gov/home.html .
20. Bodenreider O, McCray AT. Exploring semantic groups through visual approaches. J Biomed Inform 2003;36:414-32. [ PMC free article ] [ PubMed ]
21. Savova GK, Masanz JJ, Ogren PV, Zheng J, Sohn S, Kipper-Schuler KC, et al. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. J Am Med Inform Assoc 2010;17:507-13. [ PMC free article ] [ PubMed ]
22. Liao KP, Karlson EW. Classification and epidemiology of rheumatoid arthritis. In: Hochberg MC, Silman AJ, Smolen JS, Weinblatt ME, Weisman MH, eds. Rheumatology. 5th ed. Mosby Elsevier, 2011:823-4.
23. Kappelman MD, Rifas-Shiman SL, Kleinman K, Ollendorf D, Bousvaros A, Grand RJ, et al. The prevalence and geographic distribution of Crohn's disease and ulcerative colitis in the United States. Clin Gastroenterol Hepatol 2007;5:1424-9. [ PubMed ]
24. Simpson S Jr, Blizzard L, Otahal P, Van der Mei I, Taylor B. Latitude is significantly associated with the prevalence of multiple sclerosis: a meta-analysis. J Neurol Neurosurg Psychiatry 2011;82:1132-41. [ PubMed ]
25. Aletaha D, Neogi T, Silman AJ, Funovits J, Felson DT, Bingham CO 3rd, et al. 2010 Rheumatoid arthritis classification criteria: an American College of Rheumatology/European League Against Rheumatism collaborative initiative. Arthritis Rheum 2010;62:2569-81. [ PubMed ]
26. Zou H. The adaptive Lasso and its oracle properties. J Am Stat Assoc 2006;101:1418-29.
27. Kurreeman F, Liao K, Chibnik L, Hickey B, Stahl E, Gainer V, et al. Genetic basis of autoantibody positive and negative rheumatoid arthritis risk in a multi-ethnic cohort derived from electronic health records. Am J Hum Genet 2011;88:57-69. [ PMC free article ] [ PubMed ]
28. Ananthakrishnan AN, Cagan A, Gainer VS, Cai T, Cheng SC, Savova G, et al. Normalization of plasma 25-hydroxy vitamin D is associated with reduced risk of surgery in Crohn's disease. Inflamm Bowel Dis 2013;19:1921-7. [ PMC free article ] [ PubMed ]
29. Ananthakrishnan AN, Cagan A, Gainer VS, Cheng SC, Cai T, Scoville E, et al. Thromboprophylaxis is associated with reduced post-hospitalization venous thromboembolic events in patients with inflammatory bowel diseases. Clin Gastroenterol Hepatol 2014;12:1905-10. [ PMC free article ] [ PubMed ]
30. Aronson AR, Lang FM. An overview of MetaMap: historical perspective and recent advances. J Am Med Inform Assoc 2010;17:229-36. [ PMC free article ] [ PubMed ]
31. Garla VN, Brandt C. Knowledge-based biomedical word sense disambiguation: an evaluation and application to clinical document classification. J Am Med Inform Assoc 2013;20:882-6. [ PMC free article ] [ PubMed ]
32. Leaman R, Islamaj Dogan R, Lu Z. DNorm: disease name normalization with pairwise learning to rank. Bioinformatics 2013;29:2909-17. [ PMC free article ] [ PubMed ]
33. Murphy S, Churchill S, Bry L, Chueh H, Weiss S, Lazarus R, et al. Instrumenting the health care enterprise for discovery research in the genomic era. Genome Res 2009;19:1675-81. [ PMC free article ] [ PubMed ]
34. Denny JC, Ritchie MD, Basford MA, Pulley JM, Bastarache L, Brown-Gentry K, et al. PheWAS: demonstrating the feasibility of a phenome-wide scan to discover gene-disease associations. Bioinformatics 2010;26:1205-10. [ PMC free article ] [ PubMed ]
35. Kho AN, Pacheco JA, Peissig PL, Rasmussen L, Newton KM, Weston N, et al. Electronic medical records for genetic research: results of the eMERGE consortium. Sci Transl Med 2011;3:79re1. [ PMC free article ] [ PubMed ]
Articles from The BMJ are provided here courtesy of BMJ Publishing Group
Formats:
EGEMS (Wash DC). 2016; 4(2): 1206.
Published online 2016 Mar 29. doi:  10.13063/2327-9214.1206
PMCID: PMC4827782
PMID: 27141519
Extracting Electronic Health Record Data in a Practice-Based Research Network: Processes to Support Translational Research across Diverse Practice Organizations
Find articles by Laura-Mae Baldwin
iUniversity of Washington, Institute of Translational Health Sciences
All eGEMs publications are licensed under a Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 License http://creativecommons.org/licenses/by-nc-nd/3.0/
Abstract
Context:
The widespread adoption of electronic health records (EHRs) offers significant opportunities to conduct research with clinical data from patients outside traditional academic research settings. Because EHRs are designed primarily for clinical care and billing, significant challenges are inherent in the use of EHR data for clinical and translational research. Efficient processes are needed for translational researchers to overcome these challenges. The Data QUEST Coordinating Center (DQCC), which oversees Data Query Extraction Standardization Translation (Data QUEST) – a primary-care, EHR data-sharing infrastructure – created processes that guide EHR data extraction for clinical and translational research across these diverse practices. We describe these processes and their application in a case example.
Case Description:
The DQCC process for developing EHR data extractions not only supports researchers' access to EHR data, but supports this access for the purpose of answering scientific questions. This process requires complex coordination across multiple domains, including the following: (1) understanding the context of EHR data; (2) creating and maintaining a governance structure to support exchange of EHR data; and (3) defining data parameters that are used in order to extract data from the EHR. We use the Northwest-Alaska Pharmacogenomics Research Network (NWA-PGRN) as a case example that focuses on pharmacogenomic discovery and clinical applications to describe the DQCC process. The NWA-PGRN collaborates with Data QUEST to explore ways to leverage primary-care EHR data to support pharmacogenomics research.
Findings:
Preliminary analysis on the case example shows that initial decisions about how researchers define the study population can influence study outcomes.
Major Themes and Conclusions:
The experience of the DQCC demonstrates that coordinating centers provide expertise in helping researchers understand the context of EHR data, create and maintain governance structures, and guide the definition of parameters for data extractions. This expertise is critical to supporting research with EHR data. Replication of these strategies through coordinating centers may lead to more efficient translational research. Investigators must also consider the impact of initial decisions in defining study groups that may potentially affect outcomes.
Keywords: electronic health records, primary care, governance
Context
The widespread adoption of electronic health records (EHRs) offers significant opportunities to conduct research that addresses critical problems in clinical medicine. 5 , 6 In contrast to claims-based data, which may capture only demographics, diagnoses, and procedures recorded for billing purposes, EHR systems provide a broader range of clinical data. These data include vital signs, diagnostic test results, social and family histories, prescriptions, and physical examination findings. 7 EHRs are also a critical and effective source of data for studying small populations with rare conditions. 8 For example, rural residing populations are less likely to participate in clinical research. 9 Leveraging EHR data systems, which are widely adopted even in rural-serving primary care clinics, offers an important resource to address the paucity of rural subjects in clinical research studies. 10 However, compared to large urban practices, rural-serving primary care practices, which are often independently owned and operated, may have fewer resources to support participation in EHR-based research.
EHRs are designed primarily for clinical care and billing. This leads to several potential problems inherent in the use of EHR data for clinical research. 11 , 12 William Hersh and colleagues suggested that researchers consider five caveats related to data origin, or provenance, when planning to use EHR data for research: (1) Many EHR data have been transformed or coded for billing purposes; (2) Data entered in free text format may not be readily captured in easy-to-use formats; (3) EHRs may present multiple sources of data for a given measure or indicator of interest, resulting in potential inconsistencies in the data; (4) EHR data may not provide complete data; and (5) EHR data may contain inaccurate or incorrect data. 3 Understanding the origin of information from EHR data is critical for accurately analyzing and interpreting EHR data for research. 3 Researchers need efficient and feasible means of gathering this information.
There are several steps researchers may take to address problems encountered in the use of EHR data for research. When undertaking research with EHR data, researchers must consider both the technical specifications for creating data extractions and the impact of initial decisions that are needed to prepare EHR data for analysis. The DQCC team has developed an innovative three-step process to support development of EHR data extractions that includes helping investigators understand the context of EHR data, creating and maintaining data governance structures, and defining data parameters. The objectives of this paper are the following: (1) describe the process that the DQCC team uses; and (2) explore the impact of initial decisions about the study population that researchers make in preparing EHR data extractions for analysis. We suggest that the process that the DQCC has created is usable in other data sharing networks and will facilitate translational research with EHR data. We illustrate issues that arise when creating research data extractions and definitions of a patient population through a case example from pharmacogenomics research conducted in concert with the Northwest-Alaska Pharmacogenomics Research Network (NWA-PGRN).
Data Source
Data Query Extraction Standardization Translation (Data QUEST) is an infrastructure for sharing EHR data for research. Data QUEST was developed after an extensive needs and feasibility assessment with rural-serving primary care practices in the Washington, Wyoming, Alaska, Montana and Idaho (WWAMI) region Practice and Research Network (WPRN), The WPRN is a collaborative group of primary care practices across the five-state WWAMI region committed to research and quality improvement. 16 The WPRN practices that partner with Data QUEST recognized data accessibility as a powerful way to facilitate research participation. Practices were also interested in supporting the including of rural populations in translational research. Our initial assessment identified local control of data management as a critical governance issue in developing the data sharing infrastructure. 16 Our team created governance strategies to support data sharing, such as allowing partner organizations to store EHR data locally, requiring project-specific data queries to be approved by participating sites, and allowing partner organizations to physically terminate the links to the data sharing infrastructure at any time. 13
Data QUEST is implemented in over a dozen diverse primary-care practices serving primarily rural populations in the WPRN. 14 , 15 , 16 Data QUEST ( Figure 1 ) includes patient-level data stored securely within local practices' firewalls and uses a federated data-sharing infrastructure to support regulation-compliant governance of data between primary care partners and researchers. 16 Data QUEST includes a centralized data repository, hosted at the University of Washington (University of WA). Other existing distributed network solutions would have required larger digital infrastructures that are not feasible in small and rural-serving primary care practices. Data QUEST's infrastructure and governance maintain compliance and are appropriate with the Health Insurance Portability and Accountability Act (HIPAA) and the Institutional Review Board (IRB). Data QUEST targets extraction from main data domains in the EHRs, including demographics, vital signs, diagnosis codes, diagnostic test results, social and family history, prescriptions, and physical examination findings.
Figure 1.
Structure of Data QUEST Data Flow and Management
While Data QUEST's federated data sharing system provides participating organizations maximal local control of data, it may hinder the efficient use of data for research. 17 Reports indicate that many data sharing infrastructures include some elements of local data control. 18 Development of tools and processes to support data sharing can address some of these potential inefficiencies. The Scalable Architecture for Federated Translational Inquiries Network (SAFTINet) system addressed these challenges with the implementation of Master Consortium Agreements and Service Level Objective agreements, both of which are similar to processes used in the Data QUEST network. 19 As trust and governance solidified, Data QUEST has expanded to include a central, de-identified data repository, improving speed and efficiency for cohort discovery.
Data QUEST Coordinating Center
The Data QUEST Coordinating Center (DQCC) is a team of researchers and scientists that provides consultation and support to those interested in working with Data QUEST. DQCC team member roles and responsibilities are described in Table 1 . Members of the DQCC provide expertise in technical aspects of data sharing, developing and maintaining governance structures, and creating and sharing tools for efficient research. Consultations use a collaborative process to assess the feasibility of projects, ensure proper governance and regulatory compliance (e.g., HIPAA, IRB), create data extraction specifications, coordinate delivery of EHR data extractions, and advise on analyses.
Table 1.
Data QUEST Coordinating Center Roles and Responsibilities
ROLE
Develop and support maintenance of Data QUEST's technicalarchitecture
Develop and implement governance structure, including Memoranda of Understanding and Data Use Agreements
Consult with academic investigators interested in working with Data QUEST
Practice-based research network clinical research experts
Develop and support relationships with primary care practices that contribute EHR data to Data QUEST
Develop and implement governance structure
Provide primary care expertise on definition of parameters for data extractions
Consult with academic investigators interested in working with Data QUEST
Research scientists
Provide guidance on defining parameters for EHR data extractions
Serve as liaison with primary care practices that contribute EHR data to Data QUEST
Support development and implementation of governance structure
Facilitate governance requirements with practices
Work with vendor to obtain data extract for investigators (finances, logistics, troubleshoot data issues)
Consult with academic investigators interested in working with Data QUEST
Program coordinators
Coordinate communication with primary care practices, DQCC and academic investigators
Project management
Open in a separate window
Case Description
Through an iterative process, the DQCC has developed and adapted a process to successfully develop EHR data extractions that researchers can use to answer scientific questions. In this section, we describe the necessary steps and resources the DQCC uses in this process. We also describe a specific case from Data QUEST that includes a collaboration with pharmacogenomics researchers, and highlights the process the DQCC uses to develop EHR data extractions. The DQCC process of developing EHR data extractions, shown in Figure 2 , provides researchers with access to the EHR data they need for answering scientific questions. This process requires coordination across multiple domains: (1) understanding the context of EHR data, (2) creating and maintaining a governance structure to support exchange of EHR data, and (3) defining data parameters that will be used to extract data from the EHR. 1 , 2 , 3 , 4 , 20 We believe that this process can be implemented in similar practice-based research networks interested in building EHR data sharing capacity, and may be useful for supporting translational research with EHR data.
Process that the Data QUEST Coordinating Center Uses to Create EHR Data Extractions
1.  Understanding the Context of EHR Data
How EHR information is collected and entered is influenced by clinical workflow and variations in the processes of care. 20 , 21 Knowledge of data provenance, the original context under which EHR information is collected, helps researchers understand advantages and limitations of EHR data and explore how the data can be used to answer a scientific question. The DQCC has worked closely with partner practices to understand the flow of data in their EHR systems. This knowledge, which grows with time and experience, is critical in guiding the design of scientific studies using EHR data. 22 The DQCC helps researchers understand the context of EHR data through an initial consultation between the researcher and the DQCC. The DQCC meets with all potential researchers to understand their scientific questions and to provide advice and expertise about the feasibility of conducting studies using Data QUEST data. Additionally, the DQCC assesses whether data is available, complete, and valid. 20 The DQCC also advises on how to formulate research projects that are acceptable and relevant to Data QUEST partner practices. If the DQCC determines that the scientific question is unfeasible using available data, researchers must consider whether the scientific question is modifiable, or whether a different source of data may be more appropriate.
2.  Creating and Maintaining a Governance Structure to Support the Exchange of EHR Data
Data QUEST's success is a direct result of its strong collaborative relationships with primary care practices, and its ongoing engagement with our stakeholders in partner practices. 15 As research projects are proposed within the Data QUEST organizations, the DQCC works closely with partner practices to ensure that the practices understand the research and its procedures, as well as to address issues of data governance and security. Because Data QUEST works with diverse primary care practices, multiple health care systems may be involved in reviewing and approving projects. The DQCC has streamlined the governance process of working across diverse organizations by establishing the following: (1) a Memorandum of Understanding (MoU) that governs roles and responsibilities regarding Data QUEST for the DQCC and partner practices; and (2) Data Use Agreement (DUA) templates that can be easily adapted to specific research projects. To operationalize the research process, the DQCC asks researchers to create a one-page study overview following a specific template that the DQCC shares with partner practices to recruit practices to the study. The study overview describes the study protocol, potential benefits to project participation, and any potential impact on the practice that would result from participation. The DQCC and researchers then work collaboratively to create project specific DUAs. The DUAs outline the purpose of the study, list the necessary data elements, and describe how the data will be used to support this research. Data QUEST partner practices participating in a project must approve the DUA prior to the start of the project. The DQCC emphasizes the importance of compliance with HIPAA regulations and ensures that only data deemed essential to the conduct of the research study are collected. These tools and templates facilitate efficient communication between translational researchers and Data QUEST partner practices. Use of standard tools ensures that Data QUEST partner practices have all the information needed to quickly review and approve studies, and to authorize data extractions.
3.  Defining Data Parameters
The process of defining data parameters that are used to create EHR data extractions is iterative between researchers and the DQCC. During this process, knowing the provenance and context of the EHR data, as well as understanding which data elements are available, is critical to producing EHR data extractions that answer the proposed scientific questions. The DQCC uses a template, completed for each study, that describes the available EHR data as a starting point for outlining and documenting data elements.
The researcher must identify which data elements are required for the project. Potential data elements may include the following: patient demographics (e.g., age, gender, race, ethnicity, ZIP code), patient insurance status, clinic characteristics (e.g., certain clinics within the practice organizations), provider identification (e.g., particular providers or types of providers), encounters (e.g., office visits with clinicians, telephone encounters, other types of encounters), vital signs, medications (e.g., type, dose, refills), International Classification of Diseases (ICD) diagnoses, laboratory results, past medical and surgical history, social history (e.g., tobacco, alcohol, drug use), family history, procedures (e.g., Current Procedural Terminology codes), referrals, and allergies. Researchers may prioritize comprehensiveness, and ask to include the type and maximum number of data elements. However, to ensure compliance with data governance issues, the DQCC requires researchers to limit data elements to only those that are necessary to answer the scientific question. For example, the DQCC encourages researchers to minimize protected health information (e.g., extract the year of birth rather than date of birth when age is needed) whenever possible.
The DQCC also works with the researcher to define patient inclusion and exclusion criteria for the data extract. For example, a researcher might choose to include patients of a certain gender, or those with a clinic visit within a certain period. The DQCC works with the researcher to finalize the data extract parameters, and contracts with a commercial vendor to obtain the EHR data extract.
Once a researcher receives an EHR data extraction, the researcher must make initial decisions to define the study population in preparation for data analysis. To some degree, this requires considerations similar to those described above for defining EHR data parameters; however, these considerations achieve a different purpose. For example, in order to reduce missing data, researchers may wish to exclude patients with infrequent visits. The DQCC provides expert consultation to researchers about defining inclusion and exclusion criteria, such as determining the time frame in which the patient must be seen in the clinic (e.g., ever seen versus seen in a discrete period), frequency of visits, or visits with specific provider types. The DQCC also consults with researchers to understand how data parameters or exclusion criteria may inadvertently select for certain types of patients, potentially biasing the results of analysis.
Case Example
Pharmacogenomics Research Network
The NWA-PGRN is a collaborative research program supported by the National Institutes of General Medical Sciences. The program focuses specifically on pharmacogenomic discovery and application relevant to the health care of Northwest Native American, Alaska Native, and rural populations. 23 , 24 The NWA-PGRN collaborates with Data QUEST to explore how primary care EHR data can be leveraged to support pharmacogenomics research and clinical applications. For example, EHR data facilitates pharmacogenomics research by helping to identify patients with rare or uncommon adverse drug events that may suggest pharmacogenomics markers. 25 , 26 EHR systems also provide infrastructure that can help deploy pharmacogenomic tests, such as clinical decision support tools. 27 , 28 , 29
As researchers within the NWA-PGRN, the DQCC worked with pharmacogenomic scientists who brought content expertise, while we provided expertise in working with EHR data from outpatient clinical settings. We were interested in determining whether primary care clinic EHR data could identify patients who might be candidates for potential pharmacogenomics testing, and focused on statins, a category of medications common in primary care.
Creating EHR Data Extractions to Identify Patients Using Statin Medications for Pharmacogenomics Investigations
The DQCC case example involves a research project that uses EHR data to identify and describe patients using statin medications. Statins are commonly prescribed medications that are effective in treating hyperlipidemia. 30 Statin-induced myopathy is a clinical condition that includes myalgia, myositis, and rhabdomyolysis related to use of statin medications. 31 The incidence of statin myopathy reported in clinical trials ranges from 0.44 to 5.34 per 10,000 years. 32 , 33 Evidence suggests that some patients may be particularly susceptible to statin myopathy because of a genetic variation in a statin uptake protein (SLCO1B1/OATP1B1) or the cytochrome P450 enzyme system. 34 The NWA-PGRN seeks to leverage primary care EHR systems to study patients who have been prescribed a statin medication. This case example is a detailed demonstration of the processes that the DQCC uses when collaborating with researchers to create EHR data extractions. We also include results of preliminary analyses comparing two populations of primary care patients to demonstrate the potential impact of initial decisions researchers make in their studies using EHR data.
1.  Understanding the Context of the EHR Data
First, we explored whether the original scientific question—To what degree can primary care EHR data be used to identify patients who are taking statins and have evidence of statin-induced myopathy?—was feasible to answer with the data elements available in Data QUEST. When considering the study question, we found that a limitation that Data QUEST faces is its lack of specialty care and inpatient data. Severe statin myopathy that requires hospitalization, such as rhabdomyolysis, is unlikely to be found in primary care clinic EHRs. Thus the scientific question evolved to better fit the available data: How many patients in Data QUEST primary care practices are taking a statin medication, and who, based on available EHR data, have evidence of statin-based myopathy?
2.  Governance
The DQCC facilitated communication with partner practices about the project, and compliance with all regulatory and governance issues. We created a one-page summary of the project proposal that was shared with potential partner practices. Once the DQCC developed the data parameters for the EHR extract, we drafted and distributed a DUA to each partner organization that agreed to participate. The DUA outlined the data elements that would be included in the extract, and how these elements would be used for the research. Each partner reviewed and signed the study-specific DUA. The DQCC then worked with a commercial vendor to obtain the EHR data extract.
3.  Defining Data Parameters
We created a list of necessary data parameters for the project. Data elements requested included medications, diagnosis codes, specific laboratory results related to statin complications (e.g., liver, kidney, and thyroid function), family and social history variables such as history of cardiovascular disease and drug use, clinical encounter information (with anonymized provider data), height, weight, blood pressure, and patient demographics (gender, race or ethnicity, insurance, and ZIP code). To maximize patient privacy, only the month and year of birth were requested to calculate age. To limit the extract, we defined patient inclusion and exclusion criteria as adult patients aged 18 and older, since children are unlikely to use statin medications. In addition, we specified that the extract include only those patients who paid at least one visit to the clinic after July 1, 2007.
Findings
In this section, we describe the results from the process we used to define "active" patients, as well as the results from comparing the characteristics of groups of patients based on those definitions. We also describe the process of examining the medication data in the EHR data for different groups of active patients. These results highlight the impact of the initial decisions researchers make in preparing to analyze EHR data.
To illustrate differences between definitions of active patients in the NWA-PGRN study, we created two patient groups across two Data QUEST organizations that provided data from six primary care clinics. The overall group included patients with an office visit during a one-year study observation period, and was made up of two subgroups: Subgroup 1 consisted of patients with an office visit during the one-year study period; and Subgroup 2 consisted of patients with an office visit during the one-year study period, as well as both a visit in the year prior to and the year after the one-year study period. The overall group definition resulted in an additional 6,135 patients for the first organization, and an additional 2,675 patients for the second organization compared with the subgroup required to have three continuous years of visits ( Table 2 ). We compared the characteristics of patients in the two independent subgroups at each organization. For both organizations, Subgroup 2 had a higher average age than did Subgroup 1. In the first organization, the proportion of female patients was higher among those required to have three continuous years of visits (69 percent in Subgroup 2 versus 63 percent in Subgroup 1, p<0.001), and in the second organization the reverse was true (65 percent in Subgroup 1 versus 59 percent in Subgroup 2, p<0.001). In both organizations, the mean number of prescriptions, defined as the number of prescriptions with evidence, was similar (0.1) in the two Subgroups. For both organizations, the proportions of patients with a statin prescription were higher in Subgroup 2 (required to have three years of visits). The decision regarding how to create the groups based on number of patient visits was made at the time of analysis rather than at the time that the original data parameters for creating the EHR data extract were defined. Enforcing these limitations at the time of EHR data extraction would have reduced the number of patients potentially available for analysis, and prevented us from detecting the differences in rates of statin prescriptions between the group definitions prior to undertaking the primary analysis.
Table 2.
Comparison of Selected Characteristics Across Different Patient Groups in Two Primary Care Organizations1
CHARACTERISTICS
Number of patients (all ages)
9,365
Number of prescriptions documented during the 12 month study period, mean (SD)
0.1 (0.4)
Number of patients with a statin prescription documented during the 12-month study period, n(%)
620 (7%)
1Missing values are not included in this table.
2p value based on Chi-square test or t-test comparing Subgroup 1 and Subgroup 2.
3Age is defined as the patient age as of 2011.
4Four patients at Organization 1 (in Subgroup 1) were missing information about gender.
*Subgroup 1 defined as patients with an office visit during the 1-year study period, but not in all three years (before, during and after the 1-year study period). Subgroup 2 defined as patients with an office visit in the year prior, the year of, and the year after the 1-year study period.
Major Themes
The process the DQCC uses to work with researchers to develop EHR data extractions requires an understanding of the context of EHR data, knowledge of defining data parameters, and knowledge of developing and maintaining governance structures. In Table 3 , we summarize the DQCC recommendations when considering the use of EHR data for research. While each of these recommendations outlines specific processes, there are common themes of collaboration and respect for the complexity of this process. Collaboration between academic researchers, community-based practices, and a multidisciplinary coordinating center team to foster successful research partnerships will ensure that researchers can successfully and efficiently leverage EHR data for clinical and translational research.
Table 3.
DQCC Recommendations when Considering use of EHR Data for Clinical Research
CRITICAL COMPONENT
Include researchers and staff with the needed expertise (biomedical informatics, governance, clinical knowledge, project management).
Support collaborative relationships between practices and investigators.
Engage both academic investigators and primary care practices in use of EHR data for research. This requires developing and maintaining bidirectional partnerships.
Respect governance.
Limit data extractions to the minimum of data elements required to answer scientific questions.
Create standardized, project-specific Data Use Agreements (DUAs) to ensure practices understand how EHR data will be used.
Understand and explore downstream consequences of data definitions and analytic steps.
Work collaboratively with data experts to understand how the creation of patient groups could potentially bias findings.
Anticipate the complexity of the process.
Carefully consider the limitations of EHR data and the steps for creating data extractions. Obtaining a data extract and preparing it for analysis likely requires frequent and in-depth consultation with experienced teams.
Open in a separate window
Initial decisions that researchers make in preparing to analyze EHR data extractions have an impact on the study populations. Preliminary analysis of EHR data extractions from the NWA PGRN case found that varied definitions of study populations resulted in differences in patient characteristics (age, gender). Incompleteness of EHR data remains a commonly cited concern when considering use of EHR data for research. 1 In a recent systematic review of studies assessing EHR data quality, incompleteness of EHR data was the most commonly assessed dimension of EHR data quality. 1 Requiring patients to be "active" in a clinic, as measured by requiring a clinical encounter before, during, and after the study observation period, ensures capture of outcomes that might have otherwise been missing or incomplete for patients who left the clinic. However, as evidenced in our findings, there may be important clinical differences between patients that meet these stricter inclusion criteria and those who do not—such as differences in average age and average number of prescriptions received. Researchers must weigh the risk of bias with the advantages of having more complete data. Working with experienced teams such as the DQCC can ensure receipt of comprehensive EHR data extractions that offer flexibility in defining sampling methods as researchers develop appropriate phenotypes for relevant research samples. Future research could explore analytic methods to address potential biases created by the varying approaches to identifying patient groups.
The data available in EHR systems are not always exactly what researchers want or need for analyses. Medication information critical to pharmacogenomics research is available in the primary care based EHRs, but is often limited to the dates when medications were prescribed, making it difficult to measure medication adherence. 35 Despite this limitation, we were able to identify two groups of patients who had active prescriptions for a statin medication, building toward a usable phenotype algorithm for future medication adverse-event studies. Our results demonstrate the capacity of EHR data from Data QUEST to identify patients who have been prescribed a statin medication. This cohort identification process is critical for pursuing further research to study genetic aspects of statin induced myopathy. For example, future research could leverage Data QUEST to identify and contact patients for enrollment in a study to test for genetic markers that may predispose patients to statin induced myopathy. Growing work to link EHR-extracted data with additional types of data, such as claims and patient-reported outcomes, also has the potential to address this shortfall. 36 , 37
EHR data extractions can offer unprecedented opportunities to examine critical outcomes among large numbers of patients in real-world settings. Creating research-ready data sets from EHR data is complicated, however, and needs expert consultation to guide researchers in defining usable and reliable data sets. Leveraging our lessons learned in the DQCC, we identified three key components necessary for creating data extractions from EHR data for clinical research: (1) understanding the context of EHR data, (2) creating and maintaining a strong governance structure to support exchange of EHR data, and (3) defining data parameters for extracting data from the EHR. Critical among these strategies is infusing expert consultation (see Table 1 ) in the process to assist researchers in navigating the complexities inherent in using EHR data for research.
Given the rapidly growing adoption of EHR systems across diverse health care systems, there will be increasing opportunities for translational researchers to conduct research with EHR data. The development of efficient and scalable processes that support collaborative research between translational researchers and health care systems is needed. The DQCC process described here potentially addresses this existing gap. This process promotes efficient translational research with EHR data across diverse, independent practice organizations that do not share an overarching governance structure. The DQCC process is generally applicable to any clinical research network that includes distributed EHR data, regardless of the chosen data model. To our knowledge, the DQCC process has only been used with Data QUEST in the WWAMI region Practice and Research Network (WPRN), and not in other networks. This process provides general guidance for new and existing networks to facilitate data sharing across diverse partners.
We are unaware of a comprehensive report of governance processes or tools in similar networks, but such a report would be helpful to identify shared themes related to governance. These themes could be used to define a generalizable process of governance that could be disseminated more widely. As described by Paolino and colleagues, innovations in data sharing governance, specifically around DUAs and IRB approvals, can increase the efficiency and the flexibility of using EHR data sets for research. 38 Future research by the Data QUEST team to test the effect of the DQCC governance and operational model on translational research process outcomes is needed.
Conclusion
Development and implementation of Data QUEST infrastructure, which supports the sharing of EHR data for research, faces unique challenges in response to the diversity of participating primary care organizations. In this report, we demonstrate the unique capacity of Data QUEST to provide robust EHR data from across diverse, rural-serving primary care organizations, which supports essential clinical and translational research across diverse, rural-serving primary care organizations. Our process for creating EHR data extractions—which includes understanding the context of EHR data, supporting and maintaining governance structures, and defining data parameters—can be used as a guide for other distributed data networks. Collaboration with an experienced team that is familiar with working with EHR data and has existing partnerships with the practices where EHR data are collected is critical to success. Without the knowledge of data provenance and strong partnerships, it is impossible to understand the context of EHR data and to develop and maintain functional governance strategies to work with it. Sustaining efforts to maintain expertise and relationships that support data sharing infrastructures is a challenge on a national scale. Coordinating centers, such as the DQCC, play a critical role in providing this expertise and supporting these relationships. 39 With these strategies and collaborators in place, researchers can successfully use EHR data for research within and across diverse health care organizations.
Acknowledgments
We acknowledge the Northwest Alaska Pharmacogenomics Research Network group for supporting the infrastructure and data collection, and Imara West for her assistance in data cleaning and analysis. This project was funded by the National Institute of General Medical Science (U01 GM092676) and the National Center for Advancing Translational Sciences of the National Institutes of Health (UL1TR000423). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.
Footnotes
Health Information Technology | Primary Care
References
1. Weiskopf NG, Weng C. Methods and dimensions of electronic health record data quality assessment: enabling reuse for clinical research. J Am Med Inform Assoc. 2013 Jan 1;20(1):144–51. [ PMC free article ] [ PubMed ]
2. Jensen PB, Jensen LJ, Brunak S. Mining electronic health records: towards better research applications and clinical care. Nat Rev Genet. 2012 May 2;13(6):395–405. [ PubMed ]
3. Hersh WR, Weiner MG, Embi PJ, Logan JR, Payne PR, Bernstam EV, Lehmann HP, Hripcsak G, Hartzog TH, Cimino JJ, Saltz JH. Caveats for the use of operational electronic health record data in comparative effectiveness research. Med Care. 2013 Aug;51(8 Suppl 3):S30–7. [ PMC free article ] [ PubMed ]
4. Powell J, Buchan I. Electronic health records should support clinical research. J Med Internet Res. 2005 Jan-Mar;7(1):e4. [ PMC free article ] [ PubMed ]
5. Prokosch HU, Ganslandt T. Perspectives for medical informatics. Reusing the electronic medical record for clinical research. Methods Inf Med. 2009;48(1):38–44. [ PubMed ]
6. Hsiao CJ, Hing E. Use and characteristics of electronic health record systems among office-based physician practices: United States, 2001–2013. NCHS Data Brief. 2014;(143):1–8. [ PubMed ]
7. Weiner MG, Lyman JA, Murphy S, Weiner M. Electronic health records: high-quality electronic data for higher-quality clinical research. Inform Prim Care. 2007;15(2):121–7. [ PubMed ]
8. Devers K, Grey B, Ramos C, Shaw A, Blavin F, Waidmann T.  Assistant Secretary for Planning and Evaluation; 2013. Sep, The feasibility of using electronic health records and other electronic health data for research on small populations [Internet]. [place unknown] [cited 2014 July, 27]; Available from: http://aspe.hhs.gov/sp/reports/2013/ElectronicHealthData/rpt_ehealthdata.pdf .
9. Baquet CR, Commiskey P, Mullins CD, Mishra SI. Recruitment and participation in clinical trials: socio-demographic, rural/urban, and health care access predictors. Cancer Detect Prev. 2006;30(1):24–33. [ PMC free article ] [ PubMed ]
10. Mack D, Zhang S, Douglas M, Sow C, Strothers H, Rust G. Disparities in Primary Care EHR Adoption Rates. Journal of Health Care for the Poor and Underserved. 2016;27(1):327–38.
11. Dick RS, Steen EB, Detmer DE. The Computer-Based Patient Record: An Essential Technology for Health Care [Internet] Washington D.C: National Academy Press; 1997.  [cited 2015 June 26]; Available from: http://www.nap.edu/openbook.php?record_id=5306 .
12. Crossing the Quality Chasm [Internet] Washington D.C: National Academy Press; 2001.  [cited 2015 June 26]; Available from: https://www.iom.edu/~/media/Files/Report%20Files/2001/Crossing-the-Quality-Chasm/Quality%20Chasm%202001%20%20report%20brief.pdf .
13. Lin CP, Stephens KA, Baldwin LM, Keppel GA, Whitener RJ, Echo-Hawk A, Korngiebel D. Developing governance for federated community-based EHR data sharing. AMIA Jt Summits Transl Sci Proc. 2014 Apr 7;2014:71–6. [ PMC free article ] [ PubMed ]
14. Cole AM, Stephens KA, Keppel GA, Lin CP, Baldwin LM. Implementation of a health data-sharing infrastructure across diverse primary care organizations. J Ambul Care Manage. 2014;37(2):164–70. [ PMC free article ] [ PubMed ]
15. Lin CP, Black RA, Laplante J, Keppel GA, Tuzzio L, Berg AO, Whitener RJ, Buchwald DS, Baldwin LM, Fishman PA, Greene SM, Gennari JH, Tarczy-Hornoch P, Stephens KA. Facilitating health data sharing across diverse practices and communities. AMIA Jt Summits Transl Sci Proc. 2010 Mar 1;2010:16–20. [ PMC free article ] [ PubMed ]
16. Stephens KA, Lin CP, Baldwin LM, Echo-Hawk A, Keppel GA, Buchwald D, Whitener RJ, Korngiebel DM, Berg AO, Black RA, Tarczy-Hornoch P. LC Data QUEST: A Technical Architecture for Community Federated Clinical Data Sharing. AMIA Jt Summits Transl Sci Proc. 2012;2012:57–62. [ PMC free article ] [ PubMed ]
17. McCarthy DB, Propp K, Cohen A, Sabharwal R, Schachter AA, Rein AL. Learning from Health Information Exchange Technical Architecture and Implementation in Seven Beacon Communities. EGEMS (Wash DC) 2014 May 5;2(1):1060. [ PMC free article ] [ PubMed ]
18. Sittig DF, Hazlehurst BL, Brown J, Murphy S, Rosenman M, Tarczy-Hornoch P, Wilcox AB. A survey of informatics platforms that enable distributed comparative effectiveness research using multi-institutional heterogeneous clinical data. Medical care. 2012 Jul;50(Suppl):S49. [ PMC free article ] [ PubMed ]
19. Schilling LM, Kwan BM, Drolshagen CT, Hosokawa PW, Brandt E, Pace WD, Uhrich C, Kamerick M, Bunting A, Payne PR, Stephens WE, George JM, Vance M, Giacomini K, Braddy J, Green MK, Kahn MG. Scalable Architecture for Federated Translational Inquiries Network (SAFTINet) Technology Infrastructure for a Distributed Data Network. EGEMS (Wash DC) 2013 Oct 7;1(1):1027. [ PMC free article ] [ PubMed ]
20. Hersh WR, Cimino J, Payne PR, Embi P, Logan J, Weiner M, Bernstam EV, Lehmann H, Hripcsak G, Hartzog T, Saltz J. Recommendations for the use of operational electronic health record data in comparative effectiveness research. EGEMS (Wash DC) 2013;1(1):1018. [ PMC free article ] [ PubMed ]
21. Spence D. Data, data everywhere. BMJ. 2013 Feb 4;346:f725. [ PubMed ]
22. Johnson KE, Kamineni A, Fuller S, Olmstead D, Wernli KJ. How the provenance of electronic health record data matters for research: a case example using system mapping. EGEMS (Wash DC) 2014;2(1):1058. [ PMC free article ] [ PubMed ]
23. PGRN [Internet]   [cited 2015 June, 26]; Available from: http://www.nigms.nih.gov/Research/specificareas/PGRN/background/Pages/mission2010.aspx .
24. Pharmacogenomics Research Network (PGRN) [Internet]   [cited 2015 June, 26]; Available from: http://www.pgrn.org/display/pgrnweborganization/NWAP+Profile .
25. Vilar S, Harpaz R, Santana L, Uriarte E, Friedman C. Enhancing adverse drug event detection in electronic health records using molecular structure similarity: application to pancreatitis. PLoS One. 2012;7(7):e41471. [ PMC free article ] [ PubMed ]
26. Haerian K, Varn D, Vaidya S, Ena L, Chase HS, Friedman C. Detection of pharmacovigilance-related adverse events using electronic health records and automated methods. Clin Pharmacol Ther. 2012;92(2):228–34. [ PMC free article ] [ PubMed ]
27. Romano MJ, Stafford RS. Electronic health records and clinical decision support systems: impact on national ambulatory care quality. Arch Intern Med. 2011;171(10):897–903. [ PMC free article ] [ PubMed ]
28. Kawamoto K, Lobach DF, Willard HF, Ginsburg GS. A national clinical decision support infrastructure to enable the widespread and consistent practice of genomic and personalized medicine. BMC Med Inform Decis Mak. 2009;9:17. [ PMC free article ] [ PubMed ]
29. Welch BM, Kawamoto K. Clinical decision support for genetically guided personalized medicine: a systematic review. J Am Med Inform Assoc. 2013 Mar-Apr;20(2):388–400. [ PMC free article ] [ PubMed ]
30. DeWilde S, Carey IM, Bremner SA, Richards N, Hilton SR, Cook DG. Evolution of statin prescribing 1994–2001: a case of agism but not of sexism? Heart. 2003 Apr;89(4):417–21. [ PMC free article ] [ PubMed ]
31. Sathasivam S, Lecky B. Statin induced myopathy. BMJ. 2008;337:a2286. [ PubMed ]
32. Gaist D, Rodriguez LA, Huerta C, Hallas J, Sindrup SH. Lipid-lowering drugs and risk of myopathy: a population-based follow-up study. Epidemiology. 2001;12(5):565–9. [ PubMed ]
33. Graham DJ, Staffa JA, Shatin D, Andrade SE, Schech SD, La Grenade L, Gurwitz JH, Chan KA, Goodman MJ, Platt R. Incidence of hospitalized rhabdomyolysis in patients treated with lipid-lowering drugs. JAMA. 2004 Dec 1;292(21):2585–90. [ PubMed ]
34. Ghatak A, Faheem O, Thompson PD. The genetics of statin-induced myopathy. Atherosclerosis. 2010 Jun;210(2):337–43. [ PubMed ]
35. Bayley KB, Belnap T, Savitz L, Masica AL, Shah N, Fleming NS. Challenges in using electronic health record data for CER: experience of 4 learning organizations and solutions applied. Med Care. 2013;51(8 Suppl 3):S80–6. [ PubMed ]
36. Estabrooks PA, Boyle M, Emmons KM, Glasgow RE, Hesse BW, Kaplan RM, Krist AH, Moser RP, Taylor MV. Harmonized patient-reported data elements in the electronic health record: supporting meaningful use by primary care action on health behaviors and key psychosocial factors. J Am Med Inform Assoc. 2012 Jul-Aug;19(4):575–82. [ PMC free article ] [ PubMed ]
37. Howie L, Hirsch B, Locklear T, Abernethy AP. Assessing the value of patient-generated data to comparative effectiveness research. Health Aff (Millwood) 2014;33(7):1220–8. [ PubMed ]
38. Paolino AR, Lauf SL, Pieper LE, Rowe J, Vargas IM, Goff MA, Daley MF, Tuzzio L, Steiner JF. Accelerating Regulatory Progress in Multi-Institutional Research. EGEMS (Wash DC) 2014 Jul 10;2(1):1076. [ PMC free article ] [ PubMed ]
39. Wilcox A, Holve E. Sustaining the effective use of health care data: a message from the editors. EGEMS (Wash DC) 2014;2(2):1141. [ PMC free article ] [ PubMed ]
Articles from eGEMs are provided here courtesy of Academy Health
Formats:
AMIA Annu Symp Proc. 2016; 2016: 514–523.
Published online 2017 Feb 10.
PMCID: PMC5333270
PMID: 28269847
Characterizing Physicians Practice Phenotype from Unstructured Electronic Health Records
Find articles by Sanjoy Dey
Yajuan Wang
Find articles by Yajuan Wang
Roy J. Byrd
Find articles by Roy J. Byrd
Kenney Ng
Find articles by Kenney Ng
Steven R. Steinhubl
Find articles by Steven R. Steinhubl
Christopher deFilippi
Find articles by Christopher deFilippi
Walter F. Stewart
Find articles by Walter F. Stewart
1IBM Research, T.J. Watson Research Center, Yorktown Heights, NY USA
2Geisinger Health System, Danville, PA USA and Scripps Health, San Diego, CA USA
3Inova Heart and Vascular Institute, Fairfax, VA USA
4Sutter Health Research, Walnut Creek, CA USA
Author information ► Copyright and License information ► Disclaimer
Copyright ©2016 AMIA - All rights reserved.
This is an Open Access article: verbatim copying and redistribution of this article are permitted in all media for any purpose
Abstract
Clinical practice varies among physicians in ways that could lead to variation in what is documented in a patient's electronic health records (EHR) and act as a source of bias to predictive model performance that is independent of patient health status. We used EHR encounter note data on 5,187primary care patients 50 to 85 years of age selected for a separate case-control study covering 144 unique primary care physicians (PCPs). A validated text extractor tool was used to identify mentions of Framingham heartfailure signs and symptoms (FHFSS) from the notes. Hierarchical clustering analyses were performed on the encounter note data for finding subgroups of PCPs with distinct FHFSS documentation behaviors. Three distinct PCP groups were identified that differed in the rate of documenting assertions and denials of mentions. Physician subgroup differences were not explained by patient disease burden, medication use, or other factors related to health.
Introduction
The widespread adoption of electronic health records (EHR) by US health care providers [ 1 ] is motivating a rapid growth in the use of predictive models to guide clinical decisions [ 2 ], to identify patients at high risk of future events (e.g., 30-day readmission) [ 3 ], and to detect disease early [ 4 ], among other applications. Copious longitudinal structured and unstructured data are captured by EHRs to characterize the patient's demographic (e.g., age, sex, address), health and treatment status, diagnoses, lab test results, and medication orders. As much as 80% of the EHR data is thought to be in unstructured form [ 5 ]. To effectively use EHR data it is important to understand how the data comes to be.
Physicians are the dominant sources of the data captured in EHR. However, physicians vary substantially and systematically in their clinical practices [ 6 ] resulting in variation in what is ordered, diagnosed, and documented for each patient, in medication prescribing and in preferences for the intensity of practice. Such physician practice styles are not idiosyncratic. Rather, practice style is known to be directly or indirectly influenced by medical school training, regional practice standards, local practice standards, and performance incentives, among other factors. However, most of these clinical practice differences are independent of the underlying health status of the patient or other characteristics of patients such as demographics or prior genetic predisposition to the disease.
These "practice phenotype" differences can significantly influence the quality of both structured and unstructured data in the EHR and act as a source of potential bias for any downstream analysis of EHR data. To build accurate computational models, we need to detect and normalize for such variances in physician behavior. However, previous studies have largely focused on differences in physicians' practices using structured EHR data to characterize diagnostic practice [ 6 ], regional practice pattern and standards [ 7 ], expertise, prior educational and training background of doctors [ 8 ], and the patient's treatment plan [ 9 ]. Differences may be identifiable by a limited number of practice phenotypes. Prior studies indicate that patient utilization phenotypes can be identified from structured data that are strongly influenced by provider preferences [ 10 ]. But, documentation in unstructured data has not been examined for such patterns.
In this study, we aim to explore whether there are practice phenotypes that characterize differences among physicians in how information gets into the unstructured EHR data. To test the phenotype hypothesis we use physicians' notes to determine whether there are practice phenotypes in the documentation of Framingham heart failure signs and symptoms (FHFSS). FHFSS are frequently documented in progress notes by PCPs, often years before HF diagnosis [ 11 ]. We describe a systematic framework, based on a clustering approach, for characterizing the practice phenotype of PCPs using a large scale unstructured EHR data.
Methods
We focused on FHFSS to explore variations because physicians routinely document the presence or absence of the symptoms among older primary care patients independent of HF diagnosis. There are a number of challenges in extracting FHFSS from clinical notes that are actually related to practice phenotype. First, FHFSS are often documented in clinical notes much earlier than the clinical diagnosis of HF, which means that tracking the clinical notes longitudinally throughout the patient's medical history is needed. Second, a physician's practice might be affected by other confounding factors such as the patient's age, sex, prior medical history, other co-morbidities and the physician's expertise. Such confounding factors have to be removed in order to obtain an unbiased estimate of the actual practice phenotype defined by documentation behavior of FHFSS.
We carefully performed the study design and cohort selection to remove the effects of confounding factors during the feature extraction from clinical notes. A previously validated natural language processing (NLP) tool [ 12 ] was used for extracting FHFSS from clinical notes. For convenience, we used a large well characterized sample of primary care patients selected as controls for a prior nested case control study of heart failure [ 4 ]. We focused on the control group of patients to avoid any potential practice behaviors that might be a result of disease onset rather than actual practice phenotype of physicians. We were interested in broadly testing the hypothesis of physician documentation phenotypes in a representative sample of patients and not in a sample that was defined by any specific disease. Details of the patient sample and data source are summarized below along with the feature construction and analytic methods. This study was approved by the Geisinger Institutional Review Board.
Study Design, Population, Setting, Source of Data
Longitudinal EHR data were obtained on patients, 50 to 85 years of age, from the Geisinger Clinic, a multispecialty group practice that provides care to approximately 400,000 residents in central and northeastern Pennsylvania. EpicCare EHR was installed at Geisinger before 2001.
A total of 1684 incident HF cases among Geisinger primary care patients were identified over the time period from 2003 to 2010 [ 4 , 13 , 14 ]. Up to 10 eligible primary care clinic-, sex-, and age-matched (in 5-year age intervals) controls were selected for each incident HF case for a total of 13,525 Geisinger control patients. Primary care patients were eligible as controls if they had no HF diagnosis up to the one year post-HF diagnosis of the corresponding HF case. Control subjects were required to have their first office encounter within one year of the incident HF patient's first office visit and have >1 office encounters 30 days before or any time after the case's HF diagnosis date to ensure similar duration of observations among cases and controls. Nine or 10 controls were identified for 49% of the Geisinger cases; 1.5% of Geisinger cases had only 1 to 2 controls.
The primary care physician (PCP) was the unit of analysis and patient data were nested within each physician. Patient data were assigned to the physician who was the designated primary care provider as documented in the EHR. Assignment of patients to a PCP is documented in an EHR structured field. Note that different encounters of the same patients might correspond to different PCPs, if that patient utilized care from multiple providers. In that case, we mapped each patient to the unique PCP who treated that patient for the longest period of time. Of the total 13,525 patients, 11,268 were explicitly assigned to a PCP.
The EHR data used for this study were selected from the time period in controls that was 12 to 36 months before the incident diagnosis of HF in the corresponding matched case. This time period prior to the diagnosis of HF of the matched case is denoted as the "observation window" ( Figure 1 ). Patients were excluded if: 1) they did not have an encounter note in the12-36 month observation window (this observation window retained the largest amount of patient data as described elsewhere [ 4 ]); 2) if the coverage duration period (i.e., total time span covering patient encounters with the PCP that are within the observation window) for the patient was less than or equal to 180 days; and 3) if a patient had a substantial number of documented chronic diseases during the observation window. This last step was used to minimize documentation variation among physicians that could be explained by patient's comorbidies other than disease burden. We considered 1148 ICD-9 diagnosis codes coming from three different types such as Chronic Disease (200 codes), Cardiometabolic Chronic Disease (743 codes) and Chronic Episodic Disease (214 codes) with only 9 codes being common between the first two categories. Most of the patients have very low chronic conditions as shown in Figure 2 . Therefore, we only kept patient with at most five (out of 1148 codes) chronic conditions to retain maximum number of samples. The first two criteria of the obserbation windows of 12 to 36 months and the minimum coverage of 6 months led to 6862 patients in total, while imposing all three criteria reduced the samples to 5,187 qualified control samples to 144 PCPs in total. The data used for analysis was confined to this subgroup of PCPs.
Figure 1:
Relation of observation window for use of data and the diagnosis date for cases and the same relative times for controls. For each patient, a feature vector is generated by applying appropriate aggregation functions on the longitudinal EHR patient data in the observation window.
Figure 2:
Qualified patient distribution based on the frequency of chronic diseases from three different types: Chronic Disease (200 ICD-9 diagnosis codes), Cardiometabolic Chronic Disease(743 codes) and Chronic Episodic Disease (214). 284 out of 5351 patients (5%) had no chronic disease. Majority of patients (34%) had only two out of 1148 diagnosis codes.
Unstructured Data Extraction from Longitudinal EHR Patient Data
We used physician encounter note data to extract mentions of FHFSS. The FHFSS were originally published in 1971 [ 15 ] and are often a focus of encounter documentation when physicians assess a patient's cardiovascular health. A hybrid natural language processing (NLP) tool called PredMED [ 12 ] was used to identify mentions of FHFSS within the notes and to label notes according to whether, at the encounter level, each FHFSS was asserted or denied. PredMED achieves an F-score of 0.910 for mention extraction and an F-score of 0.932 for encounter labeling. Among the 17 FHFSS described in [ 15 ], 15 were deemed clinically relevant based on prior work [ 4 , 12 ] and given the known frequency of documentation ( Table 1 ). PredMED extracted mentions of these 15 FHFSS, along with modifiers that indicated the assertion (presence) or denial (absence) of each condition [except Tachycardia and WeightLoss, for which only assertion was extracted]. The resulting 28 FHFSS were used as features, for further analysis of PCP documentation.
Table 1.
28 Framingham heart failure signs and symptoms (FHFSS) extracted from text notes using the PredMED text analysis tool. The mean, median and standard deviation of the fraction of encounters where each of these 28 features was reported by PCPs is shown.
FHFSS Description
JMIR Med Inform. 2017 Apr-Jun; 5(2): e12.
Published online 2017 May 9. doi:  10.2196/medinform.7235
PMCID: PMC5442348
Effective Information Extraction Framework for Heterogeneous Clinical Reports Using Online Machine Learning and Controlled Vocabularies
Monitoring Editor: Gunther Eysenbach
Reviewed by Min Jiang and  Noa Patricia Cruz-Díaz
1Department of Biomedical Informatics, Emory University, Atlanta, GA, United States
2Department of Mathematics and Computer Science, Emory University, Atlanta, GA, United States
3Division of Cardiology, Emory School of Medicine, Emory University, Atlanta, GA, United States
4Department of Biomedical Informatics, Stony Brook University, Stony Brook, NY, United States
Fusheng Wang, Department of Biomedical Informatics, Stony Brook University, 2211 Computer Science, Stony Brook, NY, 11794-8330, United States, Phone: 1 6316327528, Fax: 1 6316381323, Email: ude.koorbynots@gnaw.gnehsuf .
Shuai Zheng
Find articles by Shuai Zheng
James J Lu
Find articles by James J Lu
Nima Ghasemzadeh
Find articles by Nima Ghasemzadeh
Salim S Hayek
Find articles by Salim S Hayek
Arshed A Quyyumi
Find articles by Arshed A Quyyumi
Fusheng Wang
Find articles by Fusheng Wang
Corresponding author.
Author information ► Article notes ► Copyright and License information ► Disclaimer
Received 2016 Dec 28; Revisions requested 2017 Feb 9; Revised 2017 Mar 16; Accepted 2017 Mar 20.
Copyright ©Shuai Zheng, James J Lu, Nima Ghasemzadeh, Salim S Hayek, Arshed A Quyyumi, Fusheng Wang. Originally published in JMIR Medical Informatics (http://medinform.jmir.org), 09.05.2017.
This is an open-access article distributed under the terms of the Creative Commons Attribution License ( http://creativecommons.org/licenses/by/2.0/ ), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on http://medinform.jmir.org/, as well as this copyright and license information must be included.
This article has been cited by other articles in PMC.
Abstract
Background
Extracting structured data from narrated medical reports is challenged by the complexity of heterogeneous structures and vocabularies and often requires significant manual effort. Traditional machine-based approaches lack the capability to take user feedbacks for improving the extraction algorithm in real time.
Objective
Our goal was to provide a generic information extraction framework that can support diverse clinical reports and enables a dynamic interaction between a human and a machine that produces highly accurate results.
Methods
A clinical information extraction system IDEAL-X has been built on top of online machine learning. It processes one document at a time, and user interactions are recorded as feedbacks to update the learning model in real time. The updated model is used to predict values for extraction in subsequent documents. Once prediction accuracy reaches a user-acceptable threshold, the remaining documents may be batch processed. A customizable controlled vocabulary may be used to support extraction.
Results
Three datasets were used for experiments based on report styles: 100 cardiac catheterization procedure reports, 100 coronary angiographic reports, and 100 integrated reports—each combines history and physical report, discharge summary, outpatient clinic notes, outpatient clinic letter, and inpatient discharge medication report. Data extraction was performed by 3 methods: online machine learning, controlled vocabularies, and a combination of these. The system delivers results with F1 scores greater than 95%.
Conclusions
IDEAL-X adopts a unique online machine learning–based approach combined with controlled vocabularies to support data extraction for clinical reports. The system can quickly learn and improve, thus it is highly adaptable.
Keywords: information extraction, natural language processing, controlled vocabulary, electronic medical records
Introduction
While immense efforts have been made to enable structured data model for electronic medical record (EMR), a large amount of medical data remain in free-form narrative text, and useful data from individual patients are usually distributed across multiple reports of heterogeneous structures and vocabularies. This poses major challenges to traditional information extraction systems, as either costly training datasets or manually crafted rules have to be prepared. These approaches also lack the capability of taking user feedbacks, to adapt and improve the extraction algorithm in real time.
Our goal is to provide a generic information extraction framework that is adaptable to diverse clinical reports, enables a dynamic interaction between a human and a machine, and produces highly accurate results with minimal human effort. We have developed a system, Information and Data Extraction using Adaptive Online Learning (IDEAL-X), to support adaptive information extraction from diverse clinical reports with heterogeneous structures and vocabularies. The system is built on top of online machine learning and customizable controlled vocabularies. A demo video can be found on YouTube [ 1 ].
IDEAL-X uses online machine learning–based approach [ 2 - 4 ] for information extraction. Traditional machine learning algorithms take a two-stage approach: batch training based on an annotated training dataset, and batch prediction for future datasets based on the model generated from stage one ( Figure 1 ). In contrast, online machine learning algorithms [ 2 , 3 ] take an iterative approach ( Figure 1 ). It learns one document at a time, and predicts values to be extracted for the next one. Learning occurs from revisions made by the user, and the updated model is applied to prediction for subsequent documents. Once the model achieves a satisfactory accuracy, the remaining documents may be processed in batch. Online machine learning not only significantly reduces human's effort for annotation but also provides the mechanism for collecting feedback from human-machine interaction to improve the system's model continuously.
Figure 1
Online machine learning versus batch learning. (a) Batch machine learning workflow; (b) Online machine learning workflow.
Besides online machine learning, IDEAL-X allows for customizable controlled vocabularies to support data extraction from clinical reports, where a vocabulary enumerates the possible values that can be extracted for a given attribute. (The X in IDEAL-X represents the controlled vocabulary plug-in.) The use of online machine learning and controlled vocabularies is not mutually exclusive; they are complementary, which provide the user with a variety of modes for working with IDEAL-X.
Background
Related Work
A number of research efforts have been made in different fields of medical information extraction. Successful systems include caTIES [ 5 ], MedEx [ 6 ], MedLEE [ 7 ], cTAKES [ 8 ], MetaMap [ 9 ], HITEx [ 10 ], and so on. These methods either take a rule-based approach, a traditional machine learning–based approach, or a combination of both.
Different online learning algorithms have been studied and developed for classification tasks [ 11 ], but their direct application to information extraction has not been studied. Especially in the clinical environment, the effectiveness of these algorithms is yet to be examined. Several pioneering projects have used learning processes that involve user interaction and certain elements of IDEAL-X. I2E2 is an early rule-based interactive information extraction system [ 12 ]. It is limited by its restriction to a predefined feature set. Amilcare [ 13 , 14 ] is adaptable to different domains. Each domain requires an initial training that can be retrained on the basis of the user's revision. Its algorithm (LP)2 is able to generalize and induce symbolic rules. RapTAT [ 15 ] is most similar to IDEAL-X in its goals. It preannotates text interactively to accelerate the annotation process. It uses a multinominal naïve Baysian algorithm for classification but does not appear to use contextual information beyond previously found values in its search process. This may limit its ability to extract certain value types.
Different from online machine learning but related is active learning [ 16 , 17 ], it assumes the ability to retrieve labels for the most informative data points while involving the users in the annotation process. DUALIST [ 18 ] allows users to select system-populated rules for feature annotation to support information extraction. Other example applications in health care informatics include word sense disambiguation [ 19 ] and phenotyping [ 20 ]. Active learning usually requires comprehending the entire corpus in order to pick the most useful data point. However, in a clinical environment, data arrive in a steaming fashion over time that limits our ability to choose data points. Hence, an online learning approach is more suitable.
IDEAL-X adopts the Hidden Markov Model for its compatibility with online learning, and for its efficiency and scalability. We will also describe a broader set of contextual information used by the learning algorithm to facilitate extraction of values of all types.
Heterogeneous Clinical Reports
A patient's electronic medical record could come with a variety of medical reports. Data in these reports provide critical information that can be used to improve clinical diagnosis and support biomedical research. For example, the Emory University Cardiovascular Biobank [ 21 ] collects records of patients with potential or confirmed coronary artery diseases undergoing cardiac catheterization, and aims to combine extracted data elements from multiple reports to identity patients for research. Report types include history and physical report, discharge summary, outpatient clinic note, outpatient clinic letter, coronary angiogram report, cardiac catheterization procedure report, echocardiogram report, inpatient report, and discharge medication lists.
We classify clinical reports into 3 forms: semistructured data, templatebased narration, and complex narration. Semistructured data represent data elements in the form of attribute and value pairs ( Figure 2 ). Reports in this form have simple structures, making data extraction relatively straightforward. Template-based narration is a very common report form. The narrative style, including sentence patterns and vocabularies, follow consistent templates and expressions ( Figure 2 ). Extracting information from this type of text (eg, "right posterior descending artery") require major linguistics expertise, to either formulate extraction rules or to annotate training data. Complex narration is essentially free-form text. It can be irregular, personal, and idiomatic ( Figure 2 ). Most medical reporting systems still allow for (and thus encourage) such a style. It is the most difficult form to interpret and process by NLP algorithms. Nevertheless, certain type of information such as diseases and medications has finite vocabulary that could be used to support data extraction.
Example snippets of different report forms. (a) Semistructured report; (b) Template based narration; and (c) Complex narration.
Methods
Overview
The interface and workflow conform to traditional annotation systems: a user browses an input document from the input document collection and fills out an output form. On loading each document, the system attempts to fill the output form automatically with its data extraction engine. Then, a user can review and revise incorrect answers. The system then updates its data extraction model automatically based on the user's feedbacks. Optionally, the user may provide a customized controlled vocabulary to further support data extraction and answer normalization. Pretraining with manually annotated data is not required, as the prediction model behind the data extraction engine can be established incrementally through online learning, customizing controlled vocabularies, or a combination of the two.
The system can operate in two modes: (1) interactive: through online learning, the system predicts values to be extracted for each report, and the user verifies or corrects the predicted values; and (2) batch: batch predicting for all unprocessed documents once the accrued accuracy is sufficient for users. Whereas interactive mode uses online machine learning to build the learning model incrementally, batch mode runs the same as the prediction phase of batch machine learning.
System Interface and User Operations
System Interface
IDEAL-X provides a GUI with two main panels: a menu and navigation buttons ( Figure 3 ). The left panel is for browsing an input report, and the right panel is the output table with predicted values of each data element in the report. The menu provides options for defining the data elements to be extracted, specifying input reports, among others.
An example screenshot of IDEAL-X's interface.
Definition of Data Elements for Extraction
The system provides a wizard for constructing the metadata of the output form. The user builds the form by specifying a list of data elements and their constraints. An example is the data element "Heart Rate," which is constrained to be a numerical value between 0 and 200. Other constraints include sections of the report that may contain the values. However, except for the names of the data elements, specifying constraints are optional, as these can be learned by the system.
Data Extraction Workflow
The user will first select a collection of input reports to be extracted from a local folder. By default, the system runs in an interactive mode, and one report will be loaded at a time on the left display panel. The user can make manual annotations by highlighting the correct value in the report text. Clicking the corresponding data field in the table assigns the value to the data element. If the system has prefilled the field of a data element with a predicted value, the user can provide feedback by fixing incorrect values. As the user navigates to the next document, the system compares the prefilled and the final values for the most recently processed document. Values that are unchanged or filled in by users are taken as positive instances, and values that have been revised are taken as negative instances. Both instances are incorporated into the online learning algorithm to be used by the data extraction for subsequent documents. By iterating through this process, the amount of information that the system is able to correctly prefill grows over time. Note that manual revision in this context is different from traditional human labeling. It is only necessary if there is a wrong prediction, thus humans' effort can be significantly saved. Once the decision model reaches an acceptable level of accuracy, the user has the option to switch to batch mode to complete extraction for the remaining documents. If a patient has multiple reports, the text input panel displays each report with a separate tab. Data extracted from all the reports are aggregated in the output.
Customization of Controlled Vocabularies
IDEAL-X also provides an interface for the user to customize a controlled vocabulary that can be used by the system for data extraction. The controlled vocabulary contains both terminology and structural properties. The terminology includes lists of values and their normalization mappings. For example, Disease terminology includes "Diabetes Mellitus" with variations "DM" and "Diabetes." It also defines inductions. For example, taking "Insulin" or "Metformin" indicates having Diabetes Mellitus. Structural properties provide positive and negative contextual information for giving terms. For example, to extract medications taken by patients, the "Allergies" section is a negative context and medicine names in the section will be skipped. Structural properties may also contain disambiguation terms that may further improve the precision of extraction. A simple example is that "intolerant" is a negative indicator for identifying "statin" as "statin intolerant" refers to different a concept. Controlled vocabularies can be a powerful tool to support data extraction: it can be used to locate sentences and chunks of possible values, and to perform normalization for extracted values, discussed in the next section.
The Data Extraction Engine
While the user interacts with IDEAL-X interface, the data extraction engine works transparently in the background. The engine has 3 major components: answer prediction, learning, and the learning model that the online learning process continuously updates ( Figure 4 ). The system combines statistical and machine learning–based approaches with controlled vocabularies for effective data extraction.
Overview of System Workflow.
Document Preprocessing
When a report is loaded, the text is first parsed into an in-memory hierarchical tree consisting of 4 layers: section, paragraph, sentence, and token. Apache OpenNLP [ 22 ] is used to support the parsing with its Sentence Detector, Tokenizer, and Part-of-Speech Tagger. A reverse index of tokens is created to support efficient keywords-based search. The index is used to find locations (eg, sections, paragraphs, sentences, and phrases) of a token, as well as its properties such as part of speech and data type. For example, given the token "DM," the system can quickly identify the section (eg, "History") and the containing sentences. Such token search is frequently performed in answer prediction, and the in-memory index structures enable high efficiency for such operations.
Answer Prediction
Predicting the value of each data element involves the following steps: (1) Identifying target sentences that are likely to contain the answer; (2) Identifying candidate chunks in the sentences; (3) Filtering the chunks to generate candidate values; (4) Ranking candidate values to generate (raw) values; (5) Normalizing values; and (6) Aggregating values from multiple reports. The workflow is shown in Figure 5 .
Precision and recall changes over processed records.
Identifying Target Sentences
Through online learning, the system accrues keywords from past answers (answer keywords) along with cooccurring words in the corresponding sentences (contextual words). For example, given the answer keywords "diabetes" and "hypertension" in the sentence "The patient reports history of diabetes and hypertension," contextual words are "patient," "report," and "history." Such answer keywords and contextual words combined with customized vocabularies can be utilized to identify sentences that are likely to contain answers with the following methods:
First, similarity-based search using the vector space model [ 23 ]. Given a collection of contextual words and their frequencies, the system computes the similarity against sentences in the document [ 23 ]. Sentences with high similarities are selected. For example, most sentences about "disease" contain "diagnosis" and "history." The past contextual keywords and their frequency weights are represented and maintained through a learning model discussed later in "Learning" section.
Second, answer keyword matching search. The answer keywords, combined with relevant user customized vocabularies, are also used to identify target sentences with keyword matching. For example, to extract diseases, if a sentence contains the disease term "myocardial infarction" defined in the vocabulary, the sentence is selected as a target. In both approaches, sections to be searched or skipped are also considered to narrow the scope of searching.
Identifying Candidate Chunks
After target sentences are selected, the system identifies potential phrases in the sentences using 2 methods: Hidden Markov model (HMM) [ 24 ] and keyword-based search. The HMM represents target words and contextual words in a sentence with different states, and marks values to be extracted based on probability distributions learned from previously collected values and their sentences. The keyword-based search finds candidate chunks using keywords collected from past answers and the controlled vocabulary.
Filtering chunks
To filter candidate chunks, the system uses rule induction [ 14 , 25 ] to generate "If-Then" rules based on historical statistics. The following filtering criteria are used: (1) Part of speech (POS): This filters a phrase by its POS tag in the sentence. Simple example phrases are noun and verb phrases. (2) String pattern: This looks for chunks that match special string patterns. For example, the first characters of all tokens are capitalized. (3) Value domain: This eliminates numerical or enumerated values that fall outside a specified range of values. (4) Negation: Based on predefined built-in rules, this removes phrases governed by words that reverse the meaning of the answer [ 26 ]. For example, if a candidate chunk "cancer" is extracted from a sentence "the patient has no history of cancer," "cancer" would not be included. (5) Certainty: Similar to negation filter, this detects and filters uncertain events or situations such as future plans, based on predefined rules. For example, a candidate chunk "radiation therapy" for treatment from a sentence "the patient is planned to take radiation therapy" should not be included. Whereas negation and certainty filtering is based on predefined rules, other filtering relies on real-time data statistics for filtering criteria.
Ranking Candidate Values
The system combines the scores of the selected sentences and chunks for ranking of candidate values. For a single-valued data element (eg, heart beat), the candidate value with the highest confidence score is selected. For a multi-valued data element (eg, medication), values with confidence scores above a threshold are selected. Based on this, each candidate value is either accepted or rejected.
Normalizing Values
This step normalizes extracted values through transformation, generalization, and induction rules given by the controlled vocabulary ( Figure 4 ). For example, "DM" is transformed into "Diabetes Mellitus." "Pindolol" is generalized to its hypernym "beta blocker." The appearance of medication term "Metformin" (a drug for treating type 2 diabetes) in the text can infer the disease "Diabetes Mellitus."
Aggregating Results
Data extracted from multiple reports of a patient will be aggregated into a single table. The aggregation process may normalize values and remove duplicates. For example, "lisinopril" and "captopril" are extracted from discharge summary and inpatient report, respectively, and they can be normalized as "ACE inhibitor." If the same data element is extracted from multiple reports, deduplication is performed. The final output is in simple structural table form that can be exported conveniently to other applications such as Excel (Microsoft) or a database.
Note that controlled vocabularies can play important roles in the answer prediction process. They are used for identifying target sentences through keyword searching, identifying candidate chunks through keyword matching, and supporting normalization for extracted values.
Learning
IDEAL-X takes an online learning–based approach to incrementally build statistical models and make predictions ( Figure 5 ). The 3 models used in IDEAL-X are all statistical based and can be continuously updated after each iteration.
System-predicted values automatically populate the output table, and the user advances to the next report with or without revision to these values. In both cases, the internal learning and prediction models of IDEAL-X are updated. For each instance, IDEAL-X collects and analyzes the following features: (1) Position: location of the answer in the text hierarchy; (2) Landmark: co-occurring contextual keywords in a sentence; (3) POS: parts of speech tag; (4) Value: the tokens of the answer; (5) String patterns: literal features such as capitalization and initial and special punctuation. These features are then used to update the 3 models.
In IDEAL-X, each data element such as attribute "disease" or "medicine," has its own statistical model, and each new instance of a data element will update the corresponding model. There are 3 models to be updated: (1) Updating Space Vector Model: This model uses "Landmark" features of positive instances. The system updates frequencies of cooccurring contextual words, used as weights of the space vector [ 23 ]. (2) Updating HMM: HMM lists all words in a sentence as a sequence, in which an extracted value is marked as target value state and other words are recognized as irrelevant contextual states. Based on this sequence, the state transition probabilities and emission probabilities are recalculated [ 24 ]. (3) Updating rule induction model: Filtering rules are induced based on the coverage percentage [ 25 ]. Features such as POS, value domain and string patterns of both positive and negative instances are analyzed and their respective coverage percentages are modified. Once the coverage of a rule reaches a predefined threshold, the rule is triggered for filtering.
In an interactive mode, the above 4 steps repeat for each report, where the learning models are continuously updated and improved.
Results
Experimental Setup
Datasets
We used 3 datasets from 100 patients that were randomly sampled from a collection of about 5000 patients in the Emory Biobank database. Dataset 1 is a set of semistructured reports and contains 100 cardiac catheterization procedure reports. Dataset 2 is a set of template-based narration and contains 100 coronary angiographic reports. Dataset 3 is a set of complex narration and contains 315 reports, including history and physical report, discharge summary, outpatient clinic notes, outpatient clinic letter, and inpatient discharge medication report.
Ground Truth
The test datasets are independently hand-annotated by domain expert annotators, including physicians, physician trainees, and students trained by the Emory Clinical Cardiovascular Research Institute for Biobank data reporting. Each record is annotated by 2 different annotators. The interrater agreement scores (kappa) of these 3 datasets are .991, .986, and .835, respectively. An arbitrator—an independent cardiovascular disease researcher reconciles incompatible outputs of the system and the manual annotations to produce the final ground truth.
Evaluation Metrics
For validation, precision, recall, and F1 scores are used to estimate the effectiveness of extraction by comparing the system predicted results (before human revision) and the ground truth.
Experiment Settings
We aimed to evaluate the effectiveness of the system with respect to using online learning and controlled vocabularies and to understand their applicability to different report forms. By analyzing the report styles and vocabularies, we discovered that online learning will be more suitable for semistructured or template-based narration reports, and controlled vocabulary-guided data extraction would be more effective on complex narration with a finite vocabulary. Thus, we designed 3 experiments: (1) Online learning–based data extraction, where controlled vocabularies are not provided, based on Dataset 1 (semistructured) and Dataset 2 (template-based narration); (2) Controlled vocabularies-based data extraction, where online learning is not used, based on Dataset 3 (complex narration); and (3) Controlled vocabularies guided data extraction combined with online learning, based on Dataset 3.
Performance Evaluation
Experiment 1: Online machine Learning–Based Data Extraction
This experiment was based on Datasets 1 and 2. The system starts in an interactive mode with an empty decision model without prior training. The defined data elements are summarized in Multimedia Appendix 1 . The user processes one report at a time, and each system-predicted value (including empty values for the first few reports) before user revision was recorded for calculating precision and recall.
Results are summarized in Table 1 for the 2 datasets, respectively. Both test cases achieved high precision as semistructured and template-based text is most easy to handle. To study the learning rate of online learning, we divided records into 10 groups, and plotted precision and recall of every 10% of the records in datasets 1 and 2. We observed that in both tests, the system maintained high precision during the learning process. Although some variability exists due to new data pattern, the recall of both cases also improved steadily. Not surprisingly, the rate of learning for dataset 1 is much faster given its semistructure.
Table 1
Results of data extraction from semistructured reports (Dataset 1) and template-based narration (Dataset 2).
Dataset
Clin Transl Sci. 2018 Jan; 11(1): 85–92.
Published online 2018 Jan 9. doi:  10.1111/cts.12514
PMCID: PMC5759745
PMID: 29084368
Electronic Health Record Phenotypes for Precision Medicine: Perspectives and Caveats From Treatment of Breast Cancer at a Single Institution
1 , 2 Hongfang Liu , 3 Kara N. Maxwell , 4 Jyotishman Pathak , 5 and Rui Zhang 6 , 7
Matthew K. Breitenstein
1 Institute for Biomedical Informatics, Perelman School of Medicine,   University of Pennsylvania,   Philadelphia,   Pennsylvania,   USA,
2 Center for Pharmacoepidemiology Research and Training, Perelman School of Medicine,   University of Pennsylvania,   Philadelphia,   Pennsylvania,   USA,
Find articles by Matthew K. Breitenstein
Hongfang Liu
Find articles by Hongfang Liu
Kara N. Maxwell
Find articles by Kara N. Maxwell
Jyotishman Pathak
Find articles by Jyotishman Pathak
Rui Zhang
6 Department of Pharmaceutical Care & Health Systems, College of Pharmacy,   University of Minnesota,   Minneapolis,   Minnesota,   USA,
7 Institute for Health Informatics,   University of Minnesota,   Minneapolis,   Minnesota,   USA,
Find articles by Rui Zhang
1 Institute for Biomedical Informatics, Perelman School of Medicine,   University of Pennsylvania,   Philadelphia,   Pennsylvania,   USA,
2 Center for Pharmacoepidemiology Research and Training, Perelman School of Medicine,   University of Pennsylvania,   Philadelphia,   Pennsylvania,   USA,
3 Division of Biomedical Statistics and Informatics,   Mayo Clinic,   Rochester,   Minnesota,   USA,
4 Department of Medicine, Division of Hematology/Oncology, Perelman School of Medicine,   University of Pennsylvania,   Pennsylvania,   USA,
5 Division of Health Informatics, Weill Cornell Medicine,   Cornell University,   New York,   New York, USA,
6 Department of Pharmaceutical Care & Health Systems, College of Pharmacy,   University of Minnesota,   Minneapolis,   Minnesota,   USA,
7 Institute for Health Informatics,   University of Minnesota,   Minneapolis,   Minnesota,   USA,
Matthew K. Breitenstein, Email: ude.nnepu@tierbkm .
Corresponding author.
Author information ► Article notes ► Copyright and License information ► Disclaimer
Received 2017 May 2; Accepted 2017 Sep 26.
Copyright © 2017 The Authors. Clinical and Translational Science published by Wiley Periodicals, Inc. on behalf of American Society for Clinical Pharmacology and Therapeutics
This is an open access article under the terms of the Creative Commons Attribution‐NonCommercial License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.
Abstract
Precision medicine is at the forefront of biomedical research. Cancer registries provide rich perspectives and electronic health records (EHRs) are commonly utilized to gather additional clinical data elements needed for translational research. However, manual annotation is resource‐intense and not readily scalable. Informatics‐based phenotyping presents an ideal solution, but perspectives obtained can be impacted by both data source and algorithm selection. We derived breast cancer (BC) receptor status phenotypes from structured and unstructured EHR data using rule‐based algorithms, including natural language processing (NLP). Overall, the use of NLP increased BC receptor status coverage by 39.2% from 69.1% with structured medication information alone. Using all available EHR data, estrogen receptor‐positive BC cases were ascertained with high precision (P = 0.976) and recall (R = 0.987) compared with gold standard chart‐reviewed patients. However, status negation (R = 0.591) decreased 40.2% when relying on structured medications alone. Using multiple EHR data types (and thorough understanding of the perspectives offered) are necessary to derive robust EHR‐based precision medicine phenotypes.
Study Highlights
WHAT IS THE CURRENT KNOWLEDGE ON THE TOPIC?
✓ Targeted therapeutics are routinely prescribed in certain diseases, including breast cancer. However, knowledge regarding optimal ascertainment of precision medicine phenotypes using electronic health record (EHR) data is limited.
WHAT QUESTION DID THIS STUDY ADDRESS?
✓ As we pursue multi‐institution biobank studies to advance translational science and develop precision medicine knowledge, we increasingly rely on EHRs to annotate necessary clinical phenotypes. This study addresses important considerations regarding development of robust EHR‐based precision medicine phenotypes needed for translational science and precision medicine Research.
WHAT THIS STUDY ADDS TO OUR KNOWLEDGE
✓ In addition to the nuanced perspectives offered by structured and unstructured data sources, this study articulates necessary considerations for robust phenotyping in the precision medicine era.
HOW THIS MIGHT CHANGE CLINICAL PHARMACOLOGY OR TRANSLATIONAL SCIENCE
✓ This study demonstrates how EHR phenotyping might empower translational science. Potential research applications include: human genetics, epidemiology, heath outcomes and utilization, quality improvement, and pharmacoepidemiology, to name a few. In the future, potential for clinical pharmacology practice implementation exists. However, enhancements and thorough validation are required first.
Advancements in our understanding of breast cancer (BC) provide powerful personalized treatment opportunities not readily available in all cancer types. At present, we commonly define molecular subtypes (estrogen receptor (ER), progesterone receptor (PR), or human epidermal growth factor receptor2 (HER2) overexpression) as positive (+) or negative (–) receptor status in BC patients. Negative status of the molecular subtypes ER, PR, and HER2 are characterized as triple‐negative breast cancer (TNBC). Standard treatment in the neoadjuvant, adjuvant, and metastatic settings is guided by expression of hormone receptors (typically ER) and HER2. 1 Generally, endocrine therapy is prescribed for ER+ (PR+ or –) patients, Her2 directed therapy for Her2+ patients, and chemotherapy alone for patients with TNBC. 2 Utilization of targeted therapies, in part, have achieved great reductions in BC mortality, with 5‐year overall survival rates approaching 90% in all‐comer BC patients and 99% in ER‐positive localized, node‐negative cases. 3
Cancer registries provide a wealth of cancer‐specific information (e.g., stage, lymph node involvement); augmenting cancer registries with electronic health record (EHR) data to gain additional perspectives is an exciting frontier in cancer control research. 4 While the structure, depth, and longitudinal nature of EHR data present considerable strengths and opportunities, EHR‐based phenotypes have potential to provide an incomplete (or inaccurate) perspective. Historically, development of EHR‐based phenotyping algorithms has focused on maximizing ascertainment of true case or control disease status. While this approach facilitated the development of reproducible and portable phenotypes, 5 a limited focus was placed on identifying granular phenotype perspectives beyond high‐level true case or control status. Applications of rule‐based natural language processing (NLP) have effectively annotated breast cancer clinical data elements from pathology reports. 6 Further, machine learning‐enhanced NLP has shown that annotation, including BC receptor status, of pathology notes can be developed with limited supervision using only a few training notes. 7 However, critical phenotype considerations remain unclear. These include: treatment perspectives contained across multiple EHR data sources, ascertaining negation of receptor status by evaluating surrounding contextual features contained within the clinical dialogs, and the impact of alternative EHR data source perspectives. With the advancement of precision medicine, disease phenotypes 8 are becoming increasingly complex—ensuring robust EHR perspectives from multiple data sources develops increasing importance. A thorough understanding of the nuances needed for EHR phenotyping in the era of precision medicine is critical to advance translational science.
In our study, we sought to understand the caveats and data considerations for creating robust precision medicine phenotypes of BC subtypes (e.g., ER, PR, HER2, and TN status) from multiple EHR perspectives. We posit that a "precision medicine phenotype" (PMP) represents perspectives based on molecular subtypes or targeted therapeutic utilization for cancer patients. Further, data sources and extraction techniques utilized are likely to have a pronounced impact on the perspective of the PMP. We aim to understand the different combinations of clinical data sources and informatics approaches necessary, including developed NLP infrastructure, to establish a robust BC PMP. We evaluate PMP perspectives by i) clinical data source coverage, and ii) performance relative to a baseline of expert‐ascertained receptor status. Finally, we highlight caveats and potential biases regarding cancer PMPs.
METHODS
Patient subgroups and data extraction
Female patients with newly diagnosed BC between 1998 and 2011 were selected for inclusion in this study under minimal risk IRB 15–003347. Cancer registry data were linked with local EHR data for these BC patients (n = 13,162) at the Mayo Clinic, Rochester, Minnesota. Patients with localized BC with more than one primary tumor (bilateral or ipsilateral disease) and ductal carcinoma in situ (DCIS) were excluded. Patients not treated at the Mayo Clinic and seen for second opinions only were excluded. A combined study design, high‐level pseudo code, and study nomenclature is included for reference (Figure ​1).1 ). A subset of patients, encompassing our "Gold Standard Cohort," were selected at random for manual chart review (in addition to EHR phenotyping) to ascertain receptor status. The Gold Standard Cohort was then randomly split into approximately equal‐sized "training" and "testing" subgroups for use in algorithm development and evaluation of performance. Receptor status was initially identified by multiple reviewers and confirmed by a single expert. BC‐specific information was ascertained from the Mayo Clinic Tumor Registry, a hospital‐based cancer registry manually curated by the Mayo Clinic Cancer Center. Structured and unstructured (i.e., clinical notes) EHR data augmented the cancer registry to ascertain BC receptor status. Specifically, rule‐based informatics phenotyping methodologies were utilized to extract structured clinical data from the enterprise data trust (EDT), 9 a local data warehouse, and unstructured clinical and pathology notes as free text from the local EHR. Phenotyping details are described in‐depth below. PMPs of receptor status were compared by perspective of prescribed medications, clinical narratives, or both.
Figure 1
Overview of cohorts and pseudo code. A series of steps were taken to develop the breast cancer precision medicine phenotype; these included: 1) Data preprocessing, where data were extracted from both structured and unstructured (i.e., free text notes) electronic health record (EHR) data sources and linked with cancer registry data; 2) Ascertainment of receptor status from multiple EHR perspectives, initiated with extraction of necessary data features and subsequently attributed via a series of rules; 3) Development of a Gold Standard Cohort, consisting of patients manually chart reviewed and annotated for receptor status, to evaluate performance of the EHR rule‐based algorithm; and 4) Perspectives and methodologies utilized to evaluate performance of the EHR rule‐based algorithm.
Prescribed medication perspective
The prescribed medication perspective focused on identifying receptor status based on targeted medication utilization. Our corpus of prescribed medication data was gathered from structured medication orders in EDT 9 and clinical notes. The medication extraction and normalization system MedXN 10 was linked to a copy of the EHR to extract medication prescription information contained within clinical notes. Receptor status was ascertained from prescribed medication by brand and generic drug names, hand‐curated by experts, targeting that specific receptor. ER+ patients were identified by prescriptions for antiestrogen therapy, aromatase inhibitors (AIs), or selective estrogen response modulators (SERMs). AIs included exemestane (Aromasin), anastrozole (Arimidex), or letrozole (Femara) prescriptions. SERM prescriptions included: tamoxifen (Nolvadex, Soltamox) or raloxifene (Evista, Keoxifene). HER2‐positive patients are commonly prescribed trastuzumab (Herceptin) and were identified accordingly. Using these perspectives, a patient was classified as positive for one or more receptor status if corresponding neoadjuvant or adjuvant therapies were prescribed. Similarly, patients were classified as negative for a particular receptor status if medication prescription information was available for a patient, but no prescriptions for a corresponding targeted therapy were identified.
Clinical narrative perspective
The clinical narrative perspective focused on elucidating BC receptor status based on the unstructured text dialog contained within the patient's EHR. Specifically, clinical diagnosis of receptor status was ascertained from the clinical narrative, clinical notes, and pathology reports, via a series of rule‐based NLP algorithms as follows: Pattern‐based information extraction functionality was utilized to extract receptor information from the clinical narrative using the open‐source pipeline MedTagger. 11 , 12 Development of the NLP algorithm included training and testing phases. Again, to ensure adequate power for each subtype, the "training" subgroup included approximately half of the total (n = 871) patients within the Gold Standard Cohort. The training subgroup included 10,182 (median: 18; range: 1–290) clinical notes and 9,077 (median: 20; range: 2–113) pathology reports. Training of the algorithm initiated using full receptor names (i.e., estrogen, progesterone, epidermal growth factor receptor 2) and standard abbreviations (e.g., ER, PR, HER2). Training results were recursively compared with the Gold Standard Cohort training subgroup to identify the most appropriate regular expression patterns. Algorithm training ceased when further manipulation of regular expression patterns did not improve the recall (i.e., sensitivity), or true negative rate (i.e., specificity). Following generation of the trained NLP algorithm, additions were made to accommodate negation (i.e., surrounding text that designates the opposite of the object) of the receptor status.
Many instances of receptor status‐positive or ‐negative were identified per patient following deployment of the NLP BC receptor status algorithm. Commonly, a patient would have some level of "chatter" between individual receptor status‐positive or ‐negative. Eventually, this chatter would resolve into a definitive clinical diagnosis. Anecdotally, chatter within clinical notes was an artifact of the diagnostic process and not necessarily reflective of clinical disagreement. For example, chatter may reflect discordance in receptor status between the diagnostic biopsy and the final surgical specimen. Following optimization, we applied a minimum threshold of 60% agreement of receptor status‐positive or ‐negative to resolve individual receptor statuses and minimize false‐negative or false‐positive receptor status ascertainment. Patients below this threshold were designated as nonresolvable (or "nr"), requiring manual chart review to resolve and removed from our analysis.
Evaluation
The Gold Standard Cohort "testing" subgroup was utilized to elucidate algorithm performance by clinical data source. Comparisons of performance were made between the Observation Cohort and the Gold Standard Cohort testing subgroup. Performance was evaluated by phenotype perspectives of individual and combinations of EHR clinical data sources. A family of measures characterized performance, including: precision (P), recall (R), and harmonic mean of precision and recall (F
1
RESULTS
Gold Standard Cohort
Overall, within the Gold Standard Cohort receptor status was unavailable for a proportion of patients during manual chart review (Table 1 ). Likely a reflection of clinical practice standards at this facility, only a small minority of patients were missing data for ER and PR, with the missing percentage varying little over time. The majority of absent data was for HER2 status, consistent with the clinically heterogeneous adoption of HER2‐directed therapy 14 over the course of the study period (1998–2011). As expected for this study period, HER2 status was unavailable for the majority of patients (75.1%). A detailed list of receptor status coverage ascertained for the Gold Standard Cohort can be found in Table 1 .
Table 1
Gold Standard Cohort receptor status
ER
ER = estrogen receptor, PR = progesterone receptor, HER2 = human epidermal growth factor receptor, TN = triple negative, (+) = status positive, (–) = status negative, na = not available.
Observation cohort
After removing patients contained within the Gold Standard Cohort, our "Observation Cohort" consisted of patients (n = 12,291) who were phenotyped using EHR data and did not undergo manual review. The median age at BC diagnosis was 57 years (range 18–98); 37.9% of the patients were Stage III–IV. Patient records were utilized to evaluate two fundamentals of EHR phenotyping: i) data reliability via clinical attribute coverage of a patient cohort by EHR perspectives, and ii) performance measurement (precision, recall, and F
1
score) of the EHR perspectives. Coverage and performance are important characteristics of robust precision phenotypes: development of approaches that maximize data ascertainment and return accurate, reliable predictions are imperative.
Coverage by clinical data source
In our application, cohort coverage refers to patients within the Observation Cohort having sufficient EHR data to ascertain a phenotype via the specified EHR perspective (i.e., some level of necessary data elements were present). Similarly, individual receptor status coverage (e.g., ER, PR, HER2, and TNBC) represents the ability to ascertain positive or negative status for that particular receptor status via an EHR perspective. Observation Cohort coverage (i.e., available EHR data) ranged from 63–92% (Table 2 ) by individual data sources. First, within the prescribed medications perspective, Observation Cohort coverage based on structured data alone was unexpectedly low, at 69.1%, potentially an artifact of legacy system integration. Inclusion of clinical notes to EDT, prescribed medications perspectives increased Observation Cohort coverage to 77.1% from 69.1%. Second, the clinical narrative perspective, ascertained via the rule‐based NLP algorithm using both clinical and pathology notes, increased Observation Cohort coverage to 92.1% (∼30% increase when compared with a baseline of structured medication information alone of 69.1%). When all data sources were utilized, representing the combined prescribed medication and clinical narrative perspectives, Observation Cohort coverage exceeded 96%.
Table 2
Coverage of individual EHR data sources and phenotypes
Prescribed medications perspective
Open in a separate window
Receptor status phenotype coverage by clinical data source Note: total cohort size n = 12,770; cohort coverage refers to coverage of that clinical data source out of the total cohort size. ER = estrogen receptor, PR = progesterone receptor, HER2 = human epidermal growth factor receptor 2, TNBC = triple negative, nr = true missing or unable to resolve status; % of patients with relevant EHR data source coverage for individual receptor status phenotypes.
The following observations and posited justifications were ascertained from the coverage analysis (Table 2 ): i) Overall, ER+ feature coverage was low (between 42.6% and 45.6%) for prescribed medications perspectives, the maximum ability to identify positive cases, while having a concerning 69.1–77.1% data source coverage. Approximately 70% of women diagnosed with BC are known to be ER+. 3 Alternatively, our developed NLP methodology identified between 68.8% and 74.5% women as being ER+. Further, utilizing all EHR data sources (including prescribed medications and clinical narrative perspectives) we identified 73.8% of our cohort as being ER+, with 96.2% EHR data source coverage. ii) PR was not resolvable via prescribed medication perspective: Treatment decisions are typically made by ER status alone. The developed NLP methodology was needed to identify PR+ cases from the clinical narrative perspective, where between 52% to 64% were identified as PR‐positive. iii) The clinical narrative perspective was needed to resolve HER2 status: HER2 status had satisfactory coverage utilizing our developed NLP methodology within clinical or pathology note clinical narratives, implying the signal was contained within the EHR. However, medication order data sources were ineffective at identifying HER2 status, suggestive of potential nomenclature (e.g., combination therapy acronyms) or data source (e.g., intravenous vs. oral delivery route) issues. Further, iv) TNBC was unreliable due to poor coverage: Reliable TNBC status could only be ascertained from clinical narrative perspectives, potentially due to the limited reliability of status negation by the medication orders data sources. We identified between 5.9% and 8.4% of patients as TNBC utilizing NLP to ascertain receptor status directly from the clinical narrative, which is close to the anticipated TNBC prevalence. v) Overall, alternative perspectives helped resolve our PMPs. For individual receptor status coverage, alternative perspectives, or combinations of perspectives helped to resolve precision phenotypes. vi) The ability to disambiguate between sparse EHR data and lack of prescription is critical when assigning a targeted therapy status based on prescribed therapeutics. In future work, inverse weighting by measures of data sparseness are recommended.
Performance evaluation in Gold Standard Cohort
For algorithm performance, receptor status phenotype precision measures were higher in clinical narrative perspective and lower in the prescribed medication perspective (Table 3 ). We found ER status to be a reliable precision medicine phenotype when all data sources were utilized in our Gold Standard Cohort: Ascertainment of ER+ status using all available data sources was excellent (P = 0.98, R = 0.99, F = 0.98) and provided very high coverage (96.2%). However, negation performance was noticeably lower (P = 0.97, R = 0.61, F = 0.75) for prescribed medications as opposed to clinical narrative (P = 0.99, R = 0.98, F = 0.99) perspectives. Further, we found PR precision medicine phenotypes to be reliable when NLP of clinical narratives was utilized: Using the perspective of the clinical narrative, PR performance was very high (P = 0.99, R = 0.94, F = 0.96). An important note: While coverage may be conditionally independent of performance, specifically precision, increases in overall cohort coverage are linked to increased R and F scores (i.e., the ability to negate an individual receptor status).
Table 3
Receptor status phenotype performance compared within manual chart reviewed Gold Standard Cohort—testing subset
Prescribed medications
Open in a separate window
All comparisons made to "gold standard" validation cohort (n = 435); P = precision, R = recall, F = F1 score (harmonic mean of precision and recall; ER = estrogen receptor, PR = progesterone receptor, HER2 = human epidermal growth factor receptor 2, TN = triple negative. PR and TN are blank because they cannot be directly inferred from a prescribed medications perspective.
While HER2 (P = 0.70, R = 0.67, F = 0.68) and TNBC (P = 0.72, R = 0.68, F = 0.70) status performances were acceptable, they remained noticeably poorer than ER or PR status. Related, we found the HER2 precision medicine phenotype to be unreliable and complicated by both defining and resolving applicable synonyms. Reducing the observation period, controlling for potential bias due to treatment advancements, to contain the years 2008 through 2011, only slightly enhanced algorithm performance (P = 0.58, R = 0.78, F = 0.67 and P = 0.75, R = 0.86, F = 0.80). The many combinations and synonyms for HER2 status (e.g., HER2/neu, HER2) complicated training. Complications and data quality issues arising from lack of standardization are likely to be encountered with other precision medicine therapeutic targets, data sources, and synonymous naming conventions.
PR and TNBC status could not be ascertained utilizing medication orders due to low coverage. However, NLP could identify most cases of either status from clinical notes. To control for potential confounding by indication (antiestrogen therapies target reactions relevant to both ER and PR status) during performance evaluation, we collapsed ER and PR status within the Gold Standard Cohort. While ER+ receptor status had a very high predictive power utilizing medication orders, the recall and the harmonic mean (F
1
score) remained extremely poor. Further complicating these efforts was the limited cohort coverage (77%) encountered via medication orders. Negation of any receptor status remained poor; negative receptor status was not reliably ascertained utilizing medication orders. Due to the likely nonrandom gaps in coverage, strong potential exists to introduce sampling bias. Future work should include evaluations of data source coverage and density to control for these potential biases.
DISCUSSION
As precision medicine matures, incorporating EHR‐driven precision medicine phenotypes into clinical pharmacology research endeavors will become increasingly important. Further, the need for informatics phenotyping approaches to develop PMPs will continue to grow.
Precision medicine treatment opportunities for BC are relatively advanced compared with other cancer sites, with mature utilization of targeted therapies. However, other cancer types, including diffuse large B‐cell lymphoma, non‐small cell lung cancer, and multiple myeloma, are rapidly increasing targeted therapy offerings. Further, potential to target alterations in select cancer pathways with therapeutics, regardless of the cancer type, remain on the near horizon. 15 The clinical reality of precision medicine will become increasingly complex, requiring disciplined application of informatics approaches to ascertain robust precision phenotypes. While we demonstrated potential utility as a research application, rigorous evaluation and expert clinician input is needed prior to implementation considerations for clinical practice applications.
Caveats of precision medicine phenotypes
While related, the concepts molecular subtype, receptor status, and precision phenotype referenced throughout this article have nuanced differences. Molecular subtype refers to altered (i.e., increased) molecular expression of a variant or wildtype gene product of known etiological significance. In BC, immunohistochemistry staining of a biopsy is performed and a pathologist establishes receptor status in accordance with the established clinical guideline. 16 While receptor status and treatment with the corresponding targeted therapy are typically congruent, in some instances a patient may be positive (or borderline) for multiple receptors types. When ambiguity exists, a tumor board, consisting of relevant oncology expertise, will convene to develop a diagnosis and assign a corresponding treatment plan. However, these nuances are likely to be unclear, or lost as noise, from the perspective of naïve phenotyping algorithm. In these instances, the precision phenotype might represent either the most clinically relevant receptor status/molecular subtype or to which targeted therapy was first prescribed, or potentially something more nuanced in between. Anecdotally, we identified a certain level of "noise," occurring primarily during the diagnostic process, while training the NLP algorithm within the clinical narrative. Our rule‐based approach was iteratively validated to resolve receptor status variation across the clinical dialog data sources at the specified threshold. While validated in our algorithm for appropriateness, inappropriate selection of this threshold could potentially impact the receptor status phenotype and introduce bias—validation is required prior to adjustments. Appropriate representation of the varying EHR perspectives (and corresponding source data integrity) is a critical design consideration. Related, more comprehensive incorporation of clinical acronyms for combination therapies also holds potential to enhance these perspectives. 17
Insights from our BC precision medicine phenotype
In our study, clinical or pathology notes alone or together provided the broadest cohort coverage and clinical notes alone provided the most precise measure of receptor status. The pathology note data source outperformed the clinical note data source for TNBC status. Utilizing all EHR data sources provided the largest cohort coverage. High coverage was accompanied by some of the highest individual receptor status performance by every clinical data source. Augmenting structured EHR data with NLP increased coverage and performance for BC PMP for both case identification and negation at our institution. We increased data coverage when NLP of EHR notes augmented the structured EHR data. Structured data alone might be insufficient to ascertain a robust precision medicine phenotype when complete patient data coverage is unavailable. Overall, our NLP approaches increased coverage, with comparable results for applications in chronic conditions. 18 As we demonstrated, balancing a combination of perspectives was necessary to ascertain a high‐quality EHR‐derived phenotype.
Overcoming potential gaps in EHR data coverage
Despite gains with NLP, negation remained problematic throughout our study, introducing potential for bias. Specifically, identifying true negation of a targeted therapy status was problematic when coverage for a patient's EHR data was poor—improvements are ongoing. When longitudinal drug exposures can be captured with a high level of coverage, we posit that negation will become more reliable. Minimizing potential bias begins by ensuring data integrity within local research data warehouses. In our study, the EDT, our local data warehouse, was documented to represent all clinical data from the local EHR. 9 However, documentation available to the researchers regarding handling of data from legacy EHR systems and maintenance/integrity was incomplete—a potential study limitation. This can be particularly problematic when certain therapeutics (e.g., trastuzumab treatment for HER2+ breast cancer vs. anastrozole for ER+ breast cancer) have varying administration routes (infusion vs. oral) and corresponding data will be captured via different clinical workflows. The integration of external data into the EHR perspective would allow for a precision phenotype to include the perspective of filled medication orders—integration of insurance claims perspectives hold great promise to overcome biases resulting from incomplete coverage. For example, in cases where longitudinal EHR coverage is initially poor, augmenting the EHR perspective with that of filled medication orders from insurance claims, to represent care received at external healthcare facilities, might increase PMP performance. Conversely, in instances where a patient is known to have near complete coverage in the EHR, performance of that phenotype will potentially be enhanced, with increased reliability. Finally, novel computational phenotyping approaches are likely needed to account for data coverage within a patient's EHR compared with patients with similar disease states and drug exposures.
Informatics opportunities
We posit informatics approaches will be critical to ascertaining PMPs in an evolving landscape of molecular targets and corresponding therapeutic agents: First, clinical utilization of evolving molecular subtypes and newly discovered targeted therapies will likely outpace mandates for annotation of applicable clinical data elements in population research registries. For example, national organizations such as the Surveillance, Epidemiology, and End Results Program (SEER) mandate collection of only specific clinical data elements. While curating these registries is of tremendous value, gaps in data (or clinical data elements of insufficient granularity) needed to define molecular subtypes and identify targeted therapy utilization may exist. Informatics approaches offer the opportunity to resolve granularity gaps with the wealth of clinical knowledge contained within the EHR. Second, curating these cancer registries commonly rely on resource‐intensive manual abstraction. As informatics‐based phenotyping advances, future opportunities to develop automated or semiautomated data annotation procedures might exist. Third, while rule‐based algorithms might sufficiently phenotype most cases, in certain instances medical oncology and pathology diagnostic decisions might remain too "unclear," from an informatics perspective, for an EHR phenotyping algorithm to disambiguate case status. In retrospective research, extraction processes that refer select "unclear" cases for expert clinical review might provide an optimal means to prioritize the manual annotation efforts needed for registry inclusion. Rule‐based approaches may not be readily scalable "out of the box"; advancements in deep‐learning techniques are needed to ensure feature extraction adaptable to nuances contained within multiple care delivery perspectives. Fourth, prescribed medications, medical oncology, and pathology data sources provide unique perspectives that might reflect certain aspects of clinical reality. While different clinical perspectives are certainly valuable for research applications and conceptually scalable to clinical informatics applications, they should not inappropriately offer a prescriptive perspective for guiding clinical care decisions. Specifically, based on observed performance and limited ability (i.e., unreliability) to negate or identify a "negative" status, improvements are needed prior to implementation of a similar algorithm. Close clinical collaboration and validation is needed prior to consideration of phenotyping algorithms for clinical care applications.
In the future, the complexity of informatics approaches needed to ascertain precision phenotypes will likely vary between simple rule‐based and novel computational approaches. Formally trained informaticians can help guide application and development of the methodologies needed to ascertain robust precision phenotypes. Cancer subtypes, such as TNBC and basal‐like subtypes, are frequently heterogeneous, 19 and characterized using multiple types of data, complicating potential EHR phenotyping. In research endeavors, phenotyping approaches that span genetic, transcriptional, histological, 20 and clinical features will likely be necessary to resolve these heterogeneous cases, 21 which hold potential to uncover novel biology. 22
Cancer registries are a particularly robust resource for personalized medicine discovery. Augmenting cancer registry data with clinical data elements from the local EHR offers a profound opportunity to gain knowledge that may have previously been hidden. For certain cancer registries, additional biological specimens are collected (e.g., tumor slides and blocks, germline DNA, somatic tumor DNA, plasma, serum) that correspond to the clinical intervention. This enables pursuit of translational bioinformatics research endeavors spanning the richness of information contained within both the local EHR and biological specimens. 23 Indeed, development of robust precision medicine phenotypes is critical for translational bioinformatics to empower pursuit of clinical pharmacology knowledge.
CONCLUSION
As precision medicine phenotypes grow increasingly complex in the era of precision medicine, nuanced informatics applications that account for multiple EHR perspectives are needed. A thorough understanding of EHR data source perspectives, data source coverage, and potential for bias are imperative to the development of robust precision medicine phenotypes.
Conflict of Interest/Disclosure
The authors declared no conflicts of interest.
Acknowledgments
This work was supported by the National Cancer Institute‐sponsored Mayo Clinic Cancer Genetic Epidemiology Training Program (R25 CA092049). The authors thank James R. Cerhan, MD, PhD, for the substantial editorial feedback provided in the development of this article. Further, the researchers thank the nurse abstraction group led by Wendy Gay for their contributions to chart review and cohort integrity assurance, and Xiaoyang Ruan, PhD, for assistance in deployment of natural language processing algorithms.
Author Contributions
M.K.B., H.L., K.N.M., J.P., and R.Z. wrote the article; M.K.B. and R.Z. designed the research; M.K.B. performed the research; M.K.B. analyzed the data; M.K.B., H.L., and J.P. contributed new reagents/analytical tools.
References
1. Harris L.N. et al Use of biomarkers to guide decisions on adjuvant systemic therapy for women with early‐stage invasive breast cancer: American Society of Clinical Oncology clinical practice guideline. J. Clin. Oncol. 34(10), 1134–1150 (2016). [ PubMed ]
2. Carels N., Spinassé L.B., Tilli T.M. & Tuszynski J.A. Toward precision medicine of breast cancer. Theor. Biol. Med. Model. 13(1), 7 (2016). [ PubMed ]
3. Howlader N. et al US incidence of breast cancer subtypes defined by joint hormone receptor and HER2 status. J. Natl. Cancer Inst. 106(5), dju055 (2014). [ PubMed ]
4. Abernethy A.P. et al Rapid‐learning system for cancer care. J. Clin. Oncol. 28(27), 4268–4274 (2010). [ PubMed ]
5. Gottesman O. et al The electronic medical records and genomics (eMERGE) network: past, present, and future. Genet. Med. 15(10), 761. [ PubMed ]
6. Buckley J.M. et al The feasibility of using natural language processing to extract clinical information from breast pathology reports. J. Pathol. Informat. 3, 23 (2012). [ PMC free article ] [ PubMed ]
7. Yala A. et al Using machine learning to parse breast pathology reports. Breast Cancer Res. Treat. 161(2), 203–211 (2017). [ PubMed ]
8. Wei W.Q. & Denny J.C. Extracting research‐quality phenotypes from electronic health records to support precision medicine. Genome Med. 7(1), 41 (2015). [ PubMed ]
9. Chute C.G., Beck S.A., Fisk T.B. & Mohr D.N. The enterprise data trust at Mayo Clinic: A semantically integrated warehouse of biomedical data. J. Am. Med. Inform. Assoc. 17(2), 131–135 (2010). [ PubMed ]
10. Sohn S., Clark C., Halgrim S.R., Murphy S.P., Chute C.G. & Liu H. MedXN: An open source medication extraction and normalization tool for clinical text. J. Am. Med. Inform. Assoc. 21(5), 858–865 (2014). [ PubMed ]
11. Torii M., Wagholikar K. & Liu H. Using machine learning for concept extraction on clinical documents from multiple data sources. J. Am. Med. Inform. Assoc. 18(5), 580–587 (2011). [ PubMed ]
12. Liu H. et al An information extraction framework for cohort identification using electronic health records. AMIA Summits Transl. Sci. Proc. 2013, 149 (2013). [ PubMed ]
13. Goutte C. & Gaussier E. A probabilistic interpretation of precision, recall and F‐score, with implication for evaluation. Eur. Conf. Inform. Retriev. Res. 2005, 345–359 (2005).
14. Nahta R.E. & Esteva F.J. Trastuzumab: triumphs and tribulations. Oncogene. 26(25), 3637–3643 (2007). [ PubMed ]
15. Renfro L.A., Mallick H., An M.W., Sargent D.J. & Mandrekar S.J. Clinical trial designs incorporating predictive biomarkers. Cancer Treat. Rev. 43, 74–82 (2016). [ PubMed ]
16. Hammond M.E.H. et al American Society of Clinical Oncology/College of American Pathologists guideline recommendations for immunohistochemical testing of estrogen and progesterone receptors in breast cancer. J. Clin. Oncol. 28(16), 2784–2795 (2010). [ PubMed ]
17. Warner J.L., Cowan A.J., Hall A.C. & Yang P.C. HemOnc.org: A collaborative online knowledge platform for oncology professionals. J. Oncol. Pract. 11(3), e336–e350 (2015). [ PubMed ]
18. Carroll R.J. et al Portability of an algorithm to identify rheumatoid arthritis in electronic health records. J. Am. Med. Inform. Assoc. 19(e1), e162–e169 (2012). [ PubMed ]
19. Rakha E.A., Reis‐Filho J.S. & Ellis I.O. Basal‐like breast cancer: A critical review. J. Clin. Oncol. 26(15), 2568–2581 (2008). [ PubMed ]
20. Kothari S., Phan J.H., Stokes T.H. & Wang M.D. Pathology imaging informatics for quantitative analysis of whole‐slide images. J. Am. Med. Inform. Assoc. 20(6), 1099–1108 (2013). [ PubMed ]
21. Pareja F., Geyer F.C., Marchiò C., Burke K.A., Weigelt B., Filho Reis‐&, J.S. Triple‐negative breast cancer: the importance of molecular and histologic subtyping, and recognition of low‐grade variants. Breast Cancer. 2, 16036 (2016). [ PubMed ]
22. Ritchie M.D., Holzinger E.R., Li R., Pendergrass S.A. & Kim D. Methods of integrating data to uncover genotype‐phenotype interactions. Nat. Rev. Genet. 16(2), 85–97 (2015). [ PubMed ]
23. Tenenbaum J.D. et al An informatics research agenda to support precision medicine: seven key areas. J. Am. Med. Inform. Assoc. 23(4), 791–795 (2016). [ PubMed ]
Articles from Clinical and Translational Science are provided here courtesy of Wiley-Blackwell
Formats:
Division of Biomedical Informatics, School of Medicine, University of California San Diego La Jolla, CA, USA.
Abstract
The database of Genotypes and Phenotypes (dbGaP) is archiving the results of different Genome Wide Association Studies (GWAS). dbGaP has a multitude of phenotype variables, but they are not harmonized across studies. We proposed a method to standardize phenotype variables by classifying similar variables based on semantic distances. We first extracted variables description, enriched them using domain knowledge, and computed the distances among them. We used clustering techniques to classify the most similar variables. We used domain experts to audit clusters, annotated the clusters with appropriate labels, and used re-clustering to build a semantically-driven Genotypes and Phenotypes (sdGaP) ontology using the UMLS semantic network and metathesaurus. The sdGaP ontology allowed us to expand user queries and retrieve information using a semantic metric called density measure (DM). We illustrated the potential improvement of information retrieval using the sdGaP ontology in one search scenario using the variables from the Cleveland Family Study.
PMID:
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA. Electronic address: wang.yanshan@mayo.edu.
2
Department of Medical Informatics & Clinical Epidemiology, Oregon Health and Science University, Portland, OR, USA. Electronic address: wst@ohsu.edu.
3
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA. Electronic address: li.dingcheng@mayo.edu.
4
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA. Electronic address: mehrabi.saeed@mayo.edu.
5
Department of Health Sciences Research, Mayo Clinic, Rochester, MN, USA. Electronic address: liu.hongfang@mayo.edu.
Abstract
In the era of digitalization, information retrieval (IR), which retrieves and ranks documents from large collections according to users' search queries, has been popularly applied in the biomedical domain. Building patient cohorts using electronic health records (EHRs) and searching literature for topics of interest are some IR use cases. Meanwhile, natural language processing (NLP), such as tokenization or Part-Of-Speech (POS) tagging, has been developed for processing clinical documents or biomedical literature. We hypothesize that NLP can be incorporated into IR to strengthen the conventional IR models. In this study, we propose two NLP-empowered IR models, POS-BoW and POS-MRF, which incorporate automatic POS-based term weighting schemes into bag-of-word (BoW) and Markov Random Field (MRF) IR models, respectively. In the proposed models, the POS-based term weights are iteratively calculated by utilizing a cyclic coordinate method where golden section line search algorithm is applied along each coordinate to optimize the objective function defined by mean average precision (MAP). In the empirical experiments, we used the data sets from the Medical Records track in Text REtrieval Conference (TREC) 2011 and 2012 and the Genomics track in TREC 2004. The evaluation on TREC 2011 and 2012 Medical Records tracks shows that, for the POS-BoW models, the mean improvement rates for IR evaluation metrics, MAP, bpref, and P@10, are 10.88%, 4.54%, and 3.82%, compared to the BoW models; and for the POS-MRF models, these rates are 13.59%, 8.20%, and 8.78%, compared to the MRF models. Additionally, we experimentally verify that the proposed weighting approach is superior to the simple heuristic and frequency based weighting approaches, and validate our POS category selection. Using the optimal weights calculated in this experiment, we tested the proposed models on the TREC 2004 Genomics track and obtained average of 8.63% and 10.04% improvement rates for POS-BoW and POS-MRF, respectively. These significant improvements verify the effectiveness of leveraging POS tagging for biomedical IR tasks.
KEYWORDS:
Bag-of-word; Biomedical information retrieval; Markov random field; Natural language processing; Part-Of-Speech
PMID:
Generate a file for use with external citation management software.
Create File
Phenotype algorithm for identifying type 2 diabetes mellitus (T2DM)...
For full functionality of ResearchGate it is necessary to enable JavaScript.             Here are the instructions how to enable JavaScript in your web browser .
Figure 1 : Phenotype algorithm for identifying type 2 diabetes mellitus (T2DM) from electronic medical records (EMR or EHR). T1DM: type 1 diabetes mellitus; Dx: diagnoses, defined as recorded using International Classification of Diseases, 9th Revision (ICD-9) codes; med: medication; physcn: physicians; Rx: prescriptions. More details can be found in the appendix and on PheKB.org. 
Download
Context
Electronic health records (EHRs) are increasingly used for clinical and translational research through the creation of phenotype algorithms , consisting of structured selection criteria designed to produce re- search-quality phenotypes. 1–7 These algorithms operate on diverse classes of EHR data to select individuals with given traits (e.g., identifying records for continuous trait analyses or marking records as a case, a control, or neither for given conditions). 8,9 Examples include identifying patients with hypothyroidism matched to hypothyroidism- free controls, 5 evaluating cardiac conduction duration in electrocardiograms of "heart-healthy" individuals, 10 and determining medication responses. 11–18 Typically, these algorithms define the workflow for querying clinical data regarding diagnoses, procedures, medications, laboratory or radiology reports, and other EHR data, and can require natural language processing (NLP) or text mining. Multi-site studies have shown that these algorithms often are portable between sites. 5,19,20 Currently, most phenotype algorithms are recorded as human-readable descriptive text documents that can be shared via knowledge bases such as the Phenotype KnowledgeBase (PheKB,   ) and PhenotypePortal (). Algorithms described via text and flowcharts (such as the type 2 diabetes mellitus [T2DM] algorithm shown in Figure 1 and the Desiderata section) require human translation to computable formats and are often ambiguous. Implementation across different institutions requires human experts to interpret the algorithm and translate it into executable operations and queries. This situation has hampered cross- institutional collaboration. 21 To enable cross-site phenotype execution, we suggest two needed initiatives: (1) creation of a common phenotype representation model (PheRM) as a computable representation of phenotype algorithms and (2) development of infrastructure to allow standards-based authoring and execution of PheRM-based algorithms for a variety of EHR systems. In this paper, we leveraged our experiences with the Electronic Medical Records and Genomics (eMERGE) Network, 22 Pharmacogenomics Research Network (PGRN), 23 Strategic Health IT Advanced Research Project (SHARP), 24 and the National Patient-Centered Clinical Research Network (PCORnet) 25 to propose desiderata for PheRM (Table 1). With the implementation of Meaningful Use (MU), EHRs have been increasing in ubiquity, functionality, and comprehensiveness. One recent advance has been the coupling of DNA bio-repositories to EHR data 27–30 to enable genomic discoveries. 31 In particular, the eMERGE network, a large scale, multi-site network of research organizations of 11 academic medical centers, has been at the forefront of mining biobank resources (both EHRs and associated DNA samples) for genomic medicine. Identification of research subjects from patient populations using phenotype algorithms is the starting point for these projects. Data components in phenotyping may include the full range of clinical data stored in the EHR, such as demographics, vital signs, laboratory tests, medication, diagnoses, procedures, and other documentation. 32 However, each EHR can have a different data model. One approach to facilitate research interoperability among different sites has been the Observational Health Data Sciences and Informatics (OHDSI) program, which has built on the Observational Medical Outcomes Partnership (OMOP) common data model (CDM). 33–37 This CDM provides a standardized data interface for a vibrant ecosystem of healthcare big- data analyses ( including tools, web applications, and application program interfaces. Similarly, PCORnet 25 and the Informatics for Integrating Biology and the Bedside (i2b2) based Shared Health Research Information Network 38,39 are advancing common data models among their groups. These CDMs typically cover more focused, common data elements to enable a broad range of queries. Phenotype algorithms are typically developed in an iterative fashion with expert review for validation 21 to rule-based models, but can also utilize machine learning methods. 40–42 The efficacy of a phenotype algorithm is usually measured with information retrieval metrics, such as sensitivity, specificity, positive predictive value, and F-measure. At present, most existing phenotype algorithms are expressed in pseudo-code and not directly executable, because there is no widely adopted standards and underlying data structures. Thus, implementation requires human experts to translate descriptive algorithms from documents to ad hoc queries in local EHR research repositories, a process which is prone to inconsistencies or errors. 43 One of the major efforts in establishing a standard language for a related task is the Quality Data Model (QDM) from National Quality Forum, which has been designed to represent electronic clinical quality measures. 44 QDM has been shown to be capable of representing many phenotype algorithms from PheKB. 45,46 Systems such as i2b2 system, 47 SHARP, 24 and Eureka! Clinical Analytics 48 all have internal data and algorithm representations, some of which may be shared across sites. In general, these systems provide graphical interfaces that can standardize queries, but complex scoring metrics, counting rules, and nested temporal references or sequencing of events—such as found in many eMERGE algorithms 49 —often exceed their capability. Phenotype algorithms have adopted a variety of logical and computational modalities. 9,50 Modalities (e.g., scoring rules, counting rules) adopted in clinical diagnostic criteria 51–53 have potential application in phenotype algorithms. In addition, machine learning and statistical model-based phenotype algorithms have been increasingly reported. 54–57 Most current phenotype algorithms (CPT) use both structured and unstructured EHR elements. 9 Structured EHR data usually include demo- graphic information (e.g., age, sex, race, death), billing codes (i.e., International Classification of Disease version 9 (ICD-9), Current Procedural Terminology), most laboratory tests, vital signs, medications, and more. Unstructured EHR elements usually include clinical notes (e.g., history and physical examinations, progress notes, discharge summaries, nursing notes), some non-billing medical problems and most family history elements, some medications records and refills, diagnostic reports (e.g., radiology, microbiology, pathology), and more. A group of clinicians and informaticians reviewed 21 eMERGE phenotype algorithms (Table 2) and several authoring tools (Measure Authoring Tool [www.emeasuretool.cms.gov], i2b2, Eureka!, PhenotypePortal, the Vanderbilt Synthetic Derivative, 27 and the Marshfield Personalized Medicine Research Project interface 58 ) for common features. These phenotyping algorithms were of different complexity and included both disease and drug response phenotypes using algorithms from the eMERGE 22 and Pharmacogenomics of Very Large Populations (PGPop) networks. We also evaluated the ability to represent selected well-known diagnostic criteria (e.g., Duke criteria for infective endocarditis, 52 CHADS2 criteria for anticoagulation therapy in atrial fibrillation (AF) 51 ) as potential phenotypes (see Supplementary Appendix Part 2). After proposal by a smaller team of investigators, the desiderata were evaluated and refined by all authors, which included investigators from eMERGE, PGRN, PGPop, SHARPn, PCORNet, and HMO Research Network. Based on our review, we propose the following desiderata for PheRM and its software implementation (see Figure 2 and Table 1). We ac- knowledge that phenotyping is not a standalone practice, and, instead, is closely coupled with EHR infrastructure and clinical practice. Therefore, we have included recommendations (representing the phenotyping community) to the EHR development community (Desiderata 1 and 2) as well as those regarding PheRM itself (Desiderata 3–10). Clinical data are practically structured to promote efficient queries of all clinical information for an individual patient. On the other hand, phenotyping requires population-wide searching of individuals with similar characteristics (e.g., elevated LDL for a hyperlipidemia phenotype). Relational databases have been widely used for data storage as parts of enterprise data warehouse solutions. To further facilitate querying, where possible, clinical data stored in such data warehouses should be atomized (as first normal form 59 ), such as storing a blood pressure into a systolic reading and a diastolic reading. Precalculating commonly derived observations (e.g., periods of drug exposure, as implemented in the OMOP drug era model 33–37 ) also facilitate more efficient querying. Currently available documents are mostly poorly structured, and require information extraction or indexing 60 to make them queryable. To achieve a common PheRM, a common EHR data representation should be implemented where possible. Huser and Cimino analyzed three public integrated data repositories (IDRs) and proposed desiderata for their common design patterns. 61 Potential candidates for CDM include Clinical Information Modeling Initiative, 62 Mini-Sentinel Common Data Model (recommended by US Food and Drug Administration, www.mini-sentinel.org), i2b2 Star Schema, 63 and OMOP CDM. 33,35,36,64 Additionally, the Institute of Medicine has re- cently initiated an effort to standardize structured capture of social and behavioral domains in the EHR. 65 EHR implementations and systems are heterogeneous, and CDMs must have the flexibility to adapt to a variety of institutional IDRs. One challenge in standardization is labeling and referencing of specific document types, and many EHR sites may have specific but nonstan- dard documents that address a particular question. 66 Custom approaches can generically circumnavigate this limitation. For example, the colon polyp phenotype in the eMERGE network 67 used colonoscopy surgical and pathology reports, which are not yet labeled in a standard manner or mapped to CDMs in most of the IDR systems in the network. This algorithm separates the implementation into trans- portable tasks (e.g., concept extraction through NLP, grouping, extraction of covariates) implemented as a fully executable Konstanz Information Miner (KNIME) package with institutional adaptation tasks (i.e., database querying for the proper document types). Creating a portable infrastructure that implements the algorithmic rules and thus only requires the user to build the "last mile" of the solution can accel- erate algorithm implementation across other sites. The investigators and initiators for most phenotype projects are clinical experts, epidemiologists, geneticists, and other subject matter experts. As important communication tools among researchers of different expertise, the phenotype representations should support a human- readable format or transformation for clinical experts to ensure medical accuracy. Additionally, phenotype algorithms should include clear scientific and clinical definitions to enable creation of gold standards for evaluation and to facilitate reuse. For example, one algorithm may allow any cause of hypothyroidism when evaluating treatment efficacy while another may focus on only on primary autoimmune hypothyroidism when evaluating genetic causes. It is strongly preferable that the human-readable format and computable formats be computationally coupled such that one can be automatically generated from the other; otherwise it would risk inconsistency between them. For example, the QDM provides a transformation from machine-readable XML to human-readable HTML via automated Extensible Stylesheet Language Transformations. Phenotyping is a population level process, which includes intersection (e.g., patients billed with ICD-9 codes for T2DM and patients treated with oral hypoglycemic medications), union (e.g., patients treated with angiotensin converting enzyme inhibitors or patients treated with angiotensin receptor blockers), or exclusion (e.g., patients who have diabetes but have never had retinopathy diagnosed). Relational algebra in database theory is a typical set model. The capability to handle set operations and seamless connections to rule-based models (see Desideratum 5) will directly affect the usability of phenotype algorithms. Virtually all phenotype algorithms explicitly exclude certain other conditions, exposures, or laboratory results operating on either the pa- tient-level or on particular episode(s) of care. Such exclusions are commonly present in control algorithms but also present in many case algorithms. For example, the methotrexate toxicity algorithm 68 excludes patients with known organic liver disease, and for cases, excludes episodes of liver function test elevation while the patient is taking leflunomide (another common rheumatoid arthritis medication with liver toxicity as a side effect). PheRM should support structured and rule-based logical representations, which has been successfully adopted in QDM, OMOP Health R Outcomes of Interest (HOI  34,37,69 and JBOSS Drools based phenotyping. 45,70 Phenotyping algorithms can involve multiple complex logical steps, integrating various operations (e.g., Boolean, comparative, aggregative, temporal). A complex, nested logical structure is supported by QDM. 46 On the other hand, while interface tools such as i2b2 may limit the number of nested levels, some allow users to reference prior patient sets to support more complex workflows. Boolean values can be generated with comparative or temporal operations (see below), set projection (i.e., nonempty set as TRUE, empty set as FALSE). Many of the phenotyping rules are Boolean operations. 50 Common Boolean operators include AND, OR, and NOT (similar to intersection and union). For example, in the T2DM algorithm 19 (Figure 1), every step generates a Boolean value for each patient to follow a decision tree to determine if the patient is a case or control. In phenotyping, comparative operators can be used to threshold a variable (e.g., the numeric result of a laboratory test, such as a white blood cell count), or to compare a numeric variable to another numeric variable (e.g., comparing the LDL value after statin treatment to LDL value before treatment). In addition, important raw data are not always ready to be used directly from an EHR. For example, body mass index (BMI) often needs to be calculated from weight and height. Thus, supporting ba- sic arithmetic functions will broaden the application. Rules to exclude nonbiologic values may also be needed, such as a BMI of 1000 kg/m 2 . Aggregative functions (e.g., COUNT, FIRST) bridge across different levels of clinical information (e.g., from events to patients). In addition, more complex counting and scoring rules should be implemented. 46 In fact, these rules are extremely popular in clinical diagnostic criteria (see Supplementary Appendix Part 2), including the Modified Duke Criteria for diagnosis of infective endocarditis, 52 the CHADS2 score for antithrombotic therapy in AF, 51 or the 2013 guidelines for cholesterol management. 71 In addition, most regression-based predictive models in phenotyping can be represented as a scoring system, such as an algorithm to find rheumatoid arthritis. 54 In phenotyping, negation has two meanings. It can be a negative assertion (e.g., "patient denies headache"), which can be extracted with NLP, 72–74 or an empty set from aggregation in many computer languages (e.g., Perl, Python), similar to exclusion (see Desideratum 5). These two interpretations can be conflicting, and need to be distin- guished. Care must be taken to not imply negation from missing values that are not available due to the variability of the EHR systems. Temporal relationships are widely used in phenotype algorithms, es- pecially for studying response and side effects of medications. 11,14 The first type is sequential clinical events, such as an algorithm to identify patients that have subsequent cardiovascular events while still on clopidogrel, 11 which requires ordered and appropriately spaced se- quences of ischemic and medication events computed from the time- stamps of records. On the other hand, temporality can also be captured through narrative text, requiring advanced NLP to parse grammatical features (past tense of verbs) and relative temporal expressions ("five years ago," "1980s," or location within a "past medical history" section). This strategy has been tested in the 2012 i2b2 challenge, 75,76 and applied in a prior analysis of colorectal cancer screening 77 and in an identification of methotrexate-induced liver toxicity. 68 Frequently in an EHR, the true incident date for a disease is not defined even when using NLP, since it may precede the patient's en- rollment in the given clinic or hospital system. To allow phenotype algorithms in PheRM to be supported in different EHR systems, accommodating non-standardized terminologies is important. Many EHR systems employ local ad hoc terminologies, but the use of local terminology should be limited in PheRM, because it will hinder the portability of algorithms. Both HL7 and OMOP CDM recommend standardized coding systems for clinical terminology, 78 such as ICD-9/10 for billed diagnoses, RxNorm for medication, Logical Observation Identifiers Names and Codes (LOINC) for laboratory tests, and Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) for describing medical conditions. Therefore, EHR databases should provide mapping between standardized terminology systems and their local systems. 61 Phenotype algorithms and quality measures often enumerate lists of concepts to define a medical condition, and these lists have been conventionally called value sets , such as all the ICD-9 codes to define T2DM. Authoring these value sets requires expertise and manual cura- tion, and such sets should be available for reuse by other investigators. To facilitate authoring, i2b2 uses the intrinsic hierarchical structure of medical ontology 79 to allow a user to select all concepts under the same semantic nodes. Local ontologies are supported in i2b2 for the convenience that it offers for their research domain. Broad pathophysiological groupings of ICD-9 codes have been developed for genetic and clinical research, including codes designed to enable phenome-wide association studies, 80–82 and groupings designed for the Agency for Healthcare Research and Quality Clinical Classifications Software. 83 The same value set sometimes can be reused in a variety of projects. For instance, the value set of all the angiotensin-converting- enzyme inhibitors can be used in research projects on diabetic nephropathy, congestive heart failure, or adverse drug reaction. Such information can be stored and managed in the Value Set Authority Center (provided by the National Library of Medicine, https://vsac.nlm. nih.gov/), and the Common Terminology Services 2 (an Object Management Group standard,  Documentation of a detailed description of a patient's clinical presen- tation and management in free text is indispensable in clinical care and in validating that a patient has a given disease. Clinical documents are commonly used for phenotype research. 6 Text searching and NLP are major strategies to validate coded data or define more granular phenotypes than what is possible via structured data, such as subtypes of multiple sclerosis, 84 physical exam findings, 85,86 or the collection of all blood pressure measures. 87 NLP-derived features have been widely applied for machine learning-based phenotype algorithms. 88 PheRM should include NLP and text searching. Patterns of NLP recur- ring in phenotype algorithms have included: identifying targeted document types (e.g., colonoscopy reports), section location, 89 concept identification, 90–92 and negation and context filtering. 72–74 Here, we propose the PheRM should allow for specification of inclusion or exclusions of elements based on: document type, section location, concept instances (with removal of non-patient and negated concepts), and keywords. In addition to NLP, keyword and regular-expression text searches have been applied widely in phenotype algorithms. For example, an AF algorithm includes a keyword search from electrocardiogram reports for different variances in phrasing AF, such as "A-fib", "Atr. Fibrillation." 6,10 With assistance from section separators and negation masks, text searching can achieve a higher accuracy and faster execution (than comprehensive de novo NLP) for many phenotypes. Development of phenotype algorithms is a rapidly evolving field, as are complementary computational algorithms and tools, such as NLP and statistical models. For example, the severe childhood obesity algorithm 2 requires age appropriate percentiles for BMI, which may require an external calculator and/or additional percentile data. These dynamic tasks are difficult to represent or program with static languages (such as XML). Likely the optimal method to "interface" with external software packages would be to allow inclusion of new specifications of data elements that could be calculated external to the phenotype algorithm. As a related endeavor, the eMERGE colon polyps algorithm 67 was delivered as a standard executable KNIME workflow, with a simple Java Snippet unit connecting to a customized NLP package to parse the colonoscopy reports. The T2DM algorithm has a KNIME workflow implementation available on PheKB. A PheRM must be developed according to current existing EHR data, but robust enough to evolve to make use of new clinical data and standards. In addition, unlike a quality measure, which only focuses on records of a limited and recent period, phenotype algorithms fre- quently use information dated back to as early as the first day of utilization of the EHR to obtain enough data for statistical significance. The information usually comes from records across multiple distinct historical eras of EHR development, and from multiple generations of EHR client software and templates. An obvious example is the need to support both ICD9 and ICD10, as well as different historical versions of ICD9 (e.g., allergic bronchopulmonary aspergillosis was billed as "518.89," but has been billed as "518.6" since 1997 93 ). Since phenotype algorithms often examine historical data, such capabilities are still required even after the United States formally adopts ICD10. Acknowledging that robust data normalization across EHRs (espe- cially for historical data) is also a difficult and yet unachieved task, we recommend prioritizing the development of functionality and support of data elements for PheRM. For example, data elements that have been widely used in previous phenotype algorithms should be standardized first: billing codes, RxNorm codes for medications, Logical Observation Identifiers Names and Codes for laboratory tests, and diagnoses on problem lists. Progressive normalization of EHR data with CDMs may simplify backward compatibility. The T2DM algorithm first ascertains T2DM diagnosis with grouped T2DM ICD-9 codes, use of oral hypoglycemic medications represented in grouped RxNorm codes (as Desideratum 7), or multiple mentions of T2DM in clinical narratives (Desiderata 5 [a counting rule] and 8); then it differentiates T2DM from type 1 diabetes mellitus (T1DM) patients by excluding patients with T1DM ICD-9 codes (as Desideratum 4 [exclusion]), enforcing absence of insulin use or oral medications should preceded insulin use (as Desiderata 5 [aggregation function of first ap- pearance] and 6); for some cases, it confirms diabetes diagnoses with laboratory values. Its implementation and inter-institutional operation requires supports of other listed desiderata (with details in Supplementary Appendix A Part 1). To develop these desiderata for a standardized PheRM, we have investigated phenotyping modalities adopted in algorithms from eMERGE, PGRN, SHARPn, and PGPop networks (Table 2), and evaluated popular clinical diagnostic and decision-making algorithms. We have also investigated currently available phenotyping tools, and find that these tools are evolving along with our proposed desiderata and are able to perform increasingly complex phenotype queries. As tests for the feasibility and sufficiency of these modalities, algorithms, and tools, the ongoing Phenotype Execution Modeling Architecture (PhEMA) () collaboration has been actively im- plementing these desiderata and delivering phenotype workflows (Supplementary Appendix Part 3). Since phenotyping is a knowledge-intensive process based on a global evaluation of each patient, 105 missing only a few features in a phenotyping platform or standard language will result in difficulty representing elementary algorithms. It is challenging to list all the techni- cal requirements and details in one paper. Thus, ongoing collaboration between developers of phenotype languages and tools, and user com- munities (including both geneticists and clinicians) will be imperative. The desiderata (D1–10) we proposed cover multiple domains:
Join ResearchGate to access over 30 million figures and 100+ million publications – all in one place. Join for free
Published in
<Domain name="Labor" minDay="1" minMonth="18" maxMonth="30">...</Domain>
<Domain name="Labor" minDay="1" minMonth="30" maxMonth="42">...</Domain>
<Domain name="Labor" minDay="1" minMonth="42" maxMonth="54">...</Domain>
</Domain>
Figure 10: query result sheet for a clinical study
5 Related Work
Our DW system can be compared to related systems in two ways. First, we can discuss
our system's pure DW aspect. Second, we can discuss the aspect of IE from clinical data.
The idea of using a DW for accessing  clinical  data  is not  new,  but there  is no
comprehensive and easily applicable out-of-the-box solution. Clinical information
systems, like any other enterprise information system, are diverse and individual. Even if
based on a standard solution there has so to be done much customizing to fit the existing
clinical processes and documentation, so that there is no standardization yet to be usable
for  different  hospitals. In  order  to construct  an  operational  and effective  DW it  is
essential to combine process work, domain expertise and  high quality  database design
[LSH08]. This  seems to correspond  to concurring  literature, where  we found  either a
holistic approach with theoretical conceptualization but lacking practical implementation
[Ka05a], [Du90], [Ku07]) or practical implementation focused on one single domain, i.e.
lacking generalizability of such concepts [Br09].
The idea of using IE for clinical data has also already been discussed for decades. The
comparison of different medical IE systems is a difficult task because neither the input
nor  the  output  data  are available  because  of  patient  privacy  protection  reasons.  We
therefore remain with the description of the system's structural differences:
Most  medical IE  approaches  handle  only one  specific  domain  like [CM05], [DP07],
[Ma03],  [Ju07] [Ra01],  [BK02],  [Zh07]  for the  domains  of  echocardiography, ECG,
radiology, nursing management, disease management, pharmaceutics or drug events. Or
they  try  to combine  one  domain  with  another  like in  [RD08]  where  the  domain  of
radiology was linked with the domain of pathology.
Most  approaches  used a  commonly  accessible  terminology  standard like  UMLS  or
RadLex  as their  vocabulary basis for IE. This  often  leads to  a  quite  poor recall
performance  because  the  processed  texts  share a  different  vocabulary  than  the  used
terminologies.
Only few systems aim at creating an integrated DW in combination with cross
domains or domain independent IE. One of these systems is MetaMap
(http://metamap.nlm.nih.gov/).  As MetaMap's  terminology  is only based on  UMLS it
also suffers from poor recall for terms which do not exists in this standard. Systems with
an  adaptable  terminology  system  are  cTAKES  [Sa10]  and  MedLee  [Fr00].  MedLee
features a high adaptability to new domains. There, as in our approach, different domains
were made accessible one by one. In the advanced stages of the MedLee project there
was a large variety of clinical domains in the system which could  be accessed
homogeneously.  In  contrast  to  our  approach, they  integrated  the  terminology  of the
different  domains in  one  big terminology, which they enriched with  every  further
domain. As MedLee is not freely available we were unable to further compare it with our
system.
A system with a similar type of query language is WAMIS [Do02] that allows posing of
queries with a temporal logic structure.
A comprehensive overview on IE and data warehousing in the clinical field can be found
at [Me08] and [PG09].
We developed a structured approach for the homogeneous integration of different data
domains used in clinical routine into a DW. We described the general architecture of the
system and the work flow for extracting information from unstructured text domains and
their  integration into  the DW.  The  IE from  the domain  of echocardiography  reports
already shows satisfying f1 score results, thus, the extracted information can already be
reliably integrated in clinical studies or applied to clinical research questions. The other
domains show promising results but still  have to be improved  to use them for clinical
studies  with the  same degree  of reliability.  We compared  two IE  methods:  CRF and
keyword matching with terminology-based disambiguation. We evaluated both methods
on a selected set of text domains and yielded very encouraging results.
In our future work we will analyze and integrate additional yet unprocessed text domains
and improve the performance of our approach on the domains we already worked on.
This  work was supported by grants from the Bundesministerium für Bildung  und
Forschung (BMBF01 EO1004).
[An11]  CE  Angermann,  S.  Störk,  G.  Gelbrich,  H.  Faller,  R.  Jahns,  S.  Frantz  ,  M.
Loeffler, G.  Ertl: Mode  of Action  and Effects  of Standardized  Collaborative Disease
Management on Mortality and Morbidity in Patients  with Systolic Heart Failure: The
Interdisciplinary Network  for Heart  Failure (INH)  Study. Competence  Network Heart
Failure. Circ Heart Fail. 2012 Jan 1;5(1):25-35. Epub 2011 Sep 28. PMID: 21956192
[At09]  Atzmueller, M.;  Beer, S.;  Puppe, F.    A Data Warehouse-Based  Approach for
Quality Management,  Evaluation and  Analysis of  Intelligent Systems  using Subgroup
Mining: Proc. 22nd International Florida Artificial Intelligence Research Society
Conference (FLAIRS), pp. 372-377, AAAI Press, 2009
[Ba05]  H.  Balzert:  Lehrbuch der  Objektmodellierung,  Analyse  und  Entwurf  mit  der
UML 2, 2005
[Bl96]  Black N.:  Why we  need observational studies to  evaluate the  effectiveness of
health care. BMJ1996; 312:1215-8.
[BK02] JS Barrett, SP Koprowski Jr.: The epiphany of data warehousing technologies in
the pharmaceutical industry. Int J Clin Pharmacol Ther 2002; 40 (3): S3–13
[Bo04]  O. Bodenreider:  The  Unified  Medical Language  System  (UMLS): integrating
biomedical  terminology.  Nucleic Acids  Research, Vol.  32,  No. suppl  1.  (01 January
2004), pp. D267-D270.
[Br09] CJ Brooks, JW  Stephen, DE Price, DV Ford, RA Lyons, SL Prior, SC Bain: Use
of a patient linked  data warehouse to facilitate  diabetes trial  recruitment from primary
care. Prim Care Diabetes. 2009 Nov;3(4):245-8. Epub 2009 Jul 14.
[CM05] J. Chung, S Murphy: Conceptvalue pair extraction from semi-structured clinical
narrative: a case study using echocardiogram reports. AMIA Annu Symp Proc, 2005.
[CN08]  R. Cornet,  K. Nicolette:  Forty years  of SNOMED:  a literature  review. BMC
Medical Informatics and Decision Making 8: S2. PMID 19007439
[DP07] J. C.  Denny, J.  F. Peterson: Identifying qt  prolongation from  ecg impressions
using natural language processing and negation detection. In MedInfo, 1283–1288, 2007
[Do02] W. Dorda, W. Gall, G. Duftschmid: Clinical data retrieval: 25 years of temporal
query management at the University of Vienna Medical School.InfMed 2002; 41: 89–97.
[DN07] V. Dinu, P. Nadkarni: Guidelines for the effective use of entity-attribute-value
modeling for biomedical databases. Int Journal of Medical Informatics 76 (11–12)
[DM09] M. Dugas, S. Amler, M. Lange, J. Gerß, B. Breil, W. Köpcke: Estimation of
Patient Accrual Rates in Clinical Trials Based on Routine Data from Hospital
Information Systems. Methods Inf Med. 2009;48(3):263-6.
[Du09] M. Dugas, B. Breil, V. Thiemann, J. Lechtenbörger, G. Vossen: Single Source
Informationssysteme - Nutzung von Routinedaten für die klinische Forschung.  54.
Jahrestagung  der Deutschen  Gesellschaft für  Medizinische  Informatik, Biometrie  und
Epidemiologie. Düsseldorf: German Medical Science GMS Publishing House
[Er11] M. Ertl: Erfassung von klinischen Untersuchungsdaten und Transfer in ein Data
Warehouse. master thesis, Würzburg 2011.
[Fe11] G.  Fette, P. Kluegl, M. Ertl,  S. Stoerk, F. Puppe:  Information Extraction  from
Echocardiography Records. In Proc. LWA 2011
[Fr00] C.  Friedman: A broad-coverage natural  language processing system. In  AMIA,
pages 270–274, 2000.
[Ju07] K. Junttila, R. Meretoja, A. Seppälä, E Tolppanen, T. Nikkola, L. Silvennoinen:
Data warehouse approach to nursing management. J Nurs Manag 2007; 15 (2): 155–161.
[Ka05a] C. Katzer, K. Weismüller, D. Brammen, R. Röhrig, G. Hempelman, T.
Chakraborty: Data-Warehouse aus klinischen und genomischen Daten zur Entwicklung
kombinatorischer  Scoring-Systeme,  50.  Jahrestagung  der  Deutschen  Gesellschaft  für
Medizinische Informatik, Biometrie und Epidemiologie (gmds). Düsseldorf, Köln:
German Medical Science; 2005.
[Ka05b] J. Kamal, K. Pasuparthi, P. Rogers, J. Buskirk, H.  Mekhjian.: Using an
information  warehouse to  screen patients  for clinical  trials: a  prototype. AMIA  Annu
Symp Proc. 2005:1004.
[Kl09a]  P. Kluegl,  M. Atzmueller,  and F.  Puppe.: TextMarker:  A tool  for rule-based
information extraction. In Proc. UIMA, 2009 Conference of the GSCL, 2009.
[Kl09b]  P.  Klügl,  M.  Atzmueller,  P.  Puppe:  Meta-level  Information  Extraction..  In:
Mertsching, B.; Hund, M. & Aziz, M. Z. (Hrsg.): KI. Springer, 2009 (Lecture Notes in
Computer Science 5803)
[Ku07] R. Kush, L. Alschuler, R. Ruggeri, S. Cassells, N. Gupta, L. Bain, K. Claise, M.
Shah, M. Nahm. 2007. Implementing Single Source: the STARBRITE proof-of-concept
study J. Am. Med. Inform. Assoc. 14:  662-673.
[LMP01] J. Lafferty, A. McCallum, F. Pereira: Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. , Proc. 18th International Conf. on
Machine Learning 2001
[LSH08] JA Lyman,  K. Scully, JH Harrison  Jr.: The development  of health care data
warehouses to support data mining. Clin Lab Med. 2008 Mar;28(1):55-71, vi.
[Ma03] B. Mamlin, D. Heinze, C.McDonald: Automated extraction and normalization of
findings from cancer-related free-text radiology reports. AMIA Annu Symp Proc, 2003
[Mc02] A. McCallum: MALLET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu. 2002.
[Me08] S.M.  Meystre, G.K.  Savova, K.C.  Kipper-Schuler, and J.F. Hurdle: Extracting
Information  from Textual  Documents in  the Electronic  Health Record:  A Review  of
Recent Research. IMIA Yearbook 2008: Access to Health Information, 2008:128–144
[PG09] HU Prokosch, T. Ganslandt: Perspectives for medical informatics  - reusing the
electronic  medical  record  for clinical  research.  Methods  of  Information  in Medicine
48/1(2009): 38-44
[Ra01] Ramick DC: Data warehousing in disease management programs. J Healthc Inf
Manag. 2001; 15 (2): 99–105
[RD08] DL  Rubin, TS Desser: A data warehouse for integrating radiologic and
pathologic data. J Am Coll Radiol 2008; 5 (3): 210-217
[Ro63] FB Rogers : Medical subject headings. Bull Med Libr Assoc 51: 114–116. ISSN
0025-7338. PMC 197951. PMID 13982385
[Sa10] G. Savova, J. Masanz, P.  Ogren, J. Zheng, S. Sohn, K. Kipper-Schuler, and  C.
Chute. Mayo clinical text analysis and knowledge extraction system (ctakes):
architecture, component evaluation and applications. JAMIA, 17(5):507–513, 2010
[Zh07] Q. Zhang, Y. Matsumura, T. Teratani, S. Yoshimoto, T. Mineno, K. Nakagawa,
M. Nagahama, S. Kuwata, H. Takeda:  The application of an  institutional clinical data
warehouse to the assessment of adverse drug reactions (ADRs). Evaluation of
aminoglycoside and cephalosporin associated nephrotoxicity. Methods Inf Med 2007; 46
(5): 516–522
Citations (7)Citations (7)
References (31)References (31)
... This task is not trivial and may especially vary across subdomains for optimal results. Encodings that mirror fine-grained concept identifiers [19] have inherent performance issues. However, coarse encodings have been used successfully for clinical named entity recognition, for example, in Swedish [36], which is a promising approach to support terminology development as well as semantic interpretation.Figure 1 shows an artificial echocardiography report which is representative for the types of reports addressed in this paper. ...
B.J. Devi
Abstract
Many applications in the area of HealthCare are generating the patients data in the form of Electronic Health Records(EHR). The data of the patients coming from these electronic gadgets are increasing day by day leading to big data. Generally this data is unstructured or semi-structured. It is converted into a standard form and extraction of knowledge can be done for better understanding and decision making. The data extracted can be stored in Resource Description Framework (RDF) store. In this paper, an algorithm is designed to generate the probability tables using Bayesian approach. This algorithm is applied on the dataset of the patients to identify the favorable chances of getting the flu.
Do you want to read the rest of this article?
Request full-text
On Mapping Textual Queries to a Common Data Model
August 2017
The widespread adoption of Electronic Health Records (EHRs) has enabled data-driven approaches to clinical care and research. However, the performance and generaliz-ability of those approaches are severely hampered by the lack of syntactic and semantic interoperability of EHR data across institutions. Towards resolving this problem, Common Data Models (CDMs) can be used to standardize the... [Show full abstract]
Evaluation of Semantic-Based Information Retrieval Methods in the
Autism Phenotype Domain
Saeed Hassanpour, MS, Martin J. O'Connor, MS, Amar K. Das, MD, PhD
Stanford Center for Biomedical Informatics Research, Stanford, CA 94305
Abstract
Biomedical ontologies are increasingly  being used to  improve information  retrieval methods. In
this paper, we present a novel information retrieval approach that exploits knowledge specified by
the Semantic Web ontology and rule languages OWL and SWRL. We evaluate our approach using
an autism ontology that has 156 SWRL rules defining 145 autism phenotypes. Our approach uses a
vector space model to correlate how well these phenotypes relate to the publications used to define
them. We compare a vector space phenotype representation using class hierarchies with one that
extends this method to incorporate additional semantics encoded in SWRL rules. From a PubMed-
extracted corpus of 75  articles, we  show that  average rank  of a  related paper  using the  class
hierarchy method is 4.6 whereas the average rank using the extended rule-based method is 3.3. Our
results indicate that incorporating  rule-based  definitions in  information  retrieval  methods  can
improve search for relevant publications.
1. Introduction
Ontologies are now widely used to provide domain knowledge in many types of biomedical
applications, ranging  from database mediation1,  2,  data annotation3,  and data  mining4. In the area  of
information  retrieval,  researchers  have developed  search  engines  that  can automatically  expand  the
user's terms to include synonyms, hypernyms, hyponyms, and concept hierarchies using terminology
resources like UMLS and to organize the query results hierarchically based on ontologies such as the
Gene Ontology. Ontologies have also been used to identify relevant domain relationships in articles5.
Although biomedical ontologies are increasingly being expressed in the W3C standard Ontology Web
Language,  or  OWL  (http://www.w3.org/TR/owl-ref/),  relatively few  information retrieval  methods
exploit knowledge in this format.
In  our work,  we  are  examining  the information  retrieval  needs  of autism  domain  experts  who are
developing an ontology-based catalog of autism phenotypes (called Phenologue). In particular, we are
interested  in  whether  information  retrieval  of  PubMed articles  relevant  to  particular  phenotypes is
improved when the rule definition of the phenotype is included with the knowledge encoded from the
ontology-based class hierarchy. As  previously described6, the  autism ontology is  an extension of the
OWL-based NIFSTD (Neuroscience Integration Framework ontology7). Figure 1 shows a part of this
ontology modeling phenotypes using an OWL-based class hierarchy. In our prior work, we have shown
that the Semantic Web Rule Language, or SWRL (http://www.w3.org/Submission/SWRL/), can be used
to provide significantly more detailed rule-based definitions of these phenotypes. In particular, we use
SWRL  rules to  define how phenotypes (e.g., delayed phrases)  can be  inferred  from available  data
collected  from  standard  autism  assessment  instrument,  such  as  the Autism  Diagnostic  Instrument.
Figure 2 shows a SWRL rule encoding the delayed phrases phenotype using a cutoff score of the age
when phrase acquisition is developmentally expected.
When creating the  Phenologue  knowledge base,  domain  experts  are interested  in finding relevant
publications for each phenotype, which  could be used to establish  literature support. To support this
knowledge acquisition effort, we have developed a method that incorporates the existing ontology and
rule definitions  for phenotypes in  the search process. Our  method extends the  standard vector space
model, which is widely used by search engines, to exploit these ontology and rule definitions. Using our
autism phenotype ontology, we undertook a validation of how well our approach can produce relevant
results for semantic search of phenotype definitions in full-text scientific articles.
569
Figure 1. Autism  phenotypes  show  as  a part of autism ontology class hierarchy. Some of these
phenotypes such as, for example, delayed  phrases have  detailed  definitions,  which are  encoded as
SWRL rules
Figure 2. An example SWRL rule encoding the delayed phrases phenotype
2. Background
Many  modern literature-search  tools  use vector  space  modeling  to  implement search  strategies8.  It
provides  an  efficient  and scalable  computational  approach  for converting  a  text-based corpus  to  a
standard mathematical format  and for  then searching for terms in that corpus.  This model  represents
each document in a corpus as a vector in Euclidian space, where each dimension of a vector corresponds
to an individual term in the overall corpus. If a document includes a term, its value in the vector is given
a non-zero  weight for that  term. One  of the most  common methods to  compute this weight is term
frequency-inverse document frequency weighting8. In this weighting scheme, weights increase
proportionally to  the number  of the  term  appearances in  the document  but are  scaled down by  the
frequency of the term in the corpus.
To perform querying, vectors assigned to documents can be compared to query vectors constructed from
search terms. They can also be compared against other documents' vectors to determine the similarity
between documents. This approach is used in PubMed's Related Articles functionality9,  for example,
and in many other document clustering methods10. While powerful and very efficient, standard vector-
based search is based only on the presence and frequency of terms in documents and does not consider
570
any additional information about terms themselves. If a user searches for the term, many synonymous or
semantically related terms are not considered.
To improve search quality, historically lexical databases and thesauri like WordNet11 are used to expand
terms in queries12. Ontologies are increasingly being used in a similar way to enhance the information
retrieval process by paraphrasing, relaxing, or expanding user queries through context identification and
disambiguation12, 13. However these methods typically do not take the advantage of the rich information
model presented in ontologies. Some recent approaches have attempted to rectify this shortcoming. Jun-
feng et  al.14 proposed annotating  web  content  with ontology  terms  and  using these annotations in
addition to  web content  in a vector  space model for  information retrieval purposes.  Castells et  al.15
proposed manually assigning weights to annotations in vector space modeling. Khan et al.13 presented a
method to extract related concepts automatically and then index a document based on a concept-based
model using ontologies. However none of these methods considers (1) incorporating the axiomatic or
rule-based semantics from ontologies in the search process and (2) quantifying ontological expansions
by a measure of semantic similarity.
The autism ontology we used  to evaluate our information retrieval approach is written in  OWL and
contains hierarchies of autism phenotypes. The goal of the ontology is to create standard encodings of
autism phenotypes, which can be used to support concept-based querying of the National Database of
Autism Research (NDAR)16 and other resources. The ontology contains both an information model that
represents research or clinical data collected through standardized instruments and a domain ontology
that  defines  terms  and relationships  among  nine major  categories  of  autism phenotypes,  such  as
language, social interaction, and behavioral abnormalities. The ontology is an extension of the NIFSTD
neuroscience  ontology, which does  not contain significant  numbers  of classes  related to autism
phenotypes. To encode the initial domain knowledge of phenotypes, a systematic review was
undertaken in PubMed  to find clinical phenotypes defined by three  common assessment instruments
used in autism2, 6. A manual analysis by two domain experts of 26 relevant papers produced a list of 145
phenotypes,  some of  which had  multiple definitions.  The NIFSTD  ontology  was then  expanded to
incorporate classes and properties related to these phenotypes as well as the items from the assessment
instruments.
As mentioned, SWRL was used to encode each of the 156 phenotype definitions in the autism OWL
ontology.  SWRL  has  emerged as  the  primary  language  for encoding  rules  in  OWL  ontologies. It
provides a Horn-like rule  language built on OWL and uses the same description logic foundation as
OWL. SWRL rules are written directly in terms of domain concepts in an OWL ontology and can be
considered as formal logical statements about ontology entities. This property allows SWRL rules to be
treated as first  class entities in an ontology  and supports automatic methods  to determine how rules
interact  with  the  ontology.  Our  extended  rule-based  method  for  information  retrieval exploits  this
property to examine the phenotype rules stored in the autism ontology and to determine automatically
the domain concepts used in those rules.
3. Methods
To address the shortcomings of prior ontology-based methods for information retrieval, we introduce a
novel  search method  in  this paper,  which  we call  the extended rule-based  method.  This  method
combines the efficiency of vector modeling with the additional expressivity provided by the semantic
information  encoded  in  ontologies.  It  automatically  extends  the  vector  space  representation  to  use
SWRL  rules  and  uses various  semantic  relationships  that  are  encoded  in  the  associated  ontology.
Instead  of simply  filtering results  based on  class hierarchies,  it quantitatively  incorporates semantic
similarity metric in  the  vector space  representation  before undertaking  the search  process. Figure  3
shows the basic steps of our method.
571
Figure 3. An overview of the extended rule-based method, which shows how we model publications
and rules in vector space and compute their correlation
To evaluate the efficacy of this extended rule-based method, we compare it to prior ontology-derived
methods. In particular, we used the framework presented by Jun-feng et al.14 to come up with a baseline
method for our evaluation. This method, which we call class hierarchy method, uses just the ontology
hierarchies to expand queries for the  domain concepts searched for in  the corpus. Like the extended
rule-based method, the class hierarchy method uses vector space modeling to represent publications. To
model concepts, the class hierarchy method uses a binary vector that assign 1 to a present concept and it
super-classes,  sub-classes and  sibling-classes; 0  is  assigned otherwise.  Correlation between  concept
binary vectors from and publication vectors are computed by a standard cosine similarity measure. After
presenting  details of  the two  information  retrieval approaches,  we discuss  how  we validated their
precision by correlating the phenotype vectors to a set of 75  PubMed-retrieved full-text publications
which contained the 26 articles that were the basis of knowledge in the autism ontology.
3.1 Vector Space Modeling
The goal of the extended rule-based method is to take a set of SWRL rules in a domain ontology and to
automatically find  research publications  that are  related to  the rules. As  a first step, we  generate a
representation of the ontology, its associated rule base, and a set of domain publications in a corpus.
As mentioned, we  used a vector space modeling approach8 to encode this representation. Because of
structural differences  between  publications and rules, we  used a slightly modified vector space
modeling  methods to  encode  each  source.  The  focus  of  our  approach  is  to  capture  the  semantic
information about terms in a rule to expand the representation of each rule.
572
Publications Model
Before converting publications to vectors of words, we applied several preprocessing procedures on the
publications in the corpus. As the first step, we converted publications from PDF format to plain text.
We eliminated stop words, which are common words that do not convey information about context. For
this step, we used a list of stop words from the natural language processing literature17. Next, we applied
a stemming algorithm to replace derivation words with their origins. For this step we used the Porter
stemming  algorithm,  which  is the de  facto standard algorithm used for English Stemming
18.  After
trimming  the text  we  converted each  publication  to a  vector of  weighted  terms. We  used  the term
frequency inverse document frequency-weighting scheme.
Rule Representation Model
We then represented each of the 156 rules that define the 145 phenotypes in the ontology as a weighted
vector of terms. To calculate this weighting, we incorporated the semantics of the OWL terms used in
the  rules. In  standard  vector space modeling, the weights  of terms  in a  vector depend only on the
relative frequency of the terms. Our method extends this approach to reflect the relationships between
the OWL terms used in the rules and ontology terms that are semantically related to those terms.
To quantify the strength of the relationship between ontology terms and the terms used in a rule, we
used a  semantic similarity-weighting scheme.  Semantic similarity is  a common  metric in  ontology-
based systems  and is  used to  capture the strength  of the various relationships between terms in an
ontology8. A number of approaches exist to quantify semantic similarity. In most of these approaches,
the ontology is considered as a directed graph, with terms  being nodes  in that graph, and the various
relationships between the terms being represented as edges and graph node properties to quantify each
term's semantic distance in the graph19, 20.
We used a method based on the hierarchy of classes and properties in an ontology in our semantic
similarity  weighting scheme.  In  this approach,  semantic  similarity  decreases exponentially with  the
distance between two ontology entities in the hierarchy. The definition of semantic similarity is:
Here, SemanticSimilaritya(b) is the semantic similarity between classes or properties a and b where
distance(a,b) is the distance between them in the hierarchy. α is a variable to scale the semantic
similarity values and can be varied based on the semantic relationship between a and b.
This semantic similarity metric was used to weigh the conceptual similarity of classes and properties
that  are  covered  by  each  SWRL  rule. After  we calculated  the  weightings  of a  set  of  classes and
properties from  each SWRL  rule, we again  used a vector  space model  to represent these  rules and
incorporated the weighting in this representation.
If a class or property was in the relevant set of multiple atoms in the rule, we summed its relevance
values in the vector representation to reflect the  relevance  of this class or property to the whole rule
base. We also considered term annotations as a part of the vector representations. Adding annotations to
the vector space captured metadata information that the rule developers added to the rule base.
3.2 Correlation Computation
After we represent the sets of rules and research publications as points in a vector space, we can
compute a similarity metric between rules and publications. We used a standard cosine similarity metric
to calculate this similarity. The cosine similarity for two vectors is equivalent to the cosine of the angle
between them. Values range from 0 for orthogonal vectors to 1 for identical vectors. The mathematical
formula for cosine similarity is:
573
Where a and b are two vectors in the Euclidean space, θ is the angle between them, a.b is their dot
product, and ||a|| and ||b|| are the magnitudes of the vectors. Since cosine similarity measure normalizes
vectors, it  does not  depend on  the size  of the vectors  and gives  an accurate and  stable measure of
similarity for any vector space dimension.
We calculated the cosine similarity between each phenotype rule in the rule base and all the publications
in the  corpus. For each phenotype rule, all publications in the  corpus were sorted according to each
one's cosine similarity score.
4. Evaluation and Results
We evaluated the accuracy of our method by assessing the correlation of the 145 phenotype classes and
their  156 associated rules  with a  corpus of 75 papers from  the autism  domain.  These papers  were
retrieved  from PubMed  using  the same  keyword  search criteria  that were  used  to find  the original
articles used to develop the autism ontology.  The corpus consists of the 26 articles that contained clear
definitions  of phenotypes and another 49 articles  that were not found to  have such phenotype
definitions. There were 1,726 classes and properties in the phenotype domain ontology. We used the
ontology, along with metadata annotations, to create a vector space of 2,765 dimensions. The addition
of the corpus created a vector space of 32,086 terms after processing.
For each rule, we sorted all the publications in corpus using the cosine similarity of each paper to its
phenotype  class  and  corresponding  rule(s).  We  used the  publications  that  were  used  to define  a
phenotype class by ontology developers as the gold standard for relevance and thus the most related
publication to the related rule. In the ideal search result, the publication or publications that were used to
define the phenotype will be at the top or near the top of the sorted list of 75 publications. Because most
of the phenotypes have only one source publication, we chose to look at the rank of this source article
rather than a more typical precision and recall statistic. We report on the average ranking of the correct
match among all 145 phenotype classes as well as the percentage of articles among the top one to ten
related publications that had at least one correctly matched publication.
In the sorted list of the related publications, the average position of the correct source article or articles
for the extended rule-based method was 3.3. For comparison, we examined our method against the class
hierarchy method, which, instead of using our semantic similarity measure to incorporate the semantics
provided  by  the  SWRL  rules  in  the  ontology,  uses  the  ontology  hierarchy  alone  to  expand  each
phenotype concept to a set of related concepts. Our comparison shows that the average position of the
correct source article or articles with the class hierarchy method was 4.6, which is significantly lower
than our extended rule-based method.
To examine how the two methods performed against each other, we varied the number of the top related
publications and looked at the percentage of correctly related paper that these methods found. Figure 4
shows this comparison. As shown, the extended rule-based method performs as well or better than the
class hierarchy method across the top one to ten returned results and reaches greater than 90% accuracy
for the top 10 articles out of the 75 that are ranked.
574
Figure 4. This figure shows  the percentage of phenotypes where the retrieved documents cover the
correct source article(s) in the extended rule-based method and the class hierarchy method
5. Discussion
We  have  developed  a  novel  semantic search  method  to  assist  biomedical  researchers  in  finding
publications directly relevant to concepts in a domain. The method incorporates the semantics provided
by rules  in a  domain ontology  to find concepts  indirectly related to  search terms.  Our approach to
semantic search differs from prior work. Many information retrieval systems use thesauri to
automatically expand search terms to other related terms so that searches are not restricted to simple
term  matching21.  However,  these  term  expansions  typically  capture  only  a  very limited  amount  of
additional  semantic  information.  Other  systems,  such as  MedMiner22 and Textpresso5,  use natural
language processing techniques to capture additional term semantics when searching. These techniques
include parsing and part-of-speech tagging to identify documents that mention a query term that a user
specifies.  However  due  to the  complexity  of  human  language, these  techniques'  performances  are
limited and are generally restricted to certain domains.
Ontology-based methods  are increasingly  being used  to enhance  search but  their  primary  use is  to
structure search results. For example, they are often used in digital libraries to categorize research paper
recommendations23.  They  can  also  be  used  to  perform  fine-grained  stratification  of  result in  some
domains. GoPubMed, for example, filters search results through the categories of the Gene Ontology24.
A  few strategies  attempt to  incorporate semantic  information into  the  search process  itself. These
strategies  include description  logic  and fuzzy  logic  based  approaches25.  While  these approaches  do
increase  expressivity,  they  generally  suffer from  scalability  problems.  Additionally,  they  are  often
limited  to domains  described by  a  predefined set  of ontologies  and  do not  support general-purpose
searches using arbitrary user-supplied ontologies.
Our preliminary results  indicate that  vector-based semantic  correlation using  rules has  a very  good
precision in finding articles related to a systematically encoded catalog of phenotypes. We are analyzing
how we can improve upon the precision of the match.  For example, we plan to evaluate the correlation
of the rules to specific sections of the articles where the rule was defined, such as methods or results,
rather than to  the entire text of  the publication. The current study of our method reports only on an
internal validation of precision.  We are currently working on an evaluation of web-based service (to be
575
available at phenologue.org), where we are using the Google custom search API to submit our weighted
set of terms for each rule and find relevant pages and articles. We are also planning to apply our method
on other ontologies and rule bases for further verification. As part of the Phenologue project, we also
plan to provide an interactive web service that allows users to upload their own SWRL rule bases with
associated ontologies and to search for related full-text publications on the Web.
Acknowledgments
The authors would like to thank Samson W. Tu, Lakshika Tennakoon, Richard Waldinger and Joachim
Hallmayer for their help on the ontology development. We would like to acknowledge support from the
National Database for Autism Research. This research was supported in part by grants R01LM009607
and R01MH87756 from the National Institutes of Health.
References
1. Jeng S, Wang K, Barbero J, Brinkley J, Tarczy-Hornoch P. A pilot bridging data integration and
analytics: BioMediator and R? AMIA Annual Symposium, Washington, DC, 2005, pp. 995.
2. Young L, Tu SW, Tennakoon L, Vismer D, Astakhov V, Gupta A, Grethe JS, Martone ME, Das
AK, McAuliffe MJ. Ontology-driven data integration for autism research. 22nd IEEE International
Symposium on Computer Based Medical Systems, Albuquerque, NM, 2009, pp. 1-7.
3. Shah NH,  Rubin DL, Supekar  KS, Musen  MA. Ontology-based annotation  and query  of tissue
microarray data. AMIA Annual Symposium, Washington, DC, 2006, pp. 709-713.
4. Raj R, O'Connor MJ, Das  AK. An ontology-driven method for hierarchical mining  of temporal
patterns: application  to HIV drug resistance research. AMIA Annual Symposium, Chicago, IL,
2007, pp. 614-619.
5. Muller HM, Kenny EE, Sternberg PW. Textpresso: an ontology-based information retrieval and
extraction system for biological literature. PLoS Biol. 2004;2(11):1984-1998.
6. Tu S, Tennakoon, L, Das AK. Using an integrated ontology and information model for querying
and reasoning about phenotypes: the case of autism. AMIA Annual Symposium, Washington, DC,
2008, pp. 727-731.
7. Bug WJ, Ascoli GA, Grethe JS, Gupta A, Fennema-Notestine C, Laird AR, Larson SD, Rubin D,
hepherd GM,  Turner JA,  and Martone  ME. The  NIFSTD and  BIRNLex vocabularies: building
comprehensive ontologies for neuroscience. Neuroinformatics. 2008;6(3):175-194
8. Manning CD, Raghavan P, Schüze H. Introduction to information retrieval, Cambridge University
Press, 2008.
9. Wilbur WJ, Coffee, L.  The  effectiveness of  document neighboring in search enhancement. Inf.
Process. Manage. 1994;30:253-266.
10. Glenisson  P, Antal  P, Mathys  J, Moreau  Y, de Moor B. Evaluation of the vector space
representation in text-based gene clustering. Pac Symp. Biocomput. 2003;8:391-402.
11. Miller GA. WordNet: a lexical database for English. Com. ACM. 1995;38(11):39-41.
12. Bhogal J,  Macfarlane A,  Smith P.  A  review  of  ontology  based  query  expansion. Inf.  Process.
Manage. 2007;43(4):866-886.
13. Khan L, McLeod D, Hovy E. Retrieval effectiveness of an ontology-based model for information
selection. VLDB J. 2004;13(1):71-85.
14. Jun-feng  S,  Wei-ming  Z, Wei-dong  X,  Guo-hui  L,  Zhen-ning  X.  Ontology-based  information
retrieval model for the semantic web. IEEE International Conference on e-Technology, e-
Commerce and e-Service, Hong Kong, 2005, pp. 152-155.
15. Castells P, Fernández M, Vallet D. An adaptation of the  vector-space model for  ontology-based
information retrieval. IEEE T Knowl Data En. 2007;19(2):261-272.
16. Hus V,  Pickles  A, Cook  EH, Risi  S, Lord C.  Using  the autism diagnostic interview-revised  to
increase phenotypic homogeneity in genetic studies of autism. Biol Psychiatry 2007;61(4):438-448.
17. Charniak E, Altun Y, de Salvo Braz R, Garrett B, Kosmala M, Moscovich T, Pang L, Pyo C, Sun
Y, Wy W, Yang Z, Zeller S, Zorn L. Reading comprehension programs in a statistical-language -
Processing Class. North American Association  for Computational  Linguistics Workshop on
Reading Comprehension Tests As Evaluation  for Computer-Based Language Understanding
Systems, Morristown, NJ, 2000, pp. 1-5.
576
18. Porter M. An algorithm for suffix stripping, Program, 1980;14(3):130-137.
19. Pesquita C, Faria D, Falcao AO, Lord P, Couto FM. Semantic similarity in biomedical ontologies.
PLoS Comput Biol. 2009;5(7):e1000443.
20. Rada R, Mili H, Bicknell E, Blettner M. Development and application of a metric on semantic nets.
IEEE T Syst Man Cyb. 1989;19(1):17-30.
21. Bhalotia G, Nakov PI, Schwartz AS, Hearst MA. BioText team report for the TREC 2003 genomics
track. 25th Text REtrievalxi Conference, Gaithersburg, MD, 2003, pp. 612-621.
22. Tanabe L, Scherf U, Smith LH, Lee JK, Hunter L, Weinstein JN, MedMiner: an internet text-
mining tool for biomedical information, with application to gene expression profiling.
Biotechniques. 1999;27:1210-1217.
23. Shum SB, Motta E, Domingue J. ScholOnto: an ontology-based digital library server for research
documents and discourse. Int J Digit Libr. 2000;3:237-248.
24. Delfs  R,  Doms  A,  Kozlenkov  A,  Schroeder  M.  GoPubMed:  ontology-based  literature search
applied to GeneOntology and PubMed.  German Bioinformatics  Conference, Bielefeld, Germany,
2004, pp. 169-178.
25. Zhang L, Yu Y, Zhou J, Lin C, Yang Y. An enhanced model for searching in semantic portals. 14th
International Conference on World Wide Web, New York, NY, 2005, pp. 453-462.
577
Citations (4)Citations (4)
References (28)References (28)
... Delayed Words phenotype SWRL rule definition ADI-R(?a) ^ adi-r2003:ADI_2003_acqorlossoflang_ aword(?a, ?wordage) ^ swrlb:greaterThan(?wordage, 24) ^ adi_r2003:SubjectKey(?a, ?subjectID) ^ swrlx: createOWLThing(?phenotype, ?subjectID) → 'Delayed word'(?phenotype) ^ autism-core:subject_has_quality_ or_disposition(?subjectID, ?phenotype) In prior work, we have presented semantic-based information retrieval that uses previously encoded knowledge about a domain, specifically a domain ontology and rule base, to identify papers relevant to phenotypes and to extract snippets of text most likely to contain their defini- tions [7,8] . In this paper, we present a novel semanticbased approach to identify which exact definition or related definition exists within a returned snippet of text in an article. ...
https://doi.org/10.1016/j.ajog.2017.11.578 Get rights and content
Background
Contraception counseling and provision is an essential preventative service. Real-time assessment of these services is critical for quality improvement and comparative study. Direct observation is not feasible on a large scale, so indirect measures (such as chart review) have been determined to be acceptable tools for this assessment. Computer-aided chart review has significant benefits over manual chart review as far as greater efficiency and ease of repeated measurements. The wide use of electronic medical records provides an opportunity to create a data extraction algorithm for computer-aided chart review that is sharable among institutions. We provide a useful schema for others who use electronic medical record systems and are interested in real-time assessment of contraception counseling and provision for the purposes of baseline assessment of services and quality improvement.
Objective
The purpose of this study was to create a comprehensive and accurate data extraction algorithm that is useful in the assessment of contraception counseling and provision rates in the outpatient setting.
Study Design
We included all visits between August 2015 and May 2016 at 8 outpatient clinics that are affiliated with a large, urban academic medical center in which nonpregnant women who were 14–45 years old were seen by a nurse practitioner, physician's assistant, or physician. Contraception-related prescriptions, International Classification of Diseases codes, current procedural terminology codes, and search-term capture were extracted with the use of structured query language from electronic medical record data that were stored in a relational database. The algorithm's hierarchy was designed to query prescription data first, followed by International Classification of Diseases and current procedural terminology codes, and finally search-term capture. Visits were censored when the first positive evidence of contraceptive service was obtained. Search terms were selected based on group discussion of investigators and providers. This algorithm was then compared with manual chart review and refined 3 times until high sensitivity and specificity, when compared with manual chart review, were achieved.
Results
There were 22,134 visits of reproductive-aged women who our inclusion criteria. Electronic medical record evidence of contraception counseling or provision was found in 56.9% of these visits. Of these, 21.3% were captured by prescriptions; 8.9% were captured by International Classification of Diseases codes, and 69.7% were captured by search-term capture with the use of our algorithm. Among visits with evidence of contraception counseling without provision, 15.7% were captured by diagnosis codes and 84.3% were captured by search-term capture. When compared with manual chart review, sensitivity and specificity improved from 0.79 and 0.85 to 0.99 and 0.98, respectively, over the 3 rounds of testing and revision.
Conclusion
Data extraction algorithms can be used effectively for computer-aided chart review of contraception counseling and provision measures, but testing and refinement are extremely important. Search-term capture from unstructured data is a critical component of a comprehensive algorithm, especially for the capture of instances of contraception counseling without provision. The algorithm that we developed here could be used by others with an electronic medical record system who are interested in real-time assessment, quality improvement, and comparative study of the delivery of contraceptive services. The ease of execution of this algorithm also allows for its repeated use for ongoing assessments over time.
Previous article in issue
Under an Elsevier user license
open archive
Abstract
The paper describes a rule-based information extraction (IE) system developed for Polish medical texts. We present two applications designed to select data from medical documentation in Polish: mammography reports and hospital records of diabetic patients. First, we have designed a special ontology that subsequently had its concepts translated into two separate models, represented as typed feature structure (TFS) hierarchies, complying with the format required by the IE platform we adopted. Then, we used dedicated IE grammars to process documents and fill in templates provided by the models. In particular, in the grammars, we addressed such linguistic issues as: ambiguous keywords, negation, coordination or anaphoric expressions. Resolving some of these problems has been deferred to a post-processing phase where the extracted information is further grouped and structured into more complex templates. To this end, we defined special heuristic algorithms on the basis of sample data. The evaluation of the implemented procedures shows their usability for clinical data extraction tasks. For most of the evaluated templates, precision and recall well above 80% were obtained.
Previous article in issue
Cookies are used by this site. For more information, visit the cookies page .
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
Cookies are used by this site. For more information, visit the cookies page .
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
Cookies are used by this site. For more information, visit the cookies page .
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
Cookies are used by this site. For more information, visit the cookies page .
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
Cookies are used by this site. For more information, visit the cookies page .
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
